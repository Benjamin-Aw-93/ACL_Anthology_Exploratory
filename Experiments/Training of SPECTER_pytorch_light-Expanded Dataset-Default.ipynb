{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import time\n",
    "\n",
    "# Reading in files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Lightning modules\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy, auroc\n",
    "from torchmetrics import F1Score\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "\n",
    "# Split dataset/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Importing own functions \n",
    "from extract.importing_data import get_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Collated_dataset_for_scientific_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Intro Concl\", \"Labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"string\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = list(df['label'].unique())\n",
    "possible_labels_num = list(range(0,len(possible_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possible_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'allenai/specter'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = zip(possible_labels, possible_labels_num)\n",
    "label_to_idx = {label: num  for label, num in mapping}\n",
    "mapping = zip(possible_labels, possible_labels_num)\n",
    "idx_to_label = {num: label for label, num in mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_COUNT = 512\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 8 # Changes: Edit the batch size here\n",
    "KFOLD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecterDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: AutoTokenizer, max_token_len: int = MAX_TOKEN_COUNT, mapping = label_to_idx):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "        self.mapping = mapping\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "    \n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row[\"string\"]\n",
    "        labels = self.mapping[data_row[\"label\"]]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_token_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding=\"max_length\",\n",
    "          truncation=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "          text=text,\n",
    "          input_ids=encoding[\"input_ids\"].flatten(),\n",
    "          attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "          labels=labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecterDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, df,\n",
    "                 tokenizer,\n",
    "                 k = 0,  # fold number\n",
    "                 split_seed = 123,  # split needs to be always the same for correct cross validation\n",
    "                 num_splits = KFOLD,\n",
    "                 batch_size = BATCH_SIZE, \n",
    "                 max_token_len = MAX_TOKEN_COUNT,\n",
    "                 num_workers = 0,\n",
    "                 pin_memory = False):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(logger=False)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # choose fold to train on\n",
    "        kf = StratifiedKFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n",
    "        all_splits = [k for k in kf.split(df, df.label)]\n",
    "        train_indexes, val_indexes = all_splits[self.hparams.k]\n",
    "        train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
    "\n",
    "        self.data_train, self.data_val = df.iloc[train_indexes], df.iloc[val_indexes]\n",
    "        \n",
    "        self.train_dataset = SpecterDataset(\n",
    "          self.data_train,\n",
    "          self.hparams.tokenizer,\n",
    "          self.hparams.max_token_len\n",
    "        )\n",
    "        self.val_dataset = SpecterDataset(\n",
    "          self.data_val,\n",
    "          self.hparams.tokenizer,\n",
    "          self.hparams.max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "          self.train_dataset,\n",
    "          batch_size = self.hparams.batch_size,\n",
    "          shuffle=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "          self.val_dataset,\n",
    "          batch_size = self.hparams.batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecterClassModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.specter = AutoModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "        # Changes: Edit model architecture forward pass here\n",
    "        self.classifier = nn.Linear(self.specter.config.hidden_size, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.acc = MulticlassAccuracy(num_classes = 21, average = 'weighted') #measure performance based on weighted average\n",
    "        self.f1 = MulticlassF1Score(num_classes = 21, average = 'weighted')\n",
    "        self.prec = MulticlassPrecision(num_classes = 21, average = 'weighted')\n",
    "        self.rec = MulticlassRecall(num_classes = 21, average = 'weighted')\n",
    "        \n",
    "        # Changes: Comment below code to remove freezing of the SPECTER embeddings\n",
    "        for name, param in self.specter.named_parameters():\n",
    "            if name.startswith('embeddings'):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.specter(input_ids, attention_mask=attention_mask)\n",
    "        # Changes: Edit model architecture forward pass here\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "            # acc = self.acc(output, labels)\n",
    "            # f1 = self.f1(output, labels)\n",
    "            # prec = self.prec(output, labels)\n",
    "            # rec = self.rec(output, labels)\n",
    "        return loss, output #, acc, f1, prec, rec\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        #loss, output, acc, f1, prec, rec = self(input_ids, attention_mask, labels)\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        #loss, output, acc, f1, prec, rec = self(input_ids, attention_mask, labels)\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = sum(output['loss'].item() for output in outputs) / len(outputs)\n",
    "        \n",
    "        predictions = torch.cat([output['predictions'] for output in outputs])\n",
    "        labels = torch.cat([output['labels'] for output in outputs])\n",
    "        \n",
    "        acc = self.acc(predictions, labels)\n",
    "        f1 = self.f1(predictions, labels)\n",
    "        prec = self.prec(predictions, labels)\n",
    "        rec = self.rec(predictions, labels)\n",
    "        print(f\"******Train epoch {self.current_epoch} eval metrics: loss {avg_loss:.8f}, f1 {f1:.4f} prec {prec:.4f} rec {rec:.4f}, acc {acc:.4f}\")\n",
    "        \n",
    "        self.logger.experiment.add_scalars('loss', {'train': avg_loss}, self.current_epoch)    \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = sum(output['loss'].item() for output in outputs) / len(outputs)\n",
    "\n",
    "        predictions = torch.cat([output['predictions'] for output in outputs])\n",
    "        labels = torch.cat([output['labels'] for output in outputs])\n",
    "        \n",
    "        acc = self.acc(predictions, labels)\n",
    "        f1 = self.f1(predictions, labels)\n",
    "        prec = self.prec(predictions, labels)\n",
    "        rec = self.rec(predictions, labels)\n",
    "        print(f\"******Val epoch {self.current_epoch} eval metrics: loss {avg_loss:.8f}, f1 {f1:.4f} prec {prec:.4f} rec {rec:.4f}, acc {acc:.4f}\")\n",
    "        \n",
    "        #For final output\n",
    "        self.log(\"Ignore/acc\", acc, logger=True)\n",
    "        self.log(\"Ignore/f1\", f1, logger=True)\n",
    "        self.log(\"Ignore/prec\", prec, logger=True)\n",
    "        self.log(\"Ignore/rec\", rec, logger=True)\n",
    "        \n",
    "        #For Tensorboard visualisaion with Epoch as x axis\n",
    "        self.logger.experiment.add_scalar('Collated/acc', acc, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Collated/f1', f1, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Collated/prec', prec, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Collated/rec', rec, self.current_epoch)\n",
    "    \n",
    "        self.logger.experiment.add_scalars('loss', {'val': avg_loss}, self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5) # Changes: Edit the learning rate\n",
    "        scheduler = get_linear_schedule_with_warmup( # Changes: Edit the scheduler\n",
    "          optimizer,\n",
    "          num_warmup_steps=self.n_warmup_steps,\n",
    "          num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        \n",
    "        # Changes: Edit the optimizer\n",
    "        return dict(\n",
    "          optimizer = optimizer,\n",
    "          lr_scheduler = dict(\n",
    "            scheduler = scheduler,\n",
    "            interval = 'step'\n",
    "          )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"Default\"\n",
    "time_now = time.strftime(\"%d_%m_%Y_%H_%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 1270)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch= round(len(df) * 0.8 // BATCH_SIZE)\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\Specter_Default_06_02_2023_01_41_run0\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Benjamin Aw\\Desktop\\ACL_Anthology_Exploratory\\Experiments\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | specter    | BertModel           | 109 M \n",
      "1 | classifier | Linear              | 16.1 K\n",
      "2 | criterion  | CrossEntropyLoss    | 0     \n",
      "3 | acc        | MulticlassAccuracy  | 0     \n",
      "4 | f1         | MulticlassF1Score   | 0     \n",
      "5 | prec       | MulticlassPrecision | 0     \n",
      "6 | rec        | MulticlassRecall    | 0     \n",
      "---------------------------------------------------\n",
      "85.7 M    Trainable params\n",
      "24.3 M    Non-trainable params\n",
      "109 M     Total params\n",
      "439.818   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                              | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.62it/s]******Val epoch 0 eval metrics: loss 3.23358631, f1 0.0000 prec 0.0000 rec 0.0000, acc 0.0000\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|██████████████████████████▍      | 255/319 [00:51<00:12,  4.93it/s, loss=1.89, v_num=0, train_loss=1.550]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████▍      | 256/319 [00:51<00:12,  4.95it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▌      | 257/319 [00:51<00:12,  4.97it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▋      | 258/319 [00:51<00:12,  4.98it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▊      | 259/319 [00:51<00:11,  5.00it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  82%|██████████████████████████▉      | 260/319 [00:51<00:11,  5.02it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 261/319 [00:51<00:11,  5.03it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 262/319 [00:51<00:11,  5.05it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████▏     | 263/319 [00:51<00:11,  5.07it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▎     | 264/319 [00:51<00:10,  5.08it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▍     | 265/319 [00:51<00:10,  5.10it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▌     | 266/319 [00:51<00:10,  5.12it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▌     | 267/319 [00:52<00:10,  5.13it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▋     | 268/319 [00:52<00:09,  5.15it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▊     | 269/319 [00:52<00:09,  5.17it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  85%|███████████████████████████▉     | 270/319 [00:52<00:09,  5.18it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████     | 271/319 [00:52<00:09,  5.19it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████▏    | 272/319 [00:52<00:09,  5.21it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▏    | 273/319 [00:52<00:08,  5.22it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▎    | 274/319 [00:52<00:08,  5.23it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▍    | 275/319 [00:52<00:08,  5.24it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▌    | 276/319 [00:52<00:08,  5.26it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▋    | 277/319 [00:52<00:07,  5.27it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 278/319 [00:52<00:07,  5.28it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 279/319 [00:52<00:07,  5.30it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  88%|████████████████████████████▉    | 280/319 [00:52<00:07,  5.31it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████    | 281/319 [00:52<00:07,  5.32it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████▏   | 282/319 [00:52<00:06,  5.33it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▎   | 283/319 [00:52<00:06,  5.35it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 284/319 [00:52<00:06,  5.36it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 285/319 [00:53<00:06,  5.37it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▌   | 286/319 [00:53<00:06,  5.38it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▋   | 287/319 [00:53<00:05,  5.40it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▊   | 288/319 [00:53<00:05,  5.41it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  91%|█████████████████████████████▉   | 289/319 [00:53<00:05,  5.42it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 290/319 [00:53<00:05,  5.43it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 291/319 [00:53<00:05,  5.44it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▏  | 292/319 [00:53<00:04,  5.46it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▎  | 293/319 [00:53<00:04,  5.47it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▍  | 294/319 [00:53<00:04,  5.48it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▌  | 295/319 [00:53<00:04,  5.50it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▌  | 296/319 [00:53<00:04,  5.51it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▋  | 297/319 [00:53<00:03,  5.52it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▊  | 298/319 [00:53<00:03,  5.53it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  94%|██████████████████████████████▉  | 299/319 [00:53<00:03,  5.54it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████  | 300/319 [00:53<00:03,  5.56it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████▏ | 301/319 [00:54<00:03,  5.57it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▏ | 302/319 [00:54<00:03,  5.58it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▎ | 303/319 [00:54<00:02,  5.59it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▍ | 304/319 [00:54<00:02,  5.60it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▌ | 305/319 [00:54<00:02,  5.62it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▋ | 306/319 [00:54<00:02,  5.63it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▊ | 307/319 [00:54<00:02,  5.64it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▊ | 308/319 [00:54<00:01,  5.65it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▉ | 309/319 [00:54<00:01,  5.66it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████ | 310/319 [00:54<00:01,  5.68it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████▏| 311/319 [00:54<00:01,  5.69it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▎| 312/319 [00:54<00:01,  5.70it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 313/319 [00:54<00:01,  5.71it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 314/319 [00:54<00:00,  5.72it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▌| 315/319 [00:54<00:00,  5.73it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▋| 316/319 [00:54<00:00,  5.75it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▊| 317/319 [00:55<00:00,  5.76it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████▉| 318/319 [00:55<00:00,  5.77it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████████| 319/319 [00:55<00:00,  5.78it/s, loss=1.89, v_num=0, train_loss=1.550]\u001b[A******Val epoch 0 eval metrics: loss 1.77033242, f1 0.4912 prec 0.5127 rec 0.5363, acc 0.5363\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:55<00:00,  5.73it/s, loss=1.89, v_num=0, train_loss=1.550, val_loss=1.770]\n",
      "                                                                                                                       \u001b[A******Train epoch 0 eval metrics: loss 2.50677397, f1 0.2749 prec 0.3728 rec 0.3050, acc 0.3050\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=1.89, v_num=0, train_loss=1.550, val_loss=1.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 255: 'val_loss' reached 1.77107 (best 1.77107), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.88it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.90it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.92it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.93it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.95it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.97it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 261/319 [00:52<00:11,  4.98it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 262/319 [00:52<00:11,  5.00it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  82%|██████████████   | 263/319 [00:52<00:11,  5.01it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 264/319 [00:52<00:10,  5.03it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 265/319 [00:52<00:10,  5.05it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.06it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.08it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 268/319 [00:52<00:10,  5.10it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.11it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.13it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.14it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.15it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 273/319 [00:52<00:08,  5.17it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 274/319 [00:52<00:08,  5.18it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  86%|██████████████▋  | 275/319 [00:52<00:08,  5.19it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  87%|██████████████▋  | 276/319 [00:53<00:08,  5.20it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 277/319 [00:53<00:08,  5.22it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 278/319 [00:53<00:07,  5.23it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 279/319 [00:53<00:07,  5.24it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.25it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.27it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  88%|███████████████  | 282/319 [00:53<00:07,  5.28it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  89%|███████████████  | 283/319 [00:53<00:06,  5.29it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.31it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.32it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.33it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 287/319 [00:53<00:05,  5.34it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.36it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 289/319 [00:53<00:05,  5.37it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 290/319 [00:53<00:05,  5.38it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  91%|███████████████▌ | 291/319 [00:53<00:05,  5.39it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 292/319 [00:54<00:04,  5.40it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 293/319 [00:54<00:04,  5.42it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 294/319 [00:54<00:04,  5.43it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 295/319 [00:54<00:04,  5.44it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.45it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.47it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.48it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.49it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.50it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  94%|████████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  95%|████████████████ | 302/319 [00:54<00:03,  5.53it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 303/319 [00:54<00:02,  5.54it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 304/319 [00:54<00:02,  5.55it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 305/319 [00:54<00:02,  5.56it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 307/319 [00:54<00:02,  5.58it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 308/319 [00:55<00:01,  5.60it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 309/319 [00:55<00:01,  5.61it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 310/319 [00:55<00:01,  5.62it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 311/319 [00:55<00:01,  5.63it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 313/319 [00:55<00:01,  5.66it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 314/319 [00:55<00:00,  5.67it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 315/319 [00:55<00:00,  5.68it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 316/319 [00:55<00:00,  5.69it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1:  99%|████████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1: 100%|████████████████▉| 318/319 [00:55<00:00,  5.71it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.770]\u001b[A******Val epoch 1 eval metrics: loss 1.27062558, f1 0.6538 prec 0.6595 rec 0.6739, acc 0.6739\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.270]\n",
      "                                                                                                                       \u001b[A******Train epoch 1 eval metrics: loss 1.42672644, f1 0.6010 prec 0.6302 rec 0.6356, acc 0.6356\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=1.36, v_num=0, train_loss=1.110, val_loss=1.270]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 510: 'val_loss' reached 1.27289 (best 1.27289), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|████████████▊   | 255/319 [00:52<00:13,  4.88it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  80%|████████████▊   | 256/319 [00:52<00:12,  4.89it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 257/319 [00:52<00:12,  4.91it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 258/319 [00:52<00:12,  4.93it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 259/319 [00:52<00:12,  4.94it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 260/319 [00:52<00:11,  4.96it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 261/319 [00:52<00:11,  4.98it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.99it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.01it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.03it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.04it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.06it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.08it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.09it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.11it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.12it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.13it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.15it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.16it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 274/319 [00:52<00:08,  5.17it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.19it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.20it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.21it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.22it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.24it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 280/319 [00:53<00:07,  5.25it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 281/319 [00:53<00:07,  5.26it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.27it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.29it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.31it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.32it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 287/319 [00:53<00:05,  5.34it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.35it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 290/319 [00:53<00:05,  5.37it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.38it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.40it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.41it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.43it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.44it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.46it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.47it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.48it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 300/319 [00:54<00:03,  5.49it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 302/319 [00:54<00:03,  5.52it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 304/319 [00:54<00:02,  5.54it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 305/319 [00:54<00:02,  5.55it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  96%|███████████████▍| 307/319 [00:55<00:02,  5.58it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 310/319 [00:55<00:01,  5.61it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 314/319 [00:55<00:00,  5.66it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 315/319 [00:55<00:00,  5.67it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 316/319 [00:55<00:00,  5.68it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2:  99%|███████████████▉| 317/319 [00:55<00:00,  5.69it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2: 100%|███████████████▉| 318/319 [00:55<00:00,  5.70it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.270]\u001b[A******Val epoch 2 eval metrics: loss 1.16559719, f1 0.6737 prec 0.6828 rec 0.6876, acc 0.6876\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.160]\n",
      "                                                                                                                       \u001b[A******Train epoch 2 eval metrics: loss 0.95491207, f1 0.7422 prec 0.7586 rec 0.7574, acc 0.7574\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.917, v_num=0, train_loss=0.535, val_loss=1.160]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 765: 'val_loss' reached 1.16398 (best 1.16398), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████████▊   | 255/319 [00:52<00:13,  4.85it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  80%|████████████▊   | 256/319 [00:52<00:12,  4.87it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 257/319 [00:52<00:12,  4.88it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 258/319 [00:52<00:12,  4.90it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 259/319 [00:52<00:12,  4.92it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 260/319 [00:52<00:11,  4.93it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 261/319 [00:52<00:11,  4.95it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.97it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 263/319 [00:52<00:11,  4.98it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  83%|█████████████▏  | 264/319 [00:52<00:11,  5.00it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.01it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.03it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.05it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.06it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.08it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 270/319 [00:53<00:09,  5.09it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 271/319 [00:53<00:09,  5.11it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  85%|█████████████▋  | 272/319 [00:53<00:09,  5.12it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 273/319 [00:53<00:08,  5.13it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 274/319 [00:53<00:08,  5.14it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.16it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.17it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.18it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.19it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.21it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 280/319 [00:53<00:07,  5.22it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 281/319 [00:53<00:07,  5.23it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.24it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.26it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.27it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.28it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  90%|██████████████▎ | 286/319 [00:54<00:06,  5.29it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 287/319 [00:54<00:06,  5.31it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 288/319 [00:54<00:05,  5.32it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  91%|██████████████▍ | 289/319 [00:54<00:05,  5.33it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 290/319 [00:54<00:05,  5.34it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.35it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.37it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.38it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.39it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.40it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.41it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.43it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.44it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.45it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 300/319 [00:54<00:03,  5.46it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 301/319 [00:54<00:03,  5.48it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 302/319 [00:55<00:03,  5.49it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 303/319 [00:55<00:02,  5.50it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 304/319 [00:55<00:02,  5.51it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 305/319 [00:55<00:02,  5.52it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 306/319 [00:55<00:02,  5.53it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  96%|███████████████▍| 307/319 [00:55<00:02,  5.55it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 308/319 [00:55<00:01,  5.56it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 309/319 [00:55<00:01,  5.57it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 310/319 [00:55<00:01,  5.58it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 311/319 [00:55<00:01,  5.59it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 312/319 [00:55<00:01,  5.60it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 313/319 [00:55<00:01,  5.62it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 314/319 [00:55<00:00,  5.63it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 315/319 [00:55<00:00,  5.64it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 316/319 [00:55<00:00,  5.65it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3:  99%|███████████████▉| 317/319 [00:55<00:00,  5.66it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3: 100%|███████████████▉| 318/319 [00:56<00:00,  5.67it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.68it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.160]\u001b[A******Val epoch 3 eval metrics: loss 1.12608612, f1 0.6841 prec 0.6830 rec 0.6935, acc 0.6935\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.63it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.130]\n",
      "                                                                                                                       \u001b[A******Train epoch 3 eval metrics: loss 0.67484986, f1 0.8307 prec 0.8395 rec 0.8379, acc 0.8379\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.63it/s, loss=0.585, v_num=0, train_loss=0.155, val_loss=1.130]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1020: 'val_loss' reached 1.12732 (best 1.12732), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.88it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.90it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.91it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.93it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.95it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.96it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  82%|█████████████▉   | 261/319 [00:52<00:11,  4.98it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  82%|█████████████▉   | 262/319 [00:52<00:11,  5.00it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  82%|██████████████   | 263/319 [00:52<00:11,  5.01it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  83%|██████████████   | 264/319 [00:52<00:10,  5.03it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  83%|██████████████   | 265/319 [00:52<00:10,  5.04it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.06it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.08it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  84%|██████████████▎  | 268/319 [00:52<00:10,  5.09it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.11it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.12it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.14it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.15it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  86%|██████████████▌  | 273/319 [00:52<00:08,  5.16it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  86%|██████████████▌  | 274/319 [00:52<00:08,  5.18it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  86%|██████████████▋  | 275/319 [00:53<00:08,  5.19it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  87%|██████████████▋  | 276/319 [00:53<00:08,  5.20it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 277/319 [00:53<00:08,  5.21it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 278/319 [00:53<00:07,  5.23it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 279/319 [00:53<00:07,  5.24it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.25it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.26it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  88%|███████████████  | 282/319 [00:53<00:07,  5.27it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  89%|███████████████  | 283/319 [00:53<00:06,  5.29it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.31it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.32it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  90%|███████████████▎ | 287/319 [00:53<00:05,  5.34it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.35it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  91%|███████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  91%|███████████████▍ | 290/319 [00:53<00:05,  5.37it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  91%|███████████████▌ | 291/319 [00:54<00:05,  5.39it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  92%|███████████████▌ | 292/319 [00:54<00:05,  5.40it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  92%|███████████████▌ | 293/319 [00:54<00:04,  5.41it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  92%|███████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  92%|███████████████▋ | 295/319 [00:54<00:04,  5.43it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.45it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.46it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.47it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.48it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.49it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  94%|████████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  95%|████████████████ | 302/319 [00:54<00:03,  5.52it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  95%|████████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  95%|████████████████▏| 304/319 [00:54<00:02,  5.54it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 305/319 [00:54<00:02,  5.55it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 307/319 [00:55<00:02,  5.58it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  97%|████████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  97%|████████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  97%|████████████████▌| 310/319 [00:55<00:01,  5.61it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  97%|████████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 314/319 [00:55<00:00,  5.66it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  99%|████████████████▊| 315/319 [00:55<00:00,  5.67it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  99%|████████████████▊| 316/319 [00:55<00:00,  5.68it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4:  99%|████████████████▉| 317/319 [00:55<00:00,  5.69it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4: 100%|████████████████▉| 318/319 [00:55<00:00,  5.71it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.130]\u001b[A******Val epoch 4 eval metrics: loss 1.13832478, f1 0.6814 prec 0.6837 rec 0.6896, acc 0.6896\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.140]\n",
      "                                                                                                                       \u001b[A******Train epoch 4 eval metrics: loss 0.50947654, f1 0.8879 prec 0.8913 rec 0.8900, acc 0.8900\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.140]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1275: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.54, v_num=0, train_loss=0.181, val_loss=1.140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\Specter_Default_06_02_2023_01_41_run1\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Benjamin Aw\\Desktop\\ACL_Anthology_Exploratory\\Experiments\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | specter    | BertModel           | 109 M \n",
      "1 | classifier | Linear              | 16.1 K\n",
      "2 | criterion  | CrossEntropyLoss    | 0     \n",
      "3 | acc        | MulticlassAccuracy  | 0     \n",
      "4 | f1         | MulticlassF1Score   | 0     \n",
      "5 | prec       | MulticlassPrecision | 0     \n",
      "6 | rec        | MulticlassRecall    | 0     \n",
      "---------------------------------------------------\n",
      "85.7 M    Trainable params\n",
      "24.3 M    Non-trainable params\n",
      "109 M     Total params\n",
      "439.818   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 54.05it/s]******Val epoch 0 eval metrics: loss 2.91295171, f1 0.1000 prec 0.2500 rec 0.0625, acc 0.0625\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|██████████████████████████▍      | 255/319 [00:52<00:13,  4.89it/s, loss=1.72, v_num=0, train_loss=1.280]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████▍      | 256/319 [00:52<00:12,  4.91it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▌      | 257/319 [00:52<00:12,  4.92it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▋      | 258/319 [00:52<00:12,  4.94it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▊      | 259/319 [00:52<00:12,  4.96it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  82%|██████████████████████████▉      | 260/319 [00:52<00:11,  4.97it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 261/319 [00:52<00:11,  4.99it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 262/319 [00:52<00:11,  5.01it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████▏     | 263/319 [00:52<00:11,  5.02it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▎     | 264/319 [00:52<00:10,  5.04it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▍     | 265/319 [00:52<00:10,  5.06it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▌     | 266/319 [00:52<00:10,  5.07it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▌     | 267/319 [00:52<00:10,  5.09it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▋     | 268/319 [00:52<00:09,  5.11it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▊     | 269/319 [00:52<00:09,  5.12it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  85%|███████████████████████████▉     | 270/319 [00:52<00:09,  5.13it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████     | 271/319 [00:52<00:09,  5.15it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████▏    | 272/319 [00:52<00:09,  5.16it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▏    | 273/319 [00:52<00:08,  5.17it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▎    | 274/319 [00:52<00:08,  5.19it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▍    | 275/319 [00:52<00:08,  5.20it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▌    | 276/319 [00:52<00:08,  5.21it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▋    | 277/319 [00:53<00:08,  5.22it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 278/319 [00:53<00:07,  5.24it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 279/319 [00:53<00:07,  5.25it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  88%|████████████████████████████▉    | 280/319 [00:53<00:07,  5.26it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████    | 281/319 [00:53<00:07,  5.27it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████▏   | 282/319 [00:53<00:06,  5.29it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▎   | 283/319 [00:53<00:06,  5.30it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 284/319 [00:53<00:06,  5.31it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 285/319 [00:53<00:06,  5.32it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▌   | 286/319 [00:53<00:06,  5.34it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▋   | 287/319 [00:53<00:05,  5.35it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▊   | 288/319 [00:53<00:05,  5.36it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  91%|█████████████████████████████▉   | 289/319 [00:53<00:05,  5.37it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 290/319 [00:53<00:05,  5.39it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 291/319 [00:53<00:05,  5.40it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▏  | 292/319 [00:53<00:04,  5.41it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▎  | 293/319 [00:54<00:04,  5.42it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▍  | 294/319 [00:54<00:04,  5.43it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▌  | 295/319 [00:54<00:04,  5.45it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▌  | 296/319 [00:54<00:04,  5.46it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▋  | 297/319 [00:54<00:04,  5.47it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▊  | 298/319 [00:54<00:03,  5.48it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  94%|██████████████████████████████▉  | 299/319 [00:54<00:03,  5.50it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████  | 300/319 [00:54<00:03,  5.51it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████▏ | 301/319 [00:54<00:03,  5.52it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▏ | 302/319 [00:54<00:03,  5.53it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▎ | 303/319 [00:54<00:02,  5.54it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▍ | 304/319 [00:54<00:02,  5.55it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▌ | 305/319 [00:54<00:02,  5.57it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▋ | 306/319 [00:54<00:02,  5.58it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▊ | 307/319 [00:54<00:02,  5.59it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▊ | 308/319 [00:54<00:01,  5.60it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▉ | 309/319 [00:55<00:01,  5.61it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████ | 310/319 [00:55<00:01,  5.63it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████▏| 311/319 [00:55<00:01,  5.64it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▎| 312/319 [00:55<00:01,  5.65it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 313/319 [00:55<00:01,  5.66it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 314/319 [00:55<00:00,  5.67it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▌| 315/319 [00:55<00:00,  5.68it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▋| 316/319 [00:55<00:00,  5.70it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▊| 317/319 [00:55<00:00,  5.71it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████▉| 318/319 [00:55<00:00,  5.72it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████████| 319/319 [00:55<00:00,  5.73it/s, loss=1.72, v_num=0, train_loss=1.280]\u001b[A******Val epoch 0 eval metrics: loss 1.79531197, f1 0.4524 prec 0.4766 rec 0.5049, acc 0.5049\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.68it/s, loss=1.72, v_num=0, train_loss=1.280, val_loss=1.800]\n",
      "                                                                                                                       \u001b[A******Train epoch 0 eval metrics: loss 2.48206579, f1 0.3003 prec 0.3668 rec 0.3153, acc 0.3153\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=1.72, v_num=0, train_loss=1.280, val_loss=1.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 255: 'val_loss' reached 1.80053 (best 1.80053), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.87it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.89it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.90it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.92it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.94it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.95it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 261/319 [00:52<00:11,  4.97it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 262/319 [00:52<00:11,  4.99it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  82%|██████████████   | 263/319 [00:52<00:11,  5.00it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 264/319 [00:52<00:10,  5.02it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 265/319 [00:52<00:10,  5.03it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.05it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.07it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 268/319 [00:52<00:10,  5.08it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.10it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.11it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.13it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.14it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 273/319 [00:52<00:08,  5.15it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 274/319 [00:53<00:08,  5.16it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  86%|██████████████▋  | 275/319 [00:53<00:08,  5.18it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  87%|██████████████▋  | 276/319 [00:53<00:08,  5.19it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 277/319 [00:53<00:08,  5.20it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 278/319 [00:53<00:07,  5.21it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 279/319 [00:53<00:07,  5.23it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.24it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.25it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  88%|███████████████  | 282/319 [00:53<00:07,  5.26it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  89%|███████████████  | 283/319 [00:53<00:06,  5.28it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.29it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.30it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.31it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 287/319 [00:53<00:06,  5.33it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.34it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 289/319 [00:54<00:05,  5.35it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 290/319 [00:54<00:05,  5.36it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  91%|███████████████▌ | 291/319 [00:54<00:05,  5.38it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 292/319 [00:54<00:05,  5.39it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 293/319 [00:54<00:04,  5.40it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 294/319 [00:54<00:04,  5.41it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 295/319 [00:54<00:04,  5.42it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.44it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.45it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.46it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.47it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.49it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  94%|████████████████ | 301/319 [00:54<00:03,  5.50it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  95%|████████████████ | 302/319 [00:54<00:03,  5.51it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 303/319 [00:54<00:02,  5.52it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 304/319 [00:54<00:02,  5.53it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 305/319 [00:55<00:02,  5.55it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 306/319 [00:55<00:02,  5.56it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 307/319 [00:55<00:02,  5.57it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 308/319 [00:55<00:01,  5.58it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 309/319 [00:55<00:01,  5.59it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 310/319 [00:55<00:01,  5.60it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 312/319 [00:55<00:01,  5.63it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 313/319 [00:55<00:01,  5.64it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 314/319 [00:55<00:00,  5.65it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 315/319 [00:55<00:00,  5.66it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 316/319 [00:55<00:00,  5.67it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1:  99%|████████████████▉| 317/319 [00:55<00:00,  5.68it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1: 100%|████████████████▉| 318/319 [00:55<00:00,  5.70it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:55<00:00,  5.71it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.800]\u001b[A******Val epoch 1 eval metrics: loss 1.38432085, f1 0.6127 prec 0.6085 rec 0.6385, acc 0.6385\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.390]\n",
      "                                                                                                                       \u001b[A******Train epoch 1 eval metrics: loss 1.41170226, f1 0.5995 prec 0.5990 rec 0.6311, acc 0.6311\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=1.29, v_num=0, train_loss=1.030, val_loss=1.390]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 510: 'val_loss' reached 1.38668 (best 1.38668), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.87it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.88it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.90it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.92it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.93it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.95it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  82%|█████████████▉   | 261/319 [00:52<00:11,  4.97it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  82%|█████████████▉   | 262/319 [00:52<00:11,  4.98it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  82%|██████████████   | 263/319 [00:52<00:11,  5.00it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  83%|██████████████   | 264/319 [00:52<00:10,  5.02it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  83%|██████████████   | 265/319 [00:52<00:10,  5.03it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.05it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.06it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  84%|██████████████▎  | 268/319 [00:52<00:10,  5.08it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.10it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.11it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.12it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.14it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  86%|██████████████▌  | 273/319 [00:53<00:08,  5.15it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  86%|██████████████▌  | 274/319 [00:53<00:08,  5.16it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  86%|██████████████▋  | 275/319 [00:53<00:08,  5.17it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  87%|██████████████▋  | 276/319 [00:53<00:08,  5.19it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  87%|██████████████▊  | 277/319 [00:53<00:08,  5.20it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  87%|██████████████▊  | 278/319 [00:53<00:07,  5.21it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  87%|██████████████▊  | 279/319 [00:53<00:07,  5.22it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.24it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.25it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  88%|███████████████  | 282/319 [00:53<00:07,  5.26it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  89%|███████████████  | 283/319 [00:53<00:06,  5.27it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.29it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.30it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.31it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  90%|███████████████▎ | 287/319 [00:53<00:06,  5.32it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.34it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  91%|███████████████▍ | 289/319 [00:54<00:05,  5.35it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  91%|███████████████▍ | 290/319 [00:54<00:05,  5.36it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  91%|███████████████▌ | 291/319 [00:54<00:05,  5.37it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  92%|███████████████▌ | 292/319 [00:54<00:05,  5.39it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  92%|███████████████▌ | 293/319 [00:54<00:04,  5.40it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  92%|███████████████▋ | 294/319 [00:54<00:04,  5.41it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  92%|███████████████▋ | 295/319 [00:54<00:04,  5.42it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.43it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.45it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.46it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.47it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.48it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  94%|████████████████ | 301/319 [00:54<00:03,  5.49it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  95%|████████████████ | 302/319 [00:54<00:03,  5.51it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  95%|████████████████▏| 303/319 [00:54<00:02,  5.52it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  95%|████████████████▏| 304/319 [00:54<00:02,  5.53it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  96%|████████████████▎| 305/319 [00:55<00:02,  5.54it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  96%|████████████████▎| 306/319 [00:55<00:02,  5.55it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  96%|████████████████▎| 307/319 [00:55<00:02,  5.57it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  97%|████████████████▍| 308/319 [00:55<00:01,  5.58it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  97%|████████████████▍| 309/319 [00:55<00:01,  5.59it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  97%|████████████████▌| 310/319 [00:55<00:01,  5.60it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  97%|████████████████▌| 311/319 [00:55<00:01,  5.61it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  98%|████████████████▋| 312/319 [00:55<00:01,  5.62it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  98%|████████████████▋| 313/319 [00:55<00:01,  5.64it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  98%|████████████████▋| 314/319 [00:55<00:00,  5.65it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  99%|████████████████▊| 315/319 [00:55<00:00,  5.66it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  99%|████████████████▊| 316/319 [00:55<00:00,  5.67it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2:  99%|████████████████▉| 317/319 [00:55<00:00,  5.68it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2: 100%|████████████████▉| 318/319 [00:55<00:00,  5.69it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A\n",
      "Epoch 2: 100%|█████████████████| 319/319 [00:55<00:00,  5.70it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.390]\u001b[A******Val epoch 2 eval metrics: loss 1.24375580, f1 0.6690 prec 0.6809 rec 0.6817, acc 0.6817\n",
      "Epoch 2: 100%|█████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.250]\n",
      "                                                                                                                       \u001b[A******Train epoch 2 eval metrics: loss 0.93149639, f1 0.7384 prec 0.7472 rec 0.7588, acc 0.7588\n",
      "Epoch 2: 100%|█████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=1.08, v_num=0, train_loss=1.410, val_loss=1.250]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 765: 'val_loss' reached 1.24673 (best 1.24673), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████████▊   | 255/319 [00:52<00:13,  4.87it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  80%|████████████▊   | 256/319 [00:52<00:12,  4.88it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 257/319 [00:52<00:12,  4.90it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 258/319 [00:52<00:12,  4.92it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 259/319 [00:52<00:12,  4.93it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 260/319 [00:52<00:11,  4.95it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 261/319 [00:52<00:11,  4.97it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.98it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.00it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.02it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.03it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.05it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.07it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.08it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.10it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.11it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.12it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.14it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 273/319 [00:53<00:08,  5.15it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 274/319 [00:53<00:08,  5.16it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.17it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.19it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.20it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.21it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.22it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 280/319 [00:53<00:07,  5.24it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 281/319 [00:53<00:07,  5.25it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.26it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.27it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.29it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.30it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.31it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 287/319 [00:53<00:06,  5.32it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.34it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  91%|██████████████▍ | 289/319 [00:54<00:05,  5.35it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 290/319 [00:54<00:05,  5.36it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.37it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.39it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.40it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.41it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.42it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.43it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.45it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.46it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.47it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 300/319 [00:54<00:03,  5.48it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 301/319 [00:54<00:03,  5.49it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 302/319 [00:54<00:03,  5.51it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 303/319 [00:54<00:02,  5.52it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 304/319 [00:54<00:02,  5.53it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 305/319 [00:55<00:02,  5.54it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 306/319 [00:55<00:02,  5.55it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  96%|███████████████▍| 307/319 [00:55<00:02,  5.57it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 308/319 [00:55<00:01,  5.58it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 309/319 [00:55<00:01,  5.59it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 310/319 [00:55<00:01,  5.60it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 311/319 [00:55<00:01,  5.61it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 312/319 [00:55<00:01,  5.62it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 313/319 [00:55<00:01,  5.64it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 314/319 [00:55<00:00,  5.65it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 315/319 [00:55<00:00,  5.66it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 316/319 [00:55<00:00,  5.67it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3:  99%|███████████████▉| 317/319 [00:55<00:00,  5.68it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3: 100%|███████████████▉| 318/319 [00:55<00:00,  5.69it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:55<00:00,  5.70it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.250]\u001b[A******Val epoch 3 eval metrics: loss 1.27481995, f1 0.6572 prec 0.6742 rec 0.6719, acc 0.6719\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.280]\n",
      "                                                                                                                       \u001b[A******Train epoch 3 eval metrics: loss 0.65981427, f1 0.8309 prec 0.8402 rec 0.8389, acc 0.8389\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=0.818, v_num=0, train_loss=1.390, val_loss=1.280]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1020: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.88it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.90it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.91it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.93it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.95it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.96it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▉   | 261/319 [00:52<00:11,  4.98it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▉   | 262/319 [00:52<00:11,  5.00it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|██████████████   | 263/319 [00:52<00:11,  5.01it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|██████████████   | 264/319 [00:52<00:10,  5.03it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|██████████████   | 265/319 [00:52<00:10,  5.05it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.06it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.08it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|██████████████▎  | 268/319 [00:52<00:10,  5.09it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.11it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.12it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.14it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.15it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|██████████████▌  | 273/319 [00:52<00:08,  5.16it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|██████████████▌  | 274/319 [00:52<00:08,  5.18it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|██████████████▋  | 275/319 [00:53<00:08,  5.19it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▋  | 276/319 [00:53<00:08,  5.20it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 277/319 [00:53<00:08,  5.21it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 278/319 [00:53<00:07,  5.23it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 279/319 [00:53<00:07,  5.24it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.25it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.26it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|███████████████  | 282/319 [00:53<00:07,  5.28it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|███████████████  | 283/319 [00:53<00:06,  5.29it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.31it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.33it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|███████████████▎ | 287/319 [00:53<00:05,  5.34it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.35it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|███████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|███████████████▍ | 290/319 [00:53<00:05,  5.38it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|███████████████▌ | 291/319 [00:54<00:05,  5.39it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▌ | 292/319 [00:54<00:05,  5.40it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▌ | 293/319 [00:54<00:04,  5.41it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▋ | 295/319 [00:54<00:04,  5.44it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.45it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.46it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.47it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.48it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.50it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|████████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|████████████████ | 302/319 [00:54<00:03,  5.52it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|████████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|████████████████▏| 304/319 [00:54<00:02,  5.55it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 305/319 [00:54<00:02,  5.56it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 307/319 [00:55<00:02,  5.58it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▌| 310/319 [00:55<00:01,  5.62it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▌| 311/319 [00:55<00:01,  5.63it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 314/319 [00:55<00:00,  5.66it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|████████████████▊| 315/319 [00:55<00:00,  5.67it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|████████████████▊| 316/319 [00:55<00:00,  5.69it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|████████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4: 100%|████████████████▉| 318/319 [00:55<00:00,  5.71it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.280]\u001b[A******Val epoch 4 eval metrics: loss 1.26956670, f1 0.6563 prec 0.6640 rec 0.6699, acc 0.6699\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.270]\n",
      "                                                                                                                       \u001b[A******Train epoch 4 eval metrics: loss 0.50197006, f1 0.8854 prec 0.8927 rec 0.8895, acc 0.8895\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.270]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1275: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.57, v_num=0, train_loss=0.720, val_loss=1.270]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\Specter_Default_06_02_2023_01_41_run2\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Benjamin Aw\\Desktop\\ACL_Anthology_Exploratory\\Experiments\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | specter    | BertModel           | 109 M \n",
      "1 | classifier | Linear              | 16.1 K\n",
      "2 | criterion  | CrossEntropyLoss    | 0     \n",
      "3 | acc        | MulticlassAccuracy  | 0     \n",
      "4 | f1         | MulticlassF1Score   | 0     \n",
      "5 | prec       | MulticlassPrecision | 0     \n",
      "6 | rec        | MulticlassRecall    | 0     \n",
      "---------------------------------------------------\n",
      "85.7 M    Trainable params\n",
      "24.3 M    Non-trainable params\n",
      "109 M     Total params\n",
      "439.818   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 38.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Val epoch 0 eval metrics: loss 2.86650538, f1 0.1964 prec 0.4583 rec 0.1250, acc 0.1250\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|██████████████████████████▍      | 255/319 [00:52<00:13,  4.89it/s, loss=1.88, v_num=0, train_loss=1.670]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████▍      | 256/319 [00:52<00:12,  4.91it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▌      | 257/319 [00:52<00:12,  4.92it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▋      | 258/319 [00:52<00:12,  4.94it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▊      | 259/319 [00:52<00:12,  4.96it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  82%|██████████████████████████▉      | 260/319 [00:52<00:11,  4.97it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 261/319 [00:52<00:11,  4.99it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 262/319 [00:52<00:11,  5.01it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████▏     | 263/319 [00:52<00:11,  5.02it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▎     | 264/319 [00:52<00:10,  5.04it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▍     | 265/319 [00:52<00:10,  5.06it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▌     | 266/319 [00:52<00:10,  5.07it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▌     | 267/319 [00:52<00:10,  5.09it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▋     | 268/319 [00:52<00:09,  5.11it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▊     | 269/319 [00:52<00:09,  5.12it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  85%|███████████████████████████▉     | 270/319 [00:52<00:09,  5.14it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████     | 271/319 [00:52<00:09,  5.15it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████▏    | 272/319 [00:52<00:09,  5.16it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▏    | 273/319 [00:52<00:08,  5.17it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▎    | 274/319 [00:52<00:08,  5.19it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▍    | 275/319 [00:52<00:08,  5.20it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▌    | 276/319 [00:52<00:08,  5.21it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▋    | 277/319 [00:53<00:08,  5.22it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 278/319 [00:53<00:07,  5.24it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 279/319 [00:53<00:07,  5.25it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  88%|████████████████████████████▉    | 280/319 [00:53<00:07,  5.26it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████    | 281/319 [00:53<00:07,  5.28it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████▏   | 282/319 [00:53<00:06,  5.29it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▎   | 283/319 [00:53<00:06,  5.30it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 284/319 [00:53<00:06,  5.31it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 285/319 [00:53<00:06,  5.32it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▌   | 286/319 [00:53<00:06,  5.34it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▋   | 287/319 [00:53<00:05,  5.35it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▊   | 288/319 [00:53<00:05,  5.36it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  91%|█████████████████████████████▉   | 289/319 [00:53<00:05,  5.37it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 290/319 [00:53<00:05,  5.39it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 291/319 [00:53<00:05,  5.40it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▏  | 292/319 [00:53<00:04,  5.41it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▎  | 293/319 [00:54<00:04,  5.42it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▍  | 294/319 [00:54<00:04,  5.43it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▌  | 295/319 [00:54<00:04,  5.45it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▌  | 296/319 [00:54<00:04,  5.46it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▋  | 297/319 [00:54<00:04,  5.47it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▊  | 298/319 [00:54<00:03,  5.48it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  94%|██████████████████████████████▉  | 299/319 [00:54<00:03,  5.49it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████  | 300/319 [00:54<00:03,  5.51it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████▏ | 301/319 [00:54<00:03,  5.52it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▏ | 302/319 [00:54<00:03,  5.53it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▎ | 303/319 [00:54<00:02,  5.54it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▍ | 304/319 [00:54<00:02,  5.55it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▌ | 305/319 [00:54<00:02,  5.57it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▋ | 306/319 [00:54<00:02,  5.58it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▊ | 307/319 [00:54<00:02,  5.59it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▊ | 308/319 [00:54<00:01,  5.60it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▉ | 309/319 [00:55<00:01,  5.61it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████ | 310/319 [00:55<00:01,  5.62it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████▏| 311/319 [00:55<00:01,  5.64it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▎| 312/319 [00:55<00:01,  5.65it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 313/319 [00:55<00:01,  5.66it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 314/319 [00:55<00:00,  5.67it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▌| 315/319 [00:55<00:00,  5.68it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▋| 316/319 [00:55<00:00,  5.69it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▊| 317/319 [00:55<00:00,  5.71it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████▉| 318/319 [00:55<00:00,  5.72it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████████| 319/319 [00:55<00:00,  5.73it/s, loss=1.88, v_num=0, train_loss=1.670]\u001b[A******Val epoch 0 eval metrics: loss 1.73338631, f1 0.5309 prec 0.5543 rec 0.5835, acc 0.5835\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.68it/s, loss=1.88, v_num=0, train_loss=1.670, val_loss=1.740]\n",
      "                                                                                                                       \u001b[A******Train epoch 0 eval metrics: loss 2.50929660, f1 0.2611 prec 0.3006 rec 0.3148, acc 0.3148\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=1.88, v_num=0, train_loss=1.670, val_loss=1.740]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 255: 'val_loss' reached 1.73662 (best 1.73662), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.87it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.88it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.90it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.92it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.93it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.95it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 261/319 [00:52<00:11,  4.97it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 262/319 [00:52<00:11,  4.98it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  82%|██████████████   | 263/319 [00:52<00:11,  5.00it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 264/319 [00:52<00:10,  5.01it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 265/319 [00:52<00:10,  5.03it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.05it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.06it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 268/319 [00:52<00:10,  5.08it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.10it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.11it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.12it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.14it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 273/319 [00:53<00:08,  5.15it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 274/319 [00:53<00:08,  5.16it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  86%|██████████████▋  | 275/319 [00:53<00:08,  5.17it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  87%|██████████████▋  | 276/319 [00:53<00:08,  5.19it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 277/319 [00:53<00:08,  5.20it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 278/319 [00:53<00:07,  5.21it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 279/319 [00:53<00:07,  5.23it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.24it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.25it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  88%|███████████████  | 282/319 [00:53<00:07,  5.26it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  89%|███████████████  | 283/319 [00:53<00:06,  5.27it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.29it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.30it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.31it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 287/319 [00:53<00:06,  5.33it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.34it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 289/319 [00:54<00:05,  5.35it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 290/319 [00:54<00:05,  5.36it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  91%|███████████████▌ | 291/319 [00:54<00:05,  5.37it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 292/319 [00:54<00:05,  5.39it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 293/319 [00:54<00:04,  5.40it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 294/319 [00:54<00:04,  5.41it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 295/319 [00:54<00:04,  5.42it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.44it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.45it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.46it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.47it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.48it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  94%|████████████████ | 301/319 [00:54<00:03,  5.50it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  95%|████████████████ | 302/319 [00:54<00:03,  5.51it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 303/319 [00:54<00:02,  5.52it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 304/319 [00:54<00:02,  5.53it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 305/319 [00:55<00:02,  5.54it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 306/319 [00:55<00:02,  5.55it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 307/319 [00:55<00:02,  5.57it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 308/319 [00:55<00:01,  5.58it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 309/319 [00:55<00:01,  5.59it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 310/319 [00:55<00:01,  5.60it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 311/319 [00:55<00:01,  5.61it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 312/319 [00:55<00:01,  5.62it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 313/319 [00:55<00:01,  5.64it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 314/319 [00:55<00:00,  5.65it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 315/319 [00:55<00:00,  5.66it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 316/319 [00:55<00:00,  5.67it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1:  99%|████████████████▉| 317/319 [00:55<00:00,  5.68it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1: 100%|████████████████▉| 318/319 [00:55<00:00,  5.70it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:55<00:00,  5.71it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.740]\u001b[A******Val epoch 1 eval metrics: loss 1.34571101, f1 0.5922 prec 0.5946 rec 0.6248, acc 0.6248\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.350]\n",
      "                                                                                                                       \u001b[A******Train epoch 1 eval metrics: loss 1.41751912, f1 0.6187 prec 0.6364 rec 0.6449, acc 0.6449\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=1.22, v_num=0, train_loss=1.410, val_loss=1.350]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 510: 'val_loss' reached 1.35000 (best 1.35000), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|████████████▊   | 255/319 [00:52<00:13,  4.87it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  80%|████████████▊   | 256/319 [00:52<00:12,  4.89it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 257/319 [00:52<00:12,  4.90it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 258/319 [00:52<00:12,  4.92it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 259/319 [00:52<00:12,  4.94it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 260/319 [00:52<00:11,  4.95it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 261/319 [00:52<00:11,  4.97it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.99it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.00it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.02it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.04it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.05it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.07it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.09it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.10it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.12it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.13it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.14it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.15it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 274/319 [00:53<00:08,  5.17it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.18it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.19it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.20it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.22it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.23it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 280/319 [00:53<00:07,  5.24it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 281/319 [00:53<00:07,  5.25it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.27it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.28it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.29it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.30it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.32it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 287/319 [00:53<00:06,  5.33it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.34it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.35it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 290/319 [00:54<00:05,  5.37it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.38it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.39it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.40it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.41it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.43it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.44it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.45it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.46it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.47it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 300/319 [00:54<00:03,  5.49it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 301/319 [00:54<00:03,  5.50it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 302/319 [00:54<00:03,  5.51it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 303/319 [00:54<00:02,  5.52it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 304/319 [00:54<00:02,  5.53it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 305/319 [00:54<00:02,  5.55it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 306/319 [00:55<00:02,  5.56it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  96%|███████████████▍| 307/319 [00:55<00:02,  5.57it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 308/319 [00:55<00:01,  5.58it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 309/319 [00:55<00:01,  5.59it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 310/319 [00:55<00:01,  5.61it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 312/319 [00:55<00:01,  5.63it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 313/319 [00:55<00:01,  5.64it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 314/319 [00:55<00:00,  5.65it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 315/319 [00:55<00:00,  5.66it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 316/319 [00:55<00:00,  5.68it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2:  99%|███████████████▉| 317/319 [00:55<00:00,  5.69it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2: 100%|███████████████▉| 318/319 [00:55<00:00,  5.70it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:55<00:00,  5.71it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.350]\u001b[A******Val epoch 2 eval metrics: loss 1.19307716, f1 0.6459 prec 0.6446 rec 0.6640, acc 0.6640\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.200]\n",
      "                                                                                                                       \u001b[A******Train epoch 2 eval metrics: loss 0.94500422, f1 0.7485 prec 0.7624 rec 0.7608, acc 0.7608\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=0.933, v_num=0, train_loss=1.490, val_loss=1.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 765: 'val_loss' reached 1.19610 (best 1.19610), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████████▊   | 255/319 [00:52<00:13,  4.87it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  80%|████████████▊   | 256/319 [00:52<00:12,  4.89it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 257/319 [00:52<00:12,  4.91it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 258/319 [00:52<00:12,  4.92it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 259/319 [00:52<00:12,  4.94it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 260/319 [00:52<00:11,  4.96it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 261/319 [00:52<00:11,  4.97it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.99it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.00it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.02it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.04it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.05it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.07it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.09it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.10it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.12it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.13it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.14it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.16it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 274/319 [00:53<00:08,  5.17it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.18it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.19it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.21it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.22it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.23it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 280/319 [00:53<00:07,  5.24it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 281/319 [00:53<00:07,  5.26it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.27it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.28it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.29it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.31it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.32it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 287/319 [00:53<00:06,  5.33it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.34it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 290/319 [00:54<00:05,  5.37it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.38it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.39it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.40it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.43it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.44it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.45it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.46it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.48it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 300/319 [00:54<00:03,  5.49it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 301/319 [00:54<00:03,  5.50it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 302/319 [00:54<00:03,  5.51it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 304/319 [00:54<00:02,  5.54it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 305/319 [00:54<00:02,  5.55it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 306/319 [00:55<00:02,  5.56it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  96%|███████████████▍| 307/319 [00:55<00:02,  5.57it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 308/319 [00:55<00:01,  5.58it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 310/319 [00:55<00:01,  5.61it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 312/319 [00:55<00:01,  5.63it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 313/319 [00:55<00:01,  5.64it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 314/319 [00:55<00:00,  5.65it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 315/319 [00:55<00:00,  5.67it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 316/319 [00:55<00:00,  5.68it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3:  99%|███████████████▉| 317/319 [00:55<00:00,  5.69it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3: 100%|███████████████▉| 318/319 [00:55<00:00,  5.70it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:55<00:00,  5.71it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.200]\u001b[A******Val epoch 3 eval metrics: loss 1.20790632, f1 0.6573 prec 0.6562 rec 0.6739, acc 0.6739\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.210]\n",
      "                                                                                                                       \u001b[A******Train epoch 3 eval metrics: loss 0.63622370, f1 0.8417 prec 0.8504 rec 0.8472, acc 0.8472\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.65it/s, loss=0.633, v_num=0, train_loss=0.678, val_loss=1.210]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1020: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  80%|███████████▉   | 255/319 [00:52<00:13,  4.89it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  80%|████████████   | 256/319 [00:52<00:12,  4.90it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  81%|████████████   | 257/319 [00:52<00:12,  4.92it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  81%|████████████▏  | 258/319 [00:52<00:12,  4.94it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  81%|████████████▏  | 259/319 [00:52<00:12,  4.95it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  82%|████████████▏  | 260/319 [00:52<00:11,  4.97it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  82%|████████████▎  | 261/319 [00:52<00:11,  4.98it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  82%|████████████▎  | 262/319 [00:52<00:11,  5.00it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  82%|████████████▎  | 263/319 [00:52<00:11,  5.02it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  83%|████████████▍  | 264/319 [00:52<00:10,  5.03it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  83%|████████████▍  | 265/319 [00:52<00:10,  5.05it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  83%|████████████▌  | 266/319 [00:52<00:10,  5.07it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  84%|████████████▌  | 267/319 [00:52<00:10,  5.08it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  84%|████████████▌  | 268/319 [00:52<00:09,  5.10it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  84%|████████████▋  | 269/319 [00:52<00:09,  5.12it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  85%|████████████▋  | 270/319 [00:52<00:09,  5.13it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  85%|████████████▋  | 271/319 [00:52<00:09,  5.14it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  85%|████████████▊  | 272/319 [00:52<00:09,  5.16it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  86%|████████████▊  | 273/319 [00:52<00:08,  5.17it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  86%|████████████▉  | 274/319 [00:52<00:08,  5.18it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  86%|████████████▉  | 275/319 [00:52<00:08,  5.19it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  87%|████████████▉  | 276/319 [00:53<00:08,  5.21it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  87%|█████████████  | 277/319 [00:53<00:08,  5.22it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  87%|█████████████  | 278/319 [00:53<00:07,  5.23it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  87%|█████████████  | 279/319 [00:53<00:07,  5.24it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  88%|█████████████▏ | 280/319 [00:53<00:07,  5.26it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  88%|█████████████▏ | 281/319 [00:53<00:07,  5.27it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  88%|█████████████▎ | 282/319 [00:53<00:07,  5.28it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  89%|█████████████▎ | 283/319 [00:53<00:06,  5.29it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  89%|█████████████▎ | 284/319 [00:53<00:06,  5.31it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  89%|█████████████▍ | 285/319 [00:53<00:06,  5.32it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  90%|█████████████▍ | 286/319 [00:53<00:06,  5.33it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  90%|█████████████▍ | 287/319 [00:53<00:05,  5.34it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  90%|█████████████▌ | 288/319 [00:53<00:05,  5.36it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  91%|█████████████▌ | 289/319 [00:53<00:05,  5.37it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  91%|█████████████▋ | 290/319 [00:53<00:05,  5.38it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  91%|█████████████▋ | 291/319 [00:53<00:05,  5.39it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  92%|█████████████▋ | 292/319 [00:54<00:04,  5.41it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  92%|█████████████▊ | 293/319 [00:54<00:04,  5.42it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  92%|█████████████▊ | 294/319 [00:54<00:04,  5.43it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  92%|█████████████▊ | 295/319 [00:54<00:04,  5.44it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  93%|█████████████▉ | 296/319 [00:54<00:04,  5.46it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  93%|█████████████▉ | 297/319 [00:54<00:04,  5.47it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  93%|██████████████ | 298/319 [00:54<00:03,  5.48it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  94%|██████████████ | 299/319 [00:54<00:03,  5.49it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  94%|██████████████ | 300/319 [00:54<00:03,  5.50it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  94%|██████████████▏| 301/319 [00:54<00:03,  5.52it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  95%|██████████████▏| 302/319 [00:54<00:03,  5.53it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  95%|██████████████▏| 303/319 [00:54<00:02,  5.54it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  95%|██████████████▎| 304/319 [00:54<00:02,  5.55it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  96%|██████████████▎| 305/319 [00:54<00:02,  5.56it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  96%|██████████████▍| 306/319 [00:54<00:02,  5.57it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  96%|██████████████▍| 307/319 [00:54<00:02,  5.59it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  97%|██████████████▍| 308/319 [00:55<00:01,  5.60it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  97%|██████████████▌| 309/319 [00:55<00:01,  5.61it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  97%|██████████████▌| 310/319 [00:55<00:01,  5.62it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  97%|██████████████▌| 311/319 [00:55<00:01,  5.63it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  98%|██████████████▋| 312/319 [00:55<00:01,  5.65it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  98%|██████████████▋| 313/319 [00:55<00:01,  5.66it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  98%|██████████████▊| 314/319 [00:55<00:00,  5.67it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  99%|██████████████▊| 315/319 [00:55<00:00,  5.68it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  99%|██████████████▊| 316/319 [00:55<00:00,  5.69it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4:  99%|██████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4: 100%|██████████████▉| 318/319 [00:55<00:00,  5.72it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A\n",
      "Epoch 4: 100%|███████████████| 319/319 [00:55<00:00,  5.73it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.210]\u001b[A******Val epoch 4 eval metrics: loss 1.20066020, f1 0.6581 prec 0.6536 rec 0.6739, acc 0.6739\n",
      "Epoch 4: 100%|███████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.200]\n",
      "                                                                                                                       \u001b[A******Train epoch 4 eval metrics: loss 0.47670168, f1 0.8912 prec 0.8957 rec 0.8934, acc 0.8934\n",
      "Epoch 4: 100%|███████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1275: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.511, v_num=0, train_loss=0.0904, val_loss=1.200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\Specter_Default_06_02_2023_01_41_run3\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Benjamin Aw\\Desktop\\ACL_Anthology_Exploratory\\Experiments\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | specter    | BertModel           | 109 M \n",
      "1 | classifier | Linear              | 16.1 K\n",
      "2 | criterion  | CrossEntropyLoss    | 0     \n",
      "3 | acc        | MulticlassAccuracy  | 0     \n",
      "4 | f1         | MulticlassF1Score   | 0     \n",
      "5 | prec       | MulticlassPrecision | 0     \n",
      "6 | rec        | MulticlassRecall    | 0     \n",
      "---------------------------------------------------\n",
      "85.7 M    Trainable params\n",
      "24.3 M    Non-trainable params\n",
      "109 M     Total params\n",
      "439.818   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                              | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 54.06it/s]******Val epoch 0 eval metrics: loss 3.06673944, f1 0.0000 prec 0.0000 rec 0.0000, acc 0.0000\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|██████████████████████████▍      | 255/319 [00:52<00:13,  4.89it/s, loss=1.72, v_num=0, train_loss=1.870]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████▍      | 256/319 [00:52<00:12,  4.91it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▌      | 257/319 [00:52<00:12,  4.93it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▋      | 258/319 [00:52<00:12,  4.94it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▊      | 259/319 [00:52<00:12,  4.96it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  82%|██████████████████████████▉      | 260/319 [00:52<00:11,  4.98it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 261/319 [00:52<00:11,  5.00it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 262/319 [00:52<00:11,  5.01it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████▏     | 263/319 [00:52<00:11,  5.03it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▎     | 264/319 [00:52<00:10,  5.04it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▍     | 265/319 [00:52<00:10,  5.06it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▌     | 266/319 [00:52<00:10,  5.08it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▌     | 267/319 [00:52<00:10,  5.10it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▋     | 268/319 [00:52<00:09,  5.11it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▊     | 269/319 [00:52<00:09,  5.13it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  85%|███████████████████████████▉     | 270/319 [00:52<00:09,  5.14it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████     | 271/319 [00:52<00:09,  5.15it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████▏    | 272/319 [00:52<00:09,  5.17it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▏    | 273/319 [00:52<00:08,  5.18it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▎    | 274/319 [00:52<00:08,  5.19it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▍    | 275/319 [00:52<00:08,  5.21it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▌    | 276/319 [00:52<00:08,  5.22it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▋    | 277/319 [00:52<00:08,  5.23it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 278/319 [00:53<00:07,  5.24it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 279/319 [00:53<00:07,  5.26it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  88%|████████████████████████████▉    | 280/319 [00:53<00:07,  5.27it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████    | 281/319 [00:53<00:07,  5.28it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████▏   | 282/319 [00:53<00:06,  5.29it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▎   | 283/319 [00:53<00:06,  5.31it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 284/319 [00:53<00:06,  5.32it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 285/319 [00:53<00:06,  5.33it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▌   | 286/319 [00:53<00:06,  5.34it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▋   | 287/319 [00:53<00:05,  5.36it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▊   | 288/319 [00:53<00:05,  5.37it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  91%|█████████████████████████████▉   | 289/319 [00:53<00:05,  5.38it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 290/319 [00:53<00:05,  5.39it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 291/319 [00:53<00:05,  5.41it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▏  | 292/319 [00:53<00:04,  5.42it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▎  | 293/319 [00:53<00:04,  5.43it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▍  | 294/319 [00:54<00:04,  5.44it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▌  | 295/319 [00:54<00:04,  5.46it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▌  | 296/319 [00:54<00:04,  5.47it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▋  | 297/319 [00:54<00:04,  5.48it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▊  | 298/319 [00:54<00:03,  5.49it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  94%|██████████████████████████████▉  | 299/319 [00:54<00:03,  5.50it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████  | 300/319 [00:54<00:03,  5.52it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████▏ | 301/319 [00:54<00:03,  5.53it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▏ | 302/319 [00:54<00:03,  5.54it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▎ | 303/319 [00:54<00:02,  5.55it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▍ | 304/319 [00:54<00:02,  5.56it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▌ | 305/319 [00:54<00:02,  5.58it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▋ | 306/319 [00:54<00:02,  5.59it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▊ | 307/319 [00:54<00:02,  5.60it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▊ | 308/319 [00:54<00:01,  5.61it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▉ | 309/319 [00:54<00:01,  5.63it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████ | 310/319 [00:54<00:01,  5.64it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████▏| 311/319 [00:55<00:01,  5.65it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▎| 312/319 [00:55<00:01,  5.66it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 313/319 [00:55<00:01,  5.67it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 314/319 [00:55<00:00,  5.68it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▌| 315/319 [00:55<00:00,  5.70it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▋| 316/319 [00:55<00:00,  5.71it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▊| 317/319 [00:55<00:00,  5.72it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████▉| 318/319 [00:55<00:00,  5.73it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████████| 319/319 [00:55<00:00,  5.74it/s, loss=1.72, v_num=0, train_loss=1.870]\u001b[A******Val epoch 0 eval metrics: loss 1.74881117, f1 0.5097 prec 0.5603 rec 0.5462, acc 0.5462\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.69it/s, loss=1.72, v_num=0, train_loss=1.870, val_loss=1.760]\n",
      "                                                                                                                       \u001b[A******Train epoch 0 eval metrics: loss 2.48864647, f1 0.2912 prec 0.3507 rec 0.3207, acc 0.3207\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.68it/s, loss=1.72, v_num=0, train_loss=1.870, val_loss=1.760]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 255: 'val_loss' reached 1.75508 (best 1.75508), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|██████████████▍   | 255/319 [00:52<00:13,  4.87it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  80%|██████████████▍   | 256/319 [00:52<00:12,  4.89it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  81%|██████████████▌   | 257/319 [00:52<00:12,  4.90it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  81%|██████████████▌   | 258/319 [00:52<00:12,  4.92it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  81%|██████████████▌   | 259/319 [00:52<00:12,  4.94it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  82%|██████████████▋   | 260/319 [00:52<00:11,  4.95it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  82%|██████████████▋   | 261/319 [00:52<00:11,  4.97it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  82%|██████████████▊   | 262/319 [00:52<00:11,  4.99it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  82%|██████████████▊   | 263/319 [00:52<00:11,  5.00it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  83%|██████████████▉   | 264/319 [00:52<00:10,  5.02it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  83%|██████████████▉   | 265/319 [00:52<00:10,  5.04it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  83%|███████████████   | 266/319 [00:52<00:10,  5.05it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  84%|███████████████   | 267/319 [00:52<00:10,  5.07it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  84%|███████████████   | 268/319 [00:52<00:10,  5.09it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  84%|███████████████▏  | 269/319 [00:52<00:09,  5.10it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  85%|███████████████▏  | 270/319 [00:52<00:09,  5.12it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  85%|███████████████▎  | 271/319 [00:52<00:09,  5.13it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  85%|███████████████▎  | 272/319 [00:52<00:09,  5.14it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  86%|███████████████▍  | 273/319 [00:52<00:08,  5.16it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  86%|███████████████▍  | 274/319 [00:53<00:08,  5.17it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  86%|███████████████▌  | 275/319 [00:53<00:08,  5.18it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  87%|███████████████▌  | 276/319 [00:53<00:08,  5.19it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  87%|███████████████▋  | 277/319 [00:53<00:08,  5.21it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  87%|███████████████▋  | 278/319 [00:53<00:07,  5.22it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  87%|███████████████▋  | 279/319 [00:53<00:07,  5.23it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  88%|███████████████▊  | 280/319 [00:53<00:07,  5.24it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  88%|███████████████▊  | 281/319 [00:53<00:07,  5.26it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  88%|███████████████▉  | 282/319 [00:53<00:07,  5.27it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  89%|███████████████▉  | 283/319 [00:53<00:06,  5.28it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  89%|████████████████  | 284/319 [00:53<00:06,  5.29it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  89%|████████████████  | 285/319 [00:53<00:06,  5.31it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  90%|████████████████▏ | 286/319 [00:53<00:06,  5.32it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  90%|████████████████▏ | 287/319 [00:53<00:06,  5.33it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  90%|████████████████▎ | 288/319 [00:53<00:05,  5.34it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  91%|████████████████▎ | 289/319 [00:53<00:05,  5.36it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  91%|████████████████▎ | 290/319 [00:54<00:05,  5.37it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  91%|████████████████▍ | 291/319 [00:54<00:05,  5.38it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  92%|████████████████▍ | 292/319 [00:54<00:05,  5.39it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  92%|████████████████▌ | 293/319 [00:54<00:04,  5.41it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  92%|████████████████▌ | 294/319 [00:54<00:04,  5.42it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  92%|████████████████▋ | 295/319 [00:54<00:04,  5.43it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  93%|████████████████▋ | 296/319 [00:54<00:04,  5.44it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  93%|████████████████▊ | 297/319 [00:54<00:04,  5.46it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  93%|████████████████▊ | 298/319 [00:54<00:03,  5.47it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  94%|████████████████▊ | 299/319 [00:54<00:03,  5.48it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  94%|████████████████▉ | 300/319 [00:54<00:03,  5.49it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  94%|████████████████▉ | 301/319 [00:54<00:03,  5.50it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  95%|█████████████████ | 302/319 [00:54<00:03,  5.52it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  95%|█████████████████ | 303/319 [00:54<00:02,  5.53it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  95%|█████████████████▏| 304/319 [00:54<00:02,  5.54it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  96%|█████████████████▏| 305/319 [00:54<00:02,  5.55it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  96%|█████████████████▎| 306/319 [00:54<00:02,  5.56it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  96%|█████████████████▎| 307/319 [00:55<00:02,  5.58it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  97%|█████████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  97%|█████████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  97%|█████████████████▍| 310/319 [00:55<00:01,  5.61it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  97%|█████████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  98%|█████████████████▌| 312/319 [00:55<00:01,  5.63it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  98%|█████████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  98%|█████████████████▋| 314/319 [00:55<00:00,  5.66it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  99%|█████████████████▊| 315/319 [00:55<00:00,  5.67it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  99%|█████████████████▊| 316/319 [00:55<00:00,  5.68it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1:  99%|█████████████████▉| 317/319 [00:55<00:00,  5.69it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1: 100%|█████████████████▉| 318/319 [00:55<00:00,  5.70it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A\n",
      "Epoch 1: 100%|██████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.760]\u001b[A******Val epoch 1 eval metrics: loss 1.36423758, f1 0.6024 prec 0.6165 rec 0.6169, acc 0.6169\n",
      "Epoch 1: 100%|██████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.370]\n",
      "                                                                                                                       \u001b[A******Train epoch 1 eval metrics: loss 1.37448338, f1 0.6195 prec 0.6560 rec 0.6488, acc 0.6488\n",
      "Epoch 1: 100%|██████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=1.1, v_num=0, train_loss=0.775, val_loss=1.370]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 510: 'val_loss' reached 1.37140 (best 1.37140), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|████████████▊   | 255/319 [00:52<00:13,  4.87it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  80%|████████████▊   | 256/319 [00:52<00:12,  4.89it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 257/319 [00:52<00:12,  4.90it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 258/319 [00:52<00:12,  4.92it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 259/319 [00:52<00:12,  4.94it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 260/319 [00:52<00:11,  4.95it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 261/319 [00:52<00:11,  4.97it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.99it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.00it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.02it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.04it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.05it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.07it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.09it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.10it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.11it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.13it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.14it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.15it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 274/319 [00:53<00:08,  5.17it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.18it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.19it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.21it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.22it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.23it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 280/319 [00:53<00:07,  5.24it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 281/319 [00:53<00:07,  5.26it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.27it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.28it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.29it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.31it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.32it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 287/319 [00:53<00:06,  5.33it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.34it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 290/319 [00:54<00:05,  5.37it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.38it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.39it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.40it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.43it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.44it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.45it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.47it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.48it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 300/319 [00:54<00:03,  5.49it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 301/319 [00:54<00:03,  5.50it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 302/319 [00:54<00:03,  5.51it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 304/319 [00:54<00:02,  5.54it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 305/319 [00:54<00:02,  5.55it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 306/319 [00:55<00:02,  5.56it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  96%|███████████████▍| 307/319 [00:55<00:02,  5.58it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 310/319 [00:55<00:01,  5.61it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 312/319 [00:55<00:01,  5.63it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 314/319 [00:55<00:00,  5.66it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 315/319 [00:55<00:00,  5.67it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 316/319 [00:55<00:00,  5.68it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2:  99%|███████████████▉| 317/319 [00:55<00:00,  5.69it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2: 100%|███████████████▉| 318/319 [00:55<00:00,  5.70it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.370]\u001b[A******Val epoch 2 eval metrics: loss 1.28783659, f1 0.6324 prec 0.6467 rec 0.6405, acc 0.6405\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.290]\n",
      "                                                                                                                       \u001b[A******Train epoch 2 eval metrics: loss 0.89994684, f1 0.7552 prec 0.7758 rec 0.7706, acc 0.7706\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.887, v_num=0, train_loss=0.546, val_loss=1.290]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 765: 'val_loss' reached 1.29498 (best 1.29498), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████████▊   | 255/319 [00:52<00:13,  4.88it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  80%|████████████▊   | 256/319 [00:52<00:12,  4.89it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 257/319 [00:52<00:12,  4.91it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 258/319 [00:52<00:12,  4.93it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 259/319 [00:52<00:12,  4.94it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 260/319 [00:52<00:11,  4.96it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 261/319 [00:52<00:11,  4.98it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.99it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.01it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.03it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.04it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.06it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.08it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.09it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.11it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.12it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.13it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.15it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.16it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 274/319 [00:52<00:08,  5.17it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.19it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.20it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.21it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.22it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.24it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 280/319 [00:53<00:07,  5.25it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 281/319 [00:53<00:07,  5.26it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.27it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.29it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.31it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.32it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 287/319 [00:53<00:05,  5.34it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.35it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 290/319 [00:53<00:05,  5.38it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.39it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.40it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.41it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.44it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.45it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.46it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.47it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.48it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 300/319 [00:54<00:03,  5.50it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 302/319 [00:54<00:03,  5.52it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 304/319 [00:54<00:02,  5.55it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 305/319 [00:54<00:02,  5.56it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  96%|███████████████▍| 307/319 [00:55<00:02,  5.58it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 310/319 [00:55<00:01,  5.62it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 311/319 [00:55<00:01,  5.63it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 314/319 [00:55<00:00,  5.66it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 315/319 [00:55<00:00,  5.68it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 316/319 [00:55<00:00,  5.69it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3:  99%|███████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3: 100%|███████████████▉| 318/319 [00:55<00:00,  5.71it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.290]\u001b[A******Val epoch 3 eval metrics: loss 1.27409437, f1 0.6357 prec 0.6483 rec 0.6424, acc 0.6424\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.280]\n",
      "                                                                                                                       \u001b[A******Train epoch 3 eval metrics: loss 0.62898470, f1 0.8391 prec 0.8474 rec 0.8463, acc 0.8463\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.639, v_num=0, train_loss=0.267, val_loss=1.280]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1020: 'val_loss' reached 1.28121 (best 1.28121), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  80%|████████████▊   | 255/319 [00:52<00:13,  4.88it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  80%|████████████▊   | 256/319 [00:52<00:12,  4.90it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|████████████▉   | 257/319 [00:52<00:12,  4.91it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|████████████▉   | 258/319 [00:52<00:12,  4.93it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|████████████▉   | 259/319 [00:52<00:12,  4.95it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████   | 260/319 [00:52<00:11,  4.96it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████   | 261/319 [00:52<00:11,  4.98it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▏  | 262/319 [00:52<00:11,  5.00it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.01it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.03it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.05it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.06it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.08it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.10it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.11it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.13it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.14it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.15it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.16it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|█████████████▋  | 274/319 [00:52<00:08,  5.18it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|█████████████▊  | 275/319 [00:52<00:08,  5.19it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.20it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.22it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.23it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.24it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|██████████████  | 280/319 [00:53<00:07,  5.25it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|██████████████  | 281/319 [00:53<00:07,  5.27it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.28it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.29it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.32it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.33it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|██████████████▍ | 287/319 [00:53<00:05,  5.34it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.35it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.37it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|██████████████▌ | 290/319 [00:53<00:05,  5.38it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|██████████████▌ | 291/319 [00:53<00:05,  5.39it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|██████████████▋ | 292/319 [00:54<00:04,  5.40it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.42it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.43it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.44it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.45it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.47it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.48it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.49it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|███████████████ | 300/319 [00:54<00:03,  5.50it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|███████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|███████████████▏| 302/319 [00:54<00:03,  5.53it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|███████████████▏| 303/319 [00:54<00:02,  5.54it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|███████████████▏| 304/319 [00:54<00:02,  5.55it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|███████████████▎| 305/319 [00:54<00:02,  5.56it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|███████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|███████████████▍| 307/319 [00:54<00:02,  5.59it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|███████████████▍| 308/319 [00:55<00:01,  5.60it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|███████████████▍| 309/319 [00:55<00:01,  5.61it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|███████████████▌| 310/319 [00:55<00:01,  5.62it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|███████████████▌| 311/319 [00:55<00:01,  5.63it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|███████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|███████████████▋| 313/319 [00:55<00:01,  5.66it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|███████████████▋| 314/319 [00:55<00:00,  5.67it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|███████████████▊| 315/319 [00:55<00:00,  5.68it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|███████████████▊| 316/319 [00:55<00:00,  5.69it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|███████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4: 100%|███████████████▉| 318/319 [00:55<00:00,  5.72it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A\n",
      "Epoch 4: 100%|████████████████| 319/319 [00:55<00:00,  5.73it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.280]\u001b[A******Val epoch 4 eval metrics: loss 1.28887820, f1 0.6354 prec 0.6448 rec 0.6424, acc 0.6424\n",
      "Epoch 4: 100%|████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.300]\n",
      "                                                                                                                       \u001b[A******Train epoch 4 eval metrics: loss 0.46859097, f1 0.8956 prec 0.8995 rec 0.8983, acc 0.8983\n",
      "Epoch 4: 100%|████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.300]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1275: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.349, v_num=0, train_loss=0.175, val_loss=1.300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\Specter_Default_06_02_2023_01_41_run4\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Benjamin Aw\\Desktop\\ACL_Anthology_Exploratory\\Experiments\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | specter    | BertModel           | 109 M \n",
      "1 | classifier | Linear              | 16.1 K\n",
      "2 | criterion  | CrossEntropyLoss    | 0     \n",
      "3 | acc        | MulticlassAccuracy  | 0     \n",
      "4 | f1         | MulticlassF1Score   | 0     \n",
      "5 | prec       | MulticlassPrecision | 0     \n",
      "6 | rec        | MulticlassRecall    | 0     \n",
      "---------------------------------------------------\n",
      "85.7 M    Trainable params\n",
      "24.3 M    Non-trainable params\n",
      "109 M     Total params\n",
      "439.818   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45.45it/s]******Val epoch 0 eval metrics: loss 2.98585367, f1 0.0000 prec 0.0000 rec 0.0000, acc 0.0000\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|██████████████████████████▍      | 255/319 [00:52<00:13,  4.90it/s, loss=1.85, v_num=0, train_loss=1.760]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████▍      | 256/319 [00:52<00:12,  4.91it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▌      | 257/319 [00:52<00:12,  4.93it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▋      | 258/319 [00:52<00:12,  4.95it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████▊      | 259/319 [00:52<00:12,  4.96it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  82%|██████████████████████████▉      | 260/319 [00:52<00:11,  4.98it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 261/319 [00:52<00:11,  5.00it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████      | 262/319 [00:52<00:11,  5.01it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████▏     | 263/319 [00:52<00:11,  5.03it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▎     | 264/319 [00:52<00:10,  5.05it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▍     | 265/319 [00:52<00:10,  5.07it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  83%|███████████████████████████▌     | 266/319 [00:52<00:10,  5.08it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▌     | 267/319 [00:52<00:10,  5.10it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▋     | 268/319 [00:52<00:09,  5.12it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  84%|███████████████████████████▊     | 269/319 [00:52<00:09,  5.13it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  85%|███████████████████████████▉     | 270/319 [00:52<00:09,  5.14it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████     | 271/319 [00:52<00:09,  5.16it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  85%|████████████████████████████▏    | 272/319 [00:52<00:09,  5.17it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▏    | 273/319 [00:52<00:08,  5.18it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▎    | 274/319 [00:52<00:08,  5.20it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  86%|████████████████████████████▍    | 275/319 [00:52<00:08,  5.21it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▌    | 276/319 [00:52<00:08,  5.22it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▋    | 277/319 [00:52<00:08,  5.23it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 278/319 [00:52<00:07,  5.25it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  87%|████████████████████████████▊    | 279/319 [00:53<00:07,  5.26it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  88%|████████████████████████████▉    | 280/319 [00:53<00:07,  5.27it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████    | 281/319 [00:53<00:07,  5.29it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  88%|█████████████████████████████▏   | 282/319 [00:53<00:06,  5.30it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▎   | 283/319 [00:53<00:06,  5.31it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 284/319 [00:53<00:06,  5.32it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  89%|█████████████████████████████▍   | 285/319 [00:53<00:06,  5.34it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▌   | 286/319 [00:53<00:06,  5.35it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▋   | 287/319 [00:53<00:05,  5.36it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████▊   | 288/319 [00:53<00:05,  5.37it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  91%|█████████████████████████████▉   | 289/319 [00:53<00:05,  5.39it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 290/319 [00:53<00:05,  5.40it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  91%|██████████████████████████████   | 291/319 [00:53<00:05,  5.41it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▏  | 292/319 [00:53<00:04,  5.42it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▎  | 293/319 [00:53<00:04,  5.44it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▍  | 294/319 [00:53<00:04,  5.45it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████▌  | 295/319 [00:54<00:04,  5.46it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▌  | 296/319 [00:54<00:04,  5.47it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▋  | 297/319 [00:54<00:04,  5.48it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████▊  | 298/319 [00:54<00:03,  5.50it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  94%|██████████████████████████████▉  | 299/319 [00:54<00:03,  5.51it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████  | 300/319 [00:54<00:03,  5.52it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████▏ | 301/319 [00:54<00:03,  5.53it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▏ | 302/319 [00:54<00:03,  5.55it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▎ | 303/319 [00:54<00:02,  5.56it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  95%|███████████████████████████████▍ | 304/319 [00:54<00:02,  5.57it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▌ | 305/319 [00:54<00:02,  5.58it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▋ | 306/319 [00:54<00:02,  5.59it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  96%|███████████████████████████████▊ | 307/319 [00:54<00:02,  5.60it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▊ | 308/319 [00:54<00:01,  5.62it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████▉ | 309/319 [00:54<00:01,  5.63it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████ | 310/319 [00:54<00:01,  5.64it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████▏| 311/319 [00:55<00:01,  5.65it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▎| 312/319 [00:55<00:01,  5.66it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 313/319 [00:55<00:01,  5.68it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  98%|████████████████████████████████▍| 314/319 [00:55<00:00,  5.69it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▌| 315/319 [00:55<00:00,  5.70it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▋| 316/319 [00:55<00:00,  5.71it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████▊| 317/319 [00:55<00:00,  5.72it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████▉| 318/319 [00:55<00:00,  5.74it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████████| 319/319 [00:55<00:00,  5.75it/s, loss=1.85, v_num=0, train_loss=1.760]\u001b[A******Val epoch 0 eval metrics: loss 1.71182464, f1 0.5045 prec 0.5298 rec 0.5580, acc 0.5580\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.69it/s, loss=1.85, v_num=0, train_loss=1.760, val_loss=1.720]\n",
      "                                                                                                                       \u001b[A******Train epoch 0 eval metrics: loss 2.51949745, f1 0.2783 prec 0.3595 rec 0.3134, acc 0.3134\n",
      "Epoch 0: 100%|█████████████████| 319/319 [00:56<00:00,  5.69it/s, loss=1.85, v_num=0, train_loss=1.760, val_loss=1.720]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 255: 'val_loss' reached 1.71933 (best 1.71933), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run4.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.88it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.89it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.91it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.93it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.94it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.96it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 261/319 [00:52<00:11,  4.98it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  82%|█████████████▉   | 262/319 [00:52<00:11,  4.99it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  82%|██████████████   | 263/319 [00:52<00:11,  5.01it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 264/319 [00:52<00:10,  5.03it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  83%|██████████████   | 265/319 [00:52<00:10,  5.04it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.06it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.08it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 268/319 [00:52<00:10,  5.09it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.11it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.12it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.14it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.15it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 273/319 [00:52<00:08,  5.16it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  86%|██████████████▌  | 274/319 [00:52<00:08,  5.17it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  86%|██████████████▋  | 275/319 [00:53<00:08,  5.19it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  87%|██████████████▋  | 276/319 [00:53<00:08,  5.20it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 277/319 [00:53<00:08,  5.21it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 278/319 [00:53<00:07,  5.23it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  87%|██████████████▊  | 279/319 [00:53<00:07,  5.24it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.25it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.26it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  88%|███████████████  | 282/319 [00:53<00:07,  5.28it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  89%|███████████████  | 283/319 [00:53<00:06,  5.29it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.31it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.33it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 287/319 [00:53<00:05,  5.34it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.35it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  91%|███████████████▍ | 290/319 [00:53<00:05,  5.38it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  91%|███████████████▌ | 291/319 [00:54<00:05,  5.39it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 292/319 [00:54<00:04,  5.40it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  92%|███████████████▌ | 293/319 [00:54<00:04,  5.41it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  92%|███████████████▋ | 295/319 [00:54<00:04,  5.44it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.45it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.46it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.47it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.49it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.50it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  94%|████████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  95%|████████████████ | 302/319 [00:54<00:03,  5.52it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  95%|████████████████▏| 304/319 [00:54<00:02,  5.55it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 305/319 [00:54<00:02,  5.56it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  96%|████████████████▎| 307/319 [00:54<00:02,  5.58it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  97%|████████████████▍| 309/319 [00:55<00:01,  5.61it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 310/319 [00:55<00:01,  5.62it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  97%|████████████████▌| 311/319 [00:55<00:01,  5.63it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  98%|████████████████▋| 314/319 [00:55<00:00,  5.67it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 315/319 [00:55<00:00,  5.68it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  99%|████████████████▊| 316/319 [00:55<00:00,  5.69it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1:  99%|████████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1: 100%|████████████████▉| 318/319 [00:55<00:00,  5.71it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.720]\u001b[A******Val epoch 1 eval metrics: loss 1.32331692, f1 0.6221 prec 0.6393 rec 0.6444, acc 0.6444\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.330]\n",
      "                                                                                                                       \u001b[A******Train epoch 1 eval metrics: loss 1.42477328, f1 0.5970 prec 0.6060 rec 0.6302, acc 0.6302\n",
      "Epoch 1: 100%|█████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=1.26, v_num=0, train_loss=1.290, val_loss=1.330]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 510: 'val_loss' reached 1.32757 (best 1.32757), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run4.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|████████████▊   | 255/319 [00:52<00:13,  4.87it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  80%|████████████▊   | 256/319 [00:52<00:12,  4.89it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 257/319 [00:52<00:12,  4.91it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 258/319 [00:52<00:12,  4.92it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  81%|████████████▉   | 259/319 [00:52<00:12,  4.94it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 260/319 [00:52<00:11,  4.96it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  82%|█████████████   | 261/319 [00:52<00:11,  4.97it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 262/319 [00:52<00:11,  4.99it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.01it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.02it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.04it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.06it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.07it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.09it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.11it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.12it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.13it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.14it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.16it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 274/319 [00:52<00:08,  5.17it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  86%|█████████████▊  | 275/319 [00:53<00:08,  5.18it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.20it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.21it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.22it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.23it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 280/319 [00:53<00:07,  5.25it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  88%|██████████████  | 281/319 [00:53<00:07,  5.26it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.27it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.28it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.31it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.32it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 287/319 [00:53<00:05,  5.33it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.35it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.36it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 290/319 [00:53<00:05,  5.37it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 291/319 [00:54<00:05,  5.38it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 292/319 [00:54<00:05,  5.40it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.41it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.42it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.43it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.44it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.46it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.47it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.48it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 300/319 [00:54<00:03,  5.49it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  94%|███████████████ | 301/319 [00:54<00:03,  5.50it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 302/319 [00:54<00:03,  5.52it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 303/319 [00:54<00:02,  5.53it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  95%|███████████████▏| 304/319 [00:54<00:02,  5.54it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 305/319 [00:54<00:02,  5.55it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  96%|███████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  96%|███████████████▍| 307/319 [00:55<00:02,  5.58it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 308/319 [00:55<00:01,  5.59it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  97%|███████████████▍| 309/319 [00:55<00:01,  5.60it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 310/319 [00:55<00:01,  5.61it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  97%|███████████████▌| 311/319 [00:55<00:01,  5.62it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 312/319 [00:55<00:01,  5.64it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 313/319 [00:55<00:01,  5.65it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  98%|███████████████▋| 314/319 [00:55<00:00,  5.66it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 315/319 [00:55<00:00,  5.67it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  99%|███████████████▊| 316/319 [00:55<00:00,  5.68it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2:  99%|███████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2: 100%|███████████████▉| 318/319 [00:55<00:00,  5.71it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:55<00:00,  5.72it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.330]\u001b[A******Val epoch 2 eval metrics: loss 1.25231994, f1 0.6309 prec 0.6343 rec 0.6405, acc 0.6405\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.260]\n",
      "                                                                                                                       \u001b[A******Train epoch 2 eval metrics: loss 0.92859082, f1 0.7410 prec 0.7590 rec 0.7579, acc 0.7579\n",
      "Epoch 2: 100%|████████████████| 319/319 [00:56<00:00,  5.66it/s, loss=0.975, v_num=0, train_loss=0.696, val_loss=1.260]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 765: 'val_loss' reached 1.25570 (best 1.25570), saving model to 'C:\\\\Users\\\\Benjamin Aw\\\\Desktop\\\\ACL_Anthology_Exploratory\\\\Experiments\\\\checkpoints\\\\best-checkpoint-Specter_Default_06_02_2023_01_41_run4.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████████▊   | 255/319 [00:52<00:13,  4.88it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  80%|████████████▊   | 256/319 [00:52<00:12,  4.90it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 257/319 [00:52<00:12,  4.91it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 258/319 [00:52<00:12,  4.93it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  81%|████████████▉   | 259/319 [00:52<00:12,  4.95it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 260/319 [00:52<00:11,  4.96it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 261/319 [00:52<00:11,  4.98it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 262/319 [00:52<00:11,  5.00it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  82%|█████████████▏  | 263/319 [00:52<00:11,  5.01it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  83%|█████████████▏  | 264/319 [00:52<00:10,  5.03it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 265/319 [00:52<00:10,  5.05it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  83%|█████████████▎  | 266/319 [00:52<00:10,  5.06it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 267/319 [00:52<00:10,  5.08it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 268/319 [00:52<00:10,  5.10it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  84%|█████████████▍  | 269/319 [00:52<00:09,  5.11it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 270/319 [00:52<00:09,  5.13it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  85%|█████████████▌  | 271/319 [00:52<00:09,  5.14it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  85%|█████████████▋  | 272/319 [00:52<00:09,  5.15it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 273/319 [00:52<00:08,  5.17it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 274/319 [00:52<00:08,  5.18it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  86%|█████████████▊  | 275/319 [00:52<00:08,  5.19it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  87%|█████████████▊  | 276/319 [00:53<00:08,  5.20it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 277/319 [00:53<00:08,  5.22it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 278/319 [00:53<00:07,  5.23it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  87%|█████████████▉  | 279/319 [00:53<00:07,  5.24it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 280/319 [00:53<00:07,  5.25it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  88%|██████████████  | 281/319 [00:53<00:07,  5.27it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  88%|██████████████▏ | 282/319 [00:53<00:07,  5.28it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 283/319 [00:53<00:06,  5.29it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  89%|██████████████▏ | 284/319 [00:53<00:06,  5.30it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  89%|██████████████▎ | 285/319 [00:53<00:06,  5.32it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  90%|██████████████▎ | 286/319 [00:53<00:06,  5.33it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 287/319 [00:53<00:05,  5.34it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  90%|██████████████▍ | 288/319 [00:53<00:05,  5.35it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  91%|██████████████▍ | 289/319 [00:53<00:05,  5.37it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 290/319 [00:53<00:05,  5.38it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  91%|██████████████▌ | 291/319 [00:53<00:05,  5.39it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 292/319 [00:54<00:04,  5.40it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 293/319 [00:54<00:04,  5.42it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  92%|██████████████▋ | 294/319 [00:54<00:04,  5.43it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  92%|██████████████▊ | 295/319 [00:54<00:04,  5.44it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  93%|██████████████▊ | 296/319 [00:54<00:04,  5.45it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 297/319 [00:54<00:04,  5.47it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  93%|██████████████▉ | 298/319 [00:54<00:03,  5.48it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  94%|██████████████▉ | 299/319 [00:54<00:03,  5.49it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 300/319 [00:54<00:03,  5.50it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  94%|███████████████ | 301/319 [00:54<00:03,  5.51it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 302/319 [00:54<00:03,  5.53it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 303/319 [00:54<00:02,  5.54it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  95%|███████████████▏| 304/319 [00:54<00:02,  5.55it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 305/319 [00:54<00:02,  5.56it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  96%|███████████████▎| 306/319 [00:54<00:02,  5.57it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  96%|███████████████▍| 307/319 [00:54<00:02,  5.59it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 308/319 [00:55<00:01,  5.60it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  97%|███████████████▍| 309/319 [00:55<00:01,  5.61it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 310/319 [00:55<00:01,  5.62it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  97%|███████████████▌| 311/319 [00:55<00:01,  5.63it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 312/319 [00:55<00:01,  5.65it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 313/319 [00:55<00:01,  5.66it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  98%|███████████████▋| 314/319 [00:55<00:00,  5.67it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 315/319 [00:55<00:00,  5.68it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  99%|███████████████▊| 316/319 [00:55<00:00,  5.69it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3:  99%|███████████████▉| 317/319 [00:55<00:00,  5.70it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3: 100%|███████████████▉| 318/319 [00:55<00:00,  5.72it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:55<00:00,  5.73it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.260]\u001b[A******Val epoch 3 eval metrics: loss 1.27332969, f1 0.6265 prec 0.6298 rec 0.6385, acc 0.6385\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.280]\n",
      "                                                                                                                       \u001b[A******Train epoch 3 eval metrics: loss 0.64182096, f1 0.8443 prec 0.8513 rec 0.8502, acc 0.8502\n",
      "Epoch 3: 100%|████████████████| 319/319 [00:56<00:00,  5.67it/s, loss=0.492, v_num=0, train_loss=0.252, val_loss=1.280]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1020: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  80%|█████████████▌   | 255/319 [00:52<00:13,  4.90it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  80%|█████████████▋   | 256/319 [00:52<00:12,  4.92it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|█████████████▋   | 257/319 [00:52<00:12,  4.93it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|█████████████▋   | 258/319 [00:52<00:12,  4.95it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  81%|█████████████▊   | 259/319 [00:52<00:12,  4.97it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▊   | 260/319 [00:52<00:11,  4.99it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▉   | 261/319 [00:52<00:11,  5.00it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|█████████████▉   | 262/319 [00:52<00:11,  5.02it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  82%|██████████████   | 263/319 [00:52<00:11,  5.04it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|██████████████   | 264/319 [00:52<00:10,  5.05it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|██████████████   | 265/319 [00:52<00:10,  5.07it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  83%|██████████████▏  | 266/319 [00:52<00:10,  5.09it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|██████████████▏  | 267/319 [00:52<00:10,  5.10it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|██████████████▎  | 268/319 [00:52<00:09,  5.12it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  84%|██████████████▎  | 269/319 [00:52<00:09,  5.14it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 270/319 [00:52<00:09,  5.15it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 271/319 [00:52<00:09,  5.16it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  85%|██████████████▍  | 272/319 [00:52<00:09,  5.18it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|██████████████▌  | 273/319 [00:52<00:08,  5.19it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|██████████████▌  | 274/319 [00:52<00:08,  5.20it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  86%|██████████████▋  | 275/319 [00:52<00:08,  5.21it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▋  | 276/319 [00:52<00:08,  5.23it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 277/319 [00:52<00:08,  5.24it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 278/319 [00:52<00:07,  5.25it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  87%|██████████████▊  | 279/319 [00:52<00:07,  5.26it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|██████████████▉  | 280/319 [00:53<00:07,  5.28it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|██████████████▉  | 281/319 [00:53<00:07,  5.29it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  88%|███████████████  | 282/319 [00:53<00:06,  5.30it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|███████████████  | 283/319 [00:53<00:06,  5.32it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|███████████████▏ | 284/319 [00:53<00:06,  5.33it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  89%|███████████████▏ | 285/319 [00:53<00:06,  5.34it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|███████████████▏ | 286/319 [00:53<00:06,  5.35it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|███████████████▎ | 287/319 [00:53<00:05,  5.37it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  90%|███████████████▎ | 288/319 [00:53<00:05,  5.38it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|███████████████▍ | 289/319 [00:53<00:05,  5.39it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|███████████████▍ | 290/319 [00:53<00:05,  5.40it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  91%|███████████████▌ | 291/319 [00:53<00:05,  5.42it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▌ | 292/319 [00:53<00:04,  5.43it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▌ | 293/319 [00:53<00:04,  5.44it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▋ | 294/319 [00:53<00:04,  5.45it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  92%|███████████████▋ | 295/319 [00:53<00:04,  5.47it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|███████████████▊ | 296/319 [00:54<00:04,  5.48it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|███████████████▊ | 297/319 [00:54<00:04,  5.49it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  93%|███████████████▉ | 298/319 [00:54<00:03,  5.50it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|███████████████▉ | 299/319 [00:54<00:03,  5.51it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|███████████████▉ | 300/319 [00:54<00:03,  5.53it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  94%|████████████████ | 301/319 [00:54<00:03,  5.54it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|████████████████ | 302/319 [00:54<00:03,  5.55it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|████████████████▏| 303/319 [00:54<00:02,  5.56it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  95%|████████████████▏| 304/319 [00:54<00:02,  5.57it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 305/319 [00:54<00:02,  5.59it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 306/319 [00:54<00:02,  5.60it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  96%|████████████████▎| 307/319 [00:54<00:02,  5.61it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▍| 308/319 [00:54<00:01,  5.62it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▍| 309/319 [00:54<00:01,  5.63it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▌| 310/319 [00:54<00:01,  5.65it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  97%|████████████████▌| 311/319 [00:54<00:01,  5.66it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 312/319 [00:55<00:01,  5.67it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 313/319 [00:55<00:01,  5.68it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  98%|████████████████▋| 314/319 [00:55<00:00,  5.69it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|████████████████▊| 315/319 [00:55<00:00,  5.70it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|████████████████▊| 316/319 [00:55<00:00,  5.72it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4:  99%|████████████████▉| 317/319 [00:55<00:00,  5.73it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4: 100%|████████████████▉| 318/319 [00:55<00:00,  5.74it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:55<00:00,  5.75it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.280]\u001b[A******Val epoch 4 eval metrics: loss 1.26998821, f1 0.6358 prec 0.6435 rec 0.6424, acc 0.6424\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:55<00:00,  5.70it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.270]\n",
      "                                                                                                                       \u001b[A******Train epoch 4 eval metrics: loss 0.48437679, f1 0.8868 prec 0.8919 rec 0.8900, acc 0.8900\n",
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.69it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.270]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1275: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████| 319/319 [00:56<00:00,  5.69it/s, loss=0.53, v_num=0, train_loss=0.290, val_loss=1.270]\n"
     ]
    }
   ],
   "source": [
    "result_acc_lst = []\n",
    "result_f1_lst = []\n",
    "result_prec_lst = []\n",
    "result_recall_lst = []\n",
    "nums_fold = 5\n",
    "split_seed = 123\n",
    "\n",
    "for k in range(nums_fold):\n",
    "    data_module = SpecterDataModule(df, tokenizer, k = k)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath = \"checkpoints\",\n",
    "        filename = f\"best-checkpoint-Specter_{experiment}_{time_now}_run{k}\",\n",
    "        save_top_k = 1,\n",
    "        verbose = True,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = \"min\"\n",
    "    )\n",
    "\n",
    "    # here we train the model on given split...\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name = f\"Specter_{experiment}_{time_now}_run{k}\")\n",
    "    model = SpecterClassModel(n_classes = 21, n_warmup_steps = warmup_steps, n_training_steps = total_training_steps)\n",
    "    trainer = pl.Trainer(logger = logger, callbacks = [checkpoint_callback], max_epochs = N_EPOCHS, accelerator = \"auto\")#, enable_progress_bar = False)\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    accuracy = trainer.callback_metrics['Ignore/acc'].item()\n",
    "    f1 = trainer.callback_metrics['Ignore/f1'].item()\n",
    "    precision = trainer.callback_metrics['Ignore/prec'].item()\n",
    "    recall = trainer.callback_metrics['Ignore/rec'].item()\n",
    "    \n",
    "    result_acc_lst.append(accuracy)\n",
    "    result_f1_lst.append(f1)\n",
    "    result_prec_lst.append(precision)\n",
    "    result_recall_lst.append(recall)\n",
    "\n",
    "average_val_acc_score = sum(result_acc_lst) / len(result_acc_lst)\n",
    "average_val_f1_score = sum(result_f1_lst) / len(result_f1_lst)\n",
    "average_val_prec_score = sum(result_prec_lst) / len(result_prec_lst)\n",
    "average_val_recall_score = sum(result_recall_lst) / len(result_recall_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of the validation set across 5 folds is: 0.6636542201042175\n",
      "The average F1 score of the validation set across 5 folds is: 0.653399920463562\n",
      "The average precision of the validation set across 5 folds is: 0.6579187512397766\n",
      "The average recall of the validation set across 5 folds is: 0.6636542201042175\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average accuracy of the validation set across 5 folds is: {average_val_acc_score}\")\n",
    "print(f\"The average F1 score of the validation set across 5 folds is: {average_val_f1_score}\")\n",
    "print(f\"The average precision of the validation set across 5 folds is: {average_val_prec_score}\")\n",
    "print(f\"The average recall of the validation set across 5 folds is: {average_val_recall_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop here for now We might want to take a look at creating a test set "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b110531817a2bf0b8e1fa59ad47ae74cf2e8602bdaf1b8cf6b96ebd8cf78ed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
