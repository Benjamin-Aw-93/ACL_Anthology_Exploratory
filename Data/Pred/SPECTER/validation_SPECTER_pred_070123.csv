string,Labels,Full Prediction,Max Score,Max Label,Actual Label Score,Label outcome
"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed singledimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices. 5 We note that conceptualizations of AAE and the accompanying terminology for the variety have shifted considerably in the last half century; see King (2020) for an overview. 6 https://bit.ly/2Yv07IL 7 https://bit.ly/3j2weZA",Ethics and NLP,"[0.039675138890743256, 0.2569012939929962, 0.0036316404584795237, 0.023004336282610893, 0.21160487830638885, 0.005376402288675308, 0.006676471326500177, 0.010488904081285, 0.08012101799249649, 0.01104767806828022, 0.01629440113902092, 0.027544738724827766, 0.04595593363046646, 0.008300005458295345, 0.13528349995613098, 0.03310057148337364, 0.009454886429011822, 0.028731580823659897, 0.01151443924754858, 0.004145446699112654, 0.008967533707618713, 0.011305969208478928, 0.01087335217744112]",0.2569012939929962,Computational Social Science and Social Media,0.21160487830638885,False
"This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English â†” (German, Romanian, Estonian, Finnish, Hungarian).",Machine Translation and Multilinguality,"[0.004381393548101187, 0.002230766462162137, 0.00596859073266387, 0.0016560598742216825, 0.006131765432655811, 0.025415243580937386, 0.005266612395644188, 0.010766778141260147, 0.012721790000796318, 0.002169858431443572, 0.058040812611579895, 0.7333033084869385, 0.029198938980698586, 0.0025858310982584953, 0.016413414850831032, 0.00404688436537981, 0.006253178231418133, 0.004720904864370823, 0.011534268967807293, 0.016136707738041878, 0.031193874776363373, 0.0069052064791321754, 0.0029576849192380905]",0.7333033084869385,Machine Translation and Multilinguality,0.7333033084869385,True
"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.02414289489388466, 0.015128253027796745, 0.025727953761816025, 0.012593305669724941, 0.017686661332845688, 0.012144242413341999, 0.005132615100592375, 0.008941900916397572, 0.06033440679311752, 0.08971154689788818, 0.007419931702315807, 0.04275330901145935, 0.020660297945141792, 0.004252851940691471, 0.03460909426212311, 0.006934857461601496, 0.008133967407047749, 0.00838245265185833, 0.5462009906768799, 0.024482015520334244, 0.005716722924262285, 0.013804704882204533, 0.005105077289044857]",0.5462009906768799,Speech and Multimodality,0.02414289489388466,False
"The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user's request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST). This information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user. We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains. We propose a recursive, hierarchical frame-based representation and show how to learn it from data. We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template. We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end. We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.",Dialogue and Interactive Systems,"[0.0010094426106661558, 0.001824130886234343, 0.9408825039863586, 0.00610873568803072, 0.0008698254823684692, 0.004670436028391123, 0.0011366610415279865, 0.0013366846833378077, 0.000832724676001817, 0.001894820830784738, 0.006696572992950678, 0.0010607072617858648, 0.001762081403285265, 0.003996540792286396, 0.005430108867585659, 0.0008990968926809728, 0.004758854396641254, 0.003997450228780508, 0.004321099258959293, 0.0015167364617809653, 0.0026553072966635227, 0.0010447152890264988, 0.0012946075294166803]",0.9408825039863586,Dialogue and Interactive Systems,0.9408825039863586,True
"The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies. From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas. From the language technologies perspective, we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-toend systems that cover sociolinguistic aspects of C-S as well. Our survey will be a step towards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S.",Computational Social Science and Social Media,"[0.026317263022065163, 0.05040346458554268, 0.004399960860610008, 0.005344812758266926, 0.03705862909555435, 0.012677686288952827, 0.01017903070896864, 0.015416056849062443, 0.04624274745583534, 0.009585335850715637, 0.02605530247092247, 0.21527844667434692, 0.04582574963569641, 0.004762251395732164, 0.31746906042099, 0.02078099176287651, 0.0267799012362957, 0.041438616812229156, 0.01183062233030796, 0.005966111086308956, 0.03918974846601486, 0.01914704404771328, 0.007851220667362213]",0.31746906042099,Resources and Evaluation,0.05040346458554268,False
"Data augmentation is an effective way to improve the performance of many neural text generation models. However, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples. In this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions. Our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee. Experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods.",Machine Learning for NLP,"[0.0055778371170163155, 0.0021472754888236523, 0.008169364184141159, 0.0022849105298519135, 0.006025589536875486, 0.4864598512649536, 0.005900020711123943, 0.032521069049835205, 0.01402734313160181, 0.0023421160876750946, 0.3527010977268219, 0.00705658970400691, 0.004892938304692507, 0.0061987051740288734, 0.014388367533683777, 0.002880952088162303, 0.016775095835328102, 0.004520267713814974, 0.0036072484217584133, 0.009252587333321571, 0.005934570915997028, 0.0036702058278024197, 0.002665966283529997]",0.4864598512649536,Generation,0.3527010977268219,False
"Recent pretrained vision-language models have achieved impressive performance on cross-modal retrieval tasks in English. Their success, however, heavily depends on the availability of many annotated image-caption datasets for pretraining, where the texts are not necessarily in English. Although we can utilize machine translation (MT) tools to translate non-English text to English, the performance still largely relies on MT's quality and may suffer from high latency problems in realworld applications. This paper proposes a new approach to learn cross-lingual cross-modal representations for matching images and their relevant captions in multiple languages. We seamlessly combine cross-lingual pretraining objectives and cross-modal pretraining objectives in a unified framework to learn image and text in a joint embedding space from available English image-caption data, monolingual and parallel corpus. We show that our approach achieves SOTA performance in retrieval tasks on two multimodal multilingual image caption benchmarks: Multi30k with German captions and MSCOCO with Japanese captions.",Machine Translation and Multilinguality,"[0.004159824922680855, 0.010710133239626884, 0.01654686965048313, 0.004689350724220276, 0.017566390335559845, 0.0199188943952322, 0.023021655157208443, 0.028949983417987823, 0.01633482053875923, 0.46452653408050537, 0.01014623325318098, 0.03258481249213219, 0.007534094154834747, 0.009036792442202568, 0.09900852292776108, 0.00584055669605732, 0.007374447304755449, 0.021989043802022934, 0.1550559550523758, 0.014842438511550426, 0.010519847273826599, 0.01307370513677597, 0.006569028832018375]",0.46452653408050537,"Language Grounding to Vision, Robotics and Beyond",0.03258481249213219,False
"Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech. 1",Ethics and NLP,"[0.028350012376904488, 0.5056862831115723, 0.004195900168269873, 0.02528328262269497, 0.16470439732074738, 0.004654914140701294, 0.0025790808722376823, 0.005030733998864889, 0.03007793240249157, 0.007297256030142307, 0.014086122624576092, 0.00945203471928835, 0.01952465996146202, 0.0070711481384932995, 0.09565778076648712, 0.012987233698368073, 0.007586183492094278, 0.028418585658073425, 0.00810257438570261, 0.002849563956260681, 0.004700805526226759, 0.004660137463361025, 0.007043456193059683]",0.5056862831115723,Computational Social Science and Social Media,0.16470439732074738,False
"Generative feature matching network (GFMN) is an approach for training implicit generative models for images by performing moment matching on features from pre-trained neural networks. In this paper, we present new GFMN formulations that are effective for sequential data. Our experimental results show the effectiveness of the proposed method, Se-qGFMN, for three distinct generation tasks in English: unconditional text generation, classconditional text generation, and unsupervised text style transfer. SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.",Generation,"[0.004114796407520771, 0.002298884792253375, 0.019856061786413193, 0.0029568527825176716, 0.003960509784519672, 0.7927462458610535, 0.0022115791216492653, 0.006630563177168369, 0.005807196721434593, 0.013357068412005901, 0.06490029394626617, 0.00479686725884676, 0.004590864293277264, 0.0035644962918013334, 0.01799994334578514, 0.0017812803853303194, 0.013002289459109306, 0.004543163813650608, 0.010327809490263462, 0.009581427089869976, 0.004607557784765959, 0.004212482366710901, 0.002151768421754241]",0.7927462458610535,Generation,0.7927462458610535,True
"An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the codeswitching data. Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.",Speech and Multimodality,"[0.0037620426155626774, 0.005788720678538084, 0.05727284774184227, 0.0066453320905566216, 0.006836549378931522, 0.009939818643033504, 0.005945384036749601, 0.007100400049239397, 0.010144035331904888, 0.021366052329540253, 0.011000094003975391, 0.042086679488420486, 0.0067700594663619995, 0.0021327869035303593, 0.011962099000811577, 0.002472307300195098, 0.005398748442530632, 0.00724803889170289, 0.7352752685546875, 0.027137145400047302, 0.00379010452888906, 0.006726717576384544, 0.0031986909452825785]",0.7352752685546875,Speech and Multimodality,0.7352752685546875,True
"Court's view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes. In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders. The attentional encoder leverages the plaintiff's claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized. The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgmentdiscriminative court's views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model. Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.",Generation,"[0.01036657765507698, 0.16906414926052094, 0.033057473599910736, 0.0608014240860939, 0.03358644247055054, 0.07564519345760345, 0.02024812065064907, 0.01947375014424324, 0.02022065594792366, 0.00804209429770708, 0.05123554915189743, 0.007771467790007591, 0.024046901613473892, 0.01411268301308155, 0.12575814127922058, 0.017089154571294785, 0.1155632734298706, 0.12365524470806122, 0.01821850799024105, 0.01826442778110504, 0.018861934542655945, 0.006078808568418026, 0.008837966248393059]",0.16906414926052094,Computational Social Science and Social Media,0.07564519345760345,False
"Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose TEXT2EVENT, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",Information Extraction,"[0.000565393187571317, 0.0012624613009393215, 0.001000901567749679, 0.0011684318305924535, 0.0010908835101872683, 0.0023169738706201315, 0.9347339272499084, 0.012493660673499107, 0.001216252101585269, 0.0020269660744816065, 0.0030417207162827253, 0.0009609837434254587, 0.0010922858491539955, 0.002334361197426915, 0.0018546672072261572, 0.0022634512279182673, 0.008753291331231594, 0.003430707845836878, 0.0016385279595851898, 0.003590980311855674, 0.009599207900464535, 0.0018476709956303239, 0.0017162907170131803]",0.9347339272499084,Information Extraction,0.9347339272499084,True
"Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content. Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content. Our method creates not only styleindependent content representation, but also content-dependent style representation in transferring style. Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation. In addition, it is also competitive in terms of style transfer accuracy and fluency.",Generation,"[0.008780083619058132, 0.005671562626957893, 0.023870116099715233, 0.00893036276102066, 0.007511138450354338, 0.6225035190582275, 0.006190124899148941, 0.011640037409961224, 0.01129897776991129, 0.012693691067397594, 0.041862308979034424, 0.010127724148333073, 0.02036471664905548, 0.0024825080763548613, 0.04074319079518318, 0.007384580094367266, 0.009804259985685349, 0.02441171556711197, 0.029692761600017548, 0.07486723363399506, 0.005642354488372803, 0.00874309241771698, 0.004783934447914362]",0.6225035190582275,Generation,0.6225035190582275,True
"Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentencelevel and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.",Interpretability and Analysis of Models for NLP,"[0.012441049329936504, 0.004560552537441254, 0.0018849930493161082, 0.008324054069817066, 0.008585129864513874, 0.0056258863769471645, 0.0025941156782209873, 0.005849538370966911, 0.8082323670387268, 0.016886135563254356, 0.015316235832870007, 0.012882793322205544, 0.00514204939827323, 0.00442994711920619, 0.02024225704371929, 0.00718039320781827, 0.025143278762698174, 0.007192221470177174, 0.00658585736528039, 0.006122738588601351, 0.006485357880592346, 0.005059375893324614, 0.003233726369217038]",0.8082323670387268,Interpretability and Analysis of Models for NLP,0.8082323670387268,True
"Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.",Machine Translation and Multilinguality,"[0.006113904062658548, 0.010293720290064812, 0.0046384138986468315, 0.0033350682351738214, 0.017860233783721924, 0.005683848634362221, 0.0033066573087126017, 0.011981659568846226, 0.025392621755599976, 0.006379973143339157, 0.013752848841249943, 0.7237842082977295, 0.009513183496892452, 0.004599090199917555, 0.09018801152706146, 0.005988236516714096, 0.0042763156816363335, 0.01715041510760784, 0.010911006480455399, 0.004919502884149551, 0.007683385629206896, 0.007611286360770464, 0.004636403638869524]",0.7237842082977295,Machine Translation and Multilinguality,0.7237842082977295,True
"The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of mental health in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced lexical diversity as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, syntactic complexity, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, therapists, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for anxiety, an exciting prospect in a time of both increased online communication and increased mental health issues.","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.09953389316797256, 0.2462823987007141, 0.020830653607845306, 0.10815228521823883, 0.043469253927469254, 0.008821881376206875, 0.0036928479094058275, 0.006227148696780205, 0.09456957876682281, 0.024500157684087753, 0.016745230183005333, 0.008649873547255993, 0.03455663099884987, 0.026580069214105606, 0.1265372931957245, 0.009115691296756268, 0.012720814906060696, 0.044614482671022415, 0.0215699952095747, 0.012142293155193329, 0.007553477305918932, 0.011156570166349411, 0.01197750587016344]",0.2462823987007141,Computational Social Science and Social Media,0.09953389316797256,False
"The importance of parameter selection in supervised learning is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multilabel classification for medical code prediction, one influential paper conducted a proper parameter selection on a set, but when moving to a subset of frequently occurring labels, the authors used the same parameters without a separate tuning. The set of frequent labels became a popular benchmark in subsequent studies, which kept pushing the state of the art. However, we discovered that most of the results in these studies cannot surpass the approach in the original paper if a parameter tuning had been conducted at the time. Thus it is unclear how much progress the subsequent developments have actually brought. The lesson clearly indicates that without enough attention on parameter selection, the research progress in our field can be uncertain or even illusive.",Interpretability and Analysis of Models for NLP,"[0.005916309542953968, 0.00498767476528883, 0.004276977851986885, 0.0019768436904996634, 0.008199557662010193, 0.007751441560685635, 0.016471968963742256, 0.059276729822158813, 0.040567848831415176, 0.0017728466773405671, 0.7551738023757935, 0.013388149440288544, 0.0030445302836596966, 0.009777983650565147, 0.011470843106508255, 0.005059442948549986, 0.013952859677374363, 0.0073543270118534565, 0.0038864838425070047, 0.003384942887350917, 0.016111481934785843, 0.0039006962906569242, 0.0022961492650210857]",0.7551738023757935,Machine Learning for NLP,0.040567848831415176,False
"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. 1",Machine Learning for NLP,"[0.004041699226945639, 0.00229572388343513, 0.0036970805376768112, 0.0014045783318579197, 0.004499147180467844, 0.013204630464315414, 0.0022072403226047754, 0.007260704413056374, 0.0463213250041008, 0.0010180657263845205, 0.8624916076660156, 0.008023067377507687, 0.0022761733271181583, 0.0029259035363793373, 0.0067373416386544704, 0.002790501806885004, 0.016171632334589958, 0.0016625110292807221, 0.0023501296527683735, 0.00229890295304358, 0.003930872306227684, 0.001445891335606575, 0.0009451978839933872]",0.8624916076660156,Machine Learning for NLP,0.8624916076660156,True
"Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an endto-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks 1 .","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0007442837813869119, 0.005470612086355686, 0.0029412624426186085, 0.004472772125154734, 0.002220477908849716, 0.0031647193245589733, 0.00875142216682434, 0.004718776326626539, 0.005063019692897797, 0.00238464935682714, 0.0014346385141834617, 0.0021394926588982344, 0.0020762793719768524, 0.001386888325214386, 0.022661739960312843, 0.0029850744176656008, 0.0019277720712125301, 0.9088504910469055, 0.003696309868246317, 0.0029317690059542656, 0.006275112275034189, 0.0022471300326287746, 0.0014553009532392025]",0.9088504910469055,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.9088504910469055,True
"Image captioning has focused on generalizing to images drawn from the same distribution as the training set, and not to the more challenging problem of generalizing to different distributions of images. Recently, Nikolaus et al. ( 2019 ) introduced a dataset to assess compositional generalization in image captioning, where models are evaluated on their ability to describe images with unseen adjective-noun and noun-verb compositions. In this work, we investigate different methods to improve compositional generalization by planning the syntactic structure of a caption. Our experiments show that jointly modeling tokens and syntactic tags enhances generalization in both RNNand Transformer-based models, while also improving performance on standard metrics.","Language Grounding to Vision, Robotics and Beyond","[0.011171446181833744, 0.01087266393005848, 0.012905292212963104, 0.008290012367069721, 0.016378765925765038, 0.0872713252902031, 0.00624559260904789, 0.010814099572598934, 0.023303108289837837, 0.4596940279006958, 0.009490611031651497, 0.00598799716681242, 0.013161544688045979, 0.006250416394323111, 0.1882656067609787, 0.00819968432188034, 0.012053715996444225, 0.022421929985284805, 0.04167035594582558, 0.01991705596446991, 0.011921584606170654, 0.009265667758882046, 0.004447455517947674]",0.4596940279006958,"Language Grounding to Vision, Robotics and Beyond",0.4596940279006958,True
"The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigrationrelated tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users' ideology and region impact framing choices, and how a message's framing influences audience responses. We find that the more commonlyused issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.",Computational Social Science and Social Media,"[0.013640501536428928, 0.7382798790931702, 0.0016055257292464375, 0.009666039608418941, 0.0551065132021904, 0.0021846238523721695, 0.002782990923151374, 0.005126209929585457, 0.012593590654432774, 0.005024251528084278, 0.004306834656745195, 0.005168233532458544, 0.011305720545351505, 0.004465492442250252, 0.058460336178541183, 0.00811257865279913, 0.0035680627916008234, 0.03894517198204994, 0.004722287878394127, 0.00210254592821002, 0.003890081075951457, 0.004142407327890396, 0.004800114780664444]",0.7382798790931702,Computational Social Science and Social Media,0.7382798790931702,True
"The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed methods significantly outperforms previous augmentation methods on 11 targets.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.003719943342730403, 0.043621014803647995, 0.007951666601002216, 0.03509426862001419, 0.008794866502285004, 0.008029822260141373, 0.002727758139371872, 0.0063458336517214775, 0.008729438297450542, 0.0032663403544574976, 0.005835770163685083, 0.0032638695556670427, 0.009019996970891953, 0.006788719445466995, 0.081794872879982, 0.010060043074190617, 0.006848165765404701, 0.7284337878227234, 0.005505343433469534, 0.0019839033484458923, 0.005472782999277115, 0.0032228524796664715, 0.003488965565338731]",0.7284337878227234,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.7284337878227234,True
"This work explores a framework for fact verification that leverages pretrained sequence-tosequence transformer models for sentence selection and label prediction, two key sub-tasks in fact verification. Most notably, improving on previous pointwise aggregation approaches for label prediction, we take advantage of T5 using a listwise approach coupled with data augmentation. With this enhancement, we observe that our label prediction stage is more robust to noise and capable of verifying complex claims by jointly reasoning over multiple pieces of evidence. Experimental results on the FEVER task show that our system attains a FEVER score of 75.87% on the blind test set. This puts our approach atop the competitive FEVER leaderboard at the time of our work, scoring higher than the second place submission by almost two points in label accuracy and over one point in FEVER score.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0029668782372027636, 0.006000370252877474, 0.025139251723885536, 0.015054532326757908, 0.0050539434887468815, 0.027060937136411667, 0.02787584625184536, 0.020805640146136284, 0.013337102718651295, 0.002846695715561509, 0.06913335621356964, 0.00255566812120378, 0.0015387190505862236, 0.031501155346632004, 0.02910696528851986, 0.008798670955002308, 0.6505428552627563, 0.016963684931397438, 0.005226299166679382, 0.004793059080839157, 0.025333667173981667, 0.003548968583345413, 0.004815775901079178]",0.6505428552627563,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.6505428552627563,True
"Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate an entity summary description. DESC-GEN consists of 37K entity descriptions from Wikipedia and Fandom, each paired with nine evidence documents on average. The documents were collected using a combination of entity linking and hyperlinks to the Wikipedia and Fandom entity pages, which together provide high quality distant supervision. The resulting summaries are more abstractive than those found in existing datasets, and provide a better proxy for the challenge of describing new and emerging entities. We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in ROUGE-L) between state-of-the-art models and human performance, suggesting that the data will support significant future work. 1",Summarization,"[0.0027178488671779633, 0.0033876379020512104, 0.004846658557653427, 0.009523368440568447, 0.004705449566245079, 0.03572839871048927, 0.4629027545452118, 0.0745815634727478, 0.00656772730872035, 0.006386296823620796, 0.009633186273276806, 0.0061658588238060474, 0.0063757817260921, 0.004744607489556074, 0.020719509571790695, 0.0066945310682058334, 0.025347821414470673, 0.013622491620481014, 0.006629912182688713, 0.23642663657665253, 0.029983054846525192, 0.012123871594667435, 0.010184885002672672]",0.4629027545452118,Information Extraction,0.23642663657665253,False
"Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders-a sequential document encoder and a graphstructured encoder-to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are finetuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors. Input Article of New York Times: John M. Fabrizi, the mayor of Bridgeport, admitted on Tuesday that he had used cocaine and abused alcohol while in office. Mr. Fabrizi, who was appointed mayor in 2003 after the former mayor, Joseph P. Ganim, went to prison on corruption charges, said he had sought help for his drug problem about 18 months ago and that he had not used drugs since. About four months ago, he added, he stopped drinking alcohol.",Summarization,"[0.000987944658845663, 0.001219634898006916, 0.0022309396881610155, 0.005519488360732794, 0.0017362955259159207, 0.009749837219715118, 0.004314830061048269, 0.005012551322579384, 0.002119890647009015, 0.00217812554910779, 0.0021214450243860483, 0.0022613052278757095, 0.002321784384548664, 0.0006642958032898605, 0.007727665361016989, 0.0007868249085731804, 0.0019267599564045668, 0.00342184747569263, 0.004922603722661734, 0.9297134280204773, 0.0030570898670703173, 0.00329211144708097, 0.0027133047115057707]",0.9297134280204773,Summarization,0.9297134280204773,True
"Images can give us insights into the contextual meanings of words, but current imagetext grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domainspecific contexts. In contrast, unlabeled multiimage, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as ""kitchen"" and ""bedroom"", and introduce metrics to assess this document similarity. We present a simple unsupervised clusteringbased method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating ""granite"" with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.","Language Grounding to Vision, Robotics and Beyond","[0.032198216766119, 0.03640943020582199, 0.007415528409183025, 0.012394994497299194, 0.029322557151317596, 0.010444685816764832, 0.012706682085990906, 0.03315052390098572, 0.059068478643894196, 0.31889230012893677, 0.012258695438504219, 0.0066773975268006325, 0.014813190326094627, 0.012908931821584702, 0.08976077288389206, 0.16138766705989838, 0.0517713762819767, 0.03041823022067547, 0.02225431054830551, 0.003817993216216564, 0.01901121810078621, 0.016050739213824272, 0.00686611095443368]",0.31889230012893677,"Language Grounding to Vision, Robotics and Beyond",0.31889230012893677,True
"Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop SEAL-Shap, an efficient source valuation framework for quantifying the usefulness of the sources (e.g., domains/languages) in transfer learning based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.",Interpretability and Analysis of Models for NLP,"[0.006537165958434343, 0.00488003296777606, 0.006843851413577795, 0.0022331790532916784, 0.010218916460871696, 0.009865870699286461, 0.005293385125696659, 0.009459054097533226, 0.09599600732326508, 0.004146752413362265, 0.28867360949516296, 0.4537201523780823, 0.008551612496376038, 0.002734774723649025, 0.02144581824541092, 0.006300307344645262, 0.015165210701525211, 0.005594470538198948, 0.012218470685184002, 0.004432705696672201, 0.017458271235227585, 0.005681456997990608, 0.002548930933699012]",0.4537201523780823,Machine Translation and Multilinguality,0.09599600732326508,False
"A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data; 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT2, ROBERTA, and XLNET. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset. mit.edu.",Interpretability and Analysis of Models for NLP,"[0.045109331607818604, 0.43548300862312317, 0.003542086109519005, 0.02315448597073555, 0.14850479364395142, 0.006081555038690567, 0.0021845230367034674, 0.0045503717847168446, 0.06711842119693756, 0.015017488040030003, 0.012230506166815758, 0.01042292732745409, 0.01612257957458496, 0.009990784339606762, 0.10686099529266357, 0.015014429576694965, 0.015202498063445091, 0.029418302699923515, 0.009929302148520947, 0.0032824662048369646, 0.006101553328335285, 0.006417389493435621, 0.0082602733746171]",0.43548300862312317,Computational Social Science and Social Media,0.06711842119693756,False
"Many types of distributional word embeddings (weakly) encode linguistic regularities as directions (the difference between jump and jumped will be in a similar direction to that of walk and walked, and so on). Several attempts have been made to explain this fact. We respond to Allen and Hospedales' recent (ICML, 2019) theoretical explanation, which claims that word2vec and GloVe will encode linguistic regularities whenever a specific relation of paraphrase holds between the four words involved in the regularity. We demonstrate that the explanation does not go through: the paraphrase relations needed under this explanation do not hold empirically.",Interpretability and Analysis of Models for NLP,"[0.26469042897224426, 0.025844411924481392, 0.0022127588745206594, 0.02084699273109436, 0.024403255432844162, 0.00943559780716896, 0.004082508850842714, 0.0074834865517914295, 0.2017129361629486, 0.019613265991210938, 0.013365591876208782, 0.016166584566235542, 0.12152111530303955, 0.013928213156759739, 0.041177261620759964, 0.13991838693618774, 0.009477119892835617, 0.016893643885850906, 0.007639413233846426, 0.00383664364926517, 0.010675058700144291, 0.016313504427671432, 0.008761740289628506]",0.26469042897224426,"Linguistic Theories, Cognitive Modeling and Psycholinguistics",0.2017129361629486,False
"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset. Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples. Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels. We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation. As a result, our method eliminates part of the spurious correlations between context representation and output labels. The code is available at https://github.com/xijiz/cfgen.",Information Extraction,"[0.0026726583018898964, 0.005471923388540745, 0.0041674175299704075, 0.003060936462134123, 0.004012535326182842, 0.005419131834059954, 0.6989875435829163, 0.03953718766570091, 0.016517287120223045, 0.004108902532607317, 0.09797205775976181, 0.0027846379671245813, 0.0038678033743053675, 0.0068363044410943985, 0.007328446488827467, 0.008173730224370956, 0.017380543053150177, 0.008644449524581432, 0.002630940405651927, 0.003995201550424099, 0.04705404117703438, 0.0052978903986513615, 0.004078468773514032]",0.6989875435829163,Information Extraction,0.6989875435829163,True
"Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) -a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering and show that SCCL significantly advances the state-of-the-art results on most benchmark datasets with 3%âˆ’11% improvement on Accuracy and 4% âˆ’ 15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and top-down clustering to achieve better intracluster and inter-cluster distances when evaluated with the ground truth cluster labels 1 .",Machine Learning for NLP,"[0.0050991615280508995, 0.00486036716029048, 0.007862403988838196, 0.007109430618584156, 0.01017071958631277, 0.024826504290103912, 0.02884017489850521, 0.24906103312969208, 0.020784035325050354, 0.002828122116625309, 0.45234057307243347, 0.010783846490085125, 0.005620491690933704, 0.007890919223427773, 0.012201192788779736, 0.01737971417605877, 0.06200915575027466, 0.009510872885584831, 0.005448398645967245, 0.01228228397667408, 0.03263301029801369, 0.005563223268836737, 0.004894319921731949]",0.45234057307243347,Machine Learning for NLP,0.45234057307243347,True
"The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.07750736176967621, 0.009655804373323917, 0.0033020535483956337, 0.006619871128350496, 0.013853915967047215, 0.020555326715111732, 0.002258263062685728, 0.00954864639788866, 0.4731521010398865, 0.004759829491376877, 0.20021642744541168, 0.013696394860744476, 0.045403823256492615, 0.005391073878854513, 0.026033032685518265, 0.014779281802475452, 0.024321701377630234, 0.008737867698073387, 0.006462955381721258, 0.004938420839607716, 0.019894208759069443, 0.0058696032501757145, 0.003042136551812291]",0.4731521010398865,Interpretability and Analysis of Models for NLP,0.07750736176967621,False
"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernelinspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformerbased universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks.",Machine Learning for NLP,"[0.010560018941760063, 0.0028497970197349787, 0.008687503635883331, 0.008651071228086948, 0.001773660653270781, 0.01671607419848442, 0.006505916360765696, 0.004885196685791016, 0.025714950636029243, 0.003350434359163046, 0.016270127147436142, 0.0023191950749605894, 0.0057073552161455154, 0.009930635802447796, 0.011682155542075634, 0.012176522053778172, 0.6936127543449402, 0.0035277758724987507, 0.0031328564509749413, 0.003972475882619619, 0.13969974219799042, 0.004015729762613773, 0.004258015658706427]",0.6936127543449402,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.016270127147436142,False
"News editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies. Previous research has investigated such persuasive effects for argumentative content. In contrast, this paper studies how important the style of news editorials is to achieve persuasion. To this end, we first compare content-and style-oriented classifiers on editorials from the liberal NYTimes with ideology-specific effect annotations. We find that conservative readers are resistant to NYTimes style, but on liberals, style even has more impact than content. Focusing on liberals, we then cluster the leads, bodies, and endings of editorials, in order to learn about writing style patterns of effective argumentation.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.006430999841541052, 0.23745720088481903, 0.0030488891061395407, 0.02542302757501602, 0.023831622675061226, 0.005643235519528389, 0.003341968171298504, 0.006715080235153437, 0.014444384723901749, 0.005635131150484085, 0.0036396465729922056, 0.0046323384158313274, 0.012173856608569622, 0.00541895255446434, 0.09223810583353043, 0.008332564495503902, 0.0038357973098754883, 0.5128695368766785, 0.004518512170761824, 0.005096415523439646, 0.0063270023092627525, 0.004869444761425257, 0.0040763081051409245]",0.5128695368766785,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.5128695368766785,True
"Manual evaluation is essential to judge progress on automatic text summarization. However, we conduct a survey on recent summarization system papers that reveals little agreement on how to perform such evaluation studies. We conduct two evaluation experiments on two aspects of summaries' linguistic quality (coherence and repetitiveness) to compare Likert-type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another. In our survey, we also find that study parameters such as the overall number of annotators and distribution of annotators to annotation items are often not fully reported and that subsequent statistical analysis ignores grouping factors arising from one annotator judging multiple summaries. Using our evaluation experiments, we show that the total number of annotators can have a strong impact on study power and that current statistical analysis methods can inflate type I error rates up to eight-fold. In addition, we highlight that for the purpose of system comparison the current practice of eliciting multiple judgements per summary leads to less powerful and reliable annotations given a fixed study budget.",Summarization,"[0.0029647445771843195, 0.006337956991046667, 0.0033066009636968374, 0.01270830538123846, 0.004789881873875856, 0.010784338228404522, 0.009014838375151157, 0.01075091864913702, 0.0046592517755925655, 0.004131846595555544, 0.003083096584305167, 0.0030573580879718065, 0.005204621236771345, 0.001745847868733108, 0.05132485181093216, 0.0017163174925372005, 0.004037208389490843, 0.014691758900880814, 0.007014790084213018, 0.8198301792144775, 0.006937731057405472, 0.0061893765814602375, 0.00571824936196208]",0.8198301792144775,Summarization,0.8198301792144775,True
"We consider a new perspective on dialog state tracking (DST), the task of estimating a user's goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, crossdomain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. 1 We describe an encoder-decoder framework for DST with hierarchical representations, which leads to 20% improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs. Turn Utterance and Annotation 1 Hi can you book me a flight to Paris please. user.flight.book.object.equals .destination.equals.location.equals.Paris Sure, when and where will you depart? system.prompt.flight.book.object.equals .source .departureDateTime",Dialogue and Interactive Systems,"[0.0012207633117213845, 0.002481738803908229, 0.9312741160392761, 0.00796640757471323, 0.0008768385159783065, 0.0036049012560397387, 0.0021898294799029827, 0.0016036435263231397, 0.000883720291312784, 0.0034117174800485373, 0.004667656496167183, 0.0010249355109408498, 0.002338900463655591, 0.003723344998434186, 0.006266201380640268, 0.0009878709679469466, 0.0053304098546504974, 0.005238981451839209, 0.0061364611610770226, 0.0017028095899149776, 0.004345238208770752, 0.0013124297838658094, 0.0014111665077507496]",0.9312741160392761,Dialogue and Interactive Systems,0.9312741160392761,True
"Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both leftside and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.",Machine Translation and Multilinguality,"[0.0012705986155197024, 0.000995025155134499, 0.0013767523923888803, 0.00045258962200023234, 0.002849605865776539, 0.002255463507026434, 0.0011902181431651115, 0.0019357767887413502, 0.003527203109115362, 0.0008071514894254506, 0.0065449802204966545, 0.954620897769928, 0.00316356192342937, 0.0005633049877360463, 0.0036304902750998735, 0.002172165084630251, 0.00127564393915236, 0.0017121767159551382, 0.002895887242630124, 0.0015874102246016264, 0.0024529730435460806, 0.00181961536873132, 0.0009006018517538905]",0.954620897769928,Machine Translation and Multilinguality,0.954620897769928,True
"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.",Machine Learning for NLP,"[0.004390822257846594, 0.002340811537578702, 0.02203640341758728, 0.002926149871200323, 0.004635187331587076, 0.7434375286102295, 0.0021861817222088575, 0.006899605039507151, 0.007121976464986801, 0.004327863920480013, 0.1164175271987915, 0.008706842549145222, 0.006643947679549456, 0.0029792641289532185, 0.015851428732275963, 0.0024816677905619144, 0.011988441459834576, 0.00459313066676259, 0.008282373659312725, 0.012020881287753582, 0.0038831864949315786, 0.0039278664626181126, 0.0019209578167647123]",0.7434375286102295,Generation,0.1164175271987915,False
"In comparison with English, due to the lack of explicit word boundary and tenses information, Chinese Named Entity Recognition (NER) is much more challenging. In this paper, we propose a boundary enhanced approach for better Chinese NER. In particular, our approach enhances the boundary information from two perspectives. On one hand, we enhance the representation of the internal dependency of phrases by an additional Graph Attention Network(GAT) layer. On the other hand, taking the entity head-tail prediction (i.e., boundaries) as an auxiliary task, we propose an unified framework to learn the boundary information and recognize the NE jointly. Experiments on both the OntoNotes and the Weibo corpora show the effectiveness of our approach.",Information Extraction,"[0.001265428145416081, 0.004409213550388813, 0.0016425870126113296, 0.002821757923811674, 0.002275691833347082, 0.001891633146442473, 0.877285897731781, 0.01873181387782097, 0.0027499538846313953, 0.0027128816582262516, 0.007790309842675924, 0.001479459460824728, 0.0031923381611704826, 0.0031723666470497847, 0.005388147197663784, 0.005588482599705458, 0.005979010369628668, 0.005723064299672842, 0.0023052634205669165, 0.003381656017154455, 0.03410150855779648, 0.0032019028440117836, 0.002909653587266803]",0.877285897731781,Information Extraction,0.877285897731781,True
"Despite the success of contextualized language models on various NLP tasks, it is still unclear what these models really learn. In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT, based on linguistically-informed insights. In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model's layers simultaneously and highlighting intra-layer properties and inter-layer differences. We show that contextualization is neither driven by polysemy nor by pure context variation. We also provide insights on why BERT fails to model words in the middle of the functionality continuum. * Contribution to the visualization part. â€  Equal contribution to the computational linguistics part.",Interpretability and Analysis of Models for NLP,"[0.04927491396665573, 0.018962569534778595, 0.0010795716661959887, 0.008379255421459675, 0.028405988588929176, 0.005511191673576832, 0.005484778434038162, 0.01457503717392683, 0.5987987518310547, 0.01955592818558216, 0.0334400050342083, 0.011735518462955952, 0.020673731341958046, 0.007169563788920641, 0.04793381690979004, 0.06923340260982513, 0.012755418196320534, 0.012450676411390305, 0.00636066822335124, 0.003595417831093073, 0.00810154527425766, 0.011653847992420197, 0.004868305753916502]",0.5987987518310547,Interpretability and Analysis of Models for NLP,0.5987987518310547,True
"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FINDa framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar traintest distributions).",Interpretability and Analysis of Models for NLP,"[0.007941766642034054, 0.007406237535178661, 0.003951865714043379, 0.0034733694046735764, 0.012870034202933311, 0.013010388240218163, 0.015466523356735706, 0.25548622012138367, 0.12333757430315018, 0.004013164900243282, 0.4078778028488159, 0.008348733186721802, 0.0028462293557822704, 0.027392441406846046, 0.025427399203181267, 0.0076999543234705925, 0.026372168213129044, 0.01359947957098484, 0.005979829002171755, 0.004412666894495487, 0.01391229685395956, 0.005704129114747047, 0.003469698131084442]",0.4078778028488159,Machine Learning for NLP,0.12333757430315018,False
"Extracting lexico-semantic relations as graphstructured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks. However, there has been no attempt to exploit GNN to create taxonomies. In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task. Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain. We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction. Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym-hypernym candidate pairs, and a set of taxonomies for some known domains for training. The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain. Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.",Machine Learning for NLP,"[0.010438745841383934, 0.010377195663750172, 0.00828282255679369, 0.01942276954650879, 0.008617192506790161, 0.03133181855082512, 0.1622055619955063, 0.08151483535766602, 0.01462960708886385, 0.0042068022303283215, 0.07464679330587387, 0.0034131561405956745, 0.012063632719218731, 0.009154675528407097, 0.023986777290701866, 0.15272817015647888, 0.2455572485923767, 0.028527813032269478, 0.00457439199090004, 0.014848966151475906, 0.06044962257146835, 0.008609801530838013, 0.01041151862591505]",0.2455572485923767,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.07464679330587387,False
"This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model. However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. Through extensive experiments, we show that (1) pruning a number of attention heads in a multilingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each. We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings.",Machine Translation and Multilinguality,"[0.00803377851843834, 0.007954459637403488, 0.006059813313186169, 0.0037870043888688087, 0.012215920723974705, 0.007350764237344265, 0.007633817847818136, 0.011377102695405483, 0.3423367142677307, 0.0051585594192147255, 0.21437789499759674, 0.14719274640083313, 0.011958288960158825, 0.004266204312443733, 0.045347098261117935, 0.01661703735589981, 0.06540291011333466, 0.012955794110894203, 0.009551643393933773, 0.004273890517652035, 0.04558148980140686, 0.007234691176563501, 0.003332377877086401]",0.3423367142677307,Interpretability and Analysis of Models for NLP,0.14719274640083313,False
"The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height â‰¤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a âˆ¼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0027504495810717344, 0.0017879493534564972, 0.0146596385166049, 0.003495544195175171, 0.0013329440262168646, 0.0068909176625311375, 0.004272781778126955, 0.0031352387741208076, 0.003403334179893136, 0.0029627815820276737, 0.00901937484741211, 0.0023652021773159504, 0.0016597966896370053, 0.00412595784291625, 0.004175929352641106, 0.008135084062814713, 0.8920372724533081, 0.0014326400123536587, 0.002453817054629326, 0.0018921239534392953, 0.02432922087609768, 0.0013281236169859767, 0.002353877993300557]",0.8920372724533081,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.8920372724533081,True
"Previous work on automatic news timeline summarization (TLS) leaves an unclear picture about how this task can generally be approached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection and date summarization, and to the previous lack of appropriate evaluation metrics for the full TLS task. In this paper, we compare different TLS strategies using appropriate evaluation frameworks, and propose a simple and effective combination of methods that improves over the stateof-the-art on all tested benchmarks. For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets. The dataset will be made available at https://github. com/complementizer/news-tls.",Summarization,"[0.0016953569138422608, 0.004090406931936741, 0.003203869564458728, 0.008656001649796963, 0.0036059280391782522, 0.012334919534623623, 0.013204139657318592, 0.013672985136508942, 0.0029735874850302935, 0.0029365387745201588, 0.0039964779280126095, 0.0043808287009596825, 0.004446953535079956, 0.0012059286236763, 0.028605032712221146, 0.0015498767606914043, 0.0033229379914700985, 0.011501368135213852, 0.006691774819046259, 0.8517375588417053, 0.005649007856845856, 0.005415103863924742, 0.00512345926836133]",0.8517375588417053,Summarization,0.8517375588417053,True
"Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. However, existing methods fail to 1) effectively incorporate the semantic information from contentrelated posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection. As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically. Extensive experiments on two realworld datasets demonstrate that our models outperform existing methods. Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.",Computational Social Science and Social Media,"[0.005539451260119677, 0.6150518655776978, 0.004129571840167046, 0.022715115919709206, 0.028317885473370552, 0.0041408054530620575, 0.005127935204654932, 0.00877775065600872, 0.006265328265726566, 0.0037404829636216164, 0.006339003331959248, 0.002421600976958871, 0.007007749751210213, 0.0049976445734500885, 0.0821572095155716, 0.007067957893013954, 0.006256467662751675, 0.15692999958992004, 0.003955396823585033, 0.004426321014761925, 0.005765553098171949, 0.003843731479719281, 0.005025149323046207]",0.6150518655776978,Computational Social Science and Social Media,0.6150518655776978,True
"Paraphrase generation plays an essential role in natural language process (NLP), and it has many downstream applications. However, training supervised paraphrase models requires many annotated paraphrase pairs, which are usually costly to obtain. On the other hand, the paraphrases generated by existing unsupervised approaches are usually syntactically similar to the source sentences and are limited in diversity. In this paper, we demonstrate that it is possible to generate syntactically various paraphrases without the need for annotated paraphrase pairs. We propose Syntactically controlled Paraphrase Generator (SynPG), an encoder-decoder based model that learns to disentangle the semantics and the syntax of a sentence from a collection of unannotated texts. The disentanglement enables SynPG to control the syntax of output paraphrases by manipulating the embedding in the syntactic space. Extensive experiments using automatic metrics and human evaluation show that SynPG performs better syntactic control than unsupervised baselines, while the quality of the generated paraphrases is competitive. We also demonstrate that the performance of SynPG is competitive or even better than supervised models when the unannotated data is large. Finally, we show that the syntactically controlled paraphrases generated by SynPG can be utilized for data augmentation to improve the robustness of NLP models.",Generation,"[0.0025201153475791216, 0.0015382218407467008, 0.014358282089233398, 0.005159352906048298, 0.0025285338051617146, 0.7276294827461243, 0.007003629580140114, 0.010103401727974415, 0.00452437624335289, 0.0024175578728318214, 0.013493284583091736, 0.004384602885693312, 0.00620451383292675, 0.0034026054199784994, 0.023940466344356537, 0.002612791955471039, 0.10118615627288818, 0.007007240317761898, 0.005571266636252403, 0.029683003202080727, 0.016650954261422157, 0.00481646042317152, 0.00326372473500669]",0.7276294827461243,Generation,0.7276294827461243,True
"The growth of social media has encouraged the written use of African American Vernacular English (AAVE), which has traditionally been used only in oral contexts. However, NLP models have historically been developed using dominant English varieties, such as Standard American English (SAE), due to text corpora availability. We investigate the performance of GPT-2 on AAVE text by creating a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating syntactic structure and AAVE-or SAE-specific language for each pair. We evaluate each sample and its GPT-2 generated text with pretrained sentiment classifiers and find that while AAVE text results in more classifications of negative sentiment than SAE, the use of GPT-2 generally increases occurrences of positive sentiment for both. Additionally, we conduct human evaluation of AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall quality.",Computational Social Science and Social Media,"[0.009041490033268929, 0.1685170829296112, 0.005571275018155575, 0.020356811583042145, 0.027391793206334114, 0.01170225627720356, 0.005330723710358143, 0.009191728197038174, 0.018627416342496872, 0.00508821289986372, 0.005871481727808714, 0.02456815540790558, 0.025284534320235252, 0.004903414286673069, 0.4349299967288971, 0.007074831984937191, 0.009103061631321907, 0.15040497481822968, 0.01399553194642067, 0.01663612760603428, 0.010471525602042675, 0.009930737316608429, 0.006006815005093813]",0.4349299967288971,Resources and Evaluation,0.1685170829296112,False
"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to ""carryover"" the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pretrained backbones: T5 (Raffel et al., 2019) and BART (Lewis et al., 2019), and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency 1 .",Dialogue and Interactive Systems,"[0.0009491007658652961, 0.0011771924328058958, 0.9468682408332825, 0.004426157101988792, 0.0007493190933018923, 0.006292589474469423, 0.0011727494420483708, 0.0012189914705231786, 0.0008090728078968823, 0.001412541838362813, 0.006860961206257343, 0.0024487588088959455, 0.00178159074857831, 0.0031721466220915318, 0.003762969048693776, 0.0007281447178684175, 0.0033920458517968655, 0.0026171619538217783, 0.004612179007381201, 0.0012696949997916818, 0.002135773189365864, 0.0009864310268312693, 0.0011561868013814092]",0.9468682408332825,Dialogue and Interactive Systems,0.9468682408332825,True
"Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation. Due to the pre-training task and corpus, BERT is taskagnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task. The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way. Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features. Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin. The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0009664656245149672, 0.009020830504596233, 0.0029199449345469475, 0.004952474497258663, 0.003799305995926261, 0.003685835748910904, 0.009391488507390022, 0.012609157711267471, 0.009460779838263988, 0.001552429748699069, 0.008366472087800503, 0.005834489595144987, 0.0025269207544624805, 0.0022748198825865984, 0.026226025074720383, 0.0033855305518954992, 0.0017860758816823363, 0.8730161786079407, 0.0040996442548930645, 0.002324723871424794, 0.0075674718245863914, 0.0024972562678158283, 0.0017357602482661605]",0.8730161786079407,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.8730161786079407,True
"In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word's similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the stateof-the-art few-shot classification model -Tap-Net, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64",Dialogue and Interactive Systems,"[0.004108966328203678, 0.007546162698417902, 0.01895664632320404, 0.00594952329993248, 0.005735337734222412, 0.020025355741381645, 0.186465322971344, 0.0904955342411995, 0.011365646496415138, 0.0050753941759467125, 0.40945401787757874, 0.005273029208183289, 0.006394388619810343, 0.012559983879327774, 0.011522653512656689, 0.014831542037427425, 0.05622384324669838, 0.016053438186645508, 0.004642465617507696, 0.004523274023085833, 0.09324836730957031, 0.0050833141431212425, 0.004465743899345398]",0.40945401787757874,Machine Learning for NLP,0.01895664632320404,False
"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft orderreward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, locationbased orders, frequency-based orders, contentbased orders, and model-based orders. Curiously, we find that for the WMT'14 English â†’ German and WMT'18 English â†’ Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English â†’ German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.",Machine Translation and Multilinguality,"[0.0022281818091869354, 0.0012850756756961346, 0.0025976095348596573, 0.0006797595997340977, 0.0036761444061994553, 0.015691367909312248, 0.001047865953296423, 0.0021958735305815935, 0.006399901583790779, 0.0010025714291259646, 0.008101683109998703, 0.9164240956306458, 0.004552540369331837, 0.001109403558075428, 0.010349909774959087, 0.0015311484457924962, 0.0034056168515235186, 0.002348143607378006, 0.003323269309476018, 0.003249615663662553, 0.004270089324563742, 0.0031885087955743074, 0.001341522904112935]",0.9164240956306458,Machine Translation and Multilinguality,0.9164240956306458,True
"Mathematical statements written in natural language are usually composed of two different modalities: mathematical elements and natural language. These two modalities have several distinct linguistic and semantic properties. State-of-the-art representation techniques have demonstrated an inability in capturing such an entangled style of discourse. In this work, we propose STAR, a model that uses crossmodal attention to learn how to represent mathematical text for the task of Natural Language Premise Selection. This task uses conjectures written in both natural and mathematical language to recommend premises that most likely will be relevant to prove a particular statement. We found that STAR not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-ofthe-art models.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.015232499688863754, 0.01459200493991375, 0.01356580015271902, 0.12679176032543182, 0.006120577454566956, 0.02814370021224022, 0.012503594160079956, 0.016458973288536072, 0.026543021202087402, 0.008324156515300274, 0.01939612440764904, 0.001728641800582409, 0.006103835999965668, 0.16425229609012604, 0.057609133422374725, 0.03121209889650345, 0.3987216353416443, 0.013059886172413826, 0.006324761547148228, 0.007654527202248573, 0.010366972535848618, 0.005341285839676857, 0.009952780790627003]",0.3987216353416443,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.3987216353416443,True
"Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years' NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that longform QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.",Summarization,"[0.00475383410230279, 0.01965065859258175, 0.03198115527629852, 0.04100639373064041, 0.011183404363691807, 0.17884264886379242, 0.009916016831994057, 0.018662620335817337, 0.0067183710634708405, 0.012037228792905807, 0.012404647655785084, 0.0060615153051912785, 0.010346946306526661, 0.005772491451352835, 0.21026457846164703, 0.0029845975805073977, 0.016618479043245316, 0.03813065215945244, 0.02064361982047558, 0.3116050362586975, 0.009142979979515076, 0.012769309803843498, 0.008502881973981857]",0.3116050362586975,Summarization,0.3116050362586975,True
"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pretrained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.",Dialogue and Interactive Systems,"[0.0020210666116327047, 0.0020142358262091875, 0.7985260486602783, 0.007439517881721258, 0.002334949327632785, 0.09742679446935654, 0.0031045787036418915, 0.0033216846641153097, 0.0019482718780636787, 0.003054951783269644, 0.019621457904577255, 0.0035556782968342304, 0.0036140151787549257, 0.005702314432710409, 0.009402190335094929, 0.0012793351197615266, 0.009254061616957188, 0.006043315399438143, 0.006278018932789564, 0.005260863341391087, 0.003912206739187241, 0.0022455118596553802, 0.002638860372826457]",0.7985260486602783,Dialogue and Interactive Systems,0.7985260486602783,True
"Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments demonstrate that our distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems. On the English WSJ corpus, our models achieve an unlabeled F1 score of 68.9 for constituency parsing. 1","Syntax: Tagging, Chunking and Parsing","[0.01712711527943611, 0.005726899951696396, 0.010888144373893738, 0.008709183894097805, 0.004936383105814457, 0.036849990487098694, 0.009143782779574394, 0.012386457063257694, 0.019531888887286186, 0.00494739506393671, 0.03953809663653374, 0.005325953010469675, 0.01481937151402235, 0.022554708644747734, 0.031245069578289986, 0.010019822977483273, 0.16497944295406342, 0.0073298863135278225, 0.004516610875725746, 0.00789012759923935, 0.5498689413070679, 0.0058571039699018, 0.005807617679238319]",0.5498689413070679,"Syntax: Tagging, Chunking and Parsing",0.5498689413070679,True
"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer (Vaswani et al., 2017) . The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT (Devlin et al., 2019). The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https: //github.com/studio-ousia/luke.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.002768631326034665, 0.0035524682607501745, 0.002378081437200308, 0.002748743398115039, 0.002935383701696992, 0.0065029156394302845, 0.7036443948745728, 0.04796948656439781, 0.011577746830880642, 0.0036701185163110495, 0.08144882321357727, 0.0030579755548387766, 0.003211046801880002, 0.010997526347637177, 0.005019062664359808, 0.013448165729641914, 0.036051198840141296, 0.004250537604093552, 0.002605148358270526, 0.0031607698183506727, 0.03985198587179184, 0.0046768467873334885, 0.0044729383662343025]",0.7036443948745728,Information Extraction,0.036051198840141296,False
"Despite the widespread success of selfsupervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SAPBERT, a pretraining scheme that selfaligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipelinebased hybrid systems, SAPBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without taskspecific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BIOBERT, SCIBERT and PUB-MEDBERT, our pretraining scheme proves to be both effective and robust. 1",Information Retrieval and Text Mining,"[0.0037473614793270826, 0.004609471652656794, 0.00351456250064075, 0.004421922378242016, 0.006042910274118185, 0.009532904252409935, 0.484438955783844, 0.1012335866689682, 0.01608712412416935, 0.0033072051592171192, 0.20860078930854797, 0.004048679023981094, 0.004105602391064167, 0.010700946673750877, 0.009196525439620018, 0.014249243773519993, 0.05157814174890518, 0.0077012209221720695, 0.003051372477784753, 0.004638897255063057, 0.03534099459648132, 0.004899532534182072, 0.004952096380293369]",0.484438955783844,Information Extraction,0.1012335866689682,False
"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. 1 I love tennis! Tennis is great! Do I look ok? Bro <title>, golf is better UR hot! Me too <3 UR hot! Canada's got no game OW",Computational Social Science and Social Media,"[0.020098859444260597, 0.5633922219276428, 0.0033057541586458683, 0.022032830864191055, 0.1369219720363617, 0.00430687889456749, 0.003605637466534972, 0.007016445510089397, 0.021852541714906693, 0.007131211459636688, 0.010451201349496841, 0.00967397727072239, 0.01718912087380886, 0.008724153973162174, 0.08837256580591202, 0.010532008484005928, 0.006812645588070154, 0.03069199062883854, 0.008388889022171497, 0.002789905993267894, 0.004831891506910324, 0.005079112946987152, 0.00679817283526063]",0.5633922219276428,Computational Social Science and Social Media,0.5633922219276428,True
"Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",Machine Translation and Multilinguality,"[0.0008546350290998816, 0.000925264204852283, 0.0010197027586400509, 0.0004302982706576586, 0.002793872030451894, 0.001211451250128448, 0.0008249590173363686, 0.0014860858209431171, 0.003423683810979128, 0.0009508015355095267, 0.003287295578047633, 0.96560138463974, 0.0016015882138162851, 0.0006537165609188378, 0.0036697781179100275, 0.0010199190583080053, 0.0006646715919487178, 0.0017445131670683622, 0.0029203842859715223, 0.0011970509076490998, 0.0011911928886547685, 0.0017126607708632946, 0.0008151096990332007]",0.96560138463974,Machine Translation and Multilinguality,0.96560138463974,True
"Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses. 1",Summarization,"[0.002558604348450899, 0.004254597704857588, 0.004198326263576746, 0.009624220430850983, 0.004099093843251467, 0.017362233251333237, 0.005448105279356241, 0.007542234845459461, 0.0034582014195621014, 0.0031448539812117815, 0.0032630078494548798, 0.002749641425907612, 0.0047178794629871845, 0.001068181125447154, 0.03225164860486984, 0.0012877930421382189, 0.0030809342861175537, 0.011727413162589073, 0.005712841637432575, 0.8571735620498657, 0.006040163803845644, 0.0052192616276443005, 0.004017247818410397]",0.8571735620498657,Summarization,0.8571735620498657,True
"Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(outof-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multilevel graph attention to explicitly model lexical relations. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.",Dialogue and Interactive Systems,"[0.00768257537856698, 0.007190411444753408, 0.008298389613628387, 0.010280651971697807, 0.007986209355294704, 0.019814595580101013, 0.1949576437473297, 0.11991627514362335, 0.02940521389245987, 0.007262078113853931, 0.29044783115386963, 0.004584079142659903, 0.009144861251115799, 0.012829973362386227, 0.008595932275056839, 0.08417361229658127, 0.08175678551197052, 0.014676399528980255, 0.005201841238886118, 0.004224394913762808, 0.05818309634923935, 0.006916142534464598, 0.006470967084169388]",0.29044783115386963,Machine Learning for NLP,0.008298389613628387,False
"The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create English-language models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning via language modelling can improve performance on these challenge datasets, but the overall performances indicate that there is still much room for improvement. We release both the datasets and the source code at https://github.com/ jerbarnes/multitask_negation_ for_targeted_sentiment.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0010024933144450188, 0.008818336762487888, 0.0028869574889540672, 0.006627100054174662, 0.0026039970107376575, 0.003434946294873953, 0.004452219232916832, 0.004183763172477484, 0.010474042035639286, 0.0016848815139383078, 0.0029674144461750984, 0.0027689679991453886, 0.0024155802093446255, 0.0016975562321022153, 0.03136507794260979, 0.0027581965550780296, 0.0019252055790275335, 0.8945382833480835, 0.00343964877538383, 0.0020843930542469025, 0.00473024370148778, 0.0017750706756487489, 0.0013655873481184244]",0.8945382833480835,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.8945382833480835,True
"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on nonisomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. ""under-training"").",Interpretability and Analysis of Models for NLP,"[0.016559919342398643, 0.018798669800162315, 0.005580361932516098, 0.005163309630006552, 0.024193238466978073, 0.011263389140367508, 0.008487453684210777, 0.012449126690626144, 0.03428026661276817, 0.005621193442493677, 0.03381534293293953, 0.5491256713867188, 0.035735808312892914, 0.005222551990300417, 0.058448199182748795, 0.07062363624572754, 0.014292710460722446, 0.023879513144493103, 0.0065778763964772224, 0.004101300612092018, 0.03461412340402603, 0.014862404204905033, 0.006303936708718538]",0.5491256713867188,Machine Translation and Multilinguality,0.03428026661276817,False
"Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",Machine Translation and Multilinguality,"[0.007640478666871786, 0.005538707133382559, 0.037026748061180115, 0.005166324321180582, 0.011608496308326721, 0.01136582251638174, 0.011989783495664597, 0.007532685995101929, 0.03921344131231308, 0.29676535725593567, 0.011015625670552254, 0.24817697703838348, 0.0095168212428689, 0.006570875644683838, 0.01615745946764946, 0.007913272827863693, 0.01056674588471651, 0.008611021563410759, 0.20067858695983887, 0.01606731303036213, 0.006966062821447849, 0.014330827631056309, 0.009580504149198532]",0.29676535725593567,"Language Grounding to Vision, Robotics and Beyond",0.24817697703838348,False
"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-theart levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve new state-ofthe-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.",Machine Translation and Multilinguality,"[0.002794308355078101, 0.003975988365709782, 0.002502226969227195, 0.0014289786340668797, 0.00920512992888689, 0.0038973537739366293, 0.0017234503757208586, 0.0057878075167536736, 0.015888584777712822, 0.003050581319257617, 0.009357730858027935, 0.8664892315864563, 0.0056771147064864635, 0.0019212127663195133, 0.03318848833441734, 0.002498851390555501, 0.0023964710999280214, 0.006839334964752197, 0.007749014068394899, 0.0032089268788695335, 0.0037120950873941183, 0.0044631995260715485, 0.0022439861204475164]",0.8664892315864563,Machine Translation and Multilinguality,0.8664892315864563,True
"Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering. However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step. To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL). The window of multilabel learning is centered on the specified emotion clause or cause clause and slides as their positions move. Finally, CMLL and EMLL are integrated to obtain the final result. We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0011121375719085336, 0.010387520305812359, 0.00662448164075613, 0.006812769919633865, 0.003551418660208583, 0.005632694344967604, 0.08140484988689423, 0.01168700773268938, 0.006214163266122341, 0.005140827037394047, 0.0036146356724202633, 0.0025685240980237722, 0.003167602000758052, 0.0035469660069793463, 0.026486849412322044, 0.004950745031237602, 0.0065452405251562595, 0.7820460796356201, 0.007392145227640867, 0.003894382156431675, 0.011745763942599297, 0.0032113995403051376, 0.002261694287881255]",0.7820460796356201,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.7820460796356201,True
"Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (Hi-AGM) with two variants. A multi-label attention variant (HiAGM-LA) learns hierarchyaware label embeddings through the hierarchy encoder and conducts inductive fusion of labelaware text features. A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.",Information Retrieval and Text Mining,"[0.003752252785488963, 0.003608106868341565, 0.004501312039792538, 0.003015384776517749, 0.005757820326834917, 0.013789253309369087, 0.027685068547725677, 0.3637453317642212, 0.027970055118203163, 0.0026181021239608526, 0.445444256067276, 0.007869125343859196, 0.002882888540625572, 0.00861828401684761, 0.00805406179279089, 0.007678675465285778, 0.01665051281452179, 0.008880293928086758, 0.0038583308923989534, 0.005438701249659061, 0.01982203871011734, 0.005056765861809254, 0.003303291043266654]",0.445444256067276,Machine Learning for NLP,0.3637453317642212,False
"Most current neural machine translation models adopt a monotonic decoding order of either left-to-right or right-to-left. In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. More specifically, our method first predicts a median word. It starts to decode the words on the right side of the median word and then generates words on the left. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental results show that the proposed method can significantly outperform strong baseline models.",Machine Translation and Multilinguality,"[0.0011170883662998676, 0.000679225311614573, 0.0011616407427936792, 0.00037679378874599934, 0.0022110696882009506, 0.0020501413382589817, 0.0005554187810048461, 0.0009664985118433833, 0.00426317797973752, 0.0005602599121630192, 0.003550721099600196, 0.966558575630188, 0.0022836357820779085, 0.0005040394607931376, 0.002477934118360281, 0.0010983938118442893, 0.0007219522376544774, 0.0015047637280076742, 0.00262594036757946, 0.0015060902805998921, 0.0012729185400530696, 0.0013494121376425028, 0.0006043307366780937]",0.966558575630188,Machine Translation and Multilinguality,0.966558575630188,True
"Pathology imaging is broadly used for identifying the causes and effects of diseases or injuries. Given a pathology image, being able to answer questions about the clinical findings contained in the image is very important for medical decision making. In this paper, we aim to develop a pathological visual question answering framework to analyze pathology images and answer medical questions related to these images. To build such a framework, we create PathVQA, a pathology VQA dataset with 32,795 questions asked from 4,998 pathology images. We also propose a three-level optimization framework which performs self-supervised pretraining and VQA finetuning end-to-end to learn powerful visual and textual representations jointly and automatically identifies and excludes noisy self-supervised examples from pretraining. We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed methods. The datasets and code are available at https://github.com/UCSD-AI4H/PathVQA",Question Answering,"[0.010431543923914433, 0.012600337155163288, 0.009187711402773857, 0.01177559420466423, 0.014711687341332436, 0.014596224762499332, 0.028108665719628334, 0.03296024724841118, 0.037614330649375916, 0.20067568123340607, 0.03304680064320564, 0.0021995732095092535, 0.005402645096182823, 0.41966742277145386, 0.03085748665034771, 0.009443779475986958, 0.05054779350757599, 0.010656328871846199, 0.02691234089434147, 0.0060343872755765915, 0.012190884910523891, 0.006426163949072361, 0.013952285051345825]",0.41966742277145386,Question Answering,0.41966742277145386,True
"Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and their compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0% in F 1 , which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind the performance of human expert, i.e. 90.8% in F 1 . It demonstrates that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid data. Our dataset is publicly available for noncommercial use at https://nextplusplus. github.io/TAT-QA/.",Question Answering,"[0.002712054643779993, 0.0024164095520973206, 0.005782260559499264, 0.005088228732347488, 0.0025791374500840902, 0.005258210003376007, 0.005356332287192345, 0.007572800386697054, 0.004792795982211828, 0.0028928278479725122, 0.015001260675489902, 0.001108371652662754, 0.0015125643694773316, 0.8963601589202881, 0.009855852462351322, 0.001923069474287331, 0.017314476892352104, 0.0027206148952245712, 0.0014789201086387038, 0.0007050229469314218, 0.0030462569557130337, 0.0013695266097784042, 0.003152932971715927]",0.8963601589202881,Question Answering,0.8963601589202881,True
"Fine-tuning pre-trained language models such as BERT has become a common practice dominating leaderboards across various NLP tasks. Despite its recent success and wide adoption, this process is unstable when there are only a small number of training samples available. The brittleness of this process is often reflected by the sensitivity to random seeds. In this paper, we propose to tackle this problem based on the noise stability property of deep nets, which is investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020) . Specifically, we introduce a novel and effective regularization method to improve fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability Regularization (LNSR). We extend the theories about adding noise to the input and prove that our method gives a stabler regularization effect. We provide supportive evidence by experimentally confirming that well-performing models show a low sensitivity to noise and fine-tuning with LNSR exhibits clearly higher generalizability and stability. Furthermore, our method also demonstrates advantages over other state-of-the-art algorithms including L 2 -",Machine Learning for NLP,"[0.004228574689477682, 0.0021981450263410807, 0.002678071614354849, 0.0017588817281648517, 0.005508756265044212, 0.005393638741225004, 0.0030868598259985447, 0.010496154427528381, 0.10189293324947357, 0.0012384215369820595, 0.8103257417678833, 0.007258706260472536, 0.0026190481148660183, 0.004700471181422472, 0.007232761941850185, 0.003290667897090316, 0.012555382214486599, 0.0030392459593713284, 0.0023467056453227997, 0.0014885816490277648, 0.003912774845957756, 0.001600302872247994, 0.001149180461652577]",0.8103257417678833,Machine Learning for NLP,0.8103257417678833,True
"Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation -A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the firstorder adjacency information. 2) The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higherorder neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.",Generation,"[0.00531301274895668, 0.002571208868175745, 0.016993122175335884, 0.00447038421407342, 0.0024692905135452747, 0.3972896337509155, 0.010600174777209759, 0.012311413884162903, 0.007897064089775085, 0.004241465125232935, 0.06442342698574066, 0.004040670581161976, 0.005585708189755678, 0.005384229589253664, 0.014211276546120644, 0.007771812379360199, 0.3851136863231659, 0.004288702737540007, 0.004513883963227272, 0.011523183435201645, 0.02214515581727028, 0.004243158269673586, 0.0025982533115893602]",0.3972896337509155,Generation,0.3972896337509155,True
"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQUAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQUAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic text corpus generated by an 8.3 billion parameter GPT-2 model and achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQUAD1.1 dev set. We further apply our methodology to SQUAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.",Question Answering,"[0.008662915788590908, 0.004209799692034721, 0.019652333110570908, 0.006504176650196314, 0.005707255098968744, 0.15855713188648224, 0.004089233465492725, 0.016975894570350647, 0.009024717845022678, 0.0076720453798770905, 0.036191392689943314, 0.0072468845173716545, 0.005546143278479576, 0.624240517616272, 0.029358113184571266, 0.0031698462553322315, 0.021906999871134758, 0.007102597504854202, 0.0040604048408567905, 0.002730509964749217, 0.0070872921496629715, 0.005156852770596743, 0.005146954674273729]",0.624240517616272,Question Answering,0.624240517616272,True
"Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs' inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/ thunlp/TR-BERT.",Question Answering,"[0.0029279175214469433, 0.002243765164166689, 0.005104059353470802, 0.0020286054350435734, 0.0033566339407116175, 0.0075948601588606834, 0.008572516031563282, 0.01650591939687729, 0.03512715548276901, 0.0016366122290492058, 0.8211227655410767, 0.00282473536208272, 0.0017679280135780573, 0.008352849632501602, 0.005476051941514015, 0.0040906197391450405, 0.050230104476213455, 0.0031364585738629103, 0.0018702545203268528, 0.0010944502428174019, 0.011861561797559261, 0.001570551423355937, 0.0015035845572128892]",0.8211227655410767,Machine Learning for NLP,0.008352849632501602,False
"Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of taskspecific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non-parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pretrained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.",Speech and Multimodality,"[0.003993501886725426, 0.006058243568986654, 0.011473152786493301, 0.0030928258784115314, 0.010234315879642963, 0.007848597131669521, 0.0048270458355546, 0.006665440741926432, 0.016311606392264366, 0.011310463771224022, 0.007986223325133324, 0.539091169834137, 0.010649967938661575, 0.0019380212761461735, 0.017792556434869766, 0.0026473484467715025, 0.003791945520788431, 0.007051968481391668, 0.2937871515750885, 0.01835661008954048, 0.003380803856998682, 0.008084836415946484, 0.0036262464709579945]",0.539091169834137,Machine Translation and Multilinguality,0.2937871515750885,False
"Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and highcoverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that Clu-BERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches. When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models. Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks. We release our sense distributions in five different languages at https://github. com/SapienzaNLP/clubert.",Semantics: Lexical Semantics,"[0.018371788784861565, 0.012527029030025005, 0.005796770099550486, 0.016753211617469788, 0.011096667498350143, 0.012095052748918533, 0.007278226315975189, 0.017294064164161682, 0.008842417038977146, 0.003287215018644929, 0.01850658468902111, 0.012192793190479279, 0.039436910301446915, 0.009604121558368206, 0.0337909571826458, 0.6745265126228333, 0.027533909305930138, 0.030544446781277657, 0.003627541707828641, 0.003498075995594263, 0.017542675137519836, 0.009186857379972935, 0.006666196975857019]",0.6745265126228333,Semantics: Lexical Semantics,0.6745265126228333,True
"State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2) domain-adaptive pre-training; and 3) taskadaptive pre-training. Experiments show that the effectiveness of pre-training is correlated with the similarity between the pre-training data and the target domain task. Moreover, we find that continuing pre-training could lead to the pre-trained model's catastrophic forgetting, and a learning method with less forgetting can alleviate this issue. Furthermore, results illustrate that a huge gap still exists between the low-resource and high-resource settings, which highlights the need for more advanced domain adaptation methods for the abstractive summarization task. 1 * * Equal contributions. Listing order is random. 1 The code and data are released at: https://github. com/TysonYu/AdaptSum",Summarization,"[0.0010294060921296477, 0.0013182052643969655, 0.0021479527931660414, 0.0052969250828027725, 0.002113578375428915, 0.007938109338283539, 0.007493357639759779, 0.007804520428180695, 0.0036100486759096384, 0.001668981509283185, 0.004458903800696135, 0.0029527214355766773, 0.0022139910142868757, 0.0006776327500119805, 0.007859466597437859, 0.0010335257975384593, 0.0024700986687093973, 0.00425099628046155, 0.00452531548216939, 0.919319748878479, 0.0031741538550704718, 0.003625545185059309, 0.0030168567318469286]",0.919319748878479,Summarization,0.919319748878479,True
"CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-ofits-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in F 1 score, property generation model achieves a respectable F 1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.",Question Answering,"[0.0035793250426650047, 0.0030028484761714935, 0.012647908180952072, 0.010152882896363735, 0.0030541226733475924, 0.00967489741742611, 0.0050323293544352055, 0.008467410691082478, 0.006816449575126171, 0.0056790015660226345, 0.020081214606761932, 0.0012693476164713502, 0.0017753296997398138, 0.7753700613975525, 0.014024526812136173, 0.005064207594841719, 0.09501615911722183, 0.003881335724145174, 0.0027121631428599358, 0.001275106449611485, 0.005748322699218988, 0.001722458633594215, 0.003952580504119396]",0.7753700613975525,Question Answering,0.7753700613975525,True
"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that emotions and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with linear time complexity, leading to a speed up. Experimental results show that our proposed model achieves the best performance, outperforming the state-of-the-art method by 2.26% (p < 0.001) in F 1 measure.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0012891022488474846, 0.009852937422692776, 0.0077498494647443295, 0.008035327307879925, 0.003005401464179158, 0.0068337321281433105, 0.07569067180156708, 0.010776601731777191, 0.00564777385443449, 0.00464516319334507, 0.003642853582277894, 0.002367919310927391, 0.004528913646936417, 0.004677654709666967, 0.025622237473726273, 0.005355436820536852, 0.005884217564016581, 0.7850282788276672, 0.006562006194144487, 0.0030178476590663195, 0.014594117179512978, 0.0029033957980573177, 0.0022885496728122234]",0.7850282788276672,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.7850282788276672,True
"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC. In an extensive analysis, we investigate factors of success and failure of our three approaches. 1",Semantics: Lexical Semantics,"[0.013564756140112877, 0.008999986574053764, 0.0049284049309790134, 0.02202502265572548, 0.005316838156431913, 0.006175862159579992, 0.00956114474684, 0.010326647199690342, 0.011580494232475758, 0.0036159376613795757, 0.012904450297355652, 0.002509960439056158, 0.006736269686371088, 0.012814582325518131, 0.01925624907016754, 0.1177278682589531, 0.675505518913269, 0.007530204486101866, 0.003852210007607937, 0.0026942482218146324, 0.029584519565105438, 0.00511907646432519, 0.007669825106859207]",0.675505518913269,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.1177278682589531,False
"Large-scale models for learning fixeddimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks. However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1",Machine Learning for NLP,"[0.007156248204410076, 0.0038570931646972895, 0.010245007462799549, 0.004482818301767111, 0.005735332146286964, 0.031650129705667496, 0.014201962389051914, 0.031050926074385643, 0.0347207672894001, 0.005927631165832281, 0.09271016716957092, 0.11005088686943054, 0.0044654482044279575, 0.007404195610433817, 0.016574779525399208, 0.011075836606323719, 0.4982684254646301, 0.006345498375594616, 0.017101123929023743, 0.007303494960069656, 0.06752325594425201, 0.008531023748219013, 0.0036178892478346825]",0.4982684254646301,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.09271016716957092,False
"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visuallygrounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents' verbal and nonverbal responses. We present experimental results obtained using the proposed VFD dataset and recent neural network models (e.g., BERT, ResNet). The results demonstrate that firstperson vision helps neural network models correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available 1 .",Dialogue and Interactive Systems,"[0.004280376248061657, 0.00952078215777874, 0.6420987844467163, 0.01725722849369049, 0.0066611310467123985, 0.007719902787357569, 0.0037041751202195883, 0.0038209110498428345, 0.004242234863340855, 0.16885671019554138, 0.005219892133027315, 0.0024290906731039286, 0.0039074313826859, 0.013116860762238503, 0.023000285029411316, 0.0027068438939750195, 0.006765973754227161, 0.008421004749834538, 0.04561719670891762, 0.005430992692708969, 0.0048780376091599464, 0.005031920969486237, 0.00531215313822031]",0.6420987844467163,Dialogue and Interactive Systems,0.6420987844467163,True
"Although deep neural networks have achieved prominent performance on many NLP tasks, they are vulnerable to adversarial examples. We propose Dirichlet Neighborhood Ensemble (DNE), a randomized method for training a robust model to defense synonym substitutionbased attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models (e.g., BERT) for NLP applications. Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.",Machine Learning for NLP,"[0.014284742064774036, 0.008209791034460068, 0.0056152744218707085, 0.008003048598766327, 0.011974160559475422, 0.02798578329384327, 0.017523815855383873, 0.03816905617713928, 0.03896182402968407, 0.0020623032469302416, 0.4962921142578125, 0.03757598251104355, 0.014224875718355179, 0.006159713491797447, 0.020305927842855453, 0.10143440216779709, 0.09053433686494827, 0.00744450930505991, 0.0038594191428273916, 0.007719589397311211, 0.02981797605752945, 0.006556155625730753, 0.005285229068249464]",0.4962921142578125,Machine Learning for NLP,0.4962921142578125,True
"Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering. Under this paradigm, a dual-encoder model is learned to encode questions and passages separately into vector representations, and all the passage vectors are then pre-computed and indexed, which can be efficiently retrieved by vector space search during inference time. In this paper, we propose a new contrastive learning method called cross momentum contrastive learning (xMoCo), for learning a dualencoder model for query-passage matching. Our method efficiently maintains a large pool of negative samples like the original MoCo, and by jointly optimizing question-to-passage and passage-to-question matching, enables using separate encoders for questions and passages. We evaluate our method on various open domain QA datasets, and the experimental results show the effectiveness of the proposed approach.",Question Answering,"[0.007327235769480467, 0.003970985300838947, 0.012162535451352596, 0.007252137642353773, 0.006657179910689592, 0.014690943993628025, 0.03877360746264458, 0.188531294465065, 0.010601134039461613, 0.0068488288670778275, 0.08647479116916656, 0.0053655486553907394, 0.0027307048439979553, 0.5205767750740051, 0.008312797173857689, 0.0074235862120985985, 0.039503298699855804, 0.005195493344217539, 0.005208458751440048, 0.0026679872535169125, 0.0071638720110058784, 0.0037620526272803545, 0.008798742666840553]",0.5205767750740051,Question Answering,0.5205767750740051,True
"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning. In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages. We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its highquality annotations to the target languages. We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline. Finally, we train different SOTA models on our novel corpus for monoand multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.006562631577253342, 0.016507569700479507, 0.021318381652235985, 0.010238968767225742, 0.00954494159668684, 0.04215051233768463, 0.01235175784677267, 0.014578931964933872, 0.014839977025985718, 0.008683500811457634, 0.030741147696971893, 0.08597200363874435, 0.013173675164580345, 0.008453852497041225, 0.17570489645004272, 0.020142000168561935, 0.2841799855232239, 0.036338549107313156, 0.0165295097976923, 0.01069851964712143, 0.1421585977077484, 0.01352017279714346, 0.005609976593405008]",0.2841799855232239,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.2841799855232239,True
"Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed eventrelated semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised ""liberal"" EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.",Information Extraction,"[0.0006843270384706557, 0.0012851449428126216, 0.0009337161900475621, 0.001351374783553183, 0.0011880865786224604, 0.0019484208896756172, 0.9238235354423523, 0.011730612255632877, 0.0014852731255814433, 0.002543047768995166, 0.0025527733378112316, 0.001225256361067295, 0.0015249564312398434, 0.0020508274901658297, 0.002639137441292405, 0.00341488397680223, 0.012478791177272797, 0.004361646715551615, 0.0020069966558367014, 0.0032335033174604177, 0.0132546192035079, 0.002232202561572194, 0.0020508323796093464]",0.9238235354423523,Information Extraction,0.9238235354423523,True
"We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-theart models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",Resources and Evaluation,"[0.007482277229428291, 0.012242710217833519, 0.009002100676298141, 0.010242135263979435, 0.02339734137058258, 0.013826477341353893, 0.00944020226597786, 0.02795702964067459, 0.1940617561340332, 0.009915427304804325, 0.37609434127807617, 0.0281821358948946, 0.008592197671532631, 0.01466930378228426, 0.1477886289358139, 0.007835417985916138, 0.026947548612952232, 0.02377835474908352, 0.009141781367361546, 0.009997627697885036, 0.017600083723664284, 0.007772350218147039, 0.004032781347632408]",0.37609434127807617,Machine Learning for NLP,0.1477886289358139,False
"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the ASR output into, hopefully, semantically self-contained chunks to be fed into the MT system. This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account. This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk. An extensive and thorough experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario. Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.",Machine Translation and Multilinguality,"[0.0037984915543347597, 0.006038271822035313, 0.03789491206407547, 0.00622748164460063, 0.008186277002096176, 0.01329289935529232, 0.007388328667730093, 0.008375734090805054, 0.008104132488369942, 0.018492426723241806, 0.010033479891717434, 0.16550764441490173, 0.01228327490389347, 0.0027088948991149664, 0.024899329990148544, 0.0032248201314359903, 0.006431447342038155, 0.009312950074672699, 0.5920552611351013, 0.036086082458496094, 0.005717654246836901, 0.009005076251924038, 0.004935139324516058]",0.5920552611351013,Speech and Multimodality,0.16550764441490173,False
"Intent classification is a major task in spoken language understanding (SLU). Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use. Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data. This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection. Our method designs a novel domain-regularized module (DRM) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases. Besides, DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement. The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons.",Dialogue and Interactive Systems,"[0.0020184533204883337, 0.007022070232778788, 0.8153882026672363, 0.017877159640192986, 0.002925450913608074, 0.004632180090993643, 0.0033937315456569195, 0.004786341451108456, 0.002414433052763343, 0.005291627254337072, 0.010413425043225288, 0.0027947186026722193, 0.0048541175201535225, 0.009297458454966545, 0.03188564255833626, 0.0029604590963572264, 0.010013289749622345, 0.014199170283973217, 0.0325503870844841, 0.004857718013226986, 0.004682464990764856, 0.00271674245595932, 0.0030247436370700598]",0.8153882026672363,Dialogue and Interactive Systems,0.8153882026672363,True
"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure. In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes. These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms. Our proposed model is a general framework and can be combined with almost all sequence taggers. Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.",Information Extraction,"[0.001277458039112389, 0.005135634448379278, 0.004542575217783451, 0.006447391584515572, 0.0033147588837891817, 0.004919279832392931, 0.05598900839686394, 0.008008889853954315, 0.007943241856992245, 0.003943031188100576, 0.0026760802138596773, 0.003600865602493286, 0.003998108673840761, 0.002004440641030669, 0.023112988099455833, 0.007217458914965391, 0.005184877663850784, 0.8227789402008057, 0.005485378205776215, 0.004831728991121054, 0.011707207188010216, 0.0034754343796521425, 0.0024054020177572966]",0.8227789402008057,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.05598900839686394,False
"There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence -image for training and tuples of source sentence -image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the ""imagined representation"" to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.",Machine Translation and Multilinguality,"[0.005661783739924431, 0.004434199538081884, 0.019277239218354225, 0.004236908629536629, 0.007929855957627296, 0.010660565458238125, 0.003962140530347824, 0.00341208023019135, 0.02105339802801609, 0.732710599899292, 0.0028680323157459497, 0.015936847776174545, 0.005119824316352606, 0.005362570285797119, 0.019301779568195343, 0.004129338078200817, 0.006641955580562353, 0.005529157351702452, 0.096603624522686, 0.0083401994779706, 0.003934487234801054, 0.007924213074147701, 0.0049691093154251575]",0.732710599899292,"Language Grounding to Vision, Robotics and Beyond",0.015936847776174545,False
"Metaphor is an indispensable part of human cognition and everyday communication. Much research has been conducted elucidating metaphor processing in the mind/brain and the role it plays in communication. In recent years, metaphor processing systems have benefited greatly from these studies, as well as the rapid advances in deep learning for natural language processing (NLP). This paper provides a comprehensive review and discussion of recent developments in automated metaphor processing, in light of the findings about metaphor in the mind, language, and communication, and from the perspective of downstream NLP tasks.",Semantics: Lexical Semantics,"[0.03993149846792221, 0.045139461755752563, 0.04266398400068283, 0.059629276394844055, 0.02259978838264942, 0.061138853430747986, 0.01087171770632267, 0.014128084294497967, 0.04542432352900505, 0.03854451701045036, 0.01650031842291355, 0.005914023611694574, 0.03387214243412018, 0.010437958873808384, 0.10234754532575607, 0.28444135189056396, 0.043855104595422745, 0.06170566752552986, 0.02097165584564209, 0.01112859882414341, 0.006128390319645405, 0.012990732677280903, 0.009634964168071747]",0.28444135189056396,Semantics: Lexical Semantics,0.28444135189056396,True
"This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 openaccess scholarly articles annotated by domain experts. A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. On all tasks, using BERT embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial. Our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.",Resources and Evaluation,"[0.00120904843788594, 0.004414450377225876, 0.0020418157801032066, 0.0021746617276221514, 0.0029266271740198135, 0.0022989956196397543, 0.8967692255973816, 0.013934309594333172, 0.0035785073414444923, 0.0023268908262252808, 0.007864323444664478, 0.002353574149310589, 0.003193104173988104, 0.0035265167243778706, 0.006703960709273815, 0.0033541389275342226, 0.004265415482223034, 0.009417080320417881, 0.002880743006244302, 0.0035918059293180704, 0.014844932593405247, 0.0033121721353381872, 0.0030175563879311085]",0.8967692255973816,Information Extraction,0.006703960709273815,False
"Relation schemas are often pre-defined for each relation dataset. Relation types can be related from different datasets and have overlapping semantics. We hypothesize we can combine these datasets according to the semantic relatedness between the relation types to overcome the problem of lack of training data. It is often easy to discover the connection between relation types based on relation names or annotation guides, but hard to measure the exact similarity and take advantage of the connection between the relation types from different datasets. We propose to use prototypical examples to represent each relation type and use these examples to augment related types from a different dataset. We obtain further improvement (ACE05) with this type augmentation over a strong baseline which uses multi-task learning between datasets to obtain better feature representation for relations. We make our implementation publicly available: https://github. com/fufrank5/relatedness * This is the work that the author has done before joining Amazon Alexa AI.",Information Extraction,"[0.00564225297421217, 0.006062043830752373, 0.010187290608882904, 0.009686230681836605, 0.008213086053729057, 0.013339905068278313, 0.34801480174064636, 0.03842676803469658, 0.0074740988202393055, 0.005129795055836439, 0.04593950882554054, 0.003686279058456421, 0.005930698476731777, 0.01056596264243126, 0.01607680507004261, 0.03317848965525627, 0.3243822157382965, 0.01605985499918461, 0.00463441014289856, 0.0075250109657645226, 0.06322836875915527, 0.006905234884470701, 0.009710850194096565]",0.34801480174064636,Information Extraction,0.34801480174064636,True
"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses. Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-ofthe-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure. When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.",Semantics: Lexical Semantics,"[0.01210575271397829, 0.009550226852297783, 0.004639836959540844, 0.013585387729108334, 0.007158562541007996, 0.005067295860499144, 0.009929944761097431, 0.01388444472104311, 0.007247083820402622, 0.00272967922501266, 0.017712898552417755, 0.00533366808667779, 0.018100764602422714, 0.006997867953032255, 0.013225237838923931, 0.7808321118354797, 0.02669207938015461, 0.01888306811451912, 0.002727385377511382, 0.0020920834504067898, 0.009819701313972473, 0.006021618843078613, 0.005663307849317789]",0.7808321118354797,Semantics: Lexical Semantics,0.7808321118354797,True
"Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.",Machine Translation and Multilinguality,"[0.0008665061322972178, 0.001139698433689773, 0.001370884943753481, 0.00048118713311851025, 0.0026327769737690687, 0.0011699562892317772, 0.0009019264834932983, 0.0013606221182271838, 0.002674465300515294, 0.0010259839473292232, 0.0018830255139619112, 0.9638333320617676, 0.0025225509889423847, 0.0005774600431323051, 0.004220201633870602, 0.0012925902847200632, 0.0005959469126537442, 0.0023282174952328205, 0.0033934356179088354, 0.0015698375646024942, 0.0014413610333576798, 0.0018101352034136653, 0.0009078009752556682]",0.9638333320617676,Machine Translation and Multilinguality,0.9638333320617676,True
"We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for Minimal Recursion Semantics. At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledgeintensive model, and Graph Neural Networks, a data-intensive model. Our parser achieves an accuracy of 92.39% in terms of ELEMENTARY DEPENDENCY MATCH, which is a 2.88 point improvement over the best data-driven model in the literature. The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scoperesolved logical form.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0033173596020787954, 0.0021367641165852547, 0.008206251077353954, 0.005933939479291439, 0.0012818054528906941, 0.00449552433565259, 0.005751859396696091, 0.0026676016859710217, 0.003952343016862869, 0.0028864287305623293, 0.005567818880081177, 0.0010663752909749746, 0.002223608549684286, 0.004683829378336668, 0.0046984003856778145, 0.01737581007182598, 0.8889788389205933, 0.0019263367867097259, 0.0020903816912323236, 0.0016583697870373726, 0.024531520903110504, 0.0016050724079832435, 0.0029637939296662807]",0.8889788389205933,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.8889788389205933,True
"Knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications. However, researchers have mainly poured attention to knowledge inference on binary facts. The studies on n-ary facts are relatively scarcer, although they are also ubiquitous in the real world. Therefore, this paper addresses knowledge inference on n-ary facts. We represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value pair(s). We further propose a neural network model, NeuInfer, for knowledge inference on n-ary facts. Besides handling the common task to infer an unknown element in a whole fact, NeuInfer can cope with a new type of task, flexible knowledge inference. It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description(s). Experimental results demonstrate the remarkable superiority of NeuInfer.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.002422482008114457, 0.0036198380403220654, 0.004287170246243477, 0.005612560082226992, 0.0035023679956793785, 0.007015308830887079, 0.680345356464386, 0.037213247269392014, 0.005914920940995216, 0.003718502353876829, 0.031875431537628174, 0.0015378707321360707, 0.002194599714130163, 0.02176906354725361, 0.004404943436384201, 0.014440960250794888, 0.12309547513723373, 0.007805517874658108, 0.0035941507667303085, 0.0038680911529809237, 0.021309368312358856, 0.003879836294800043, 0.0065728831104934216]",0.680345356464386,Information Extraction,0.12309547513723373,False
"Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.","Syntax: Tagging, Chunking and Parsing","[0.006149885710328817, 0.0038077232893556356, 0.011286040768027306, 0.008038311265408993, 0.00296517345122993, 0.01442541554570198, 0.011170553974807262, 0.007940413430333138, 0.011174838058650494, 0.0051458533853292465, 0.01510535180568695, 0.003977915272116661, 0.008146614767611027, 0.005788205191493034, 0.029229678213596344, 0.005002423655241728, 0.2610628008842468, 0.009067012928426266, 0.004795657936483622, 0.0077598365023732185, 0.5573394894599915, 0.0054960185661911964, 0.005124841351062059]",0.5573394894599915,"Syntax: Tagging, Chunking and Parsing",0.5573394894599915,True
"The performance of natural language generation systems has improved substantially with modern neural networks. At test time they typically employ beam search to avoid locally optimal but globally suboptimal predictions. However, due to model errors, a larger beam size can lead to deteriorating performance according to the evaluation metric. For this reason, it is common to rerank the output of beam search, but this relies on beam search to produce a good set of hypotheses, which limits the potential gains. Other alternatives to beam search require changes to the training of the model, which restricts their applicability compared to beam search. This paper proposes incremental beam manipulation, i.e. reranking the hypotheses in the beam during decoding instead of only at the end. This way, hypotheses that are unlikely to lead to a good final output are discarded, and in their place hypotheses that would have been ignored will be considered instead. Applying incremental beam manipulation leads to an improvement of 1.93 and 5.82 BLEU points over vanilla beam search for the test sets of the E2E and WebNLG challenges respectively. The proposed method also outperformed a strong reranker by 1.04 BLEU points on the E2E challenge, while being on par with it on the WebNLG dataset.",Generation,"[0.009164455346763134, 0.0033763840328902006, 0.02960270270705223, 0.003008567728102207, 0.004776598419994116, 0.56560218334198, 0.002337404992431402, 0.006588516291230917, 0.02886560559272766, 0.0037997872568666935, 0.1883631944656372, 0.0116110322996974, 0.011027414351701736, 0.00941941887140274, 0.03838048130273819, 0.003455149242654443, 0.050579287111759186, 0.005011158064007759, 0.0038620405830442905, 0.004278860054910183, 0.010219062678515911, 0.00420944020152092, 0.0024612736888229847]",0.56560218334198,Generation,0.56560218334198,True
"Transformers have advanced the field of natural language processing (NLP) in many ways. At the heart of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which underutilizes the capacity of the model. To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective. We present cascaded head-colliding attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency. 1",Machine Learning for NLP,"[0.003930790349841118, 0.0018773545743897557, 0.003226440167054534, 0.0013174613704904914, 0.0036906676832586527, 0.005935571156442165, 0.0029588763136416674, 0.007201252039521933, 0.05535653233528137, 0.001229389337822795, 0.8534032106399536, 0.016178758814930916, 0.0031124395318329334, 0.0028234899509698153, 0.00545198330655694, 0.003451310796663165, 0.013223490677773952, 0.0020810088608413935, 0.002265184884890914, 0.0016324517782777548, 0.007275344803929329, 0.0015205468516796827, 0.0008565958123654127]",0.8534032106399536,Machine Learning for NLP,0.8534032106399536,True
"Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese -Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multigranularity representations. 1",Machine Learning for NLP,"[0.003721582470461726, 0.001984848640859127, 0.003602149896323681, 0.0014066686853766441, 0.0025473227724432945, 0.008215231820940971, 0.0037659199442714453, 0.012209251523017883, 0.03221636638045311, 0.0013426635414361954, 0.8463379144668579, 0.0035811346024274826, 0.0020341998897492886, 0.0063110338523983955, 0.005739240441471338, 0.00405195914208889, 0.0421808585524559, 0.0015167809324339032, 0.0019896691665053368, 0.0012640082277357578, 0.011547785252332687, 0.0013871608534827828, 0.0010462345089763403]",0.8463379144668579,Machine Learning for NLP,0.8463379144668579,True
"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with finetuning.",Machine Learning for NLP,"[0.003612370230257511, 0.0021040767896920443, 0.0029100547544658184, 0.0011751519050449133, 0.006266155280172825, 0.0075372676365077496, 0.004527955316007137, 0.00952257588505745, 0.06242290139198303, 0.0015442578587681055, 0.7966629862785339, 0.05294974520802498, 0.003260168246924877, 0.003277877578511834, 0.008278226479887962, 0.0028730961494147778, 0.01133368257433176, 0.002237575827166438, 0.003437996609136462, 0.0018814429640769958, 0.008455277420580387, 0.0024219162296503782, 0.0013072255533188581]",0.7966629862785339,Machine Learning for NLP,0.7966629862785339,True
"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-toface settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial. In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.",Computational Social Science and Social Media,"[0.011334492824971676, 0.3094221353530884, 0.08493082970380783, 0.06848208606243134, 0.03470137342810631, 0.007759162690490484, 0.006192270666360855, 0.006612858735024929, 0.013173372484743595, 0.014817846938967705, 0.007730405777692795, 0.004930718801915646, 0.009692749008536339, 0.01468441542237997, 0.13414397835731506, 0.005345707759261131, 0.010590476915240288, 0.20217514038085938, 0.022562537342309952, 0.007226621266454458, 0.00768878310918808, 0.006601196713745594, 0.009200786240398884]",0.3094221353530884,Computational Social Science and Social Media,0.3094221353530884,True
"We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling-a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine-tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machinegenerated in the domain of short stories.",Generation,"[0.00937402993440628, 0.011544300243258476, 0.0025294364895671606, 0.004416050389409065, 0.008661651983857155, 0.1093786358833313, 0.010211287997663021, 0.05082042142748833, 0.05352555960416794, 0.003953129053115845, 0.550050675868988, 0.004774556029587984, 0.005828485824167728, 0.01163784135133028, 0.04519008845090866, 0.008772152476012707, 0.06223137676715851, 0.005182344000786543, 0.004389193374663591, 0.011079940013587475, 0.018009569495916367, 0.006021843291819096, 0.002417455893009901]",0.550050675868988,Machine Learning for NLP,0.1093786358833313,False
"We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine. The interactive approach helps the creation of flexible dictionaries with precise granularity required in text analysis. This paper introduces the first formulation of interactive dictionary construction to address this issue. To optimize the interaction, we propose a new algorithm that effectively captures an analyst's intention starting from only a small number of sample terms. Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task. Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions. Also, we provide our dataset for future studies 1 .",Information Retrieval and Text Mining,"[0.007613316178321838, 0.010387243703007698, 0.004185556434094906, 0.013929116539657116, 0.010657621547579765, 0.04026775807142258, 0.06708446890115738, 0.2471885234117508, 0.06891924887895584, 0.004596857354044914, 0.25481584668159485, 0.004693546798080206, 0.006162244826555252, 0.017010223120450974, 0.05009656772017479, 0.01188237126916647, 0.07544448971748352, 0.03662056475877762, 0.006221750285476446, 0.02818474918603897, 0.021132683381438255, 0.007145093288272619, 0.005760143510997295]",0.25481584668159485,Machine Learning for NLP,0.2471885234117508,False
"Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance of language identification, a tool commonly used for data filtering in the MT community and identify specific weaknesses. Based on our findings, we then propose several novel methods for data filtering, based on cross-lingual word embeddings. We compare our approaches to one of the winning methods from the WMT 2018 shared task on parallel corpus filtering on three real-life, high resource MT tasks. We find that said method, which was performing very strong in the WMT shared task, does not perform well within our more realistic task conditions. While we find that our approaches come out at the top on all three tasks, different variants perform best on different tasks. Further experiments on the WMT 2020 shared task for parallel corpus filtering show that our methods achieve comparable results to the strongest submissions of this campaign.",Machine Translation and Multilinguality,"[0.0028357296250760555, 0.007312361150979996, 0.003684898605570197, 0.0019410572713240981, 0.00995743740350008, 0.006928987801074982, 0.004660903476178646, 0.010422345250844955, 0.008931792341172695, 0.0016064090887084603, 0.025577669963240623, 0.7913058400154114, 0.00790268462151289, 0.0030335180927067995, 0.05938119813799858, 0.0032680081203579903, 0.0047002085484564304, 0.015758154913783073, 0.005792558658868074, 0.004037813283503056, 0.01165776140987873, 0.005808315705507994, 0.003494307864457369]",0.7913058400154114,Machine Translation and Multilinguality,0.7913058400154114,True
"Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse -but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.",Interpretability and Analysis of Models for NLP,"[0.019870536401867867, 0.01609247364103794, 0.0183833297342062, 0.7083415389060974, 0.011569766327738762, 0.006082224193960428, 0.004778483882546425, 0.01254539005458355, 0.014507369138300419, 0.005810269620269537, 0.014649735763669014, 0.00202000280842185, 0.013495274819433689, 0.039966974407434464, 0.02732102759182453, 0.021042760461568832, 0.02651846967637539, 0.00680811470374465, 0.003953232895582914, 0.007783112116158009, 0.006545380223542452, 0.004269341006875038, 0.00764520512893796]",0.7083415389060974,Discourse and Pragmatics,0.014507369138300419,False
"Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a selfsupervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and robustness of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.",Dialogue and Interactive Systems,"[0.0008925882284529507, 0.0020366553217172623, 0.9419312477111816, 0.006340527441352606, 0.0011796214384958148, 0.0023873941972851753, 0.0018057459965348244, 0.001397162675857544, 0.0008068501483649015, 0.0013993869069963694, 0.006943566724658012, 0.0019113820744678378, 0.0018822631100192666, 0.003039032919332385, 0.004595465958118439, 0.0012521171011030674, 0.0026699744630604982, 0.005290511529892683, 0.005478361621499062, 0.0017628850182518363, 0.002288977848365903, 0.0010546608828008175, 0.0016536532202735543]",0.9419312477111816,Dialogue and Interactive Systems,0.9419312477111816,True
"The distributions of orthographic word types are very different across languages due to typological characteristics, different writing traditions, and other factors. The wide range of cross-linguistic diversity is still a major challenge for NLP, and for the study of language more generally. We use BPE and informationtheoretic measures to investigate if distributions become more similar under specific levels of subword tokenization. We perform a cross-linguistic comparison, following incremental BPE merges (we go from characters to words) for 47 diverse languages. We show that text entropy values (a feature of probability distributions) converge at specific subword levels: relatively few BPE merges (around 200 for our corpus) lead to the most similar distributions across languages. Additionally, we analyze the interaction between subword and word-level distributions and show that our findings can be interpreted in light of the ongoing discussion about different morphological complexity types. 1","Phonology, Morphology and Word Segmentation","[0.11526603251695633, 0.024351783096790314, 0.003555938834324479, 0.013909936882555485, 0.018183495849370956, 0.010486524552106857, 0.0052499920129776, 0.007458815351128578, 0.03510621190071106, 0.004764751996845007, 0.01419877354055643, 0.04108704254031181, 0.5166395902633667, 0.008276586420834064, 0.05988021939992905, 0.045326244086027145, 0.00468589598312974, 0.022901633754372597, 0.004816464614123106, 0.004987719934433699, 0.020314428955316544, 0.011500387452542782, 0.007051497232168913]",0.5166395902633667,"Phonology, Morphology and Word Segmentation",0.5166395902633667,True
"Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information's interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a Slot Information Sharing. The sharing improve the models ability to deduce value from related slots. Our model yields a significantly improved performance compared to previous state-of-the-art models on the Multi-WOZ dataset.",Dialogue and Interactive Systems,"[0.0009998366003856063, 0.002144017955288291, 0.9394683837890625, 0.007004570681601763, 0.0011005247943103313, 0.002275790087878704, 0.00312720681540668, 0.0018353541381657124, 0.0007561268866993487, 0.0016231302870437503, 0.007869639433920383, 0.0016137236962094903, 0.001897210837341845, 0.004442120902240276, 0.0040965513326227665, 0.001442838809452951, 0.0036766291595995426, 0.004292245954275131, 0.0038795373402535915, 0.001204229425638914, 0.0029398268088698387, 0.0009149523684754968, 0.0013956056209281087]",0.9394683837890625,Dialogue and Interactive Systems,0.9394683837890625,True
"Current models for Word Sense Disambiguation (WSD) struggle to disambiguate rare senses, despite reaching human performance on global WSD metrics. This stems from a lack of data for both modeling and evaluating rare senses in existing WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of Word Senses), a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary. FEWS has high sense coverage across different natural language domains and provides: (1) a large training set that covers many more senses than previous datasets and (2) a comprehensive evaluation set containing few-and zero-shot examples of a wide variety of senses. We establish baselines on FEWS with knowledgebased and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with FEWS better capture rare senses in existing WSD datasets. Finally, we find humans outperform the best baseline models on FEWS, indicating that FEWS will support significant future work on low-shot WSD.",Resources and Evaluation,"[0.014637722633779049, 0.013790087774395943, 0.007513632997870445, 0.017860209569334984, 0.009667599573731422, 0.007539473474025726, 0.009660704992711544, 0.017319828271865845, 0.0075296079739928246, 0.0036260103806853294, 0.014295185916125774, 0.0058167739771306515, 0.026362821459770203, 0.009139160625636578, 0.0294899120926857, 0.7090662121772766, 0.024179615080356598, 0.036583349108695984, 0.00438765250146389, 0.002871463308110833, 0.014201353304088116, 0.00764797069132328, 0.006813643500208855]",0.7090662121772766,Semantics: Lexical Semantics,0.0294899120926857,False
"Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom. However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets. Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting RECESS and REMORSE, not just CATS, DOGS and BRIDGES. We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base (LKB). BabelPic explicitly targets nonconcrete concepts, thus providing refreshing new data for the community. We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB. BabelPic is available for download at http://babelpic.org.",Resources and Evaluation,"[0.00718450453132391, 0.018785903230309486, 0.006678292062133551, 0.008542627096176147, 0.013375862501561642, 0.005703761707991362, 0.006752700079232454, 0.007209075149148703, 0.023985588923096657, 0.7033825516700745, 0.003671281272545457, 0.0018782641272991896, 0.004747800063341856, 0.008844646625220776, 0.06585871428251266, 0.015573058277368546, 0.023141568526625633, 0.013949481770396233, 0.03784026950597763, 0.002831144956871867, 0.007243121974170208, 0.007751269731670618, 0.005068489816039801]",0.7033825516700745,"Language Grounding to Vision, Robotics and Beyond",0.06585871428251266,False
"Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines.",Machine Translation and Multilinguality,"[0.0008777192560955882, 0.0011648285435512662, 0.0010505930986255407, 0.00042660595499910414, 0.0027615081053227186, 0.0012147704837843776, 0.0007843768689781427, 0.0014718895545229316, 0.0027170272078365088, 0.0007939052302390337, 0.0017142008291557431, 0.9657220244407654, 0.0021591070108115673, 0.0004906118265353143, 0.004365487955510616, 0.0012307713041082025, 0.000661898753605783, 0.0020488318987190723, 0.002813556930050254, 0.0015810297336429358, 0.0014050607569515705, 0.0017392230220139027, 0.0008051033364608884]",0.9657220244407654,Machine Translation and Multilinguality,0.9657220244407654,True
"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.014828363433480263, 0.006796447094529867, 0.040376391261816025, 0.4913972020149231, 0.006194591987878084, 0.01218598522245884, 0.017291516065597534, 0.020037628710269928, 0.015458893962204456, 0.011206838302314281, 0.01801411621272564, 0.0022574991453438997, 0.0209125354886055, 0.04594625160098076, 0.02428145706653595, 0.027637092396616936, 0.12609031796455383, 0.008417953737080097, 0.0071641746908426285, 0.020340844988822937, 0.04471852630376816, 0.006243566051125526, 0.01220178697258234]",0.4913972020149231,Discourse and Pragmatics,0.12609031796455383,False
"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets. We introduce INQUISITIVE, a dataset of âˆ¼19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 (Radford et al., 2019)  and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUIS-ITIVE questions.",Generation,"[0.010655785910785198, 0.004841332323849201, 0.03312019258737564, 0.014963914640247822, 0.005049576051533222, 0.5657244920730591, 0.0029464999679476023, 0.011912683956325054, 0.008151565678417683, 0.008329056203365326, 0.012384621426463127, 0.0028638013172894716, 0.007737242616713047, 0.15977567434310913, 0.06989999860525131, 0.0044469269923865795, 0.0346994549036026, 0.013829146511852741, 0.004534113686531782, 0.005042506847530603, 0.008558831177651882, 0.005172851961106062, 0.005359758622944355]",0.5657244920730591,Generation,0.5657244920730591,True
"Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoderdecoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines. 1",Summarization,"[0.0008270730031654239, 0.001132223755121231, 0.002071249997243285, 0.004325231071561575, 0.001678831991739571, 0.00819966197013855, 0.0037194795440882444, 0.005243751686066389, 0.0018704350804910064, 0.0019144269172102213, 0.0019276827806606889, 0.002234966726973653, 0.0018749739974737167, 0.0005007105064578354, 0.006200885865837336, 0.0008063328568823636, 0.0015468536876142025, 0.0029325010254979134, 0.005223091691732407, 0.9383543133735657, 0.0023267653305083513, 0.0027832959312945604, 0.0023051768075674772]",0.9383543133735657,Summarization,0.9383543133735657,True
"Video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events. Most previous methods simplify this task by using groundtruth event segments. In this work, we propose a novel framework by taking this task as a text summarization task. We first generate lots of sentence-level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption. Our method does not depend on ground-truth event segments. Experiments on two popular datasets ActivityNet Captions and YouCookII demonstrate the advantages of our new framework. On the ActivityNet dataset, our method even outperforms some previous methods using ground-truth event segment labels.","Language Grounding to Vision, Robotics and Beyond","[0.003827586304396391, 0.01857917197048664, 0.019606858491897583, 0.018307777121663094, 0.007887253537774086, 0.044099606573581696, 0.02235904522240162, 0.01760745979845524, 0.00703032361343503, 0.2144310176372528, 0.008136944845318794, 0.004811869468539953, 0.008017071522772312, 0.008435861207544804, 0.10217424482107162, 0.003940231166779995, 0.016110002994537354, 0.020097799599170685, 0.08578846603631973, 0.3351716101169586, 0.012324686162173748, 0.013021070510149002, 0.008234038949012756]",0.3351716101169586,Summarization,0.2144310176372528,False
"Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units. Previous work has shown that prosody can help with parsing disfluent speech (Tran et al., 2018) , but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn't true in existing speech applications. We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SUbased model). In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model. However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SUbased model (90.79 vs. 90.65 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency -a distinction that the model otherwise struggles to make.",Speech and Multimodality,"[0.005360538139939308, 0.008274009451270103, 0.43016546964645386, 0.023638706654310226, 0.0078530702739954, 0.013908909633755684, 0.0037017175927758217, 0.007820160128176212, 0.009069040417671204, 0.012414485216140747, 0.011766626499593258, 0.02181922271847725, 0.022554943338036537, 0.0062234969809651375, 0.05347403138875961, 0.002947574947029352, 0.013331064954400063, 0.022280409932136536, 0.26691141724586487, 0.027734091505408287, 0.01653701812028885, 0.007400389760732651, 0.004813624545931816]",0.43016546964645386,Dialogue and Interactive Systems,0.26691141724586487,False
"This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 million instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https:// github.com/tag-and-generate/ Introduction Politeness plays a crucial role in social interaction, and is closely tied with power dynamics, social distance between the participants of a conversation, and gender (Brown et al., 1987; Danescu-Niculescu-Mizil et al., 2013) . It is also imperative to use the appropriate level of politeness for smooth communication in conversations (Coppock, 2005) , organizational settings like emails (Peterson et al., 2011) , memos, official documents, and many other settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we study the task of converting non-polite sentences to polite sentences while preserving the meaning.",Generation,"[0.02180859073996544, 0.08018174767494202, 0.06351199746131897, 0.05001537501811981, 0.02312804013490677, 0.11138270050287247, 0.0023372529540210962, 0.004699436482042074, 0.009785504080355167, 0.007634153589606285, 0.013704744167625904, 0.014416860416531563, 0.036264512687921524, 0.007693346589803696, 0.34782055020332336, 0.007524130400270224, 0.016991954296827316, 0.11258890479803085, 0.021693114191293716, 0.016291914507746696, 0.013142078183591366, 0.009641247801482677, 0.007741860579699278]",0.34782055020332336,Resources and Evaluation,0.11138270050287247,False
"Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a Super-Transformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT'14 translation task on Raspberry Pi-4, HAT can achieve 3Ã— speedup, 3.7Ã— smaller size over baseline Transformer; 2.7Ã— speedup, 3.6Ã— smaller size over Evolved Transformer with 12,041Ã— less search cost and no performance loss. HAT is open-sourced.",Machine Translation and Multilinguality,"[0.00454800995066762, 0.003608386032283306, 0.009702174924314022, 0.0020680343732237816, 0.007868094369769096, 0.011876505799591541, 0.0037109507247805595, 0.007154047954827547, 0.06975357979536057, 0.0033275489695370197, 0.5379690527915955, 0.24300739169120789, 0.005699425004422665, 0.0057174284011125565, 0.020137999206781387, 0.004043505992740393, 0.025836823508143425, 0.004675908945500851, 0.008778795599937439, 0.002412972040474415, 0.012683275155723095, 0.0035167059395462275, 0.0019033709540963173]",0.5379690527915955,Machine Learning for NLP,0.24300739169120789,False
"Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But the improvement is limited due to the noise and instability of dependency trees. To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graphbased representations learnt from the corresponding dependency graph in an iterative interaction manner. Specifically, a dualtransformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning. The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.",Machine Learning for NLP,"[0.0007285324390977621, 0.004631785675883293, 0.004116202238947153, 0.0034738178364932537, 0.002113552764058113, 0.0028724682051688433, 0.011894099414348602, 0.006091129034757614, 0.006258468143641949, 0.002478068694472313, 0.0027814405038952827, 0.0026422159280627966, 0.0020335388835519552, 0.001294244546443224, 0.014597207307815552, 0.004313062410801649, 0.0025714749936014414, 0.9088817834854126, 0.004144907463341951, 0.001577348681166768, 0.0072460053488612175, 0.002005350776016712, 0.0012532295659184456]",0.9088817834854126,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.0027814405038952827,False
"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%-3.75% in terms of F 1 scores. The code and data for SeqMix can be found at https://github. com/rz-zhang/SeqMix.",Information Extraction,"[0.0033676535822451115, 0.004924407694488764, 0.014928873628377914, 0.004206306766718626, 0.0057450151070952415, 0.0384465754032135, 0.26925286650657654, 0.07455606758594513, 0.0077215232886374, 0.0041182986460626125, 0.3309330642223358, 0.007584710605442524, 0.005254211835563183, 0.012614055536687374, 0.015315580181777477, 0.007384612690657377, 0.0738125741481781, 0.008979578502476215, 0.004746607970446348, 0.011413109488785267, 0.08319557458162308, 0.005746209062635899, 0.005752487573772669]",0.3309330642223358,Machine Learning for NLP,0.26925286650657654,False
"Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means. With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms. In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions. Thus traditional text-based methods are insufficient to detect multimodal sarcasm. To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context. Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net). The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context. Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.",Speech and Multimodality,"[0.013008561916649342, 0.17105646431446075, 0.029660645872354507, 0.031490594148635864, 0.03200709447264671, 0.005172514356672764, 0.00923206377774477, 0.010120467282831669, 0.03479539602994919, 0.17906533181667328, 0.007155862171202898, 0.007190026342868805, 0.008893558755517006, 0.006824013311415911, 0.11653422564268112, 0.010596228763461113, 0.016816649585962296, 0.07116519659757614, 0.1873282492160797, 0.0208286102861166, 0.006071125622838736, 0.012766771949827671, 0.01222020573914051]",0.1873282492160797,Speech and Multimodality,0.1873282492160797,True
"Word sense disambiguation (WSD) is a longstanding problem in natural language processing. One significant challenge in supervised all-words WSD is to classify among senses for a majority of words that lie in the longtail distribution. For instance, 84% of the annotated words have less than 10 examples in the SemCor training data. This issue is more pronounced as the imbalance occurs in both word and sense distributions. In this work, we propose MetricWSD, a non-parametric few-shot learning approach to mitigate this data imbalance issue. By learning to compute distances among the senses of a given word through episodic training, MetricWSD transfers knowledge (a learned metric space) from high-frequency words to infrequent ones. MetricWSD constructs the training episodes tailored to word frequencies and explicitly addresses the problem of the skewed distribution, as opposed to mixing all the words trained with parametric models in previous work. Without resorting to any lexical resources, MetricWSD obtains strong performance against parametric alternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark (Raganato et al., 2017b) . Our analysis further validates that infrequent words and senses enjoy significant improvement. 1",Semantics: Lexical Semantics,"[0.02209564484655857, 0.012909199111163616, 0.006896303966641426, 0.01533842645585537, 0.01176159642636776, 0.010090276598930359, 0.008240735158324242, 0.02315141074359417, 0.010104076005518436, 0.0032573100179433823, 0.030707886442542076, 0.010989973321557045, 0.03472728282213211, 0.012700129300355911, 0.02633255533874035, 0.6735100150108337, 0.023032156750559807, 0.025406623259186745, 0.004103135317564011, 0.002961238846182823, 0.015871712937951088, 0.008383490145206451, 0.007428862154483795]",0.6735100150108337,Semantics: Lexical Semantics,0.6735100150108337,True
"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.",Discourse and Pragmatics,"[0.0077120293863117695, 0.011633278802037239, 0.06157461553812027, 0.7058413624763489, 0.00523448595777154, 0.006996988318860531, 0.004802145063877106, 0.009697514586150646, 0.0033171349205076694, 0.006626470014452934, 0.007246425375342369, 0.0027870796620845795, 0.010766197927296162, 0.029676254838705063, 0.026427287608385086, 0.012457331642508507, 0.025142358615994453, 0.010853814892470837, 0.007330903317779303, 0.014225093647837639, 0.014512698166072369, 0.005348063074052334, 0.009790495969355106]",0.7058413624763489,Discourse and Pragmatics,0.7058413624763489,True
"This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice. Our code is available online.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.04336374253034592, 0.35656991600990295, 0.0038152297493070364, 0.032915178686380386, 0.2122751772403717, 0.004400181118398905, 0.003647987963631749, 0.006027929484844208, 0.08462231606245041, 0.00834731012582779, 0.021666714921593666, 0.01689976640045643, 0.03059057891368866, 0.008916282095015049, 0.07579401880502701, 0.019758010283112526, 0.009883851744234562, 0.024416286498308182, 0.010353422723710537, 0.003436468541622162, 0.005562528036534786, 0.0074815815314650536, 0.009255548939108849]",0.35656991600990295,Computational Social Science and Social Media,0.024416286498308182,False
"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of an early exit probability. We then introduce SKY-LINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.",Question Answering,"[0.003423224901780486, 0.0022317790426313877, 0.002989624859765172, 0.003148713381960988, 0.0031169040594249964, 0.004018762148916721, 0.002851758385077119, 0.00963720865547657, 0.005417630076408386, 0.004562870133668184, 0.010222741402685642, 0.0015643710503354669, 0.0019097330514341593, 0.9220750331878662, 0.006660202983766794, 0.0015104501508176327, 0.0033475742675364017, 0.0023462388198822737, 0.0015730832237750292, 0.0007215216755867004, 0.0022094992455095053, 0.0015373647911474109, 0.0029237449634820223]",0.9220750331878662,Question Answering,0.9220750331878662,True
"It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during finetuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Textto-SQL semantic parsing and logical reading comprehension. In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state-of-the-art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider 1 . We achieve this by deriving a novel Data-dependent Transformer Fixedupdate initialization scheme (DT-Fixup), inspired by the prior T-Fixup work (Huang et al., 2020) . Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.",Machine Learning for NLP,"[0.0026923774275928736, 0.0020669689401984215, 0.014176801778376102, 0.004693604074418545, 0.0014348893892019987, 0.007736665662378073, 0.00683051161468029, 0.00499656330794096, 0.004229528363794088, 0.002191919134929776, 0.01676723174750805, 0.0015490333316847682, 0.001380230183713138, 0.01120233628898859, 0.007517755497246981, 0.004261798691004515, 0.8659641146659851, 0.0018324750708416104, 0.0027349977754056454, 0.0018550457898527384, 0.030323928222060204, 0.001359947957098484, 0.002201442839577794]",0.8659641146659851,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.01676723174750805,False
"Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose an incidentally supervised model, JEANS , which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a selflearning based alignment learning process to iteratively induce the matching of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs. 1",Semantics: Lexical Semantics,"[0.001725743873976171, 0.003187042661011219, 0.0015417432878166437, 0.002183375181630254, 0.003999296110123396, 0.00435214675962925, 0.8117768168449402, 0.038599368184804916, 0.004913369659334421, 0.0025703406427055597, 0.028301818296313286, 0.007872674614191055, 0.003997856751084328, 0.003114142920821905, 0.006082736887037754, 0.01029007788747549, 0.011688186787068844, 0.004721029195934534, 0.002640374004840851, 0.005155009217560291, 0.032329898327589035, 0.004931134637445211, 0.004025795962661505]",0.8117768168449402,Information Extraction,0.01029007788747549,False
"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.",Machine Learning for NLP,"[0.027928641065955162, 0.015703916549682617, 0.01993134431540966, 0.006392913870513439, 0.012232217006385326, 0.10958876460790634, 0.021925270557403564, 0.01519524585455656, 0.041636861860752106, 0.005503867287188768, 0.185339093208313, 0.020574375987052917, 0.04041096940636635, 0.020216437056660652, 0.12999972701072693, 0.008140956982970238, 0.0727299377322197, 0.020099785178899765, 0.011949882842600346, 0.01709921658039093, 0.17897960543632507, 0.012762117199599743, 0.005658929236233234]",0.185339093208313,Machine Learning for NLP,0.185339093208313,True
"Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformerbased model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.",Interpretability and Analysis of Models for NLP,"[0.01890089549124241, 0.00371457077562809, 0.004269933328032494, 0.007337877992540598, 0.005714524537324905, 0.020497670397162437, 0.005078259855508804, 0.010182544589042664, 0.44118794798851013, 0.0066363876685500145, 0.26870623230934143, 0.008048629388213158, 0.011311319656670094, 0.012237738817930222, 0.02491491474211216, 0.014592515304684639, 0.09446272253990173, 0.005012105219066143, 0.00892847403883934, 0.004130941350013018, 0.01611482724547386, 0.005074113607406616, 0.0029447947163134813]",0.44118794798851013,Interpretability and Analysis of Models for NLP,0.44118794798851013,True
"Automatic personalized corrective feedback can help language learners from different backgrounds better acquire a new language. This paper introduces a learner English dataset in which learner errors are accompanied by information about possible error sources. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset will enable second language acquisition researchers to computationally analyze a large quantity of learner errors that are related to language transfer from the learners' first language. The dataset can also be applied in personalizing grammatical error correction systems according to the learners' first language and in providing feedback that is informed by the cause of an error.",Resources and Evaluation,"[0.07467471808195114, 0.04312308505177498, 0.005875695962458849, 0.007156795356422663, 0.039710041135549545, 0.02949746884405613, 0.00638686865568161, 0.01852666400372982, 0.2187555879354477, 0.03352050855755806, 0.06412598490715027, 0.08193714171648026, 0.03908141329884529, 0.030368555337190628, 0.15795013308525085, 0.020098233595490456, 0.0256207138299942, 0.02046438679099083, 0.024044252932071686, 0.004640069790184498, 0.028784168884158134, 0.016934076324105263, 0.008723495528101921]",0.2187555879354477,Interpretability and Analysis of Models for NLP,0.15795013308525085,False
The contribution of Ms. Eva Katakalou was restricted to the creation and the validation of the datasets as well as to the authoring of the corresponding parts of the manuscript. Figure 1: Number of legislative acts issued by the EU per year. The gold color of the bars indicates how many of the published acts are amendments to older ones. 6 legislation.gov.uk 7 See Appendix A for details on the dataset curation.,Information Retrieval and Text Mining,"[0.010598991066217422, 0.3459441363811493, 0.005795883014798164, 0.01371967513114214, 0.06397946923971176, 0.007740707602351904, 0.01792248897254467, 0.010212736204266548, 0.01934153586626053, 0.008974908851087093, 0.020722771063447, 0.01367765199393034, 0.023658867925405502, 0.00629471754655242, 0.26754137873649597, 0.017987145110964775, 0.013695432804524899, 0.08441057056188583, 0.010058490559458733, 0.006337873637676239, 0.014445346780121326, 0.009372261352837086, 0.007566927466541529]",0.3459441363811493,Computational Social Science and Social Media,0.010212736204266548,False
"Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extracted from the debate text and features encoding the characteristics of the audience are both critical in persuasion studies. In this paper, we aim to further investigate the role of discourse structure of the arguments from online debates in their persuasiveness. In particular, we use the factor graph model to obtain features for the argument structure of debates from an online debating platform and incorporate these features to an LSTM-based model to predict the debater that makes the most convincing arguments. We find that incorporating argument structure features play an essential role in achieving the better predictive performance in assessing the persuasiveness of the arguments in online debates.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0041072736494243145, 0.09907443076372147, 0.004984798841178417, 0.03432442992925644, 0.014465775340795517, 0.006630732212215662, 0.003091336926445365, 0.01068317610770464, 0.012685418128967285, 0.004481123760342598, 0.00491850171238184, 0.002762811491265893, 0.007136636413633823, 0.008756858296692371, 0.0903395488858223, 0.006407943554222584, 0.004618281964212656, 0.6613608598709106, 0.004114954732358456, 0.0032103690318763256, 0.005139228887856007, 0.003254912793636322, 0.0034505994990468025]",0.6613608598709106,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.6613608598709106,True
"We take up the task of largescale evaluation of neural machine transliteration between English and Indian languages, with a focus on multilin gual transliteration to utilize orthographic sim ilarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved mul tilingual training pipeline for Indic languages. We analyse various factors affecting transliter ation quality like language family, translitera tion direction and word origin.",Resources and Evaluation,"[0.006045333109796047, 0.005396721884608269, 0.003939709160476923, 0.002979467622935772, 0.00969634298235178, 0.0055639902129769325, 0.0020204356405884027, 0.0034624601248651743, 0.007970298640429974, 0.0032662581652402878, 0.004257028456777334, 0.8341696858406067, 0.019389385357499123, 0.0018165339715778828, 0.0391593798995018, 0.0032915412448346615, 0.0015014315722510219, 0.007401417475193739, 0.017197024077177048, 0.009916954673826694, 0.0028955924790352583, 0.005502724554389715, 0.003160357242450118]",0.8341696858406067,Machine Translation and Multilinguality,0.0391593798995018,False
"Models pre-trained on large-scale regular text corpora often do not work well for usergenerated data where the language styles differ significantly from the mainstream text. Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST). CARI injects multiple rules into an end-to-end BERT-based encoder and decoder model. It learns to select optimal rules based on context. The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset. Our extrinsic evaluation showed that CARI can greatly improve the regular pretrained models' performance on several tweet sentiment analysis tasks.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.00859257485717535, 0.009244178421795368, 0.04969576746225357, 0.016934538260102272, 0.005672774743288755, 0.3636876046657562, 0.008645604364573956, 0.01051695179194212, 0.02667277865111828, 0.006453340873122215, 0.042766064405441284, 0.005994630977511406, 0.01804927922785282, 0.004417803604155779, 0.10362221300601959, 0.014657878316938877, 0.08147716522216797, 0.1493104100227356, 0.01627751812338829, 0.026018831878900528, 0.01869724877178669, 0.007942792028188705, 0.0046520475298166275]",0.3636876046657562,Generation,0.1493104100227356,False
"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with A Purpose (CAPWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use questionanswer (QA) pairs-a natural expression of information need-from users, instead of reference captions, for both training and postinference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CAP-WAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.","Language Grounding to Vision, Robotics and Beyond","[0.004378488752990961, 0.014901971444487572, 0.01100486982613802, 0.004478096030652523, 0.012381862848997116, 0.02795831859111786, 0.008244071155786514, 0.009149440564215183, 0.01134333573281765, 0.6644271612167358, 0.004748360253870487, 0.004086475819349289, 0.004158517345786095, 0.010840517468750477, 0.10900116711854935, 0.0038274924736469984, 0.008998838253319263, 0.013761555776000023, 0.04180111736059189, 0.010244395583868027, 0.008262035436928272, 0.007927576079964638, 0.004074394702911377]",0.6644271612167358,"Language Grounding to Vision, Robotics and Beyond",0.6644271612167358,True
"We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps: (i) projecting documents of a web domain to a shared multilingual space; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TFÃ—IDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the robustness of CDA in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.",Machine Translation and Multilinguality,"[0.0011470679892227054, 0.0019724708981812, 0.0012905893381685019, 0.0007363647455349565, 0.003940232563763857, 0.002129912143573165, 0.002099759643897414, 0.004523992072790861, 0.0027618445456027985, 0.0010288660414516926, 0.004563178401440382, 0.9389963150024414, 0.003748323768377304, 0.0008740604389458895, 0.010139347054064274, 0.0018494739197194576, 0.001236479263752699, 0.0028996861074119806, 0.003784328233450651, 0.0028584327083081007, 0.0029612653888761997, 0.0031072739511728287, 0.0013508753618225455]",0.9389963150024414,Machine Translation and Multilinguality,0.9389963150024414,True
"We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.1 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.","Syntax: Tagging, Chunking and Parsing","[0.028720485046505928, 0.012083680368959904, 0.01712164469063282, 0.02081235870718956, 0.005870284978300333, 0.044993456453084946, 0.007307410705834627, 0.01353051234036684, 0.0423286072909832, 0.007351716514676809, 0.10458110272884369, 0.010434265248477459, 0.028579263016581535, 0.032174091786146164, 0.038415540009737015, 0.022305065765976906, 0.22231830656528473, 0.007204574532806873, 0.005976386833935976, 0.010453414171934128, 0.3036606013774872, 0.007537501398473978, 0.0062396894209086895]",0.3036606013774872,"Syntax: Tagging, Chunking and Parsing",0.3036606013774872,True
"We propose a Transformer-based sequence-tosequence model for automatic speech recognition (ASR) capable of simultaneously transcribing and annotating audio with linguistic information such as phonemic transcripts or part-of-speech (POS) tags. Since linguistic information is important in natural language processing (NLP), the proposed ASR is especially useful for speech interface applications, including spoken dialogue systems and speech translation, which combine ASR and NLP. To produce linguistic annotations, we train the ASR system using modified training targets: each grapheme or multi-grapheme unit in the target transcript is followed by an aligned phoneme sequence and/or POS tag. Since our method has access to the underlying audio data, we can estimate linguistic annotations more accurately than pipeline approaches in which NLP-based methods are applied to a hypothesized ASR transcript. Experimental results on Japanese and English datasets show that the proposed ASR system is capable of simultaneously producing highquality transcriptions and linguistic annotations.",Speech and Multimodality,"[0.0038763489574193954, 0.005737732630223036, 0.05794727802276611, 0.008419194258749485, 0.007587478496134281, 0.012368553318083286, 0.007082059979438782, 0.008379790931940079, 0.007709315977990627, 0.02383316680788994, 0.006715110503137112, 0.02461535856127739, 0.008505347184836864, 0.0024492526426911354, 0.01900671422481537, 0.002325410023331642, 0.00643352884799242, 0.008905396796762943, 0.718638002872467, 0.044151198118925095, 0.004556702449917793, 0.007063342723995447, 0.0036938462872058153]",0.718638002872467,Speech and Multimodality,0.718638002872467,True
"Expressive text encoders such as RNNs and Transformer Networks have been at the center of NLP models in recent work. Most of the effort has focused on sentence-level tasks, capturing the dependencies between words in a single sentence, or pairs of sentences. However, certain tasks, such as argumentation mining, require accounting for longer texts and complicated structural dependencies between them. Deep structured prediction is a general framework to combine the complementary strengths of expressive neural encoders and structured inference for highly structured domains. Nevertheless, when the need arises to go beyond sentences, most work relies on combining the output scores of independently trained classifiers. One of the main reasons for this is that constrained inference comes at a high computational cost. In this paper, we explore the use of randomized inference to alleviate this concern and show that we can efficiently leverage deep structured prediction and expressive neural encoders for a set of tasks involving complicated argumentative structures.",Machine Learning for NLP,"[0.005897661671042442, 0.009705786593258381, 0.027200311422348022, 0.024474812671542168, 0.0053150467574596405, 0.053075626492500305, 0.03524608910083771, 0.04925505816936493, 0.06319336593151093, 0.0052037485875189304, 0.18886440992355347, 0.0048433942720294, 0.004984039813280106, 0.052534155547618866, 0.04335172474384308, 0.008893349207937717, 0.0846576914191246, 0.2676122486591339, 0.0053817881271243095, 0.005984884686768055, 0.04523284733295441, 0.0053575122728943825, 0.003734365338459611]",0.2676122486591339,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.18886440992355347,False
"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers. 1",Machine Learning for NLP,"[0.028032179921865463, 0.015593224205076694, 0.004211443476378918, 0.006557696033269167, 0.022904587909579277, 0.018489327281713486, 0.0021436717361211777, 0.011225496418774128, 0.4319274425506592, 0.009766466915607452, 0.1605624109506607, 0.0243084579706192, 0.009813942015171051, 0.026652779430150986, 0.1282266229391098, 0.009492365643382072, 0.050520218908786774, 0.007548740599304438, 0.006271266378462315, 0.0030305783730000257, 0.01245117001235485, 0.0063089532777667046, 0.003961037844419479]",0.4319274425506592,Interpretability and Analysis of Models for NLP,0.1605624109506607,False
"Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice. â€  * Work done during internship at BorealisAI â€  Code at https://github.com/BorealisAI/code-gen-TAE","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.002272946760058403, 0.0018088370561599731, 0.008485160768032074, 0.005117529071867466, 0.0014093572972342372, 0.00438604224473238, 0.007094866130501032, 0.00239803665317595, 0.002582352375611663, 0.00197921902872622, 0.006829484365880489, 0.0024027375038713217, 0.0017808920238167048, 0.004287777002900839, 0.006386508699506521, 0.008734703063964844, 0.9023074507713318, 0.002157944953069091, 0.002180685056373477, 0.001494812429882586, 0.019555244594812393, 0.0013700781855732203, 0.00297734746709466]",0.9023074507713318,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.9023074507713318,True
"This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010) . This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.",Information Retrieval and Text Mining,"[0.006861334666609764, 0.005681198555976152, 0.0090269660577178, 0.02082078345119953, 0.007316596806049347, 0.049468014389276505, 0.022396404296159744, 0.37608346343040466, 0.037707168608903885, 0.004871327430009842, 0.19340644776821136, 0.010463031940162182, 0.0064674862660467625, 0.01513319555670023, 0.011533965356647968, 0.04109049215912819, 0.11426891386508942, 0.00974357035011053, 0.004338859114795923, 0.013435391709208488, 0.025539282709360123, 0.008387417532503605, 0.005958644673228264]",0.37608346343040466,Information Retrieval and Text Mining,0.37608346343040466,True
"We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches. 1",Generation,"[0.00914150569587946, 0.010684715583920479, 0.03614387661218643, 0.009570267051458359, 0.005066641140729189, 0.3753126859664917, 0.004428267478942871, 0.01092316210269928, 0.006251486949622631, 0.0072643752209842205, 0.027050809934735298, 0.011205621063709259, 0.013972085900604725, 0.015183975920081139, 0.07855040580034256, 0.00993505772203207, 0.23425248265266418, 0.01255718618631363, 0.0072819809429347515, 0.025094853714108467, 0.07681513577699661, 0.00836234726011753, 0.004950974602252245]",0.3753126859664917,Generation,0.3753126859664917,True
"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance. To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows. Through the attention flows, At-tnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context. Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDi-alKG dataset. Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.",Dialogue and Interactive Systems,"[0.0009065846679732203, 0.0014111364725977182, 0.9493898153305054, 0.0056143952533602715, 0.0009585113730281591, 0.002496096072718501, 0.0018620226765051484, 0.0014785361709073186, 0.0009458124404773116, 0.0013702531578019261, 0.0069994498044252396, 0.0017113885842263699, 0.0017063462873920798, 0.002970125526189804, 0.0037995134480297565, 0.0009484521578997374, 0.0022329490166157484, 0.00397083954885602, 0.0034920484758913517, 0.0013212006306275725, 0.0024040495045483112, 0.0008470231550745666, 0.001163297682069242]",0.9493898153305054,Dialogue and Interactive Systems,0.9493898153305054,True
"Data privacy is an important issue for ''machine learning as a service'' providers. We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model's API, determine whether the sample existed in the model's training data. Our contribution is an investigation of this problem in the context of sequence-tosequence models, which are important in applications such as machine translation and video captioning. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.",Interpretability and Analysis of Models for NLP,"[0.007376544643193483, 0.0061313798651099205, 0.0075988974422216415, 0.00343033648096025, 0.013111193664371967, 0.05899015814065933, 0.00845571793615818, 0.013613014481961727, 0.039715658873319626, 0.013197369873523712, 0.647784411907196, 0.042755842208862305, 0.006283935625106096, 0.006099402438849211, 0.04893830046057701, 0.0030055211391299963, 0.028965337201952934, 0.00620324956253171, 0.009102581068873405, 0.005554355680942535, 0.016180982813239098, 0.005013233050704002, 0.002492489293217659]",0.647784411907196,Machine Learning for NLP,0.039715658873319626,False
"In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance. Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model.",Summarization,"[0.0012136900331825018, 0.0015665574464946985, 0.0022325103636831045, 0.009803392924368382, 0.0019733256194740534, 0.005835351534187794, 0.005900458432734013, 0.005989788565784693, 0.0023237725254148245, 0.0022866802755743265, 0.0016767310444265604, 0.002393146511167288, 0.002703574951738119, 0.0009442276787012815, 0.00904736015945673, 0.0010471486020833254, 0.0017386318650096655, 0.004284966737031937, 0.005667290184646845, 0.9214654564857483, 0.0032275451812893152, 0.0033533163368701935, 0.003325030440464616]",0.9214654564857483,Summarization,0.9214654564857483,True
"Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a highresource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both languageand syntax-independence. Consequently, our model narrows the gap in zero-shot crosslingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness.",Question Answering,"[0.004956243559718132, 0.003193881595507264, 0.007486509624868631, 0.005316403228789568, 0.006452500820159912, 0.01224143709987402, 0.010195070877671242, 0.019030673429369926, 0.009501582011580467, 0.007700907066464424, 0.024416165426373482, 0.012120618484914303, 0.0030024764128029346, 0.8276914954185486, 0.012494604103267193, 0.0021381527185440063, 0.006547338329255581, 0.005534261930733919, 0.003759972285479307, 0.0016332520171999931, 0.005697774700820446, 0.0032619729172438383, 0.0056266807951033115]",0.8276914954185486,Question Answering,0.8276914954185486,True
"Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples. 1",Information Extraction,"[0.00065633311169222, 0.0016176765784621239, 0.0008954978547990322, 0.0014708421658724546, 0.0012235055910423398, 0.0015351606998592615, 0.9383265972137451, 0.012061692774295807, 0.001376502332277596, 0.0015828108880668879, 0.0033156389836221933, 0.0010698175756260753, 0.00139148673042655, 0.0022052505519241095, 0.00255132676102221, 0.002028673654422164, 0.004536109510809183, 0.0040278867818415165, 0.001462385873310268, 0.003381769172847271, 0.009929240681231022, 0.0017538073007017374, 0.0016000311588868499]",0.9383265972137451,Information Extraction,0.9383265972137451,True
"Text representation models are prone to exhibit a range of societal biases, reflecting the noncontrolled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final performance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender, race, religion, and queerness. Further, we develop an evaluation framework which simultaneously 1) measures bias on the developed REDDITBIAS resource, and 2) evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.",Ethics and NLP,"[0.007350192405283451, 0.012622612528502941, 0.697282075881958, 0.028750723227858543, 0.013283190317451954, 0.014144491404294968, 0.002401025267317891, 0.005421762354671955, 0.029579687863588333, 0.008805351331830025, 0.03856475651264191, 0.005441703367978334, 0.007263042964041233, 0.010392924770712852, 0.050405245274305344, 0.004366337321698666, 0.018239568918943405, 0.018079929053783417, 0.010933320969343185, 0.002833743579685688, 0.0063895583152771, 0.003194570541381836, 0.004254133440554142]",0.697282075881958,Dialogue and Interactive Systems,0.013283190317451954,False
"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model's action token distribution is used to perform observation pruning that removes irrelevant tokens. A second bootstrapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes. 1",Machine Learning for NLP,"[0.008571049198508263, 0.010552397929131985, 0.05560176819562912, 0.00954378955066204, 0.010211030021309853, 0.03423687070608139, 0.012604878284037113, 0.010474342852830887, 0.025171294808387756, 0.5860710740089417, 0.044454123824834824, 0.007202032022178173, 0.007479281630367041, 0.028999481350183487, 0.03354659304022789, 0.007807330694049597, 0.02720208279788494, 0.013096404261887074, 0.026520997285842896, 0.005888374987989664, 0.01803271658718586, 0.010331020690500736, 0.006401063874363899]",0.5860710740089417,"Language Grounding to Vision, Robotics and Beyond",0.044454123824834824,False
"Understanding privacy policies is crucial for users as it empowers them to learn about the information that matters to them. Sentences written in a privacy policy document explain privacy practices, and the constituent text spans convey further specific information about that practice. We refer to predicting the privacy practice explained in a sentence as intent classification and identifying the text spans sharing specific information as slot filling. In this work, we propose PolicyIE, an English corpus consisting of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of websites and mobile applications. PolicyIE corpus is a challenging real-world benchmark with limited labeled examples reflecting the cost of collecting large-scale annotations from domain experts. We present two alternative neural approaches as baselines, (1) intent classification and slot filling as a joint sequence tagging and (2) modeling them as a sequence-tosequence (Seq2Seq) learning task. The experiment results show that both approaches perform comparably in intent classification, while the Seq2Seq method outperforms the sequence tagging approach in slot filling by a large margin. We perform a detailed error analysis to reveal the challenges of the proposed corpus.",Dialogue and Interactive Systems,"[0.011273217387497425, 0.1527259647846222, 0.01846645399928093, 0.012649184092879295, 0.050194598734378815, 0.017449984326958656, 0.16291764378547668, 0.04232581704854965, 0.013668536208570004, 0.007445644587278366, 0.11961059272289276, 0.010200739838182926, 0.008387265726923943, 0.025177204981446266, 0.13882756233215332, 0.010676046833395958, 0.07776885479688644, 0.0479385182261467, 0.016768136993050575, 0.010439342819154263, 0.025961901992559433, 0.007850081659853458, 0.011276690289378166]",0.16291764378547668,Information Extraction,0.01846645399928093,False
"Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Finetuned language models using large-scale indomain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0067011830396950245, 0.2087773084640503, 0.005987153388559818, 0.02677535079419613, 0.01922583021223545, 0.006870458368211985, 0.004087344277650118, 0.006545407231897116, 0.011206468567252159, 0.004583097528666258, 0.005940693896263838, 0.005003734957426786, 0.01447272952646017, 0.0052271350286901, 0.12046781927347183, 0.007492221891880035, 0.005192724987864494, 0.5072182416915894, 0.008322722278535366, 0.003454872639849782, 0.0061128390952944756, 0.005406840704381466, 0.004927853122353554]",0.5072182416915894,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.5072182416915894,True
"We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pretrained BART models on a large number of potential labels generated by state of the art nonneural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.",Machine Learning for NLP,"[0.0051696100272238255, 0.004489989951252937, 0.005752135533839464, 0.009624620899558067, 0.009312054142355919, 0.031671084463596344, 0.03302232176065445, 0.45953625440597534, 0.051398519426584244, 0.00524824857711792, 0.2391352653503418, 0.008206209167838097, 0.0035595649387687445, 0.01137500535696745, 0.008084887638688087, 0.020445624366402626, 0.0443803109228611, 0.011366146616637707, 0.004492311272770166, 0.010403240099549294, 0.011290239170193672, 0.006944903638213873, 0.005091377999633551]",0.45953625440597534,Information Retrieval and Text Mining,0.2391352653503418,False
"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies' earnings calls are well studied for risk modeling, offering unique investment insight into stock performance. However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk. Additionally, most existing approaches ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.",Speech and Multimodality,"[0.02681172639131546, 0.23596599698066711, 0.024176590144634247, 0.038927122950553894, 0.025397662073373795, 0.008344621397554874, 0.05448418855667114, 0.02600613608956337, 0.09220568835735321, 0.03201063722372055, 0.08251077681779861, 0.008100868202745914, 0.03752938657999039, 0.01856076531112194, 0.05729558318853378, 0.023028336465358734, 0.013839855790138245, 0.05347520112991333, 0.08437766879796982, 0.01787463389337063, 0.010760104283690453, 0.016291702166199684, 0.012024716474115849]",0.23596599698066711,Computational Social Science and Social Media,0.08437766879796982,False
"Novel Object Captioning is a zero-shot Image Captioning task requiring describing objects not seen in the training captions, but for which information is available from external object detectors. The key challenge is to select and describe all salient detected novel objects in the input images. In this paper, we focus on this challenge and propose the ECOL-R model (Encouraging Copying of Object Labels with Reinforced Learning), a copy-augmented transformer model that is encouraged to accurately describe the novel object labels. This is achieved via a specialised reward function in the SCST reinforcement learning framework (Rennie et al., 2017) that encourages novel object mentions while maintaining the caption quality. We further restrict the SCST training to the images where detected objects are mentioned in reference captions to train the ECOL-R model. We additionally improve our copy mechanism via Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which determines the appropriate inflected forms of novel object labels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016)  benchmarks.","Language Grounding to Vision, Robotics and Beyond","[0.007134884130209684, 0.007923321798443794, 0.01791185326874256, 0.009012710303068161, 0.013138986192643642, 0.06511902809143066, 0.013312015682458878, 0.01594158075749874, 0.018867485225200653, 0.48975256085395813, 0.013088611885905266, 0.006106536369770765, 0.010473123751580715, 0.008944708853960037, 0.15125350654125214, 0.007081868592649698, 0.010920263826847076, 0.017988059669733047, 0.05663512274622917, 0.029678693041205406, 0.01238784659653902, 0.010929606854915619, 0.0063975476659834385]",0.48975256085395813,"Language Grounding to Vision, Robotics and Beyond",0.48975256085395813,True
"Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of 'probing' tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks. 1 * Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion.",Interpretability and Analysis of Models for NLP,"[0.02770852856338024, 0.004523534327745438, 0.004368993919342756, 0.006526699289679527, 0.00646827882155776, 0.01733626425266266, 0.0035730667877942324, 0.005923064425587654, 0.5549554228782654, 0.017407963052392006, 0.06868249922990799, 0.005113307852298021, 0.0075475010089576244, 0.017384586855769157, 0.021321063861250877, 0.011414662003517151, 0.17963364720344543, 0.0035381626803427935, 0.006583002861589193, 0.0025681783445179462, 0.019037608057260513, 0.005551469977945089, 0.0028325188905000687]",0.5549554228782654,Interpretability and Analysis of Models for NLP,0.5549554228782654,True
"Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their robustness to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when stateof-the-art systems are not robust to counterfactuals. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between robustness and generalization, demonstrating the predictive power of RAD for performance on unseen augmentations. 1  Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, and Giedrius Burachas. 2019. Sunny and dark outside?! improving answer consistency in vqa through entailed question generation. In EMNLP.","Language Grounding to Vision, Robotics and Beyond","[0.009116321802139282, 0.0067972405813634396, 0.00846946518868208, 0.009897064417600632, 0.009553618729114532, 0.01303785014897585, 0.0069846841506659985, 0.00949211698025465, 0.037402957677841187, 0.4278472661972046, 0.010085860267281532, 0.0039679971523582935, 0.004502363037317991, 0.327903151512146, 0.03567281365394592, 0.005908085033297539, 0.02062535472214222, 0.006091348361223936, 0.018470607697963715, 0.0028993715532124043, 0.00801880843937397, 0.00858378130942583, 0.008671966381371021]",0.4278472661972046,"Language Grounding to Vision, Robotics and Beyond",0.4278472661972046,True
"This paper describes the first report on crosslingual transfer for semantic dependency parsing. We present the insight that there are two different kinds of cross-linguality, namely surface level and semantic level, and try to capture both kinds of cross-linguality by combining annotation projection and model transfer of pre-trained language models. Our experiments showed that the performance of our graph-based semantic dependency parser almost achieved the approximated upper bound.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.002922459039837122, 0.00381472148001194, 0.00982421450316906, 0.011653282679617405, 0.002222299575805664, 0.00428213644772768, 0.029859967529773712, 0.00555863231420517, 0.0034449384547770023, 0.003707579104229808, 0.005887828301638365, 0.004443794023245573, 0.003893989371135831, 0.004378344863653183, 0.009399719536304474, 0.02352435514330864, 0.7865006327629089, 0.007808378431946039, 0.004211557097733021, 0.0041854786686599255, 0.05969583988189697, 0.002628859132528305, 0.006151038687676191]",0.7865006327629089,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.7865006327629089,True
"Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.",Machine Learning for NLP,"[0.005390475038439035, 0.005591643508523703, 0.0055467295460402966, 0.00206049345433712, 0.00767681235447526, 0.006418573670089245, 0.005206505302339792, 0.010711885057389736, 0.05788526311516762, 0.0022319855634123087, 0.7614562511444092, 0.01210938859730959, 0.0037689998280256987, 0.005436388775706291, 0.02177252061665058, 0.005756048019975424, 0.0485021136701107, 0.004552151542156935, 0.0057039628736674786, 0.0021767248399555683, 0.016193142160773277, 0.0022127381525933743, 0.0016391058452427387]",0.7614562511444092,Machine Learning for NLP,0.7614562511444092,True
"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.","Phonology, Morphology and Word Segmentation","[0.037262868136167526, 0.011173118837177753, 0.012391884811222553, 0.030344737693667412, 0.01142257172614336, 0.016331177204847336, 0.009139304049313068, 0.01109126303344965, 0.017405498772859573, 0.007568598724901676, 0.033119477331638336, 0.035077065229415894, 0.5834580063819885, 0.017516417428851128, 0.03752683848142624, 0.029716333374381065, 0.010631822980940342, 0.010688373818993568, 0.012549794279038906, 0.0142970597371459, 0.03277033194899559, 0.009079629555344582, 0.009437846019864082]",0.5834580063819885,"Phonology, Morphology and Word Segmentation",0.5834580063819885,True
"Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification. We argue that this division has become counterproductive and propose a new unified framework to remedy the situation. We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them. We perform experiments on five datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines. Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0011954897781834006, 0.009758667089045048, 0.0030669209081679583, 0.006548136007040739, 0.0025184282567352057, 0.004644366912543774, 0.011188537813723087, 0.005507327616214752, 0.005565323866903782, 0.0019462640630081296, 0.002536842366680503, 0.002077854936942458, 0.002977384952828288, 0.0019917809404432774, 0.0294939111918211, 0.0036007692106068134, 0.0033649825491011143, 0.8804072141647339, 0.0029712424147874117, 0.003463600529357791, 0.011234235018491745, 0.0022652444895356894, 0.001675303908996284]",0.8804072141647339,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.8804072141647339,True
"Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available at https://gitlab.com/erniecyc/ few-selector. We hope that this work will call for more attention on this largely unexplored area.",Generation,"[0.0025303377769887447, 0.001118538435548544, 0.007243710570037365, 0.0016036333981901407, 0.002707621082663536, 0.8620371222496033, 0.0023923583794385195, 0.012265671975910664, 0.0035729778464883566, 0.0016418549930676818, 0.03873509168624878, 0.004440616350620985, 0.003308402607217431, 0.0030986578203737736, 0.01566617377102375, 0.0013417111476883292, 0.01202464196830988, 0.003537980141118169, 0.0024116323329508305, 0.00889559742063284, 0.004780926275998354, 0.002832170110195875, 0.0018127289367839694]",0.8620371222496033,Generation,0.8620371222496033,True
"Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its ""core classes"", and then check core classes' ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document's core classes and utilizes confident core classes to train a taxonomyenhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%.",Information Retrieval and Text Mining,"[0.005144472233951092, 0.007003903854638338, 0.012922066263854504, 0.006853404920548201, 0.00749177997931838, 0.033871494233608246, 0.0475359782576561, 0.27072107791900635, 0.011812454089522362, 0.004419218748807907, 0.22201116383075714, 0.0037315920926630497, 0.002785947872325778, 0.02155410312116146, 0.019646549597382545, 0.01936643011868, 0.21134862303733826, 0.014434576965868473, 0.005769032984972, 0.008269806392490864, 0.04985838755965233, 0.0061224098317325115, 0.0073255631141364574]",0.27072107791900635,Information Retrieval and Text Mining,0.27072107791900635,True
"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.",Question Answering,"[0.002648219233378768, 0.001960286172106862, 0.0068574449978768826, 0.003380585927516222, 0.0022132599260658026, 0.005630966275930405, 0.0038564708083868027, 0.006803074385970831, 0.0035942764952778816, 0.004168157000094652, 0.01600695215165615, 0.001182576990686357, 0.0012502969475463033, 0.9137202501296997, 0.005991509184241295, 0.0015594454016536474, 0.007978654466569424, 0.0019344656029716134, 0.0018390289042145014, 0.000625300221145153, 0.0027456802781671286, 0.0012785852886736393, 0.0027745782863348722]",0.9137202501296997,Question Answering,0.9137202501296997,True
"A common issue in real-world applications of named entity recognition and classification (NERC) is the absence of annotated data for target entity classes during training. Zeroshot learning approaches address this issue by learning models that can transfer information from observed classes in the training data to unseen classes. This paper presents the first approach for zero-shot NERC, introducing a novel architecture that leverage the fact that textual descriptions for many entity classes occur naturally. Our architecture addresses the zero-shot NERC specific challenge that the not-an-entity class is not well defined, since different entity classes are considered in training and testing. For evaluation, we adapt two datasets, OntoNotes and MedMentions, emulating the difficulty of real-world zero-shot learning by testing models on the rarest entity classes. Our proposed approach outperforms baselines adapted from machine reading comprehension and zero-shot text classification. Furthermore, we assess the effect of different class descriptions for this task.",Information Extraction,"[0.001307399827055633, 0.0024590585380792618, 0.0021641748026013374, 0.0017790966667234898, 0.0024486479815095663, 0.005860690027475357, 0.8213901519775391, 0.039239976555109024, 0.0036174149718135595, 0.002505591604858637, 0.02392476238310337, 0.0020356576424092054, 0.0021927738562226295, 0.004619706887751818, 0.006481203716248274, 0.0043479036539793015, 0.016382580623030663, 0.006128224078565836, 0.0022371679078787565, 0.004189368803054094, 0.03809652477502823, 0.0035475168842822313, 0.003044357756152749]",0.8213901519775391,Information Extraction,0.8213901519775391,True
"Multimodal summarization becomes increasingly significant as it is the basis for question answering, Web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we present a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in English from CNN and Daily Mail. To our best knowledge, this is the first collection that spans all modalities and nearly comprises all types of materials available in this community. In addition, we devise a baseline model based on the novel dataset, which employs a newly proposed Jump-Attention mechanism based on transcripts. The experimental results validate the important assistance role of the external information for multimodal summarization.",Summarization,"[0.001787611865438521, 0.0050460584461688995, 0.006725880783051252, 0.013024969957768917, 0.0037497638259083033, 0.007409159559756517, 0.005483120679855347, 0.007272733841091394, 0.0031445890199393034, 0.007201184518635273, 0.0030932456720620394, 0.0023603285662829876, 0.0036666332744061947, 0.001315104542300105, 0.024289162829518318, 0.0010539023205637932, 0.0025238716043531895, 0.006942104082554579, 0.021202119067311287, 0.8617275953292847, 0.002985432744026184, 0.0037380883004516363, 0.00425735441967845]",0.8617275953292847,Summarization,0.8617275953292847,True
"Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations. To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision. Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision. Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weaklysupervised baselines, and is also comparable to the supervised method.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0006773514323867857, 0.004489910788834095, 0.003868740750476718, 0.003211663570255041, 0.0022823645267635584, 0.0035152395721524954, 0.009868276305496693, 0.006663030944764614, 0.007498873397707939, 0.0024049074854701757, 0.0030979691073298454, 0.004609005991369486, 0.0021445155143737793, 0.0013944604434072971, 0.019365988671779633, 0.0031968564726412296, 0.0016218433156609535, 0.9019672870635986, 0.00502245407551527, 0.002841060748323798, 0.006304340902715921, 0.0024883970618247986, 0.0014655152335762978]",0.9019672870635986,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.9019672870635986,True
"The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. The losses are then back-propagated to better calibrate our model via multi-task training. Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline. Our code is available at http://github.com/ Soistesimmer/AMR-multiview.",Generation,"[0.002573886886239052, 0.0012171220732852817, 0.00795283168554306, 0.0012999695027247071, 0.002174034947529435, 0.8931553363800049, 0.0021317587234079838, 0.0048398543149232864, 0.002332864096388221, 0.0029153807554394007, 0.02271272987127304, 0.002160667208954692, 0.003099336987361312, 0.0029190434142947197, 0.01108141615986824, 0.0012767695588991046, 0.01639426127076149, 0.0024495243560522795, 0.002013446530327201, 0.00592844607308507, 0.005203896667808294, 0.002485723467543721, 0.0016816623974591494]",0.8931553363800049,Generation,0.8931553363800049,True
"As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graphbased neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a singledocument setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github 1 .",Summarization,"[0.0012739280937239528, 0.002218216424807906, 0.0020770872943103313, 0.007036514114588499, 0.0027304752729833126, 0.006889654323458672, 0.012625197879970074, 0.012176460586488247, 0.003132764482870698, 0.0024696069303900003, 0.0026012505404651165, 0.003298161318525672, 0.0029341827612370253, 0.0008556664106436074, 0.011480665765702724, 0.0015120445750653744, 0.0023540782276540995, 0.007889676839113235, 0.004937273450195789, 0.8969591856002808, 0.004588708747178316, 0.004316648002713919, 0.0036424584686756134]",0.8969591856002808,Summarization,0.8969591856002808,True
"Fake news articles often stir the readers' attention by means of emotional appeals that arouse their feelings. Unlike in short news texts, authors of longer articles can exploit such affective factors to manipulate readers by adding exaggerations or fabricating events, in order to affect the readers' emotions. To capture this, we propose in this paper to model the flow of affective information in fake news articles using a neural architecture. The proposed model, FakeFlow, learns this flow by combining topic and affective information extracted from text. We evaluate the model's performance with several experiments on four real-world datasets. The results show that FakeFlow achieves superior results when compared against state-ofthe-art methods, thus confirming the importance of capturing the flow of the affective information in news articles.",Machine Learning for NLP,"[0.0067192926071584225, 0.505423903465271, 0.004663355182856321, 0.02982461079955101, 0.02671516127884388, 0.0044049727730453014, 0.009324227459728718, 0.01624656282365322, 0.018339326605200768, 0.012938609346747398, 0.008245465345680714, 0.00439482694491744, 0.0071760136634111404, 0.006487642414867878, 0.10991878062486649, 0.008218078874051571, 0.007081546355038881, 0.1607440561056137, 0.02204631082713604, 0.016521086916327477, 0.00392806064337492, 0.00566609064117074, 0.004972066264599562]",0.505423903465271,Computational Social Science and Social Media,0.008245465345680714,False
"Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.",Machine Learning for NLP,"[0.004783789161592722, 0.0029233251698315144, 0.00682428851723671, 0.005636756774038076, 0.0043773832730948925, 0.04241589084267616, 0.027636025100946426, 0.15075373649597168, 0.029257100075483322, 0.002810810226947069, 0.49719756841659546, 0.009614381939172745, 0.003726963885128498, 0.007047148887068033, 0.0072584375739097595, 0.010167534463107586, 0.13021212816238403, 0.0042864419519901276, 0.0089179128408432, 0.01588781177997589, 0.018804363906383514, 0.005747350864112377, 0.0037128711119294167]",0.49719756841659546,Machine Learning for NLP,0.49719756841659546,True
"Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019)  usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. EN-PAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pretrain an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.",Information Extraction,"[0.000582247506827116, 0.0013083619996905327, 0.001031446852721274, 0.0014706276124343276, 0.0010336723644286394, 0.0012991599505767226, 0.9424866437911987, 0.008539674803614616, 0.001292235217988491, 0.0017449152655899525, 0.002638719044625759, 0.0010244648437947035, 0.0014848358696326613, 0.002014350611716509, 0.002530599944293499, 0.002040172228589654, 0.004906060639768839, 0.0033184972126036882, 0.0015230914577841759, 0.002900731284171343, 0.011211598291993141, 0.001948556979186833, 0.0016693009529262781]",0.9424866437911987,Information Extraction,0.9424866437911987,True
"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",Machine Learning for NLP,"[0.008287972770631313, 0.012701260857284069, 0.004850578494369984, 0.007349507883191109, 0.018271030858159065, 0.014497232623398304, 0.009568358771502972, 0.027775077149271965, 0.10555842518806458, 0.0020000680815428495, 0.6421448588371277, 0.012955174781382084, 0.008502611890435219, 0.008214175701141357, 0.032592833042144775, 0.010199827142059803, 0.03089011088013649, 0.008756086230278015, 0.0037834299728274345, 0.004818068351596594, 0.019133679568767548, 0.0038931285962462425, 0.003256435738876462]",0.6421448588371277,Machine Learning for NLP,0.6421448588371277,True
"Social networks face a major challenge in the form of rumors and fake news, due to their intrinsic nature of connecting users to millions of others, and of giving any individual the power to post anything. Given the rapid, widespread dissemination of information in social networks, manually detecting suspicious news is sub-optimal. Thus, research on automatic rumor detection has become a necessity. Previous works in the domain have utilized the reply relations between posts, as well as the semantic similarity between the main post and its context, consisting of replies, in order to obtain state-of-the-art performance. In this work, we demonstrate that semantic oppositeness can improve the performance on the task of rumor detection. We show that semantic oppositeness captures elements of discord, which are not properly covered by previous efforts, which only utilize semantic similarity or reply structure. Our proposed model learns both explicit and implicit relations between the main tweet and its replies, by utilizing both semantic similarity and semantic oppositeness. Both of these employ the self-attention mechanism in neural text modeling, with semantic oppositeness utilizing word-level self-attention, and with semantic similarity utilizing post-level self-attention. We show, with extensive experiments on recent data sets for this problem, that our proposed model achieves state-of-theart performance. Further, we show that our model is more resistant to the variances in performance introduced by randomness.",Computational Social Science and Social Media,"[0.008979755453765392, 0.6994745135307312, 0.004098446574062109, 0.022930920124053955, 0.041745707392692566, 0.0035446863621473312, 0.003787366673350334, 0.0068792314268648624, 0.008771300315856934, 0.005249056499451399, 0.01034089457243681, 0.002462534001097083, 0.006506914272904396, 0.005592306610196829, 0.06823191791772842, 0.007363332435488701, 0.006082084961235523, 0.0671243891119957, 0.0041775573045015335, 0.004347231704741716, 0.003640206763520837, 0.003701580222696066, 0.0049680741503834724]",0.6994745135307312,Computational Social Science and Social Media,0.6994745135307312,True
"Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical selfdisclosure in online communities is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (NO SELF-DISCLOSURE, POSSI-BLE SELF-DISCLOSURE, and CLEAR SELF-DISCLOSURE) with high inter-annotator agreement (Îº = 0.88). We make this data available to the research community. We also release a predictive model trained on this dataset that achieves an accuracy of 81.02%, establishing a strong performance benchmark for this task.",Resources and Evaluation,"[0.00930088572204113, 0.6133438348770142, 0.012550102546811104, 0.04141772910952568, 0.04367248713970184, 0.0059975082986056805, 0.009654931724071503, 0.010563086718320847, 0.007608197163790464, 0.0055166445672512054, 0.01201895996928215, 0.0021498999558389187, 0.005428939592093229, 0.026641277596354485, 0.09278929978609085, 0.008014248684048653, 0.015851760283112526, 0.049897655844688416, 0.0073613994754850864, 0.004077838268131018, 0.004417185205966234, 0.003967953845858574, 0.007758192252367735]",0.6133438348770142,Computational Social Science and Social Media,0.09278929978609085,False
"We develop Process Execution Graphs (PEG), a document-level representation of real-world wet lab biochemistry protocols, addressing challenges such as cross-sentence relations, long-range coreference, grounding, and implicit arguments. We manually annotate PEGs in a corpus of complex lab protocols with a novel interactive textual simulator that keeps track of entity traits and semantic constraints during annotation. We use this data to develop graph-prediction models, finding them to be good at entity identification and local relation extraction, while our corpus facilitates further exploration of challenging long-range relations. 1",Resources and Evaluation,"[0.0036080197896808386, 0.009493310004472733, 0.009135532192885876, 0.006244961638003588, 0.006600521504878998, 0.016100889071822166, 0.6601422429084778, 0.02946130558848381, 0.008483655750751495, 0.006749976892024279, 0.03328782320022583, 0.0028595211915671825, 0.005882004741579294, 0.010404339991509914, 0.02587888389825821, 0.004427764564752579, 0.05677314102649689, 0.01916518434882164, 0.005500259343534708, 0.010162945836782455, 0.058101020753383636, 0.0059443372301757336, 0.005592291709035635]",0.6601422429084778,Information Extraction,0.02587888389825821,False
"Much recent work suggests that incorporating syntax information from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these models implicitly encode syntax. In this work, we systematically study the utility of incorporating dependency trees into pretrained transformers on three representative information extraction tasks: semantic role labeling (SRL), named entity recognition, and relation extraction. We propose and investigate two distinct strategies for incorporating dependency structure: a late fusion approach, which applies a graph neural network on the output of a transformer, and a joint fusion approach, which infuses syntax structure into the transformer attention layers. These strategies are representative of prior work, but we introduce additional model design elements that are necessary for obtaining improved performance. Our empirical analysis demonstrates that these syntax-infused transformers obtain state-of-the-art results on SRL and relation extraction tasks. However, our analysis also reveals a critical shortcoming of these models: we find that their performance gains are highly contingent on the availability of human-annotated dependency parses, which raises important questions regarding the viability of syntax-augmented transformers in real-world applications. 1",Information Extraction,"[0.0015023008454591036, 0.0019258225802332163, 0.0015743031399324536, 0.0028144847601652145, 0.002071681898087263, 0.0029189831111580133, 0.846343994140625, 0.0211026631295681, 0.005968201905488968, 0.0021732645109295845, 0.01826854981482029, 0.0015617688186466694, 0.0022376340348273516, 0.00517978984862566, 0.003960244357585907, 0.0047754948027431965, 0.035931430757045746, 0.004159421660006046, 0.002393445000052452, 0.002839042106643319, 0.02510005235671997, 0.0023540526162832975, 0.002843403024598956]",0.846343994140625,Information Extraction,0.846343994140625,True
"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.",Information Extraction,"[0.0009740661480464041, 0.002411018591374159, 0.001508053159341216, 0.0021378181409090757, 0.0016082648653537035, 0.001916976529173553, 0.8923045992851257, 0.01896125264465809, 0.0022869729436933994, 0.003071559127420187, 0.0054895514622330666, 0.0009404878946952522, 0.0016764746978878975, 0.003873987589031458, 0.003281054086983204, 0.0047098807990550995, 0.014309006743133068, 0.00813149381428957, 0.002083788625895977, 0.002518866676837206, 0.020667091012001038, 0.0025099655613303185, 0.0026277548167854548]",0.8923045992851257,Information Extraction,0.8923045992851257,True
"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser's output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and does not require human judgements for training. Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data). We also explore using our model for the task of quality estimation as a metric-conditioning on the source instead of the reference-and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair. Word-level paraphraser log probabilities H(out|in) sBLEU LASER Copy Jason went to school at the University of Madrid . <EOS> -0.",Machine Translation and Multilinguality,"[0.0026761076878756285, 0.004140200559049845, 0.0039020217955112457, 0.0016976569313555956, 0.008106899447739124, 0.013770542107522488, 0.0019981684163212776, 0.007529065478593111, 0.009512890130281448, 0.0021586650982499123, 0.01623258925974369, 0.836329996585846, 0.0056097074411809444, 0.0023366715759038925, 0.04189241677522659, 0.0020946396980434656, 0.0059263743460178375, 0.0066808364354074, 0.007968425750732422, 0.006211447063833475, 0.005867308937013149, 0.005112055689096451, 0.0022454424761235714]",0.836329996585846,Machine Translation and Multilinguality,0.836329996585846,True
"Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pre-trained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.",Machine Translation and Multilinguality,"[0.0018924530595541, 0.0020941882394254208, 0.0017801286885514855, 0.0005429867887869477, 0.005035536829382181, 0.0025823458563536406, 0.0009173447033390403, 0.002505723387002945, 0.004071241710335016, 0.001096888561733067, 0.006973737850785255, 0.9398206472396851, 0.00294516165740788, 0.0010418110759928823, 0.009895911440253258, 0.001846311497502029, 0.0011180108413100243, 0.0024167499504983425, 0.00422885175794363, 0.0017300277249887586, 0.0017716065049171448, 0.002450484549626708, 0.0012417823309078813]",0.9398206472396851,Machine Translation and Multilinguality,0.9398206472396851,True
"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a ""Two-Teacher One-Student"" learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a highquality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods. For reproducibility, we release the code and data at https://github.com/siat-nlp/TTOS.",Dialogue and Interactive Systems,"[0.0007561981328763068, 0.0013572723837569356, 0.9569743871688843, 0.00398215651512146, 0.0008954252698458731, 0.0027951234951615334, 0.001050446880981326, 0.0008847194258123636, 0.0006489548250101507, 0.001407998614013195, 0.005407614167779684, 0.0016439615283161402, 0.0013006071094423532, 0.0026746192015707493, 0.0036440615076571703, 0.0006160703487694263, 0.0019275264348834753, 0.0027466253377497196, 0.004417751915752888, 0.0012845067540183663, 0.0015429817140102386, 0.0008455063798464835, 0.001195441815070808]",0.9569743871688843,Dialogue and Interactive Systems,0.9569743871688843,True
"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.025473730638623238, 0.006345069967210293, 0.07290957123041153, 0.2431647777557373, 0.00826980359852314, 0.05751750245690346, 0.008232067339122295, 0.031800709664821625, 0.03785045072436333, 0.013765498995780945, 0.10930834710597992, 0.004028857685625553, 0.013892634771764278, 0.08349058777093887, 0.01652698591351509, 0.026465794071555138, 0.17809650301933289, 0.004038380458950996, 0.013343161903321743, 0.014790057204663754, 0.016686176881194115, 0.005583085585385561, 0.008420203812420368]",0.2431647777557373,Discourse and Pragmatics,0.17809650301933289,False
"Compressive summarization systems typically rely on a crafted set of syntactic rules to determine what spans of possible summary sentences can be deleted, then learn a model of what to actually delete by optimizing for content selection (ROUGE). In this work, we propose to relax the rigid syntactic constraints on candidate spans and instead leave compression decisions to two data-driven criteria: plausibility and salience. Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and spans are salient if they contain important information from the summary. Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied. When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark summarization datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain: our system fine-tuned on only 500 samples from a new domain can match or exceed an in-domain extractive model trained on much more data. 1",Summarization,"[0.0012062869500368834, 0.0018704811809584498, 0.0022350025828927755, 0.004784918390214443, 0.0023787200916558504, 0.008332600817084312, 0.005205261055380106, 0.007506402675062418, 0.002181651070713997, 0.0019253931241109967, 0.002709184307605028, 0.002857103245332837, 0.0028097594622522593, 0.0007646733429282904, 0.010102818720042706, 0.0009057865827344358, 0.0017388795968145132, 0.004706422332674265, 0.005018295254558325, 0.920112133026123, 0.004124143626540899, 0.00355702661909163, 0.0029670156072825193]",0.920112133026123,Summarization,0.920112133026123,True
"In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. One exemplar publication, titled ""Show Your Work: Improved Reporting of Experimental Results"" (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget. In the present work, we critically examine this paper. As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach. We analytically show that their estimator is biased and uses error-prone assumptions. We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. Our codebase is at https://github.com/ castorini/meanmax.",Machine Learning for NLP,"[0.0161955077201128, 0.01126911398023367, 0.004274843726307154, 0.0038137093652039766, 0.016228104010224342, 0.013740630820393562, 0.0042535592801868916, 0.009417522698640823, 0.46070870757102966, 0.00555624533444643, 0.21467065811157227, 0.04404338076710701, 0.015093467198312283, 0.006388199049979448, 0.08524458110332489, 0.007345902733504772, 0.027901461347937584, 0.013313952833414078, 0.006452842615544796, 0.004535612650215626, 0.019624045118689537, 0.006945072207599878, 0.0029828567057847977]",0.46070870757102966,Interpretability and Analysis of Models for NLP,0.21467065811157227,False
"Vision-and-Language Navigation wayfinding agents can be enhanced by exploiting automatically generated navigation instructions. However, existing instruction generators have not been comprehensively evaluated, and the automatic evaluation metrics used to develop them have not been validated. Using human wayfinders, we show that these generators perform on par with or only slightly better than a template-based generator and far worse than human instructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are ineffective for evaluating grounded navigation instructions. To improve instruction evaluation, we propose an instruction-trajectory compatibility model that operates without reference instructions. Our model shows the highest correlation with human wayfinding outcomes when scoring individual instructions. For ranking instruction generation systems, if reference instructions are available we recommend using SPICE.","Language Grounding to Vision, Robotics and Beyond","[0.0057874578051269054, 0.004535991232842207, 0.024328909814357758, 0.0069200871512293816, 0.007949494756758213, 0.009263518266379833, 0.0037339436821639538, 0.005438880063593388, 0.00727906683459878, 0.8323832154273987, 0.0025349841453135014, 0.002478659385815263, 0.004431369714438915, 0.016780341044068336, 0.018772369250655174, 0.003991540987044573, 0.005407202988862991, 0.0037632472813129425, 0.015127060003578663, 0.004309211857616901, 0.005518645513802767, 0.004858677741140127, 0.004406301770359278]",0.8323832154273987,"Language Grounding to Vision, Robotics and Beyond",0.8323832154273987,True
"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as ""black boxes"". We propose a filter-and-refine solution based on the stackedensemble learning paradigm to address this black-box limitation. We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning. Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.","Phonology, Morphology and Word Segmentation","[0.023373931646347046, 0.009800935164093971, 0.012255212292075157, 0.010370323434472084, 0.00946937408298254, 0.023030003532767296, 0.01339972484856844, 0.017064036801457405, 0.08796164393424988, 0.006787566468119621, 0.4423152506351471, 0.067545086145401, 0.10036271810531616, 0.014449534006416798, 0.019281182438135147, 0.02819698303937912, 0.01676962710916996, 0.007936944253742695, 0.018945911899209023, 0.009630915708839893, 0.04564967378973961, 0.009991677477955818, 0.0054117185063660145]",0.4423152506351471,Machine Learning for NLP,0.10036271810531616,False
"We introduce the largest transcribed Arabic speech corpus, QASR 1 , collected from the broadcast domain. This multi-dialect speech dataset contains 2, 000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others. QASR is suitable for training and evaluating speech recognition systems, acoustics-and/or linguistics-based Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other NLP modules for spoken data. In addition to QASR transcription, we release a dataset of 130M words to aid in designing and training a better language model. We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus. We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. We also report the first baseline for Arabic punctuation restoration. We make the corpus available for the research community. 16 ANERcorp contains 150K words. NEs are 11%. Distribution: PER= 39%, LOC= 30%, ORG= 21%, MISC= 10%. 17 Disfluency example: (OK you are isn't it I mean are you not ashamed?) 18 https://arabicspeech.org/",Resources and Evaluation,"[0.009043296799063683, 0.015227976255118847, 0.05643580108880997, 0.01982896402478218, 0.012341626919806004, 0.01434482354670763, 0.006264481693506241, 0.007336577400565147, 0.014355841092765331, 0.02442493475973606, 0.007466509938240051, 0.030623238533735275, 0.022658981382846832, 0.005373749416321516, 0.09719795733690262, 0.003811206202954054, 0.011617709882557392, 0.014580964110791683, 0.5628269910812378, 0.038914140313863754, 0.008456978015601635, 0.01071787066757679, 0.0061494349502027035]",0.5628269910812378,Speech and Multimodality,0.09719795733690262,False
"Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pretrained models. We first propose a contextaware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWord-Net. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks 1 .","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0017821292858570814, 0.00933137908577919, 0.002764314878731966, 0.006368337199091911, 0.0053534903563559055, 0.006057775113731623, 0.008880390785634518, 0.01793043687939644, 0.05257100239396095, 0.0024547853972762823, 0.015649959444999695, 0.0094142472371459, 0.0041380226612091064, 0.002152294386178255, 0.034029796719551086, 0.006488163024187088, 0.003874273970723152, 0.7853332757949829, 0.006668345537036657, 0.004172210115939379, 0.009001859463751316, 0.003647413570433855, 0.0019361644517630339]",0.7853332757949829,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.7853332757949829,True
"Podcast episodes often contain material extraneous to the main content, such as advertisements, interleaved within the audio and the written descriptions. We present classifiers that leverage both textual and listening patterns in order to detect such content in podcast descriptions and audio transcripts. We demonstrate that our models are effective by evaluating them on the downstream task of podcast summarization and show that we can substantively improve ROUGE scores and reduce the extraneous content generated in the summaries.",Interpretability and Analysis of Models for NLP,"[0.005175958853214979, 0.03873945772647858, 0.006295717786997557, 0.02011137828230858, 0.011113074608147144, 0.02160901576280594, 0.007482430897653103, 0.015337295830249786, 0.008144759573042393, 0.01469304971396923, 0.00919847097247839, 0.004308466333895922, 0.008561951108276844, 0.002068259986117482, 0.08519570529460907, 0.0026281739119440317, 0.00621769018471241, 0.015981871634721756, 0.022193165495991707, 0.6742281913757324, 0.005858203861862421, 0.00855463370680809, 0.006302990019321442]",0.6742281913757324,Summarization,0.008144759573042393,False
"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos; a priori, we expect this domain to be relatively ""easy:"" speakers in instructional videos will often reference the literal objects/actions being depicted. We ask: can similar models be trained on more diverse video corpora? And, if so, what types of videos are ""grounded"" and what types are not? We fit a representative pretraining model to the diverse YouTube8M dataset, and study its success and failure cases. We find that visualtextual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set results in representations that generalize to both noninstructional and instructional domains.","Language Grounding to Vision, Robotics and Beyond","[0.006376112345606089, 0.012909282930195332, 0.006489591673016548, 0.00527561828494072, 0.009783845394849777, 0.008118920959532261, 0.0030473070219159126, 0.0046552796848118305, 0.015088465996086597, 0.7871338725090027, 0.0028574212919920683, 0.002360071986913681, 0.0032251884695142508, 0.006070372648537159, 0.028141655027866364, 0.002979088807478547, 0.0077678244560956955, 0.0038512388709932566, 0.06232258677482605, 0.008095821365714073, 0.003045618999749422, 0.007075109984725714, 0.0033297573681920767]",0.7871338725090027,"Language Grounding to Vision, Robotics and Beyond",0.7871338725090027,True
"The cross-database context-dependent Text-to-SQL (XDTS) problem has attracted considerable attention in recent years due to its wide range of potential applications. However, we identify two biases in existing datasets for XDTS: (1) a high proportion of contextindependent questions and (2) a high proportion of easy SQL queries. These biases conceal the major challenges in XDTS to some extent. In this work, we present CHASE, a large-scale and pragmatic Chinese dataset for XDTS. It consists of 5,459 coherent question sequences (17,940 questions with their SQL queries annotated) over 280 databases, in which only 35% of questions are contextindependent, and 28% of SQL queries are easy. We experiment on CHASE with three state-ofthe-art XDTS approaches. The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that CHASE highlights the challenging problems of XDTS. We believe that CHASE can provide fertile soil for addressing the problems.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.005612405017018318, 0.006654562894254923, 0.04058193787932396, 0.017678067088127136, 0.003973573446273804, 0.023639973253011703, 0.008082333952188492, 0.019212504848837852, 0.006641128566116095, 0.0049851518124341965, 0.02139991894364357, 0.0025828019715845585, 0.00296795554459095, 0.1529945731163025, 0.05407475307583809, 0.007298959884792566, 0.5770853757858276, 0.008325489237904549, 0.005231583956629038, 0.0036060938145965338, 0.01792038045823574, 0.003547497559338808, 0.005902954842895269]",0.5770853757858276,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.5770853757858276,True
"End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work -by up to 9 BLEU on our low-resource setting.",Speech and Multimodality,"[0.004771825857460499, 0.006557951681315899, 0.0326467864215374, 0.005404060240834951, 0.010045523755252361, 0.007590739522129297, 0.006736451294273138, 0.007426910102367401, 0.014843450859189034, 0.01703718863427639, 0.010800160467624664, 0.18745172023773193, 0.01127689890563488, 0.002525986172258854, 0.018206879496574402, 0.0030159843154251575, 0.004319872707128525, 0.008947988972067833, 0.6042594909667969, 0.020136579871177673, 0.0033973311074078083, 0.008442789316177368, 0.004157470539212227]",0.6042594909667969,Speech and Multimodality,0.6042594909667969,True
"Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGTspecific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT.","Phonology, Morphology and Word Segmentation","[0.04317059367895126, 0.018175700679421425, 0.020049501210451126, 0.029459083452820778, 0.008226227015256882, 0.07543937116861343, 0.009399858303368092, 0.006177651230245829, 0.014513798989355564, 0.005504237487912178, 0.018002783879637718, 0.009595120325684547, 0.265352725982666, 0.011795930564403534, 0.2292921394109726, 0.024839166551828384, 0.05088435113430023, 0.03796924650669098, 0.008722426369786263, 0.015008107759058475, 0.07768361270427704, 0.011514602228999138, 0.009223819710314274]",0.265352725982666,"Phonology, Morphology and Word Segmentation",0.265352725982666,True
"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains -the act of ""request"" carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MASKAUGMENT, a controllable mechanism that augments text input by leveraging the pre-trained MASK token from BERT model. Inspired by consistency regularization, we use MASKAUGMENT to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MASKAUGMENT is useful in improving the cross-domain generalization for DA tagging.",Dialogue and Interactive Systems,"[0.0011375071480870247, 0.002755621448159218, 0.9306039214134216, 0.01204636599868536, 0.0012084412155672908, 0.002911399118602276, 0.002075817435979843, 0.0013729484053328633, 0.0008302817586809397, 0.0017544074216857553, 0.005201682914048433, 0.001681526773609221, 0.001973608508706093, 0.0038512595929205418, 0.007628822699189186, 0.0011154967360198498, 0.003412959398701787, 0.005864629056304693, 0.004921913146972656, 0.001926217693835497, 0.0026712703984230757, 0.0012905106414109468, 0.001763268024660647]",0.9306039214134216,Dialogue and Interactive Systems,0.9306039214134216,True
"We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment. We compare our approach against multiple baselines using both automatic metrics and human evaluation. Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.",Generation,"[0.014509891159832478, 0.04354533925652504, 0.0328800305724144, 0.008588218130171299, 0.012468243017792702, 0.14887785911560059, 0.0223903339356184, 0.022179564461112022, 0.01239306665956974, 0.012568209320306778, 0.0840321034193039, 0.015060455538332462, 0.012943293899297714, 0.013766407035291195, 0.292874276638031, 0.008654589764773846, 0.08307630568742752, 0.03640430048108101, 0.016768110916018486, 0.03309604898095131, 0.054604582488536835, 0.012692929245531559, 0.00562587846070528]",0.292874276638031,Resources and Evaluation,0.14887785911560059,False
"This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments. 1","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0007068203995004296, 0.0046861860901117325, 0.0044634840451180935, 0.004397290293127298, 0.002300767693668604, 0.004543712362647057, 0.0203066635876894, 0.01065309438854456, 0.00839641410857439, 0.0026208460330963135, 0.004611756186932325, 0.0043016946874558926, 0.002089383080601692, 0.00208566733635962, 0.022716231644153595, 0.0033184306230396032, 0.001984410220757127, 0.8739895224571228, 0.005517381243407726, 0.002586367540061474, 0.0090108597651124, 0.0029494459740817547, 0.0017635307740420103]",0.8739895224571228,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.8739895224571228,True
"Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher's soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student's performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher's hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher's hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student's performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7Ã— âˆ¼3.4Ã—.",Machine Learning for NLP,"[0.0023858165368437767, 0.0027121740859001875, 0.007260116748511791, 0.002382941311225295, 0.007104778196662664, 0.0201000664383173, 0.05844197794795036, 0.06557227671146393, 0.035861387848854065, 0.0034330144990235567, 0.6847233176231384, 0.027196194976568222, 0.0037584237288683653, 0.004428439773619175, 0.007022390142083168, 0.0033333436585962772, 0.00907934457063675, 0.010394451208412647, 0.006654266268014908, 0.012345717288553715, 0.018093174323439598, 0.004943128675222397, 0.002773380372673273]",0.6847233176231384,Machine Learning for NLP,0.6847233176231384,True
"The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation: mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user's interest vector, and adapting collaborative filtering techniques to estimate the current user's preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user's sentiment towards an entity from the conversation context, and 2) transforms the ratings of ""similar"" external reviewers to predict the current user's preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent . Our results demonstrate that Con-vExtr can improve the accuracy of predicting users' ratings for new movies by exploiting conversation content and external data.",Information Retrieval and Text Mining,"[0.006438610143959522, 0.06028458848595619, 0.12246333807706833, 0.026024358347058296, 0.014878194779157639, 0.03407237306237221, 0.050824858248233795, 0.0957990437746048, 0.014645203948020935, 0.06318184733390808, 0.0604599267244339, 0.006630534306168556, 0.007617300841957331, 0.0500226765871048, 0.15455958247184753, 0.004612372722476721, 0.01601221226155758, 0.06820385903120041, 0.07038497179746628, 0.042834360152482986, 0.010966068133711815, 0.012391331605613232, 0.006692457012832165]",0.15455958247184753,Resources and Evaluation,0.0957990437746048,False
"Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding train sets. In addition, we find that 60-70% of answers in the test sets are also present in the train sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can generalize, and what drives their overall performance. We find that all models perform substantially worse on questions that cannot be memorized from train sets, with a mean absolute performance difference of 61% between repeated and nonrepeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that train set memorization plays in these benchmarks.",Question Answering,"[0.00442714337259531, 0.0027533334214240313, 0.004108854569494724, 0.0036713264416903257, 0.0036951315123587847, 0.005114681087434292, 0.0023418739438056946, 0.007456558756530285, 0.01019523199647665, 0.007431558798998594, 0.0085862772539258, 0.0020233923569321632, 0.00207972782664001, 0.9011150598526001, 0.014650608412921429, 0.001593603752553463, 0.005219632294028997, 0.003274732269346714, 0.002181327436119318, 0.0008345586829818785, 0.002625352004542947, 0.001982415094971657, 0.0026376626919955015]",0.9011150598526001,Question Answering,0.9011150598526001,True
"Recent studies strive to incorporate various human rationales into neural networks to improve model performance, but few pay attention to the quality of the rationales. Most existing methods distribute their models' focus to distantly-labeled rationale words entirely and equally, while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words. In this paper, we propose two novel auxiliary loss functions to make better use of distantly-labeled rationales, which encourage models to maintain their focus on important words beyond labeled rationales (PINs) and alleviate redundant training on non-helpful rationales (NoIRs). Experiments on two representative classification tasks show that our proposed methods can push a classification model to effectively learn crucial clues from nonperfect rationales while maintaining the ability to spread its focus to other unlabeled important words, thus significantly outperform existing methods.",Machine Learning for NLP,"[0.006178804207593203, 0.00502045638859272, 0.006173308938741684, 0.0028831553645431995, 0.011559342965483665, 0.006178957410156727, 0.00731562077999115, 0.024121636524796486, 0.38620972633361816, 0.003874216927215457, 0.44874703884124756, 0.021136607974767685, 0.003516928991302848, 0.004636811092495918, 0.010464055463671684, 0.0053134155459702015, 0.00981635507196188, 0.013895079493522644, 0.0065980227664113045, 0.0034892703406512737, 0.007369240280240774, 0.003459400497376919, 0.0020424744579941034]",0.44874703884124756,Machine Learning for NLP,0.44874703884124756,True
"Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.",Information Extraction,"[0.006155500188469887, 0.006064245011657476, 0.007973158732056618, 0.008789519779384136, 0.007890072651207447, 0.011387932114303112, 0.1874966025352478, 0.06424731761217117, 0.06016230210661888, 0.006523001939058304, 0.36418190598487854, 0.0036164962220937014, 0.0032239058054983616, 0.02581114135682583, 0.011360536329448223, 0.01309377234429121, 0.13208666443824768, 0.007466144394129515, 0.004218355752527714, 0.004803447984158993, 0.050453051924705505, 0.006195137742906809, 0.006799759343266487]",0.36418190598487854,Machine Learning for NLP,0.1874966025352478,False
"Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using shortpremise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths. * In our experiments, we broadly consider long texts, and do not differentiate between long single sentences and multiple sentences.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.00492795929312706, 0.006817921996116638, 0.007535633165389299, 0.02086610719561577, 0.004768754355609417, 0.03066893480718136, 0.01801193132996559, 0.018949147313833237, 0.007742541842162609, 0.0037350812926888466, 0.028866145759820938, 0.0011838464997708797, 0.0038235520478338003, 0.03512340411543846, 0.06877826899290085, 0.007508516777306795, 0.6650851964950562, 0.007924556732177734, 0.004787861835211515, 0.013243192806839943, 0.02766328491270542, 0.003769336733967066, 0.008218714967370033]",0.6650851964950562,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.6650851964950562,True
"Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary. Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge. In this work, we propose a Transformer-based model to enhance the copy mechanism. Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer. We use the centrality of each source word to guide the copy process explicitly. Experimental results show that the self-attention graph provides useful guidance for the copy distribution. Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.",Summarization,"[0.001004084711894393, 0.0010418840683996677, 0.0022900965996086597, 0.005072099156677723, 0.0015785768628120422, 0.010378587990999222, 0.004339348524808884, 0.005592648405581713, 0.0025653454940766096, 0.001984677743166685, 0.0030098448041826487, 0.002684815553948283, 0.002474795561283827, 0.0006039296858943999, 0.006093317177146673, 0.0009724685223773122, 0.0015627978136762977, 0.0028378041461110115, 0.004960272926837206, 0.9317486882209778, 0.0022064081858843565, 0.002734975190833211, 0.0022625226993113756]",0.9317486882209778,Summarization,0.9317486882209778,True
"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformerbased models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.",Summarization,"[0.000989117776043713, 0.001200457918457687, 0.0024817083030939102, 0.00798382330685854, 0.0017518408130854368, 0.012063143774867058, 0.009517538361251354, 0.01259236503392458, 0.002797911176458001, 0.0023946629371494055, 0.003427258227020502, 0.0022580840159207582, 0.002267845207825303, 0.0011679846793413162, 0.00689835287630558, 0.0015372911002486944, 0.0033084675669670105, 0.004525890573859215, 0.004914961289614439, 0.9061412215232849, 0.0026447430718690157, 0.0037558183539658785, 0.0033794515766203403]",0.9061412215232849,Summarization,0.9061412215232849,True
"The design of expressive representations of entities and relations in a knowledge graph is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic complexity of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities -a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the graph level and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics.",Information Extraction,"[0.0065939235500991344, 0.007930510677397251, 0.007405038457363844, 0.006070741452276707, 0.01007643062621355, 0.0099503668025136, 0.16003504395484924, 0.055350545793771744, 0.051433444023132324, 0.005869683809578419, 0.49578484892845154, 0.005127764772623777, 0.005626693833619356, 0.01108245737850666, 0.009461263194680214, 0.032905541360378265, 0.05848706141114235, 0.013267149217426777, 0.0034845664631575346, 0.0033534627873450518, 0.030450835824012756, 0.004999883938580751, 0.005252777133136988]",0.49578484892845154,Machine Learning for NLP,0.16003504395484924,False
"Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon textonly models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra-and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.",Machine Translation and Multilinguality,"[0.012909171171486378, 0.008266658522188663, 0.004073538817465305, 0.0024162253830581903, 0.02156081236898899, 0.011926119215786457, 0.0033079315908253193, 0.005930969957262278, 0.05094947665929794, 0.0726827010512352, 0.0064827981404960155, 0.6633793711662292, 0.015452827326953411, 0.0029819163028150797, 0.029322514310479164, 0.012844007462263107, 0.0037258174270391464, 0.00810141209512949, 0.03479094058275223, 0.006611378397792578, 0.00497091980651021, 0.012687891721725464, 0.00462443707510829]",0.6633793711662292,Machine Translation and Multilinguality,0.6633793711662292,True
"Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSUM. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSUM summarization models by allowing us to identify non-factual tokens in the training data. 1 1450 Reference Summary: An early-medieval gold pendant created from an imitation of a Byzantine coin that was found in a Norfolk field is a ""rare find"", a museum expert has said. Source Article Fragment: Discovered on land at North Elmham, near Dereham, the circa 600 AD coin was created by French rulers of the time to increase their available currency. [â€¦] The pendant was declared treasure by the Norfolk coroner on Wednesday. An 18th century coin believed to be worth more than #1m has been discovered. A gold pendant created from a necklace was found in a field Entitycentric (Ent-C) The pendant was declared a treasure by the Norfolk coroner on Wednesday. The pendant was declared a treasure by the Ohio coroner on March. Generation centric (Gen-C) Label Human Annotation non-factual span non-factual arc (factual arcs not shown) Sentences Training Dataset",Summarization,"[0.0014477588701993227, 0.0022964859381318092, 0.0022767474874854088, 0.007263202220201492, 0.0025856492575258017, 0.008089348673820496, 0.0069109369069337845, 0.005704650655388832, 0.0027380024548619986, 0.0026537603698670864, 0.0019311350770294666, 0.0025460505858063698, 0.0032582685817033052, 0.0009401781717315316, 0.015328336507081985, 0.0010066963732242584, 0.002200222807005048, 0.007019902113825083, 0.005128217861056328, 0.9056635499000549, 0.004573931451886892, 0.004606151022017002, 0.0038308401126414537]",0.9056635499000549,Summarization,0.9056635499000549,True
"A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis V1.0, the first largescale corpus for phonetic typology, with aligned segments and estimated phonemelevel labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is nontrivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at https:// voxclamantisproject.github.io.",Resources and Evaluation,"[0.09793636202812195, 0.04006626456975937, 0.0064469859935343266, 0.01435400452464819, 0.026832493022084236, 0.010174672119319439, 0.003746533067896962, 0.007032398134469986, 0.029964618384838104, 0.006212606560438871, 0.012794826179742813, 0.10184825211763382, 0.42725223302841187, 0.009178216569125652, 0.10558099299669266, 0.02056042104959488, 0.004655815195292234, 0.02000742219388485, 0.011939799413084984, 0.005841948091983795, 0.015011776238679886, 0.01316126435995102, 0.009400038979947567]",0.42725223302841187,"Phonology, Morphology and Word Segmentation",0.10558099299669266,False
"Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a longtailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up during very early few iterations of the model training. These two observations are further employed to formulate a measurement which can quantify the shortcut degree of each training sample. Based on this shortcut measurement, we propose a shortcut mitigation framework LTGR, to suppress the model from making overconfident predictions for samples with large shortcut degree. Experimental results on three NLU benchmarks demonstrate that our long-tailed distribution explanation accurately reflects the shortcut learning behavior of NLU models. Experimental analysis further indicates that LTGR can improve the generalization accuracy on OOD data, while preserving the accuracy on in-distribution data.",Interpretability and Analysis of Models for NLP,"[0.014504802413284779, 0.0061826566234230995, 0.0020180237479507923, 0.0036133313551545143, 0.012529564090073109, 0.00894229207187891, 0.0033192893024533987, 0.0152503727003932, 0.2774682343006134, 0.0021037617698311806, 0.557500958442688, 0.0076738628558814526, 0.006951553281396627, 0.006555755157023668, 0.01910773292183876, 0.011161279864609241, 0.022556619718670845, 0.005626939237117767, 0.003247467102482915, 0.0021354700438678265, 0.006712334230542183, 0.00298634497448802, 0.0018513597315177321]",0.557500958442688,Machine Learning for NLP,0.2774682343006134,False
"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks' properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.",Interpretability and Analysis of Models for NLP,"[0.035873714834451675, 0.014915823005139828, 0.00476097920909524, 0.013357053510844707, 0.017916060984134674, 0.02981276996433735, 0.028714118525385857, 0.08090570569038391, 0.13569197058677673, 0.007143279537558556, 0.2230888456106186, 0.014786151237785816, 0.03876720741391182, 0.01667262427508831, 0.11193055659532547, 0.04733733460307121, 0.04533577710390091, 0.030717922374606133, 0.0055630998685956, 0.008626238442957401, 0.06867323815822601, 0.012636628933250904, 0.006772823631763458]",0.2230888456106186,Machine Learning for NLP,0.13569197058677673,False
"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.",Machine Learning for NLP,"[0.007681075017899275, 0.014662147499620914, 0.011853997595608234, 0.009711619466543198, 0.007603300269693136, 0.018458345904946327, 0.03575482591986656, 0.016274303197860718, 0.06282145529985428, 0.004664244595915079, 0.16638551652431488, 0.0038909981958568096, 0.008981101214885712, 0.009810271672904491, 0.067912258207798, 0.021386735141277313, 0.2881433665752411, 0.11119963973760605, 0.005502108484506607, 0.004143453668802977, 0.11452803760766983, 0.004468690603971481, 0.0041625769808888435]",0.2881433665752411,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.16638551652431488,False
"Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios. Although the pre-trained models (PTMs) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive. To alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks. However, existing early-exit mechanisms are specifically designed for sequencelevel tasks, rather than sequence labeling. In this paper, we first propose SENTEE: a simple extension of SENTence-level Early-Exit for sequence labeling tasks. To further reduce computational cost, we also propose TOKEE: a TOKen-level Early-Exit mechanism that allows partial tokens to exit early at different layers. Considering the local dependency inherent in sequence labeling, we employed a window-based criterion to decide for a token whether or not to exit. The token-level earlyexit brings the gap between training and inference, so we introduce an extra self-sampling fine-tuning stage to alleviate it. The extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66%âˆ¼75% inference cost with minimal performance degradation. Compared with competitive compressed models such as DistilBERT, our approach can achieve better performance under the same speed-up ratios of 2Ã—, 3Ã—, and 4Ã—. 1",Information Extraction,"[0.0032248101197183132, 0.0028959792107343674, 0.026426265016198158, 0.0035953025799244642, 0.0035477003548294306, 0.0328364335000515, 0.05254843831062317, 0.03954414650797844, 0.013525505550205708, 0.003115850267931819, 0.581635594367981, 0.01550238486379385, 0.003444993868470192, 0.008222609758377075, 0.008581489324569702, 0.005964948330074549, 0.11144860833883286, 0.006298508495092392, 0.005008062347769737, 0.0054404400289058685, 0.059996362775564194, 0.003969603683799505, 0.003225994063541293]",0.581635594367981,Machine Learning for NLP,0.05254843831062317,False
"Conversational systems enable numerous valuable applications, and question-answering is an important component underlying many of these. However, conversational questionanswering remains challenging due to the lack of realistic, domain-specific training data. Inspired by this bottleneck, we focus on conversational question generation as a means to generate synthetic conversations for training and evaluation purposes. We present a number of novel strategies to improve conversational flow and accommodate varying question types and overall fluidity. Specifically, we design ChainCQG as a two-stage architecture that learns question-answer representations across multiple dialogue turns using a flow propagation training strategy. ChainCQG significantly outperforms both answer-aware and answer-unaware SOTA baselines (e.g., up to 48% BLEU-1 improvement). Additionally, our model is able to generate different types of questions, with improved fluidity and coreference alignment.",Generation,"[0.007867458276450634, 0.009566382504999638, 0.46470025181770325, 0.02611117996275425, 0.005087371449917555, 0.0962144210934639, 0.003097704378888011, 0.00952187366783619, 0.00431267824023962, 0.006563227623701096, 0.02563365362584591, 0.0064126974903047085, 0.006502480246126652, 0.21760769188404083, 0.05127245560288429, 0.0029325762297958136, 0.01665431261062622, 0.011949063278734684, 0.0076139094308018684, 0.003460691077634692, 0.006202914286404848, 0.004410748369991779, 0.006304154172539711]",0.46470025181770325,Dialogue and Interactive Systems,0.0962144210934639,False
"In this paper, we suggest a minimallysupervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.",Computational Social Science and Social Media,"[0.011399966664612293, 0.7479180097579956, 0.002071524504572153, 0.01739291474223137, 0.04910336434841156, 0.0025007440708577633, 0.0027532607782632113, 0.00579781224951148, 0.009573071263730526, 0.003654584288597107, 0.005842195358127356, 0.006410821806639433, 0.013847213238477707, 0.0051630460657179356, 0.04571501538157463, 0.009208831004798412, 0.003784553147852421, 0.03591303154826164, 0.005301603116095066, 0.0026560663245618343, 0.004115300718694925, 0.004155535716563463, 0.005721462890505791]",0.7479180097579956,Computational Social Science and Social Media,0.7479180097579956,True
"Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an offthe-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-ofthe-art performance on both datasets. 1 * Equal contribution.",Information Retrieval and Text Mining,"[0.000828842690680176, 0.0020641859155148268, 0.0010186303406953812, 0.0020106316078454256, 0.001495328382588923, 0.001358764711767435, 0.9248741865158081, 0.010703086853027344, 0.0017003390239551663, 0.001771001610904932, 0.0029832033906131983, 0.001242877566255629, 0.002278899075463414, 0.0018928266363218427, 0.003136776853352785, 0.0032456957269459963, 0.005759069696068764, 0.006048784591257572, 0.0018603566568344831, 0.003150704549625516, 0.016515212133526802, 0.0020786907989531755, 0.0019819377921521664]",0.9248741865158081,Information Extraction,0.010703086853027344,False
"Entity linking (EL), the task of disambiguating mentions in text by linking them to entities in a knowledge graph, is crucial for text understanding, question answering or conversational systems. Entity linking on short text (e.g., single sentence or question) poses particular challenges due to limited context. While prior approaches use either heuristics or blackbox neural methods, here we propose LNN-EL, a neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to using rules, LNN-EL performs competitively against SotA black-box neural approaches, with the added benefits of extensibility and transferability. In particular, we show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even scores resulting from previous EL methods, thus improving on such methods. For instance, on the LC-QuAD-1.0 dataset, we show more than 4% increase in F1 score over previous SotA. Finally, we show that the inductive bias offered by using logic results in learned rules that transfer well across datasets, even without fine tuning, while maintaining high accuracy.",Information Extraction,"[0.0018937046406790614, 0.004223554395139217, 0.0018138788873329759, 0.003565159160643816, 0.0032343766652047634, 0.0028400083538144827, 0.8489913940429688, 0.03437822312116623, 0.00280177965760231, 0.003217854769900441, 0.013359795324504375, 0.001210835762321949, 0.0021300052758306265, 0.013267912901937962, 0.005654590204358101, 0.0058992463164031506, 0.014378969557583332, 0.0033791351597756147, 0.0017999982228502631, 0.0031584175303578377, 0.020630864426493645, 0.0034826970659196377, 0.00468751834705472]",0.8489913940429688,Information Extraction,0.8489913940429688,True
"Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation. 1",Generation,"[0.0019184892298653722, 0.0010776353301480412, 0.004920994862914085, 0.0014195129042491317, 0.0016501812497153878, 0.9266725778579712, 0.001359931891784072, 0.003663451876491308, 0.0022790259681642056, 0.0018133239354938269, 0.007904652506113052, 0.002820660825818777, 0.0029758878517895937, 0.0016762916930019855, 0.011534556746482849, 0.0010861059417948127, 0.006440398283302784, 0.0026438452769070864, 0.002384892664849758, 0.007727192249149084, 0.002332147443667054, 0.0024166079238057137, 0.0012816197704523802]",0.9266725778579712,Generation,0.9266725778579712,True
"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, singlehop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HOTPOTQA over a strong baseline on the original, out-ofdomain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using subquestions is promising for shedding light on why a QA system makes a prediction. 1 * KC was a part-time research scientist at Facebook AI Research while working on this paper. 1 Our code, data, and pretrained models are available at https://github.com/facebookresearch/ UnsupervisedDecomposition.",Question Answering,"[0.0036134012043476105, 0.002275827107951045, 0.009715845808386803, 0.004559120163321495, 0.0023313926067203283, 0.01291731558740139, 0.0034353560768067837, 0.007919525727629662, 0.0038708990905433893, 0.005476986523717642, 0.013920526020228863, 0.001993655227124691, 0.0022387742064893246, 0.8869394063949585, 0.011936655268073082, 0.0017081396654248238, 0.009204608388245106, 0.002894470002502203, 0.0021885663736611605, 0.0010975978802889585, 0.004895323887467384, 0.002031943527981639, 0.002834581071510911]",0.8869394063949585,Question Answering,0.8869394063949585,True
"Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style. Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance. In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance.",Dialogue and Interactive Systems,"[0.005158575717359781, 0.00998973660171032, 0.6709389686584473, 0.0273725762963295, 0.006664510350674391, 0.0856252908706665, 0.0024876073002815247, 0.003449053503572941, 0.004465069156140089, 0.006940837949514389, 0.012315955944359303, 0.005259709898382425, 0.010392840020358562, 0.005349250510334969, 0.04944607615470886, 0.0029066496063023806, 0.007898576557636261, 0.02939637191593647, 0.02179623581469059, 0.018460212275385857, 0.003936822526156902, 0.004891781602054834, 0.004857165738940239]",0.6709389686584473,Dialogue and Interactive Systems,0.6709389686584473,True
"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.","Syntax: Tagging, Chunking and Parsing","[0.0035897712223231792, 0.001990880584344268, 0.02002860978245735, 0.0027233592700213194, 0.0025024041533470154, 0.02350219525396824, 0.02155957743525505, 0.023440957069396973, 0.018411511555314064, 0.004785644821822643, 0.6412529945373535, 0.039044760167598724, 0.005519017577171326, 0.006568537559360266, 0.004739099182188511, 0.006560480687767267, 0.06562035530805588, 0.0039024034049361944, 0.006305867340415716, 0.007132422644644976, 0.08207937330007553, 0.005985605996102095, 0.0027541175950318575]",0.6412529945373535,Machine Learning for NLP,0.08207937330007553,False
"This paper studies the task of Relation Extraction (RE) that aims to identify the semantic relations between two entity mentions in text. In the deep learning models for RE, it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences. In such models, the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning. The major problems with these approaches are the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for RE. In order to overcome these issues, we propose a novel deep learning model for RE that uses the dependency trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information into the models with greater generalization. In particular, we leverage Ordered-Neuron Long-Short Term Memory Networks (ON-LSTM) to infer the model-based importance scores for RE for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection. We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets.",Information Extraction,"[0.0008209391962736845, 0.0019502409268170595, 0.0008234228007495403, 0.0016918531619012356, 0.0014163888990879059, 0.0012243932578712702, 0.9339421987533569, 0.012868351303040981, 0.0014178667915984988, 0.0015665636165067554, 0.003413269529119134, 0.0009519620216451585, 0.0014635458355769515, 0.0030192388221621513, 0.002634263364598155, 0.002908437279984355, 0.00657173665240407, 0.0028629405423998833, 0.0015133628621697426, 0.00274315825663507, 0.009931623004376888, 0.0019225430442020297, 0.002341706771403551]",0.9339421987533569,Information Extraction,0.9339421987533569,True
"Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW). WoW is shown to perform endto-end cross-lingual FVSQA at same levels of accuracy across 3 languages -English, Hindi, and Turkish.",Speech and Multimodality,"[0.0046574631705880165, 0.007415356580168009, 0.04132262244820595, 0.00798754021525383, 0.007593577727675438, 0.0114838145673275, 0.008606198243796825, 0.007950305938720703, 0.009539193473756313, 0.6467739939689636, 0.0035519073717296124, 0.0059927357360720634, 0.004810568410903215, 0.03623342141509056, 0.03843235597014427, 0.004161860328167677, 0.01134446170181036, 0.007703154813498259, 0.10793965309858322, 0.005240788217633963, 0.005071235354989767, 0.008656504563987255, 0.007531287148594856]",0.6467739939689636,"Language Grounding to Vision, Robotics and Beyond",0.10793965309858322,False
"In knowledge graph embedding, the theoretical relationship between the softmax crossentropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.",Machine Learning for NLP,"[0.010580443777143955, 0.012578015215694904, 0.009578450582921505, 0.005156956613063812, 0.015150466002523899, 0.009826538152992725, 0.007465871050953865, 0.022522635757923126, 0.05463006719946861, 0.004605800844728947, 0.7094241976737976, 0.009588092565536499, 0.008108846843242645, 0.009710796177387238, 0.019994646310806274, 0.024181287735700607, 0.01799829863011837, 0.014958902262151241, 0.003831234062090516, 0.0029713474214076996, 0.019985634833574295, 0.003549829823896289, 0.003601512871682644]",0.7094241976737976,Machine Learning for NLP,0.7094241976737976,True
"Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity. At training time, additional inputs based on these evaluation measures are given to the dialogue model. At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence. We also investigate the usage of additional controls at decoding time using resampling techniques. In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems.",Dialogue and Interactive Systems,"[0.0016624302370473742, 0.001783639658242464, 0.9203609824180603, 0.00686962204053998, 0.0016793913673609495, 0.015224382281303406, 0.0009960004827007651, 0.0014433799078688025, 0.0015719060320407152, 0.002077545505017042, 0.0064162942580878735, 0.002689570654183626, 0.002883902285248041, 0.002754166256636381, 0.008494609966874123, 0.0007451866986230016, 0.003499436192214489, 0.004369610920548439, 0.006266549229621887, 0.002811931073665619, 0.002108430489897728, 0.0014917071675881743, 0.0017994546797126532]",0.9203609824180603,Dialogue and Interactive Systems,0.9203609824180603,True
"The number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes it applicable to any language. Code and data are publicly available 1 . * Equal contribution. CX handled the data, generated the rankings, and sampled the examples of section 7. AJPT computed the results, plots, and wrote the paper. Both authors participated in the design of the study.",Semantics: Lexical Semantics,"[0.07052087038755417, 0.04213210567831993, 0.005150944460183382, 0.026851395145058632, 0.032504141330718994, 0.010227706283330917, 0.005607961677014828, 0.029017161577939987, 0.046040795743465424, 0.005320703610777855, 0.03707542270421982, 0.01702653057873249, 0.06558547914028168, 0.01613631099462509, 0.11427542567253113, 0.3614010810852051, 0.0222360547631979, 0.04100687801837921, 0.005670748185366392, 0.006428995635360479, 0.01606348343193531, 0.01477773766964674, 0.008941968902945518]",0.3614010810852051,Semantics: Lexical Semantics,0.3614010810852051,True
"Hypernymy detection, a.k.a. lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the lowresource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.",Semantics: Lexical Semantics,"[0.013759958557784557, 0.021347902715206146, 0.005831034388393164, 0.05028637871146202, 0.014009885489940643, 0.012205881997942924, 0.02071932703256607, 0.01790691912174225, 0.01302463561296463, 0.00370591483078897, 0.025329457595944405, 0.008201593533158302, 0.017990434542298317, 0.008479664102196693, 0.046899501234292984, 0.5459408164024353, 0.10393550246953964, 0.025498993694782257, 0.0066182492300868034, 0.00716758007183671, 0.011281351558864117, 0.008332752622663975, 0.011526210233569145]",0.5459408164024353,Semantics: Lexical Semantics,0.5459408164024353,True
"Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA ('TA' means tweet act, i.e., speech act in Twitter) dataset called EmoTA collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants.",Information Extraction,"[0.004877532832324505, 0.05046907439827919, 0.21720869839191437, 0.03959159180521965, 0.012792241759598255, 0.010349538177251816, 0.009432938881218433, 0.01067336741834879, 0.011269587092101574, 0.019332673400640488, 0.009347507730126381, 0.008241531439125538, 0.01180143840610981, 0.004291085060685873, 0.09398918598890305, 0.006015532650053501, 0.012720336206257343, 0.14157234132289886, 0.27549904584884644, 0.02526496723294258, 0.006439517717808485, 0.010863089933991432, 0.00795725267380476]",0.27549904584884644,Speech and Multimodality,0.009432938881218433,False
"Implicit discourse relation classification is a challenging task, in particular when the text domain is different from the standard Penn Discourse Treebank (PDTB; Prasad et al., 2008) training corpus domain (Wall Street  Journal in 1990s). We here tackle the task of implicit discourse relation classification on the biomedical domain, for which the Biomedical Discourse Relation Bank (BioDRB; Prasad et al., 2011) is available. We show that entity information can be used to improve discourse relational argument representation. In a first step, we show that explicitly marked instances that are content-wise similar to the target relations can be used to achieve good performance in the cross-domain setting using a simple unsupervised voting pipeline. As a further step, we show that with the linked entity information from the first step, a transformer which is augmented with entity-related information (K-BERT; Liu et al., 2020) sets the new state of the art performance on the dataset, outperforming the large pre-trained BioBERT (Lee et al.,  2020)  model by 2% points.",Discourse and Pragmatics,"[0.007185955997556448, 0.017197072505950928, 0.02322569116950035, 0.710704505443573, 0.0097053162753582, 0.0042268759571015835, 0.009953737258911133, 0.010167393833398819, 0.005216942168772221, 0.004318878520280123, 0.008125421591103077, 0.00198296457529068, 0.010669461451470852, 0.026142515242099762, 0.03291257098317146, 0.026782726868987083, 0.03842959925532341, 0.021962592378258705, 0.004416670650243759, 0.0066282665356993675, 0.006855955813080072, 0.003908952698111534, 0.009279965423047543]",0.710704505443573,Discourse and Pragmatics,0.710704505443573,True
"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for openended text generation including story or dialog generation because of the notorious oneto-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-theart metrics.",Generation,"[0.008342576213181019, 0.008057801052927971, 0.011994500644505024, 0.005464828107506037, 0.005984095856547356, 0.733246922492981, 0.00231878156773746, 0.007444743067026138, 0.0074621327221393585, 0.009011980146169662, 0.013173080049455166, 0.0021749953739345074, 0.007899677380919456, 0.005772792734205723, 0.1036207526922226, 0.003849167376756668, 0.02149314433336258, 0.014396585524082184, 0.0035187071189284325, 0.008792918175458908, 0.00782732479274273, 0.00505041005089879, 0.0031020219903439283]",0.733246922492981,Generation,0.733246922492981,True
"We reduce the task of (span-based) PropBankstyle semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format. This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art. Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.004688669461756945, 0.005766339600086212, 0.009190932847559452, 0.012525278143584728, 0.0028378439601510763, 0.008233265019953251, 0.03492060303688049, 0.007126351818442345, 0.005900549236685038, 0.00459851510822773, 0.007743826601654291, 0.002697186078876257, 0.008116544224321842, 0.004171046428382397, 0.020026665180921555, 0.01253201998770237, 0.41900599002838135, 0.012212340719997883, 0.005333475768566132, 0.009407282806932926, 0.391749769449234, 0.004887147806584835, 0.006328273564577103]",0.41900599002838135,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.41900599002838135,True
"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., part-ofspeech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.",Machine Learning for NLP,"[0.004125939682126045, 0.001996740000322461, 0.0034245552960783243, 0.0018554826965555549, 0.004627611953765154, 0.007717903703451157, 0.006132457870990038, 0.015758424997329712, 0.053909264504909515, 0.001410710159689188, 0.8187218904495239, 0.010606466792523861, 0.0022201163228601217, 0.008593479171395302, 0.009298241697251797, 0.003937253262847662, 0.02689855545759201, 0.0016714853700250387, 0.002186187542974949, 0.001453762291930616, 0.010146066546440125, 0.001900159171782434, 0.0014071969781070948]",0.8187218904495239,Machine Learning for NLP,0.8187218904495239,True
"We propose MULTIOPED 1 , an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MULTIOPED aims at supporting the study of multiple tasks relevant to automatic perspective discovery, where a system is expected to produce a singlesentence thesis statement summarizing the arguments presented. We argue that identifying and abstracting such natural language perspectives from editorials is a crucial step toward studying the implicit argumentation structure in news editorials. We first discuss the challenges and define a few conceptual tasks towards our goal. To demonstrate the utility of MULTIOPED and the induced tasks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that MULTIOPED will be a useful resource for future studies on argumentation in the news editorial domain.",Resources and Evaluation,"[0.0020976124797016382, 0.01710427924990654, 0.005165254231542349, 0.01644553802907467, 0.004705381579697132, 0.01664084941148758, 0.010022741742432117, 0.012978000566363335, 0.01179393008351326, 0.0036534760147333145, 0.00397493364289403, 0.003197461599484086, 0.00380107038654387, 0.003134927712380886, 0.06138470396399498, 0.004820316564291716, 0.004984416998922825, 0.7804573178291321, 0.003967130556702614, 0.014731667935848236, 0.008390657603740692, 0.0039464388974010944, 0.002601838205009699]",0.7804573178291321,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.06138470396399498,False
"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020) , we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020 ) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT'14 Enâ†’De, WMT'16 Roâ†’En and IWSLT'16 Deâ†’En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT'14 Enâ†’De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).",Machine Translation and Multilinguality,"[0.0009989829268306494, 0.0008107267785817385, 0.0013354775728657842, 0.0004894306766800582, 0.002300070133060217, 0.001966512994840741, 0.0006535246502608061, 0.001890308572910726, 0.003825145773589611, 0.0007223300635814667, 0.009643475525081158, 0.957237720489502, 0.0016160111408680677, 0.0011099963448941708, 0.002920271595939994, 0.0008612575475126505, 0.001382608083076775, 0.001192488707602024, 0.003395118284970522, 0.0014569470658898354, 0.0017209240468218923, 0.0015704401303082705, 0.0009001694852486253]",0.957237720489502,Machine Translation and Multilinguality,0.957237720489502,True
"State-of-the-art machine translation (MT) systems are typically trained to generate ""standard"" target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (sourcevariety) data. This also includes adaptation of MT systems to low-resource typologicallyrelated target languages. 1 We experiment with adapting an English-Russian MT system to generate Ukrainian and Belarusian, an English-Norwegian BokmÃ¥l system to generate Nynorsk, and an English-Arabic system to generate four Arabic dialects, obtaining significant improvements over competitive baselines.",Machine Translation and Multilinguality,"[0.0027192984707653522, 0.00413137674331665, 0.003171162446960807, 0.001021724776364863, 0.006916787941008806, 0.005984751041978598, 0.0012021271977573633, 0.002403884194791317, 0.0075264484621584415, 0.0023102520499378443, 0.0044311219826340675, 0.8901242017745972, 0.009574090130627155, 0.0013073500012978911, 0.02771281637251377, 0.0023457373026758432, 0.0018313948530703783, 0.005922610405832529, 0.005677098874002695, 0.0032545169815421104, 0.004656883422285318, 0.003748859977349639, 0.0020254948176443577]",0.8901242017745972,Machine Translation and Multilinguality,0.8901242017745972,True
"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient. A base model first predicts draft labels, and then a novel twostream self-attention model makes refinements on these draft predictions based on longrange label dependencies, which can achieve parallel decoding for a faster prediction. In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation. The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.","Syntax: Tagging, Chunking and Parsing","[0.004338081460446119, 0.0055033802054822445, 0.014569398015737534, 0.003487872891128063, 0.005393665283918381, 0.02282877080142498, 0.05278746411204338, 0.08336412161588669, 0.012724572792649269, 0.00421105744317174, 0.6214089393615723, 0.007126047741621733, 0.004356576595455408, 0.014930118806660175, 0.012830706313252449, 0.009526393376290798, 0.03425750881433487, 0.007229360286146402, 0.0032053960021585226, 0.0034109142143279314, 0.06336835026741028, 0.005059701856225729, 0.004081615712493658]",0.6214089393615723,Machine Learning for NLP,0.06336835026741028,False
"External syntactic and semantic information has been largely ignored by existing neural coreference resolution models. In this paper, we present a heterogeneous graph-based model to incorporate syntactic and semantic structures of sentences. The proposed graph contains a syntactic sub-graph where tokens are connected based on a dependency tree, and a semantic sub-graph that contains arguments and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model. 1",Discourse and Pragmatics,"[0.01223684847354889, 0.015156869776546955, 0.05771363899111748, 0.3671311140060425, 0.007019557058811188, 0.008603242225944996, 0.020510731264948845, 0.016723791137337685, 0.013509456999599934, 0.00914717186242342, 0.021891510114073753, 0.002205834724009037, 0.00964313093572855, 0.022503221407532692, 0.024397289380431175, 0.08227686583995819, 0.2266046404838562, 0.03250395879149437, 0.009070095606148243, 0.010411480441689491, 0.015720397233963013, 0.004120552446693182, 0.01089860126376152]",0.3671311140060425,Discourse and Pragmatics,0.3671311140060425,True
"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality. The implementation of the proposed framework is released 1 .",Dialogue and Interactive Systems,"[0.0011791398283094168, 0.0021906935144215822, 0.9375147223472595, 0.00746784545481205, 0.0016967214178293943, 0.0027437361422926188, 0.0011099607218056917, 0.0012663613306358457, 0.0011307448148727417, 0.0013593247858807445, 0.005942665506154299, 0.003148428164422512, 0.002015954814851284, 0.003190365619957447, 0.0073393103666603565, 0.0010612714104354382, 0.0021235004533082247, 0.005523134954273701, 0.005900354124605656, 0.0014844497200101614, 0.0017825403483584523, 0.001309851766563952, 0.0015187085373327136]",0.9375147223472595,Dialogue and Interactive Systems,0.9375147223472595,True
"The general format of natural language inference (NLI) makes it tempting to be used for zero-shot text classification by casting any target label into a sentence of hypothesis and verifying whether or not it could be entailed by the input, aiming at generic classification applicable on any specified label space. In this opinion piece, we point out a few overlooked issues that are yet to be discussed in this line of work. We observe huge variance across different classification datasets amongst standard BERT-based NLI models and surprisingly find that pre-trained BERT without any fine-tuning can yield competitive performance against BERT fine-tuned for NLI. With the concern that these models heavily rely on spurious lexical patterns for prediction, we also experiment with preliminary approaches for more robust NLI, but the results are in general negative. Our observations reveal implicit but challenging difficulties in entailmentbased zero-shot text classification.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.002914826385676861, 0.004400912206619978, 0.0061675990000367165, 0.009036879055202007, 0.0040068854577839375, 0.013379296287894249, 0.01536063477396965, 0.027688518166542053, 0.011095929890871048, 0.002321484498679638, 0.04357549548149109, 0.0021535216365009546, 0.0017531758639961481, 0.014570033177733421, 0.02695646695792675, 0.014415020123124123, 0.7519221901893616, 0.010413601994514465, 0.004553853999823332, 0.004013214726001024, 0.02180386707186699, 0.0028899195604026318, 0.004606685601174831]",0.7519221901893616,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.7519221901893616,True
"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifierbased probing that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.",Interpretability and Analysis of Models for NLP,"[0.0459451898932457, 0.006102048791944981, 0.002812459599226713, 0.016724709421396255, 0.012126237154006958, 0.009172840043902397, 0.008884369395673275, 0.014244110323488712, 0.5525389909744263, 0.014814945869147778, 0.07651715725660324, 0.003558517899364233, 0.01675080694258213, 0.009290662594139576, 0.028297239914536476, 0.06112423539161682, 0.060552697628736496, 0.013541131280362606, 0.00781952403485775, 0.002791601698845625, 0.023677481338381767, 0.008287293836474419, 0.004425773862749338]",0.5525389909744263,Interpretability and Analysis of Models for NLP,0.5525389909744263,True
"The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems. A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as ""teacher AG""). The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. To provide a feasible solution to the multimodality problem of NAG, we propose incorporating linguistic structure (Part-of-Speech sequence in particular) into NAG inference instead of relying on teacher AG. More specifically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at providing a specific POS sequence to constrain the NAG model during decoding. Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation. This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.",Generation,"[0.006249371450394392, 0.0032438195776194334, 0.12726517021656036, 0.0071462588384747505, 0.005937434267252684, 0.6337411403656006, 0.003952716011554003, 0.005810348782688379, 0.007030948530882597, 0.012298192828893661, 0.05795871838927269, 0.008050531148910522, 0.010561510920524597, 0.005555500276386738, 0.031052429229021072, 0.002393755130469799, 0.015646174550056458, 0.00571664422750473, 0.022873425856232643, 0.013701074756681919, 0.0049957577139139175, 0.005393067374825478, 0.0034259750973433256]",0.6337411403656006,Generation,0.6337411403656006,True
"Translating text into a language unknown to the text's author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.",Machine Translation and Multilinguality,"[0.002925136126577854, 0.00484408950433135, 0.0029456992633640766, 0.001311781001277268, 0.00842953473329544, 0.006205643527209759, 0.0011781919747591019, 0.0023043309338390827, 0.0063722808845341206, 0.002783995820209384, 0.0036090670619159937, 0.8907589316368103, 0.007547713816165924, 0.0015190783888101578, 0.026932522654533386, 0.0026100820396095514, 0.0018764609703794122, 0.005397073924541473, 0.007187661714851856, 0.003952799830585718, 0.0030060531571507454, 0.004330322612076998, 0.001971476012840867]",0.8907589316368103,Machine Translation and Multilinguality,0.8907589316368103,True
"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT-a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.",Machine Learning for NLP,"[0.0033283394295722246, 0.001322538242675364, 0.0032014218159019947, 0.0013088728301227093, 0.002773991785943508, 0.007645756937563419, 0.00485881557688117, 0.009289794601500034, 0.0275441724807024, 0.0009055966511368752, 0.8975374102592468, 0.00471615232527256, 0.002967164386063814, 0.002756549511104822, 0.003800562582910061, 0.002095091389492154, 0.011717782355844975, 0.0015442122239619493, 0.0012014656094834208, 0.0013464889489114285, 0.005958685651421547, 0.0013099029893055558, 0.0008692748961038888]",0.8975374102592468,Machine Learning for NLP,0.8975374102592468,True
"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner's perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task. * This work was done during an internship at Bloomberg.",Information Retrieval and Text Mining,"[0.004261486232280731, 0.003555176081135869, 0.004667307250201702, 0.003942899405956268, 0.010932228527963161, 0.010259736329317093, 0.03566250577569008, 0.038496293127536774, 0.09797309339046478, 0.005061409901827574, 0.6739047765731812, 0.00832029152661562, 0.0026484986301511526, 0.012051311321556568, 0.010944683104753494, 0.012882283888757229, 0.03605498746037483, 0.00613012071698904, 0.002717838855460286, 0.0023796139284968376, 0.011780139990150928, 0.0028561234939843416, 0.002517290413379669]",0.6739047765731812,Machine Learning for NLP,0.038496293127536774,False
"The intensity relationship that holds between scalar adjectives (e.g., nice < great < wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using monolingual and multilingual contextual language models. Additionally, we introduce a new binary classification task for English scalar adjective identification which examines the models' ability to distinguish scalar from relational adjectives. We probe contextualised representations and report baseline results for future comparison on this task.",Semantics: Lexical Semantics,"[0.10440170764923096, 0.027508126571774483, 0.007862660102546215, 0.11240358650684357, 0.017892302945256233, 0.020210690796375275, 0.011953222565352917, 0.01471958588808775, 0.029955316334962845, 0.01966509409248829, 0.020516203716397285, 0.007831791415810585, 0.11589773744344711, 0.03081912361085415, 0.06719942390918732, 0.25430750846862793, 0.019307535141706467, 0.052843138575553894, 0.01067532692104578, 0.006493058986961842, 0.015172221697866917, 0.01848260685801506, 0.013882053084671497]",0.25430750846862793,Semantics: Lexical Semantics,0.25430750846862793,True
"Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",Machine Learning for NLP,"[0.006004136521369219, 0.0069139981642365456, 0.007208420895040035, 0.004224423319101334, 0.007859567180275917, 0.024090152233839035, 0.007765359710901976, 0.011049488559365273, 0.05622711405158043, 0.0027842242270708084, 0.5816041231155396, 0.0062166196294128895, 0.005102770868688822, 0.009004972875118256, 0.049970000982284546, 0.006656499579548836, 0.16424445807933807, 0.007959212176501751, 0.0036085902247577906, 0.0026266491040587425, 0.024069324135780334, 0.0025349524803459644, 0.002274861326441169]",0.5816041231155396,Machine Learning for NLP,0.5816041231155396,True
"Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema. Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations. Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.",Question Answering,"[0.007187799084931612, 0.0042479559779167175, 0.013439594767987728, 0.01594547927379608, 0.0032381266355514526, 0.01954907365143299, 0.016880156472325325, 0.026338819414377213, 0.007442309986799955, 0.014708612114191055, 0.011077863164246082, 0.0025732030626386404, 0.005165021400898695, 0.4007895290851593, 0.02667783573269844, 0.015729960054159164, 0.34072989225387573, 0.003791739232838154, 0.00669343676418066, 0.004087631590664387, 0.03966448828577995, 0.00460860226303339, 0.009432867169380188]",0.4007895290851593,Question Answering,0.4007895290851593,True
"Grounding events into a precise timeline is important for natural language understanding but has received limited attention in recent work. This problem is challenging due to the inherent ambiguity of language and the requirement for information propagation over inter-related events. This paper first formulates this problem based on a 4-tuple temporal representation used in entity slot filling, which allows us to represent fuzzy time spans more conveniently. We then propose a graph attention networkbased approach to propagate temporal information over document-level event graphs constructed by shared entity arguments and temporal relations. To better evaluate our approach, we present a challenging new benchmark on the ACE2005 corpus, where more than 78% of events do not have time spans mentioned explicitly in their local contexts. The proposed approach yields an absolute gain of 7.0% in match rate over contextualized embedding approaches, and 16.3% higher match rate compared to sentence-level manual event time argument annotation. 1",Information Extraction,"[0.0014744822401553392, 0.002671747235581279, 0.0018250818829983473, 0.0029116785153746605, 0.0020365200471132994, 0.003485198365524411, 0.8357069492340088, 0.01764647290110588, 0.0028670006431639194, 0.0030602284241467714, 0.007303815800696611, 0.0015231594443321228, 0.002250227378681302, 0.004264662507921457, 0.003441624576225877, 0.013429347425699234, 0.05325956270098686, 0.004616288002580404, 0.0026451318990439177, 0.0034641665406525135, 0.022787388414144516, 0.002920033410191536, 0.004409301560372114]",0.8357069492340088,Information Extraction,0.8357069492340088,True
"Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-theart results with different few-shot sizes. The source code is available at https://github. com/JiaweiSheng/FAAN.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.00336815370246768, 0.0041899424977600574, 0.007542001083493233, 0.005001446232199669, 0.005003411788493395, 0.019978022202849388, 0.5486020445823669, 0.10155250132083893, 0.008816046640276909, 0.00525754364207387, 0.16060008108615875, 0.002984680701047182, 0.0031474418938159943, 0.02160138636827469, 0.006291402969509363, 0.007981952279806137, 0.03178643807768822, 0.00966767780482769, 0.004451397806406021, 0.007576207630336285, 0.024898137897253036, 0.004725394770503044, 0.0049767810851335526]",0.5486020445823669,Information Extraction,0.03178643807768822,False
"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We collect manual annotations of neutralised techniques used in these texts, and explore semi-supervised models to automatically classify them.",Computational Social Science and Social Media,"[0.010706824250519276, 0.5193176865577698, 0.0025903431233018637, 0.013493448495864868, 0.10219085961580276, 0.004145129583775997, 0.008375972509384155, 0.00793995801359415, 0.029304800555109978, 0.006486610043793917, 0.011419291608035564, 0.01579279638826847, 0.011852419935166836, 0.009231089614331722, 0.08840207010507584, 0.015130131505429745, 0.009727569296956062, 0.0982448160648346, 0.00946237612515688, 0.0029129385948181152, 0.010220842435956001, 0.0064500174485147, 0.00660186680033803]",0.5193176865577698,Computational Social Science and Social Media,0.5193176865577698,True
"Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.",Dialogue and Interactive Systems,"[0.005475593730807304, 0.01658964529633522, 0.7194742560386658, 0.08135036379098892, 0.005737863481044769, 0.01106438972055912, 0.002673179842531681, 0.002851455472409725, 0.0025615468621253967, 0.007775617763400078, 0.0031497215386480093, 0.0034491117112338543, 0.008577747270464897, 0.006612799130380154, 0.04227229952812195, 0.003283121157437563, 0.00724270474165678, 0.022229058668017387, 0.021723272278904915, 0.008890185505151749, 0.005807379726320505, 0.004627850838005543, 0.006580905988812447]",0.7194742560386658,Dialogue and Interactive Systems,0.7194742560386658,True
"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",Machine Translation and Multilinguality,"[0.0016859743045642972, 0.0013375998241826892, 0.0029564243741333485, 0.001028439262881875, 0.004105191677808762, 0.004241960123181343, 0.002399117685854435, 0.0034541860222816467, 0.013998502865433693, 0.0025745946913957596, 0.013945390470325947, 0.9042078852653503, 0.002879603300243616, 0.0015399424592033029, 0.009523661807179451, 0.002313310746103525, 0.004805445671081543, 0.0031168435234576464, 0.007402390241622925, 0.001770976115949452, 0.005369550082832575, 0.003614806802943349, 0.0017281277105212212]",0.9042078852653503,Machine Translation and Multilinguality,0.9042078852653503,True
"Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowledge bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on ""hard"" co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve ""soft"" augmentation by proposing a knowledge graph enrichment and embedding framework named EDGE. Given an original knowledge graph, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a graph alignment term in a shared embedding space between the original and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of EDGE in link prediction and node classification.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.006765628233551979, 0.011203893460333347, 0.005100274458527565, 0.005168188828974962, 0.01318906806409359, 0.012949381023645401, 0.17959053814411163, 0.19374854862689972, 0.03137771040201187, 0.004441139288246632, 0.3815867602825165, 0.004374608863145113, 0.005156016442924738, 0.012330528348684311, 0.00996833574026823, 0.03172886744141579, 0.03747658059000969, 0.014367817901074886, 0.0032178580295294523, 0.004923756700009108, 0.02040538378059864, 0.005342427175492048, 0.005586662795394659]",0.3815867602825165,Machine Learning for NLP,0.03747658059000969,False
"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A method to combine symbolic and linguistic reasoning is also explored for this task. Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.00504472479224205, 0.008337457664310932, 0.01989029534161091, 0.01357187982648611, 0.0056849573738873005, 0.014168885536491871, 0.18709932267665863, 0.0437300018966198, 0.011220719665288925, 0.004794015083462, 0.044866230338811874, 0.0018957305001094937, 0.0028023747727274895, 0.05722542107105255, 0.0167904831469059, 0.00959846843034029, 0.4833967983722687, 0.01001287903636694, 0.006397285498678684, 0.006645923014730215, 0.03538961336016655, 0.0036764226388186216, 0.007760100532323122]",0.4833967983722687,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.4833967983722687,True
"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a graph embedding model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiC-GRL) framework inspired by the human's ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.",Information Extraction,"[0.010741956532001495, 0.01196875050663948, 0.013047470711171627, 0.006430050823837519, 0.010433938354253769, 0.015007857233285904, 0.06444496661424637, 0.04217365011572838, 0.030565781518816948, 0.006729708984494209, 0.5209330320358276, 0.004295511171221733, 0.005880970973521471, 0.012086188420653343, 0.009224467910826206, 0.04001668840646744, 0.1189643070101738, 0.01367933489382267, 0.004741949960589409, 0.004454010631889105, 0.04236677289009094, 0.00493786484003067, 0.006874755024909973]",0.5209330320358276,Machine Learning for NLP,0.06444496661424637,False
"Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. The goal of the task is to generate a ""bridging"" utterance connecting the new topic to the topic of the previous conversation turn. We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. We first collect a new dataset of human one-turn topic transitions, which we call OTTers 1 . We then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. A: i like to eat the same thing as ninja turtles. T: I love pizza. I eat it while I skateboard. B: i enjoy riding around on a plank with wheels. Bridging: Missing Link A: i prefer things to be authentic. T: I think children are the truest form of authenticity because they say things unfiltered. B: i am not a fan of children. Disjunctive A: i like american made cars. T: I like liver cooked in butter -just throwing that in! B: i avoid eating broccoli. Off-Task A: i prefer things to be authentic. T: my bro just made some authentic thai chicken. B: i am not a fan of children. Off-Topic A: i learnt to drive. T: I had a rough night sleeping in my new bed last night. B: i like making a salmon entree.",Dialogue and Interactive Systems,"[0.0016996345948427916, 0.002483624964952469, 0.8575181365013123, 0.023857107385993004, 0.0015429313061758876, 0.02403213270008564, 0.0023201294243335724, 0.0033818972297012806, 0.001584366662427783, 0.004405030980706215, 0.0042499639093875885, 0.002389016328379512, 0.003987861797213554, 0.007149331271648407, 0.018147297203540802, 0.0015181602211669087, 0.008194192312657833, 0.007385270670056343, 0.007111675105988979, 0.007309476379305124, 0.00479457713663578, 0.002380115445703268, 0.0025581615045666695]",0.8575181365013123,Dialogue and Interactive Systems,0.8575181365013123,True
"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model's posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networkscontrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity. 1","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.004309363663196564, 0.0026193358935415745, 0.00798123236745596, 0.004313615150749683, 0.0024114360567182302, 0.010724098421633244, 0.004400273319333792, 0.005045655183494091, 0.006695652846246958, 0.0037486457731574774, 0.025017976760864258, 0.007051847875118256, 0.0023444457910954952, 0.0036683131475001574, 0.0048129502683877945, 0.03633320331573486, 0.8468603491783142, 0.0016424652421846986, 0.004057940095663071, 0.002225119387730956, 0.00852938462048769, 0.002489642472937703, 0.0027169876266270876]",0.8468603491783142,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.8468603491783142,True
"We present HERO, a novel framework for large-scale video+language omnirepresentation learning. HERO  encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities. 1","Language Grounding to Vision, Robotics and Beyond","[0.005909476894885302, 0.006735897623002529, 0.037491217255592346, 0.00903702899813652, 0.007279134355485439, 0.013797273859381676, 0.006162695586681366, 0.005348878912627697, 0.014952138997614384, 0.6735756397247314, 0.012948003597557545, 0.004395316820591688, 0.004557727370411158, 0.008092320524156094, 0.01947079412639141, 0.003009639447554946, 0.014289820566773415, 0.0037569815758615732, 0.11508230119943619, 0.01552779320627451, 0.004566570743918419, 0.008182445541024208, 0.0058308010920882225]",0.6735756397247314,"Language Grounding to Vision, Robotics and Beyond",0.6735756397247314,True
"Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a selfcontained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset.",Dialogue and Interactive Systems,"[0.004458976443856955, 0.01330158207565546, 0.5274212956428528, 0.2439131736755371, 0.004678691271692514, 0.00822519138455391, 0.007841384038329124, 0.007461347617208958, 0.0030124643817543983, 0.006739133503288031, 0.012242903001606464, 0.0018830926856026053, 0.006149020045995712, 0.019348111003637314, 0.028182534500956535, 0.00924195908010006, 0.03554113209247589, 0.019888579845428467, 0.008934404700994492, 0.010409770533442497, 0.012183461338281631, 0.0032012255396693945, 0.005740548484027386]",0.5274212956428528,Dialogue and Interactive Systems,0.5274212956428528,True
"Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning crosslingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article, we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of refinement procedures sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, refinement with Procrustes solution and refinement with symmetric re-weighting. Extensive experimentations with high-and low-resource languages from two different data sets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the supervised system. Along with performing comprehensive ablation studies to understand the contribution of different components of our adversarial model, we also conduct a thorough analysis of the refinement procedures to understand their effects.",Machine Translation and Multilinguality,"[0.0033324745018035173, 0.0018760371021926403, 0.0012473812093958259, 0.0008596261614002287, 0.005111760925501585, 0.0035291824024170637, 0.0012978222221136093, 0.003020159900188446, 0.01158646959811449, 0.0009563752682879567, 0.027849577367305756, 0.904187023639679, 0.004384548868983984, 0.0012973359553143382, 0.006340798456221819, 0.007129654288291931, 0.00250055524520576, 0.002581516280770302, 0.002820422174409032, 0.0015656172763556242, 0.0024645221419632435, 0.0027266701217740774, 0.0013344855979084969]",0.904187023639679,Machine Translation and Multilinguality,0.904187023639679,True
"While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-ofdistribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.010496309958398342, 0.007116709835827351, 0.013180972076952457, 0.011805660091340542, 0.023869413882493973, 0.041786689311265945, 0.007681580260396004, 0.012748789973556995, 0.3780905604362488, 0.008543228730559349, 0.2326943725347519, 0.012979154475033283, 0.005048253107815981, 0.014580574817955494, 0.04075642675161362, 0.010434666648507118, 0.1226022019982338, 0.014591626822948456, 0.00495563680306077, 0.005143847782164812, 0.010409260168671608, 0.005686926655471325, 0.004797163885086775]",0.3780905604362488,Interpretability and Analysis of Models for NLP,0.014591626822948456,False
"To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates 'the amount of effort' needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes. 1",Interpretability and Analysis of Models for NLP,"[0.023815857246518135, 0.004262979608029127, 0.00197783624753356, 0.004532909020781517, 0.006084920838475227, 0.007175203412771225, 0.0019001583568751812, 0.004384305328130722, 0.8141313195228577, 0.02004174143075943, 0.01981397345662117, 0.005250704940408468, 0.007888694293797016, 0.00419028103351593, 0.01692597195506096, 0.010848159901797771, 0.016490275040268898, 0.006597010884433985, 0.00648492993786931, 0.0024570864625275135, 0.0072298552840948105, 0.00564948283135891, 0.0018662390066310763]",0.8141313195228577,Interpretability and Analysis of Models for NLP,0.8141313195228577,True
"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, eQASC, contains over 98K explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer. The second dataset eQASC-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC. We show that this data can be used to significantly improve explanation quality (+14% absolute F1 over a strong retrieval baseline) using a BERT-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: ""X is a Y"" AND ""Y has Z"" IMPLIES ""X has Z""). We find that generalized chains maintain performance while also being more robust to certain perturbations. 1",Question Answering,"[0.004712095018476248, 0.0029885047115385532, 0.004259457811713219, 0.0064285509288311005, 0.004047991707921028, 0.0076057626865804195, 0.002462089527398348, 0.007894582115113735, 0.014876450411975384, 0.007496209815144539, 0.009797201491892338, 0.0015627361135557294, 0.0019491197308525443, 0.8749785423278809, 0.013583335094153881, 0.0024783313274383545, 0.016507966443896294, 0.004338080063462257, 0.002324535045772791, 0.001167629612609744, 0.0032300439197570086, 0.00212855520658195, 0.003182298270985484]",0.8749785423278809,Question Answering,0.8749785423278809,True
"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.",Semantics: Lexical Semantics,"[0.05958167463541031, 0.059723515063524246, 0.002941227052360773, 0.009203145280480385, 0.05156930536031723, 0.0085898507386446, 0.0075755552388727665, 0.02272225171327591, 0.14202499389648438, 0.0066365450620651245, 0.23361095786094666, 0.03309658169746399, 0.024569502100348473, 0.015022004954516888, 0.04955016449093819, 0.19168436527252197, 0.029544422402977943, 0.014490975067019463, 0.005923530086874962, 0.002862651366740465, 0.009853629395365715, 0.01214244868606329, 0.007080683950334787]",0.23361095786094666,Machine Learning for NLP,0.19168436527252197,False
"Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications for which they are purportedly developed has largely been neglected. With the advancements made by neural approaches in applications such as machine translation (MT), summarization and dialog systems, the need for coherence evaluation of these tasks is now more crucial than ever. However, coherence models are typically evaluated only on synthetic tasks, which may not be representative of their performance in downstream applications. To investigate how representative the synthetic tasks are of downstream use cases, we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks, and contrast this with their performance on three downstream applications: coherence evaluation for MT and summarization, and next utterance prediction in retrieval-based dialog. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, motivating alternate training and evaluation methods for coherence models. 1",Discourse and Pragmatics,"[0.00685808714479208, 0.009158561937510967, 0.44277387857437134, 0.05584219470620155, 0.008315757848322392, 0.05408666282892227, 0.009053132496774197, 0.010989914648234844, 0.015733778476715088, 0.009215792641043663, 0.06425287574529648, 0.006830343510955572, 0.016634967178106308, 0.009859517216682434, 0.09006146341562271, 0.0036495819222182035, 0.040671076625585556, 0.019192276522517204, 0.02880045771598816, 0.07343032956123352, 0.011690102517604828, 0.006689812522381544, 0.006209373474121094]",0.44277387857437134,Dialogue and Interactive Systems,0.05584219470620155,False
"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.",Information Extraction,"[0.0030618535820394754, 0.09668748080730438, 0.005936000030487776, 0.01732730306684971, 0.015391777269542217, 0.002755078487098217, 0.504130482673645, 0.044806115329265594, 0.007845599204301834, 0.007989955134689808, 0.012448683381080627, 0.004565800540149212, 0.008234359323978424, 0.007402242161333561, 0.06007799506187439, 0.02029646746814251, 0.03405086323618889, 0.07193581759929657, 0.01614474691450596, 0.012645750306546688, 0.02674437128007412, 0.00872880406677723, 0.010792485438287258]",0.504130482673645,Information Extraction,0.504130482673645,True
"Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets. We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA. We further demonstrate that our approach can learn effectively from limited data. 1 1 Our code and data is available at https://github. com/AkariAsai/logic_guided_qa. Q: The ceramic vase was less flexible than the plastic ball so it was A: more breakable Q: The ceramic vase was more flexible than the plastic ball so it was A: less breakable Q: If it is silent, does the outer ear collect less sound waves? A: more [positive causal relationship] Q: If the outer ear collect less sound waves, is less sound being detected? A: more [positive causal relationship] Q: If it is silent, is less sound being detected? A: more [positive causal relationship] RoBERTa more breakable more breakable RoBERTa more more less Conflict Conflict",Question Answering,"[0.008089262992143631, 0.003977911081165075, 0.006987742614001036, 0.012196562252938747, 0.0036582613829523325, 0.005891106091439724, 0.016093675047159195, 0.013176142238080502, 0.016662409529089928, 0.009591050446033478, 0.023883691057562828, 0.0011292011477053165, 0.0020130532793700695, 0.6348538994789124, 0.015121612697839737, 0.009177399799227715, 0.18727606534957886, 0.0033678559120744467, 0.003450084012001753, 0.0019137953640893102, 0.008107492700219154, 0.003417843021452427, 0.00996389240026474]",0.6348538994789124,Question Answering,0.6348538994789124,True
"Human conversations naturally evolve around related concepts and scatter to multi-hop concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, Con-ceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/ thunlp/ConceptFlow.",Dialogue and Interactive Systems,"[0.0018733784090727568, 0.004242148716002703, 0.8871457576751709, 0.019722793251276016, 0.002150747226551175, 0.012580224312841892, 0.0019021340413019061, 0.0019459162140265107, 0.0016846252838149667, 0.0022652773186564445, 0.007677071262151003, 0.00169632479082793, 0.0031632576137781143, 0.006016884930431843, 0.012410908937454224, 0.001667096046730876, 0.005011147353798151, 0.01238152477890253, 0.004524747375398874, 0.0028711932245641947, 0.0031543346121907234, 0.0016136813210323453, 0.002298810752108693]",0.8871457576751709,Dialogue and Interactive Systems,0.8871457576751709,True
"Many NLP tasks such as tagging and machine reading comprehension (MRC) are faced with the severe data imbalance issue: negative examples significantly outnumber positive ones, and the huge number of easy-negative examples overwhelms training. The most commonly used cross entropy criteria is actually accuracy-oriented, which creates a discrepancy between training and test. At training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.",Theory and Formalism in NLP (Linguistic and Mathematical),"[0.009221645072102547, 0.00932469591498375, 0.0043986099772155285, 0.008582240901887417, 0.012787056155502796, 0.013093617744743824, 0.016268210485577583, 0.053959012031555176, 0.09391709417104721, 0.005799397360533476, 0.4064282178878784, 0.005568934604525566, 0.004133124835789204, 0.12562400102615356, 0.0683564841747284, 0.009606000036001205, 0.10234509408473969, 0.006848616059869528, 0.004366809036582708, 0.0028494838625192642, 0.027148056775331497, 0.004465669393539429, 0.0049078441224992275]",0.4064282178878784,Machine Learning for NLP,0.004465669393539429,False
"The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on finetuning pre-trained language models. First, we study and report three HPO algorithms' performances on fine-tuning two state-of-the-art language models on the GLUE dataset. We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting. We propose two general strategies and an experimental procedure to systematically troubleshoot HPO's failure cases. By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains. Finally, we make suggestions for future work. Our implementation can be found in https://github.c om/microsoft/FLAML/tree/main/flaml /nlp/.",Resources and Evaluation,"[0.0038404050283133984, 0.0019155339105054736, 0.002234969288110733, 0.0011287976521998644, 0.003607915947213769, 0.007258641067892313, 0.0047067380510270596, 0.016360167413949966, 0.06615445017814636, 0.0011344888480380177, 0.8346864581108093, 0.006432863883674145, 0.002404004568234086, 0.0038670410867780447, 0.0066177430562675, 0.0027625756338238716, 0.018251707777380943, 0.0034961949568241835, 0.0017895456403493881, 0.0014040338573977351, 0.007188416551798582, 0.0017542417626827955, 0.0010031498968601227]",0.8346864581108093,Machine Learning for NLP,0.0066177430562675,False
"Humans use language to accomplish a wide variety of tasks -asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of adviceseeking online, as well as advice-giving in natural language generation systems. We present a dataset in English from two Reddit advice forums -r/AskParents and r/needadvice -annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present preliminary models showing that while pre-trained language models are able to capture advice better than rulebased systems, advice identification is challenging, and we identify directions for future research.",Computational Social Science and Social Media,"[0.010646962560713291, 0.10197602212429047, 0.030154334381222725, 0.07842376828193665, 0.027811435982584953, 0.030933380126953125, 0.01902247592806816, 0.029241295531392097, 0.020556362345814705, 0.03245805948972702, 0.01825665682554245, 0.0013313634553924203, 0.005311841145157814, 0.13056431710720062, 0.2811216413974762, 0.007699559908360243, 0.0465889573097229, 0.07687783986330032, 0.009627961553633213, 0.013786187395453453, 0.008657380007207394, 0.008543985895812511, 0.010408241301774979]",0.2811216413974762,Resources and Evaluation,0.10197602212429047,False
"We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems. 1","Phonology, Morphology and Word Segmentation","[0.08393418043851852, 0.012576870620250702, 0.011727520264685154, 0.02118644118309021, 0.009586264379322529, 0.03182314708828926, 0.0050946553237736225, 0.004684195853769779, 0.014318986795842648, 0.0054248021915555, 0.013949025422334671, 0.02068992145359516, 0.5820502042770386, 0.01422672625631094, 0.0537196584045887, 0.02726789191365242, 0.008210532367229462, 0.018643571063876152, 0.006453337147831917, 0.006006011739373207, 0.03141452744603157, 0.009602646343410015, 0.007408838719129562]",0.5820502042770386,"Phonology, Morphology and Word Segmentation",0.5820502042770386,True
"We present MLSUM, the first large-scale Mul-tiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.",Summarization,"[0.002148980973288417, 0.003875948255881667, 0.0029498334042727947, 0.008007088676095009, 0.003982757683843374, 0.012807194143533707, 0.006214524619281292, 0.00897242408245802, 0.003309212625026703, 0.003084156196564436, 0.0027070960495620966, 0.006056631449609995, 0.005404622294008732, 0.0010459922486916184, 0.032020390033721924, 0.0015130313113331795, 0.0026155828963965178, 0.008857977576553822, 0.00728501845151186, 0.8618268370628357, 0.005371937993913889, 0.005481569096446037, 0.004461287055164576]",0.8618268370628357,Summarization,0.8618268370628357,True
"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10, 012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hatespeech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.",Interpretability and Analysis of Models for NLP,"[0.007030362728983164, 0.7238200306892395, 0.003449121955782175, 0.01628047600388527, 0.04485572129487991, 0.0028476526495069265, 0.0032726929057389498, 0.004528558347374201, 0.005570001434534788, 0.004174711182713509, 0.006983206607401371, 0.004742291755974293, 0.007960209622979164, 0.0047459383495152, 0.07431619614362717, 0.005970922764390707, 0.0042647202499210835, 0.05172601342201233, 0.006597914267331362, 0.00332396081648767, 0.0038029742427170277, 0.004013203550130129, 0.005723141599446535]",0.7238200306892395,Computational Social Science and Social Media,0.005570001434534788,False
"In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as positive, neutral, negative in sentiment analysis. Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes). In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory. Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously. In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.",Resources and Evaluation,"[0.010537377558648586, 0.015984222292900085, 0.006535506807267666, 0.013946855440735817, 0.022625965997576714, 0.016696710139513016, 0.01825442723929882, 0.09947305172681808, 0.15220296382904053, 0.010073287412524223, 0.3381126821041107, 0.010861697606742382, 0.00623318599537015, 0.019644439220428467, 0.09568458795547485, 0.024963030591607094, 0.03668864443898201, 0.04802143946290016, 0.008801235817372799, 0.005900111515074968, 0.023842576891183853, 0.009587394073605537, 0.005328545346856117]",0.3381126821041107,Machine Learning for NLP,0.09568458795547485,False
"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and wordsense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SUBJQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.",Question Answering,"[0.009585746563971043, 0.048283543437719345, 0.012313314713537693, 0.05319005250930786, 0.031140776351094246, 0.017329605296254158, 0.02214798703789711, 0.03067517839372158, 0.04209854081273079, 0.009403837844729424, 0.02838946133852005, 0.0037859014701098204, 0.005456121638417244, 0.05132633075118065, 0.22068095207214355, 0.02537558600306511, 0.151099294424057, 0.18401503562927246, 0.008997726254165173, 0.0058435858227312565, 0.017733586952090263, 0.009903225116431713, 0.011224612593650818]",0.22068095207214355,Resources and Evaluation,0.05132633075118065,False
"Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP. Despite this, current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters. Here, we propose a model that can disambiguate between mappings and convert between the two scripts. The model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences. We further construct benchmark datasets for topic classification and script conversion. Our proposed method outperforms previous Chinese Character conversion approaches by 6 points in accuracy. These results are further confirmed in a downstream application, where 2kenize is used to convert pretraining dataset for topic classification. An error analysis reveals that our method's particular strengths are in dealing with code mixing and named entities. The code and dataset is available at https: //github.com/pranav-ust/2kenize","Phonology, Morphology and Word Segmentation","[0.012157637625932693, 0.010246580466628075, 0.013941837474703789, 0.011558225378394127, 0.00994342751801014, 0.1600596159696579, 0.016828007996082306, 0.04046616330742836, 0.0365782156586647, 0.0038992450572550297, 0.12327751517295837, 0.04315657913684845, 0.06120166927576065, 0.006786154117435217, 0.11252638697624207, 0.041363492608070374, 0.13177773356437683, 0.02440173365175724, 0.012089856900274754, 0.026362240314483643, 0.08346117287874222, 0.01203251164406538, 0.005884003359824419]",0.1600596159696579,Generation,0.06120166927576065,False
"Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only English corpus alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings: fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.",Machine Translation and Multilinguality,"[0.0037647364661097527, 0.006832120940089226, 0.06488601863384247, 0.014435330405831337, 0.0033529940992593765, 0.014758211560547352, 0.00555827934294939, 0.003176449565216899, 0.0030113079119473696, 0.004002148285508156, 0.010070759803056717, 0.011192037723958492, 0.00719020189717412, 0.008921777829527855, 0.0369497574865818, 0.009579071775078773, 0.7243189215660095, 0.007529753260314465, 0.010471593588590622, 0.004603105131536722, 0.036092646420001984, 0.003260745434090495, 0.006042072549462318]",0.7243189215660095,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.011192037723958492,False
"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.",Interpretability and Analysis of Models for NLP,"[0.03198898211121559, 0.00392570998519659, 0.005658971145749092, 0.002848872449249029, 0.005445684306323528, 0.013221659697592258, 0.005943531636148691, 0.00870390422642231, 0.558116614818573, 0.008440906181931496, 0.1250770390033722, 0.02097911201417446, 0.013883253559470177, 0.007852988317608833, 0.013718567788600922, 0.01835072971880436, 0.07062683254480362, 0.004047276917845011, 0.004050077870488167, 0.003838076489046216, 0.06213882938027382, 0.007963527925312519, 0.0031788302585482597]",0.558116614818573,Interpretability and Analysis of Models for NLP,0.558116614818573,True
"We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images. By multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting. We show that such multi-tasking improves over a BERT pretrained baseline, largely due to multi-tasking with very large dialogue datasets in a similar domain, and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the finetune and task transfer settings. We obtain stateof-the-art results on many of the tasks, providing a strong baseline for this challenge.",Dialogue and Interactive Systems,"[0.003027913859114051, 0.006456808652728796, 0.8514295220375061, 0.014594064094126225, 0.003779251128435135, 0.00742950476706028, 0.0017214345280081034, 0.0018563023768365383, 0.0020624962635338306, 0.028950657695531845, 0.004305945243686438, 0.0016376265557482839, 0.0032427911646664143, 0.011287710629403591, 0.018270883709192276, 0.001619343413040042, 0.004609997384250164, 0.008666092529892921, 0.014318588189780712, 0.002167715923860669, 0.0028643389232456684, 0.0025563938543200493, 0.0031446642242372036]",0.8514295220375061,Dialogue and Interactive Systems,0.8514295220375061,True
"Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing, and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",Machine Translation and Multilinguality,"[0.0016435073921456933, 0.0011833282187581062, 0.0012794533977285028, 0.0005964552983641624, 0.003169165924191475, 0.0016308885533362627, 0.0011265737703070045, 0.0026986338198184967, 0.009209693409502506, 0.0008020921959541738, 0.011819826439023018, 0.9409775733947754, 0.002558256732299924, 0.001100608380511403, 0.003877336857840419, 0.003995315637439489, 0.0014165893662720919, 0.0024163834750652313, 0.0030714250169694424, 0.0009395857341587543, 0.0018603825010359287, 0.0017647576751187444, 0.0008621856686659157]",0.9409775733947754,Machine Translation and Multilinguality,0.9409775733947754,True
"Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis -basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.",Interpretability and Analysis of Models for NLP,"[0.04965771734714508, 0.012746228836476803, 0.006072069983929396, 0.022763043642044067, 0.013660326600074768, 0.005903915502130985, 0.0047590164467692375, 0.008387060835957527, 0.4184674918651581, 0.019162513315677643, 0.07628306746482849, 0.00859037321060896, 0.014795796945691109, 0.15072080492973328, 0.06924387067556381, 0.023338086903095245, 0.05154724419116974, 0.008158364333212376, 0.008027181960642338, 0.0016493919538334012, 0.014013923704624176, 0.005473860539495945, 0.0065786223858594894]",0.4184674918651581,Interpretability and Analysis of Models for NLP,0.4184674918651581,True
"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",Machine Learning for NLP,"[0.005353027489036322, 0.006989465095102787, 0.003567456267774105, 0.004756807815283537, 0.006588937249034643, 0.018591586500406265, 0.019702136516571045, 0.45210346579551697, 0.03431417793035507, 0.003077616449445486, 0.32405173778533936, 0.004072448238730431, 0.002785125281661749, 0.011927202343940735, 0.00991010945290327, 0.01396477222442627, 0.03443353250622749, 0.00975869782269001, 0.0036601105239242315, 0.005549190100282431, 0.01566922664642334, 0.005381820257753134, 0.003791427705436945]",0.45210346579551697,Information Retrieval and Text Mining,0.32405173778533936,False
"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of schema linking. We also build a simple BERT-based baseline, called Schema-Linking SQL (SLSQL) to perform a data-driven study. We find when schema linking is done well, SLSQL demonstrates good performance on Spider despite its structural simplicity. Many remaining errors are attributable to corpus noise. This suggests schema linking is the crux for the current textto-SQL task. Our analytic studies provide insights on the characteristics of schema linking for future developments of text-to-SQL tasks. 1 * Equal contribution.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.00349981221370399, 0.0037816360127180815, 0.01609308086335659, 0.010560291819274426, 0.0026889874134212732, 0.008831839077174664, 0.006991968490183353, 0.005516997072845697, 0.006255780812352896, 0.0025110929273068905, 0.01141582801938057, 0.0014562823344022036, 0.00196429924108088, 0.012028093449771404, 0.020729640498757362, 0.008391122333705425, 0.8471428751945496, 0.004324793815612793, 0.003361843526363373, 0.00231715920381248, 0.015213078819215298, 0.0018429571064189076, 0.0030807277653366327]",0.8471428751945496,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.8471428751945496,True
"A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining labeled data can be costly, especially for multiple language varieties and dialects. We propose to self-train pre-trained language models in zero-and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as 10% F 1 (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks. 1","Syntax: Tagging, Chunking and Parsing","[0.014401132240891457, 0.016931315883994102, 0.03358398377895355, 0.014507283456623554, 0.016084151342511177, 0.022807782515883446, 0.019921541213989258, 0.02164723351597786, 0.04331612586975098, 0.005987304728478193, 0.25701904296875, 0.10670642554759979, 0.07700183987617493, 0.01229123305529356, 0.1013559028506279, 0.016211437061429024, 0.03797709196805954, 0.027309313416481018, 0.016946667805314064, 0.007556730881333351, 0.11284738034009933, 0.011712100356817245, 0.0058770193718373775]",0.25701904296875,Machine Learning for NLP,0.11284738034009933,False
"This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrievalbased Question Answering (QA) systems. Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers. For this purpose, we build a three-way multiclassifier, which decides if an answer supports, refutes, or is neutral with respect to another one. More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components. We tested our models on Wik-iQA, TREC-QA, and a real-world dataset. The results show that our models obtain the new state of the art in AS2. * Work done while the author was an intern at Amazon Alexa Claim: Joe Walsh was inducted in 2001. Ev1: As a member of the Eagles, Walsh was inducted into the Rock and Roll Hall of Fame in 1998, and into the Vocal Group Hall of Fame in 2001. Ev2: Joseph Fidler Walsh (born November 20, 1947) is an American singer songwriter, composer, multiinstrumentalist and record producer. Ev3: Walsh was awarded with the Vocal Group Hall of Fame in 2001.",Question Answering,"[0.006072253920137882, 0.0038958534132689238, 0.006211990024894476, 0.007088715210556984, 0.00749961705878377, 0.020855696871876717, 0.008291306905448437, 0.05486717075109482, 0.01013920921832323, 0.00757972477003932, 0.025060249492526054, 0.006083212327212095, 0.004284490831196308, 0.7640339136123657, 0.02073243446648121, 0.004301649518311024, 0.012264121323823929, 0.00845328252762556, 0.004412584938108921, 0.002489227568730712, 0.006067680660635233, 0.0038951216265559196, 0.0054204934276640415]",0.7640339136123657,Question Answering,0.7640339136123657,True
"Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic and useful, we propose to study open-ended commonsense reasoning (OpenCSR) -the task of answering a commonsense question without any predefined choices -using as a resource only a knowledge corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DRFACT, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt three popular multiple-choice datasets, and collect multiple new answers to each test question via crowd-sourcing. Experiments show that DRFACT outperforms strong baseline methods by a large margin. 1",Question Answering,"[0.0032011782750487328, 0.005122913513332605, 0.005251048598438501, 0.005321389064192772, 0.004898355808109045, 0.00499735726043582, 0.003927547950297594, 0.010755850002169609, 0.007089166436344385, 0.0052300128154456615, 0.011194029822945595, 0.00129214720800519, 0.0016859133029356599, 0.8696437478065491, 0.018048526719212532, 0.0036774585023522377, 0.021221483126282692, 0.005464649293571711, 0.0018546771025285125, 0.0007912066648714244, 0.0037427707575261593, 0.0018488537753000855, 0.0037397437263280153]",0.8696437478065491,Question Answering,0.8696437478065491,True
"Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to identify and classify named entity mentions. Prototypical network shows superior performance on few-shot NER. However, existing prototypical methods fail to differentiate rich semantics in other-class words, which will aggravate overfitting under few shot scenario. To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different undefined classes from the other class to improve few-shot NER. With these extra-labeled undefined classes, our method will improve the discriminative ability of NER classifier and enhance the understanding of predefined classes with stand-by semantic knowledge. Experimental results demonstrate that our model outperforms five state-of-the-art models in both 1shot and 5-shots settings on four NER benchmarks. We will release the code upon acceptance. The source code is released on https: //github.com/shuaiwa16/OtherClassNER.git.",Information Extraction,"[0.001272614928893745, 0.0038630771450698376, 0.001885920180939138, 0.002559061860665679, 0.0024076104164123535, 0.0028213877230882645, 0.85845547914505, 0.03180800378322601, 0.0025089262053370476, 0.002395935822278261, 0.01032928004860878, 0.0010476424358785152, 0.0024644408840686083, 0.00478064501658082, 0.007179442327469587, 0.004771730862557888, 0.009021253325045109, 0.007510852999985218, 0.0018769863527268171, 0.004580086097121239, 0.030239656567573547, 0.003091108752414584, 0.003128831507638097]",0.85845547914505,Information Extraction,0.85845547914505,True
"Speech disfluencies are prevalent in spontaneous speech. The rising popularity of voice assistants presents a growing need to handle naturally occurring disfluencies. Semantic parsing is a key component for understanding user utterances in voice assistants, yet most semantic parsing research to date focuses on written text. In this paper, we investigate semantic parsing of disfluent speech with the ATIS dataset. We find that a state-of-the-art semantic parser does not seamlessly handle disfluencies. We experiment with adding real and synthetic disfluencies at training time and find that adding synthetic disfluencies not only improves model performance by up to 39% but can also outperform adding real disfluencies in the ATIS dataset.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.007810784503817558, 0.011937793344259262, 0.4259745478630066, 0.03540143370628357, 0.006958732847124338, 0.013443845324218273, 0.0040558273904025555, 0.0063653867691755295, 0.010364721529185772, 0.02410515397787094, 0.011631357483565807, 0.007525938097387552, 0.010541696101427078, 0.006599400192499161, 0.03696262463927269, 0.005444209557026625, 0.04581372067332268, 0.013136330060660839, 0.27266061305999756, 0.024761786684393883, 0.007315682712942362, 0.005508176516741514, 0.005680066533386707]",0.4259745478630066,Dialogue and Interactive Systems,0.04581372067332268,False
"Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT Englishâ‡’German and Englishâ‡’Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. 1 * Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab.",Machine Translation and Multilinguality,"[0.002986293053254485, 0.0018121030880138278, 0.0037964950315654278, 0.0012514570262283087, 0.005821550264954567, 0.021569643169641495, 0.0064011625945568085, 0.014285550452768803, 0.013587831519544125, 0.0015002733562141657, 0.10110200196504593, 0.7550470232963562, 0.005190413445234299, 0.002130209468305111, 0.011588852852582932, 0.0028948395047336817, 0.009591520763933659, 0.004942610859870911, 0.00656595453619957, 0.006676652934402227, 0.013435409404337406, 0.0055530741810798645, 0.0022690899204462767]",0.7550470232963562,Machine Translation and Multilinguality,0.7550470232963562,True
"Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a languageagnostic probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 15 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method -multilingual contrastive pretraining (MCP). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R L ) 1 . Birds have [mask] . LAMA Probe Where do adults usually use glue sticks? A) school B) drawer C) office CommonsenseQA SWAG/CODAH The chef drops the piece of shrimp in the fryer. â†’ 3",Machine Translation and Multilinguality,"[0.011771890334784985, 0.009754053317010403, 0.004847335163503885, 0.006801380310207605, 0.017577512189745903, 0.006117180455476046, 0.005686647724360228, 0.013964925892651081, 0.3760647177696228, 0.008479878306388855, 0.12259019911289215, 0.101371169090271, 0.0075399000197649, 0.011643582955002785, 0.04725176468491554, 0.020738478749990463, 0.16435809433460236, 0.009392554871737957, 0.013117115013301373, 0.0034116795286536217, 0.022714713588356972, 0.00869300588965416, 0.00611224165186286]",0.3760647177696228,Interpretability and Analysis of Models for NLP,0.101371169090271,False
"Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. In MMT, equally treating text and images may encode too much irrelevant information from images which may introduce noise. In this paper, we propose the multimodal self-attention in Transformer to solve the issues above. The proposed method learns the representations of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",Speech and Multimodality,"[0.006034495774656534, 0.008741016499698162, 0.03745885565876961, 0.008246599696576595, 0.018083225935697556, 0.00927718635648489, 0.010298819281160831, 0.011625006794929504, 0.021649163216352463, 0.36681023240089417, 0.011969645507633686, 0.06136665865778923, 0.005387960001826286, 0.0068144965916872025, 0.031105834990739822, 0.006450545974075794, 0.007114065811038017, 0.00676768971607089, 0.32298368215560913, 0.01690203696489334, 0.003895929316058755, 0.0119779659435153, 0.009038863703608513]",0.36681023240089417,"Language Grounding to Vision, Robotics and Beyond",0.32298368215560913,False
"In this work, we introduce X-FACT: the largest publicly available multilingual dataset for factual verification of naturally existing realworld claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models. Using state-of-the-art multilingual transformer-based models, we develop several automated fact-checking models that, along with textual claims, make use of additional metadata and evidence from news stories retrieved using a search engine. Empirically, our best model attains an F-score of around 40%, suggesting that our dataset is a challenging benchmark for evaluation of multilingual fact-checking models.",Computational Social Science and Social Media,"[0.009124995209276676, 0.0684734359383583, 0.010664866305887699, 0.03246840089559555, 0.028378339484333992, 0.013419384136795998, 0.011150695383548737, 0.01928251050412655, 0.051018282771110535, 0.008035270497202873, 0.016383640468120575, 0.008944379165768623, 0.00398605689406395, 0.024252893403172493, 0.20729057490825653, 0.024978885427117348, 0.3174493610858917, 0.09658803790807724, 0.00872256513684988, 0.005038048140704632, 0.017719892784953117, 0.00664667971432209, 0.009982866235077381]",0.3174493610858917,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.0684734359383583,False
"This paper empirically studies whether BERT can really learn to conduct natural language inference (NLI) without utilizing hidden dataset bias; and how efficiently it can learn if it could. This is done via creating a simple entailment judgment case which involves only binary predicates in plain English. The results show that the learning process of BERT is very slow. However, the efficiency of learning can be greatly improved (data reduction by a factor of 1,500) if task-related features are added. This suggests that domain knowledge greatly helps when conducting NLI with neural networks.",Machine Learning for NLP,"[0.0028632069006562233, 0.0018958613509312272, 0.005952263250946999, 0.005200687795877457, 0.0013848054222762585, 0.006754053756594658, 0.010348317213356495, 0.005138852633535862, 0.006235574837774038, 0.0023625276517122984, 0.020593250170350075, 0.001191006158478558, 0.0013400902971625328, 0.016393650323152542, 0.007616401184350252, 0.00720352353528142, 0.8740749955177307, 0.0019084560917690396, 0.0027791080065071583, 0.001361335744149983, 0.01275777816772461, 0.0014164949534460902, 0.003227816428989172]",0.8740749955177307,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.020593250170350075,False
"Selecting input features of top relevance has become a popular method for building selfexplaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity. Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations. We evaluate our model on the Stack-Exchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models. â€  * Denotes equal contribution.",Interpretability and Analysis of Models for NLP,"[0.005211031995713711, 0.0036968549247831106, 0.0030568577349185944, 0.002720803255215287, 0.006150634028017521, 0.026271915063261986, 0.03963907063007355, 0.14715227484703064, 0.04782653599977493, 0.0020128521136939526, 0.626794159412384, 0.011071105487644672, 0.003328675404191017, 0.009469364769756794, 0.008076456375420094, 0.004654998425394297, 0.021349238231778145, 0.005074704065918922, 0.004228577483445406, 0.007851390168070793, 0.007205510046333075, 0.004306186456233263, 0.0028508591931313276]",0.626794159412384,Machine Learning for NLP,0.04782653599977493,False
"In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input. The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions. Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature. In this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy. Earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as Shapley value) for individual feature attributions and introduced their own extensions to model interactions. In contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature. We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts. We demonstrate that our proposed method, IDG, satisfies all the axioms. Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis. Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions.",Interpretability and Analysis of Models for NLP,"[0.007509893737733364, 0.008977042511105537, 0.0021090086083859205, 0.00990100484341383, 0.012581264600157738, 0.004966054577380419, 0.011499756015837193, 0.016241218894720078, 0.6900690197944641, 0.012323368340730667, 0.05039599537849426, 0.006066291593015194, 0.003991334233433008, 0.005926416255533695, 0.028337741270661354, 0.018953653052449226, 0.02449565939605236, 0.05553138256072998, 0.0069734216667711735, 0.004276352934539318, 0.008955203928053379, 0.006500910967588425, 0.0034179596696048975]",0.6900690197944641,Interpretability and Analysis of Models for NLP,0.6900690197944641,True
"Knowledge distillation has proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the large neural networks, e.g., parameters. Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88âˆ¼2.94Ã— faster than the large network but with competitive BLEU performance. When fixing the size of the small networks, weight distillation outperforms knowledge distillation by 0.51âˆ¼1.82 BLEU points.",Machine Learning for NLP,"[0.0033819451928138733, 0.002578476909548044, 0.004955987446010113, 0.0015526399947702885, 0.00866884645074606, 0.01249665766954422, 0.014805563725531101, 0.02056255377829075, 0.08009380847215652, 0.0023711638059467077, 0.45464998483657837, 0.33342447876930237, 0.004780858289450407, 0.003048331942409277, 0.007677052635699511, 0.00300373462960124, 0.005869411863386631, 0.006856098305433989, 0.0069001223891973495, 0.006950994487851858, 0.008201568387448788, 0.0048507810570299625, 0.0023190330248326063]",0.45464998483657837,Machine Learning for NLP,0.45464998483657837,True
"Recent advances in Named Entity Recognition (NER)  show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains. 1",Information Extraction,"[0.0011575566604733467, 0.002689085667952895, 0.001775059849023819, 0.001990570919588208, 0.0017870890442281961, 0.0026682086754590273, 0.8618593215942383, 0.025260936468839645, 0.0021368430461734533, 0.0023477047216147184, 0.010093823075294495, 0.0010469942353665829, 0.0018644239753484726, 0.005754041485488415, 0.004645160399377346, 0.004529978148639202, 0.017755022272467613, 0.004397991579025984, 0.0018726317211985588, 0.0034213836770504713, 0.03489312902092934, 0.002760462462902069, 0.0032925191335380077]",0.8618593215942383,Information Extraction,0.8618593215942383,True
"While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the parsers. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversityaware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation.",Machine Learning for NLP,"[0.0037348729092627764, 0.006502856500446796, 0.011234037578105927, 0.005707013886421919, 0.003571911249309778, 0.008325112983584404, 0.05244472250342369, 0.012022783979773521, 0.007478577084839344, 0.0030835734214633703, 0.03700431063771248, 0.0056059942580759525, 0.0077800205908715725, 0.006145699415355921, 0.021222902461886406, 0.00570286251604557, 0.08264189213514328, 0.018807413056492805, 0.004201485309749842, 0.007992919534444809, 0.6796953082084656, 0.005156471859663725, 0.0039372518658638]",0.6796953082084656,"Syntax: Tagging, Chunking and Parsing",0.03700431063771248,False
"Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models. Introducing the benefits of structure to inform neural models presents a methodological challenge. In this paper, we present a structured tuning framework to improve models using softened constraints only at training time. Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components. We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.004661333281546831, 0.0027754439506679773, 0.017719727009534836, 0.006221995688974857, 0.003554445691406727, 0.008117152377963066, 0.009090506471693516, 0.006069838535040617, 0.016097910702228546, 0.0064196595922112465, 0.04894466698169708, 0.00283918809145689, 0.0018811076879501343, 0.009825474582612514, 0.008070365525782108, 0.028184087947010994, 0.7889891862869263, 0.003693555947393179, 0.003714428748935461, 0.0019474246073514223, 0.01600409485399723, 0.0015674358000978827, 0.003611015621572733]",0.7889891862869263,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.7889891862869263,True
"Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems. In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples. We first show the current NMT adversarial attacks may be improperly estimated by the commonly used monodirectional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks. Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result. We then propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures. Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation. Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators.",Machine Translation and Multilinguality,"[0.0011216412531211972, 0.001222514547407627, 0.0011547153117135167, 0.0005037866067141294, 0.0038259366992861032, 0.00143501628190279, 0.0008464658749289811, 0.001519112498499453, 0.00563006242737174, 0.0011722419876605272, 0.0044156466610729694, 0.9555733799934387, 0.00207176199182868, 0.000721834076102823, 0.006289830897003412, 0.001151052420027554, 0.0008190840599127114, 0.002002756344154477, 0.002817438915371895, 0.001292363042011857, 0.0015243099769577384, 0.001893462031148374, 0.0009956029243767262]",0.9555733799934387,Machine Translation and Multilinguality,0.9555733799934387,True
"Most fact checking models for automatic fake news detection are based on reasoning: given a claim with associated evidence, the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence. When these models perform well, it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim. In this paper, we investigate this assumption of reasoning, by exploring the relationship and importance of both claim and evidence. Surprisingly, we find on political fact checking datasets that most often the highest effectiveness is obtained by utilizing only the evidence, as the impact of including the claim is either negligible or harmful to the effectiveness. This highlights an important problem in what constitutes evidence in existing approaches for automatic fake news detection.",Computational Social Science and Social Media,"[0.005623821634799242, 0.6591158509254456, 0.002768503502011299, 0.017941324040293694, 0.04059959203004837, 0.0028554496821016073, 0.004440310411155224, 0.007233939599245787, 0.011597701348364353, 0.006090330891311169, 0.005138371139764786, 0.0031755599193274975, 0.006699690129607916, 0.007223375607281923, 0.08132629841566086, 0.007824152708053589, 0.0067580696195364, 0.09928975999355316, 0.006080117076635361, 0.002937775570899248, 0.005922740790992975, 0.004089105874300003, 0.0052681840024888515]",0.6591158509254456,Computational Social Science and Social Media,0.6591158509254456,True
"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training opendomain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.",Dialogue and Interactive Systems,"[0.0009420074056833982, 0.0016285348683595657, 0.944462776184082, 0.005763303954154253, 0.001148819224908948, 0.0021704710088670254, 0.0018694655736908317, 0.0018731462769210339, 0.0008796919137239456, 0.0012746715219691396, 0.00685684522613883, 0.002600762527436018, 0.0018690137658268213, 0.0034347388427704573, 0.004911960568279028, 0.0010685942834243178, 0.0023790323175489902, 0.0042761038057506084, 0.004676912445574999, 0.0014042563270777464, 0.002060342114418745, 0.0010641897097229958, 0.0013843165943399072]",0.944462776184082,Dialogue and Interactive Systems,0.944462776184082,True
"Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into questionanswer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.",Generation,"[0.007408688776195049, 0.002873760648071766, 0.015104308724403381, 0.004660116508603096, 0.0030064452439546585, 0.7542351484298706, 0.002560844412073493, 0.007048581726849079, 0.005326160695403814, 0.004332184791564941, 0.007307345513254404, 0.003939782734960318, 0.007524081505835056, 0.039462994784116745, 0.049772802740335464, 0.0033707458060234785, 0.04131808131933212, 0.011483395472168922, 0.00324655557051301, 0.0037625522818416357, 0.01272515021264553, 0.005454633384943008, 0.004075636155903339]",0.7542351484298706,Generation,0.7542351484298706,True
"Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event. To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources. Next, we propose several documentlevel neural-network models to automatically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-theart performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research 1 .",Discourse and Pragmatics,"[0.008919061161577702, 0.0358441025018692, 0.009255597367882729, 0.6332733631134033, 0.014080916531383991, 0.004560557194054127, 0.02634863369166851, 0.027978451922535896, 0.006452525965869427, 0.007504662964493036, 0.006374788470566273, 0.0030394436325877905, 0.011326663196086884, 0.018992261961102486, 0.036724500358104706, 0.04656176641583443, 0.01961309090256691, 0.03550468757748604, 0.007833044975996017, 0.016414444893598557, 0.006718567572534084, 0.005750727839767933, 0.010928085073828697]",0.6332733631134033,Discourse and Pragmatics,0.6332733631134033,True
"Automatic unreliable news detection is a research problem with great potential impact. Recently, several papers have shown promising results on large-scale news datasets with models that only use the article itself without resorting to any fact-checking mechanism or retrieving any supporting evidence. In this work, we take a closer look at these datasets. While they all provide valuable resources for future research, we observe a number of problems that may lead to results that do not generalize in more realistic settings. Specifically, we show that selection bias during data collection leads to undesired artifacts in the datasets. In addition, while most systems train and predict at the level of individual articles, overlapping article sources in the training and evaluation data can provide a strong confounding factor that models can exploit. In the presence of this confounding factor, the models can achieve good performance by directly memorizing the site-label mapping instead of modeling the real task of unreliable news detection. We observed a significant drop (>10%) in accuracy for all models tested in a clean split with no train/test source overlap. Using the observations and experimental results, we provide practical suggestions on how to create more reliable datasets for the unreliable news detection task. We suggest future dataset creation include a simple model as a difficulty/bias probe and future model development use a clean non-overlapping site and date split. 1",Interpretability and Analysis of Models for NLP,"[0.0076515888795256615, 0.11169332265853882, 0.0030797056388109922, 0.022338971495628357, 0.031406987458467484, 0.010187329724431038, 0.06591281294822693, 0.2510528266429901, 0.042034588754177094, 0.00899181142449379, 0.01967109926044941, 0.010381038300693035, 0.008393942378461361, 0.016174085438251495, 0.18773603439331055, 0.01565687358379364, 0.014242873527109623, 0.07178434729576111, 0.007513849530369043, 0.05747310444712639, 0.014489092864096165, 0.01374400407075882, 0.008389738388359547]",0.2510528266429901,Information Retrieval and Text Mining,0.042034588754177094,False
"Product-related question answering platforms nowadays are widely employed in many Ecommerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce Answer-Fact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.",Question Answering,"[0.004475281108170748, 0.008576726540923119, 0.0036899703554809093, 0.007404756266623735, 0.008175130933523178, 0.007511010393500328, 0.004262726288288832, 0.018714195117354393, 0.012570132501423359, 0.005437139887362719, 0.020793650299310684, 0.002136101247742772, 0.0022403919138014317, 0.8338916301727295, 0.025111021474003792, 0.003195778001099825, 0.010044216178357601, 0.008555571548640728, 0.0024702418595552444, 0.0013920923229306936, 0.002902898471802473, 0.002367215696722269, 0.004082090221345425]",0.8338916301727295,Question Answering,0.8338916301727295,True
"Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks. 1",Machine Learning for NLP,"[0.0029217100236564875, 0.0016255315858870745, 0.00657565938308835, 0.00143699801992625, 0.006210661493241787, 0.012297668494284153, 0.005502541083842516, 0.010917638428509235, 0.061119671911001205, 0.002609079470857978, 0.7582218050956726, 0.08478828519582748, 0.002926259534433484, 0.003572949441149831, 0.005561952944844961, 0.002118603093549609, 0.007215763907879591, 0.0032731492538005114, 0.005297576542943716, 0.0028622043319046497, 0.008429443463683128, 0.0029198131524026394, 0.0015950534725561738]",0.7582218050956726,Machine Learning for NLP,0.7582218050956726,True
"This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.",Question Answering,"[0.009586604312062263, 0.0053344969637691975, 0.017762839794158936, 0.008309387601912022, 0.00839928351342678, 0.021502671763300896, 0.013466684147715569, 0.012253819033503532, 0.01953176036477089, 0.43513742089271545, 0.007472679018974304, 0.002369793364778161, 0.005563199520111084, 0.31783974170684814, 0.017843252047896385, 0.011159559711813927, 0.0375472828745842, 0.007335759233683348, 0.0135806268081069, 0.002247400349006057, 0.010933887213468552, 0.00555741973221302, 0.009264442138373852]",0.43513742089271545,"Language Grounding to Vision, Robotics and Beyond",0.31783974170684814,False
"We describe Mega-COV, a billion-scale dataset from Twitter for studying COVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as back as 2007), multilingual (comes in 100+ languages), and has a significant number of location-tagged tweets (âˆ¼ 169M tweets). We release tweet IDs from the dataset. We also develop two powerful models, one for identifying whether or not a tweet is related to the pandemic (best F 1 =97%) and another for detecting misinformation about COVID-19 (best F 1 =92%). A human annotation study reveals the utility of our models on a subset of Mega-COV. Our data and models can be useful for studying a wide host of phenomena related to the pandemic. Mega-COV and our models are publicly available.",Resources and Evaluation,"[0.011100846342742443, 0.5081393122673035, 0.003919035196304321, 0.014720818027853966, 0.047348469495773315, 0.00514649786055088, 0.00584424240514636, 0.012055826373398304, 0.01606369949877262, 0.009314625523984432, 0.012116006575524807, 0.008697064593434334, 0.01592244766652584, 0.005391791928559542, 0.19917750358581543, 0.012010600417852402, 0.009034616872668266, 0.06504548341035843, 0.008623402565717697, 0.0060364906676113605, 0.00944345723837614, 0.008735544048249722, 0.006112284958362579]",0.5081393122673035,Computational Social Science and Social Media,0.19917750358581543,False
"In entity linking, mentions of named entities in raw text are disambiguated against a knowledge base (KB). This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training. Our approach relies on methods to flexibly convert entities with several attribute-value pairs from arbitrary KBs into flat strings, which we use in conjunction with state-of-the-art models for zero-shot linking. We further improve the generalization of our model using two regularization schemes based on shuffling of entity attributes and handling of unseen attributes. Experiments on English datasets where models are trained on the CoNLL dataset, and tested on the TAC-KBP 2010 dataset show that our models are 12% (absolute) more accurate than baseline models that simply flatten entities from the target KB. Unlike prior work, our approach also allows for seamlessly combining multiple training datasets. We test this ability by adding both a completely different dataset (Wikia), as well as increasing amount of training data from the TAC-KBP 2010 training set. Our models are more accurate across the board compared to baselines.",Information Extraction,"[0.0028674034401774406, 0.006959399674087763, 0.0030334594193845987, 0.00671416986733675, 0.005424929317086935, 0.005511153489351273, 0.7538549304008484, 0.045356765389442444, 0.004339390899986029, 0.0031861374154686928, 0.03225846588611603, 0.002150420332327485, 0.004160718992352486, 0.018684877082705498, 0.009503231383860111, 0.017275825142860413, 0.02527295984327793, 0.005797248799353838, 0.0022344535682350397, 0.003439675085246563, 0.03147859126329422, 0.004030611831694841, 0.006465306971222162]",0.7538549304008484,Information Extraction,0.7538549304008484,True
"A dialogue is essentially a multi-turn interaction among interlocutors. Effective evaluation metrics should reflect the dynamics of such interaction. Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics. To this end, we propose DynaEval 1 , a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted to model a dialogue in totality, where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances. A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples. Experiments show that DynaEval significantly outperforms the state-of-the-art dialogue coherence model, and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level.",Dialogue and Interactive Systems,"[0.0018616935703903437, 0.005025101359933615, 0.8813921809196472, 0.014570250175893307, 0.002665593521669507, 0.003819329896941781, 0.001738960389047861, 0.0024070092476904392, 0.0027519098948687315, 0.004113170318305492, 0.009002845734357834, 0.0032988532911986113, 0.0032874825410544872, 0.00492063257843256, 0.018936486914753914, 0.0015632109716534615, 0.0038971654139459133, 0.013448375277221203, 0.011118976399302483, 0.003123234724625945, 0.002979111624881625, 0.001886191195808351, 0.002192216459661722]",0.8813921809196472,Dialogue and Interactive Systems,0.8813921809196472,True
"Recent pretrained language models ""solved"" many reading comprehension benchmarks, where questions are written with the access to the evidence document. However, datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging. We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise, on Natural Questions and TyDi QA. Our controlled experiments suggest two headrooms -paragraph selection and answerability prediction, i.e. whether the paired evidence document contains the answer to the query or not. When provided with a gold paragraph and knowing when to abstain from answering, existing models easily outperform a human annotator. However, predicting answerability itself remains challenging. We manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer. With this new data, we conduct percategory answerability prediction, revealing issues in the current dataset collection as well as task formulation. Together, our study points to avenues for future research in informationseeking question answering, both for dataset creation and model development. 1",Question Answering,"[0.005539577454328537, 0.0034773864317685366, 0.003861213568598032, 0.008363423869013786, 0.006017686333507299, 0.00989726185798645, 0.004558922722935677, 0.025722641497850418, 0.015022290870547295, 0.005428296979516745, 0.02135736495256424, 0.0016682749846950173, 0.0030030752532184124, 0.8137640953063965, 0.02735220640897751, 0.0035442274529486895, 0.02168653905391693, 0.004222577903419733, 0.0025448589585721493, 0.0017618454294279218, 0.004668392241001129, 0.0022788115311414003, 0.00425900099799037]",0.8137640953063965,Question Answering,0.8137640953063965,True
"Conditional masked language model (CMLM) training has proven successful for nonautoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard ""mask-predict"" algorithm, and provide analyses of its behavior on machine translation tasks.",Machine Translation and Multilinguality,"[0.005052546970546246, 0.002093758201226592, 0.012609925121068954, 0.0030145072378218174, 0.0033674202859401703, 0.059484876692295074, 0.004547140561044216, 0.012890798039734364, 0.024215837940573692, 0.002669604029506445, 0.7249895334243774, 0.016901107504963875, 0.004832678008824587, 0.004160851240158081, 0.008616182021796703, 0.0034568682312965393, 0.07208544760942459, 0.001935036270879209, 0.003563615959137678, 0.004614686127752066, 0.02024933323264122, 0.003195196855813265, 0.0014530187472701073]",0.7249895334243774,Machine Learning for NLP,0.016901107504963875,False
"This paper proposes an open-domain method for automatically annotating modifier constituents (""20th-century"") within Wikipedia categories (""20th-century male writers"") with properties (""date of birth""). The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories. In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semanticallyanchored properties, rather than as mere strings in a previous method. It does so at a better trade-off between precision and recall.",Semantics: Lexical Semantics,"[0.055305104702711105, 0.024050286039710045, 0.006588499061763287, 0.026668556034564972, 0.012635991908609867, 0.02397550828754902, 0.021658984944224358, 0.015462828800082207, 0.04394609108567238, 0.008976399898529053, 0.031887151300907135, 0.004811623599380255, 0.10313806682825089, 0.026355991140007973, 0.16181765496730804, 0.16560998558998108, 0.08586885035037994, 0.05401044338941574, 0.005141057074069977, 0.006229748018085957, 0.08544899523258209, 0.01934652216732502, 0.011065708473324776]",0.16560998558998108,Semantics: Lexical Semantics,0.16560998558998108,True
"Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models' application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fillin-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language-bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.","Language Grounding to Vision, Robotics and Beyond","[0.007877734489738941, 0.01002763956785202, 0.01862099952995777, 0.01242732722312212, 0.011365191079676151, 0.012147567234933376, 0.006017721723765135, 0.012830265797674656, 0.0098514873534441, 0.3789004385471344, 0.008885572664439678, 0.004470743704587221, 0.005753825884312391, 0.37542906403541565, 0.053294017910957336, 0.003933582920581102, 0.012556339614093304, 0.006015512626618147, 0.023598290979862213, 0.0034789112396538258, 0.005922515876591206, 0.008006439544260502, 0.008588851429522038]",0.3789004385471344,"Language Grounding to Vision, Robotics and Beyond",0.3789004385471344,True
"Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent. 1",Generation,"[0.004460289143025875, 0.00201610941439867, 0.004722870886325836, 0.001938625588081777, 0.003387424862012267, 0.8116773962974548, 0.0018625721568241715, 0.011601476930081844, 0.009704370982944965, 0.0024290920700877905, 0.05339141935110092, 0.0029223323799669743, 0.004194107372313738, 0.004768589045852423, 0.027459658682346344, 0.0024491138756275177, 0.027375124394893646, 0.00435678381472826, 0.0023273315746337175, 0.006491557694971561, 0.005407916847616434, 0.003123725298792124, 0.001932065119035542]",0.8116773962974548,Generation,0.8116773962974548,True
"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9%), the accuracy of a highperformance transfer learning model is reasonably low (76.0%). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research. 1","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.00366994715295732, 0.004422895610332489, 0.013967840000987053, 0.018806805834174156, 0.003144210670143366, 0.010681524872779846, 0.09298334270715714, 0.015360778197646141, 0.010105371475219727, 0.004346353467553854, 0.036264076828956604, 0.0015566566726192832, 0.0026904940605163574, 0.09142579883337021, 0.014753150753676891, 0.010802511125802994, 0.6226392388343811, 0.007715855725109577, 0.0037512436974793673, 0.0033789861481636763, 0.01853400468826294, 0.0027868954930454493, 0.006212127860635519]",0.6226392388343811,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.6226392388343811,True
"The state-of-the-art on basic, singleantecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in Time-Warner is considering a legal challenge to Telecommunications Inc's plan to buy half of Showtime Networks Inc-a move that could lead to all-out war between the two powerful companies. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora; as a result, it is not annotated in many datasets designed to test coreference, and previous work on resolving this type of anaphora was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics. 1",Discourse and Pragmatics,"[0.005937013775110245, 0.01574605703353882, 0.02271328493952751, 0.41510844230651855, 0.0132747245952487, 0.014398389495909214, 0.05082153156399727, 0.025543205440044403, 0.016325874254107475, 0.005970339756458998, 0.01452779769897461, 0.0030234763398766518, 0.011926760897040367, 0.012315201573073864, 0.0993218719959259, 0.022524474188685417, 0.074087955057621, 0.08917610347270966, 0.009019959717988968, 0.04005475342273712, 0.01970062404870987, 0.0072885905392467976, 0.011193608865141869]",0.41510844230651855,Discourse and Pragmatics,0.41510844230651855,True
"Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pretraining a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pretrained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledgegrounded dialogue benchmarks indicate that our model achieves better performance over baseline models.",Dialogue and Interactive Systems,"[0.001040591043420136, 0.0016747744521126151, 0.9247555136680603, 0.00839910190552473, 0.0011069313623011112, 0.006775634828954935, 0.003179420018568635, 0.0024729925207793713, 0.0011791069991886616, 0.0016000763280317187, 0.008689693175256252, 0.002310045063495636, 0.0019841238390654325, 0.004781786352396011, 0.005567237734794617, 0.0010008568642660975, 0.0039025440346449614, 0.006697810720652342, 0.00460817152634263, 0.0019882512278854847, 0.0033254309091717005, 0.0012899679131805897, 0.0016699850093573332]",0.9247555136680603,Dialogue and Interactive Systems,0.9247555136680603,True
"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a reevaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was the investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.",Machine Translation and Multilinguality,"[0.004498529247939587, 0.00858974177390337, 0.002886832458898425, 0.002146130194887519, 0.015463612973690033, 0.00444326875731349, 0.0027887565083801746, 0.006159667856991291, 0.01942150481045246, 0.004116402473300695, 0.00811716914176941, 0.7934789657592773, 0.00826946645975113, 0.0020850012078881264, 0.07198887318372726, 0.0041966806165874004, 0.0026813300792127848, 0.010410511866211891, 0.00815674476325512, 0.004420300014317036, 0.005855731200426817, 0.006549685727804899, 0.003275019582360983]",0.7934789657592773,Machine Translation and Multilinguality,0.7934789657592773,True
"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require humanannotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).",Speech and Multimodality,"[0.07423582673072815, 0.030874988064169884, 0.043528903275728226, 0.15537136793136597, 0.02926533669233322, 0.017671111971139908, 0.008706865832209587, 0.013670294545590878, 0.09647322446107864, 0.01624373346567154, 0.13641199469566345, 0.011999932117760181, 0.05310824140906334, 0.03968800976872444, 0.1019996628165245, 0.02124384418129921, 0.018190179020166397, 0.03135591372847557, 0.050968315452337265, 0.017749687656760216, 0.013745816424489021, 0.008696149103343487, 0.008800615556538105]",0.15537136793136597,Discourse and Pragmatics,0.050968315452337265,False
"In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019)  to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.",Machine Translation and Multilinguality,"[0.006311838980764151, 0.007885099388659, 0.04432440549135208, 0.004654938355088234, 0.008093316107988358, 0.2516012489795685, 0.004781585652381182, 0.008599037304520607, 0.02448936551809311, 0.004411172587424517, 0.15495996177196503, 0.1327534317970276, 0.013397104106843472, 0.006727123167365789, 0.10633236169815063, 0.00633428618311882, 0.13682886958122253, 0.011287051253020763, 0.015284677967429161, 0.00752675486728549, 0.03130090609192848, 0.008380193263292313, 0.0037352098152041435]",0.2516012489795685,Generation,0.1327534317970276,False
"Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling. Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.",Discourse and Pragmatics,"[0.005324346479028463, 0.005901624448597431, 0.06535723060369492, 0.1143355444073677, 0.0032031831797212362, 0.016404571011662483, 0.01829385571181774, 0.011636737734079361, 0.003465520218014717, 0.005641967058181763, 0.016568729653954506, 0.001733351033180952, 0.007246948312968016, 0.034173984080553055, 0.016396820545196533, 0.01996266283094883, 0.5890339612960815, 0.006979195401072502, 0.005294247530400753, 0.007719962392002344, 0.031889066100120544, 0.0032654572278261185, 0.010170995257794857]",0.5890339612960815,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.1143355444073677,False
"This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce CHARTDIALOGS, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15, 000 dialog turns from 3, 200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the bestperforming method achieving 61% plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.",Resources and Evaluation,"[0.0013458121102303267, 0.002463952638208866, 0.9138588309288025, 0.0092899389564991, 0.0014055852079764009, 0.01264105923473835, 0.00148713868111372, 0.0011107503669336438, 0.0011428252328187227, 0.006162737030535936, 0.0038172618951648474, 0.0010382753098383546, 0.002120747696608305, 0.0045850868336856365, 0.011927527375519276, 0.0009477255516685545, 0.004911624360829592, 0.004804740194231272, 0.004814572166651487, 0.0030864435248076916, 0.0035126744769513607, 0.0016986881382763386, 0.0018259648932144046]",0.9138588309288025,Dialogue and Interactive Systems,0.011927527375519276,False
"The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts. Existing approaches generally predict the dialogue state at every turn from scratch. However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation. To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue. The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn. Empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and Multi-WOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements. 1",Dialogue and Interactive Systems,"[0.0012947269715368748, 0.002827686257660389, 0.9173327684402466, 0.00725186662748456, 0.0011194712715223432, 0.004120355471968651, 0.003862818470224738, 0.0023978049866855145, 0.000878749240655452, 0.0021938560530543327, 0.011238537728786469, 0.0020629141945391893, 0.0028719459660351276, 0.004740937612950802, 0.005758602172136307, 0.0014451101887971163, 0.006276063155382872, 0.0061805094592273235, 0.007174019701778889, 0.0014519544783979654, 0.0044196611270308495, 0.0012945783091709018, 0.0018049903446808457]",0.9173327684402466,Dialogue and Interactive Systems,0.9173327684402466,True
"The goal of Knowledge graph embedding (KGE) is to learn how to represent the lowdimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al., 2018)   takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions. The recent KBGAT (Nathani et al.,  2019)  provides another way to learn embeddings by adaptively utilizing structural information. In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE). Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings. Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information. Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReIncep-tionE achieves competitive performance compared with state-of-the-art methods.",Information Extraction,"[0.004229758400470018, 0.006152501329779625, 0.004850665107369423, 0.005839685909450054, 0.007097022607922554, 0.006082513369619846, 0.4570522904396057, 0.0784299448132515, 0.01742240972816944, 0.003653836902230978, 0.2840035855770111, 0.00530717009678483, 0.0037444503977894783, 0.013888624496757984, 0.004942660685628653, 0.02899629808962345, 0.027106018736958504, 0.010050426237285137, 0.0043858629651367664, 0.0029343042988330126, 0.013892985880374908, 0.0043302481062710285, 0.005606686230748892]",0.4570522904396057,Information Extraction,0.4570522904396057,True
"Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction's context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future. 1",Interpretability and Analysis of Models for NLP,"[0.02321580797433853, 0.014630867168307304, 0.004892569500952959, 0.005488693248480558, 0.029150987043976784, 0.008017544634640217, 0.005926957819610834, 0.007259035483002663, 0.6101387143135071, 0.024423370137810707, 0.030693314969539642, 0.1054542139172554, 0.023778943344950676, 0.0066618467681109905, 0.031173862516880035, 0.009145994670689106, 0.004816781729459763, 0.014295476488769054, 0.010412650182843208, 0.008368096314370632, 0.00934643018990755, 0.00874858908355236, 0.003959235735237598]",0.6101387143135071,Interpretability and Analysis of Models for NLP,0.6101387143135071,True
"We present a novel approach to efficiently learn a simultaneous translation model with coupled programmer-interpreter policies. First, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality while keeping the translation delay low.",Machine Translation and Multilinguality,"[0.0018171138362959027, 0.0009032347588799894, 0.0047570462338626385, 0.0008145102183334529, 0.0029215977992862463, 0.006780258379876614, 0.0015071351081132889, 0.0023839338682591915, 0.00802732165902853, 0.001552136498503387, 0.03318679332733154, 0.9048410058021545, 0.002307337708771229, 0.001257663476280868, 0.0032374393194913864, 0.0013436811277642846, 0.0047719660215079784, 0.001348891295492649, 0.00620629359036684, 0.00303070736117661, 0.0029639829881489277, 0.0026644980534911156, 0.001375512802042067]",0.9048410058021545,Machine Translation and Multilinguality,0.9048410058021545,True
"We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word's tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.","Syntax: Tagging, Chunking and Parsing","[0.007657400798052549, 0.006033820565789938, 0.015207922086119652, 0.008667212910950184, 0.0030186644289642572, 0.015186943113803864, 0.010258561000227928, 0.0067536733113229275, 0.008594553917646408, 0.006009061820805073, 0.01103907823562622, 0.002899713348597288, 0.017514456063508987, 0.007198949344456196, 0.01991961896419525, 0.009452318772673607, 0.08868954330682755, 0.009661130607128143, 0.0032890676520764828, 0.008754530921578407, 0.7240951061248779, 0.0052598691545426846, 0.004838797729462385]",0.7240951061248779,"Syntax: Tagging, Chunking and Parsing",0.7240951061248779,True
"Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.",Information Retrieval and Text Mining,"[0.0060121482238173485, 0.007372886873781681, 0.0034893725533038378, 0.005810764152556658, 0.008635579608380795, 0.014608521945774555, 0.014902033843100071, 0.631713330745697, 0.029851723462343216, 0.003322256961837411, 0.12838666141033173, 0.008560356684029102, 0.003922572825103998, 0.01307134423404932, 0.021900875493884087, 0.01653541438281536, 0.027215655893087387, 0.015153656713664532, 0.007031203247606754, 0.006187110673636198, 0.01523557584732771, 0.006811552215367556, 0.00426937872543931]",0.631713330745697,Information Retrieval and Text Mining,0.631713330745697,True
"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickrshift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work 1 .","Language Grounding to Vision, Robotics and Beyond","[0.019132737070322037, 0.006908893119543791, 0.007265743333846331, 0.005188384559005499, 0.009490333497524261, 0.01709941029548645, 0.0028351808432489634, 0.0049307141453027725, 0.0838598981499672, 0.7004727721214294, 0.007498549297451973, 0.0035046758130192757, 0.008784329518675804, 0.007270456291735172, 0.0248540248721838, 0.005142544396221638, 0.007789776660501957, 0.003715217113494873, 0.05028506740927696, 0.007099544629454613, 0.005174714140594006, 0.008037954568862915, 0.003659158246591687]",0.7004727721214294,"Language Grounding to Vision, Robotics and Beyond",0.7004727721214294,True
"Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as goldstandard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only smallscale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations.",Machine Learning for NLP,"[0.0020863825920969248, 0.008615242317318916, 0.004982601851224899, 0.0037833640817552805, 0.005292896647006273, 0.006472329143434763, 0.7333967089653015, 0.03264245763421059, 0.005215377081185579, 0.003791894530877471, 0.05106731131672859, 0.0020061435643583536, 0.003352802013978362, 0.0074272844940423965, 0.01876748353242874, 0.0048782494850456715, 0.0185119416564703, 0.019929025322198868, 0.003483155509456992, 0.004172201734036207, 0.05302217975258827, 0.0037219938822090626, 0.0033808972220867872]",0.7333967089653015,Information Extraction,0.05106731131672859,False
"Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present COINS, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply COINS to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of COINS holds the potential for controlled generation of longer sequences.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.010270069353282452, 0.006875589955598116, 0.03129110112786293, 0.015280133113265038, 0.004045035690069199, 0.36556994915008545, 0.010265354067087173, 0.018698284402489662, 0.01567547582089901, 0.026814304292201996, 0.02434556744992733, 0.0014037322252988815, 0.005337201990187168, 0.19109228253364563, 0.03539751470088959, 0.006362381391227245, 0.1779322326183319, 0.00870214868336916, 0.004571123514324427, 0.006757790222764015, 0.02068045362830162, 0.006631488911807537, 0.0060007572174072266]",0.36556994915008545,Generation,0.1779322326183319,False
"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-ofspeech taggers across two data sets and three evaluation metrics.",Machine Learning for NLP,"[0.02634413354098797, 0.023870060220360756, 0.009139231406152248, 0.006255163811147213, 0.015891261398792267, 0.035951338708400726, 0.010075164958834648, 0.04659901186823845, 0.09339658915996552, 0.00999475084245205, 0.17115236818790436, 0.03037009760737419, 0.09088126569986343, 0.00876546185463667, 0.20828764140605927, 0.022652849555015564, 0.038892120122909546, 0.03567168489098549, 0.02737046778202057, 0.008362497203052044, 0.05903923884034157, 0.01597144454717636, 0.005066148936748505]",0.20828764140605927,Resources and Evaluation,0.17115236818790436,False
"Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that ""the instance does not belong to these complementary labels"". Since the probability of selecting a true label as a complementary label is low, NT provides less noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a relabeling process to transform the noisy data into useful training data, thus further benefiting the model's performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect.",Information Extraction,"[0.0008639289881102741, 0.0018043618183583021, 0.001044951961375773, 0.0022585743572562933, 0.00210735690779984, 0.0019793615210801363, 0.9170570373535156, 0.01662687212228775, 0.001998858293518424, 0.0017477702349424362, 0.00597270717844367, 0.0012771275360137224, 0.0016314859967678785, 0.002591109834611416, 0.00395082775503397, 0.002634570701047778, 0.00968922022730112, 0.004679850302636623, 0.00192295724991709, 0.0031325039453804493, 0.011036350391805172, 0.0019094933522865176, 0.0020827630069106817]",0.9170570373535156,Information Extraction,0.9170570373535156,True
"Despite the recent success of deep neural networks in natural language processing, the extent to which they can demonstrate human-like generalization capacities for natural language understanding remains unclear. We explore this issue in the domain of natural language inference (NLI), focusing on the transitivity of inference relations, a fundamental property for systematically drawing inferences. A model capturing transitivity can compose basic inference patterns and draw new inferences. We introduce an analysis method using synthetic and naturalistic NLI datasets involving clauseembedding verbs to evaluate whether models can perform transitivity inferences composed of veridical inferences and arbitrary inference types. We find that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples. The data and code for our analysis are publicly available at https://github.com/ verypluming/transitivity.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.007663961499929428, 0.00480187451466918, 0.008208053186535835, 0.017465025186538696, 0.002470269799232483, 0.0105453971773386, 0.007087877485901117, 0.0037907876539975405, 0.012653239071369171, 0.00454882113263011, 0.01111789420247078, 0.0009883330203592777, 0.002208900172263384, 0.0351407565176487, 0.0210094191133976, 0.023397892713546753, 0.7941589951515198, 0.00566228199750185, 0.003149227472022176, 0.001388086355291307, 0.014539219439029694, 0.0023869741708040237, 0.005616570822894573]",0.7941589951515198,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.7941589951515198,True
"Linguistic accommodation is the process in which speakers adjust their accent, diction, vocabulary, and other aspects of language according to the communication style of one another. Previous research has shown how linguistic accommodation correlates with gaps in the power and status of the speakers and the way it promotes approval and discussion efficiency. In this work, we provide a novel perspective on the phenomena, exploring its correlation with the open-mindedness of a speaker, rather than to her social status. We process thousands of unstructured argumentative discussions that took place in Reddit's Change My View (CMV) subreddit, demonstrating that open-mindedness relates to the assumed role of a speaker in different contexts. On the discussion level, we surprisingly find that discussions that reach agreement present lower levels of accommodation.","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.0686258003115654, 0.2703016996383667, 0.007846148684620857, 0.0717601627111435, 0.06426732242107391, 0.008717098273336887, 0.0033094468526542187, 0.0068888189271092415, 0.0801137313246727, 0.00949740968644619, 0.009528185240924358, 0.014080328866839409, 0.06744261085987091, 0.01012301817536354, 0.16456909477710724, 0.01745672896504402, 0.007292779628187418, 0.07816768437623978, 0.011643284000456333, 0.005797778721898794, 0.007196557242423296, 0.00830109789967537, 0.007073189131915569]",0.2703016996383667,Computational Social Science and Social Media,0.0686258003115654,False
"Visual dialog is a vision-language task where an agent needs to answer a series of questions grounded in an image based on the understanding of the dialog history and the image. The occurrences of coreference relations in the dialog makes it a more challenging task than visual question-answering. Most previous works have focused on learning better multi-modal representations or on exploring different ways of fusing visual and language features, while the coreferences in the dialog are mainly ignored. In this paper, based on linguistic knowledge and discourse features of human dialog we propose two soft constraints that can improve the model's ability of resolving coreferences in dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset shows that our model, which integrates two novel and linguistically inspired soft constraints in a deep transformer neural architecture, obtains new state-of-the-art performance in terms of recall at 1 and other evaluation metrics compared to current existing models and this without pretraining on other visionlanguage datasets. Our qualitative results also demonstrate the effectiveness of the method that we propose. 1",Dialogue and Interactive Systems,"[0.003211231203749776, 0.005462782923132181, 0.6080068349838257, 0.014559485949575901, 0.004182967822998762, 0.007388399448245764, 0.0048138732090592384, 0.004779653623700142, 0.004015557002276182, 0.22219586372375488, 0.005475407466292381, 0.002502680756151676, 0.003035652218386531, 0.024091724306344986, 0.01660878211259842, 0.0034070394467562437, 0.005848867353051901, 0.00960344634950161, 0.03325267508625984, 0.004066591151058674, 0.005047403275966644, 0.00439887261018157, 0.004044152330607176]",0.6080068349838257,Dialogue and Interactive Systems,0.6080068349838257,True
"Volatility prediction is complex due to the stock market's stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives' speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous stateof-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gendersensitive audio features and fewer female executives in earnings calls of one of the world's biggest stock indexes, the S&P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes. 1",Ethics and NLP,"[0.021719753742218018, 0.3021196722984314, 0.009099677205085754, 0.026477934792637825, 0.0689203292131424, 0.004744549281895161, 0.028267700225114822, 0.01607343554496765, 0.10995694249868393, 0.03271954879164696, 0.017905959859490395, 0.022721733897924423, 0.022126492112874985, 0.008438211865723133, 0.08018148690462112, 0.013192533515393734, 0.010142603889107704, 0.045396361500024796, 0.1071639284491539, 0.015996377915143967, 0.00750600453466177, 0.01560246292501688, 0.013526328839361668]",0.3021196722984314,Computational Social Science and Social Media,0.0689203292131424,False
"Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings -while more mature than those available for their dynamic counterparts -are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models that share the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings. 2 A humanist's outlook on the (in)accessibility of BERT: https://tedunderwood.com/2019/07/15/ do-humanists-need-bert/ The first is subword pooling: the application of a pooling mechanism over the k subword representations generated for w in context c in order to compute a single representation for w in c, i.e. {w 1 c , . . . , w k c } â†’ w c . Beyond this, we define context combination to be the mapping from representations w c 1 , . . . , w cn of w in different contexts c 1 , . . . , c n to a single static embedding w that is agnostic of context. Subword Pooling. The tokenization procedure for BERT can be decomposed into two steps: performing a simple word-level tokenization and then potentially deconstructing a word into multiple subwords, yielding w 1 , . . . , w k such that cat(w 1 , . . . , w k ) = w where cat(â€¢) indicates concatenation. Then, every layer of the model computes vectors w 1 c , . . . , w k c . Given these vectors, we consider four pooling mechanisms to compute w c : mean, last} min(â€¢), max(â€¢) are element-wise min/max pooling, mean(â€¢) is the arithmetic mean and last(â€¢) indicates selecting the last vector, w k c . Context Combination. Next, we describe two approaches for specifying contexts c 1 , . . . , c n and combining the associated representations w c 1 , . . . , w cn .",Interpretability and Analysis of Models for NLP,"[0.042013853788375854, 0.02519306167960167, 0.001766012399457395, 0.008002296090126038, 0.04163132980465889, 0.004364566411823034, 0.005142124369740486, 0.006400866433978081, 0.549403190612793, 0.007843616418540478, 0.06044400855898857, 0.040342237800359726, 0.026292698457837105, 0.0050886040553450584, 0.0436343215405941, 0.07731393724679947, 0.01131248939782381, 0.012130539864301682, 0.00498766265809536, 0.0030602551996707916, 0.00851981807500124, 0.009745938703417778, 0.005366639234125614]",0.549403190612793,Interpretability and Analysis of Models for NLP,0.549403190612793,True
"Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions. Lang Sentence Predicted Topic EN Blackmore's Night is a British/American traditional folk rock duo [...] rock, band, bass, formed IT I Blackmore's Night sono la band fondatrice del renaissance rock [...] rock, band, bass, formed PT Blackmore's Night Ã© uma banda de folk rock de estilo renascentista [...] rock, band, bass, formed EN Langton's ant is a two-dimensional Turing machine with [..",Interpretability and Analysis of Models for NLP,"[0.007252926006913185, 0.012480645440518856, 0.006159932818263769, 0.01966887153685093, 0.016377761960029602, 0.06075476482510567, 0.015959860756993294, 0.34488674998283386, 0.0968923270702362, 0.007235176395624876, 0.05878171697258949, 0.093324214220047, 0.011328899301588535, 0.007224199362099171, 0.05369795486330986, 0.025920258834958076, 0.034284934401512146, 0.03274644911289215, 0.011396544054150581, 0.04869994893670082, 0.015442238189280033, 0.014005614444613457, 0.005478020757436752]",0.34488674998283386,Information Retrieval and Text Mining,0.0968923270702362,False
"Pre-trained language models have achieved human-level performance on many Machine Reading Comprehension (MRC) tasks, but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. Here, we demonstrate a simple yet effective method to attack MRC models and reveal the statistical biases in these models. We apply the method to the RACE dataset, for which the answer to each MRC question is selected from 4 options. It is found that several pre-trained language models, including BERT, ALBERT, and RoBERTa, show consistent preference to some options, even when these options are irrelevant to the question. When interfered by these irrelevant options, the performance of MRC models can be reduced from human-level performance to the chance-level performance. Human readers, however, are not clearly affected by these irrelevant options. Finally, we propose an augmented training method that can greatly reduce models' statistical biases.",Interpretability and Analysis of Models for NLP,"[0.01826571114361286, 0.009493258781731129, 0.003987059462815523, 0.013439390808343887, 0.016330787912011147, 0.0098005635663867, 0.005523220635950565, 0.024957215413451195, 0.09029466658830643, 0.02038099430501461, 0.03228380158543587, 0.003253461793065071, 0.006881564389914274, 0.6356315612792969, 0.04560218006372452, 0.006681644823402166, 0.024356743320822716, 0.004958574194461107, 0.006018087267875671, 0.001972271827980876, 0.006893562152981758, 0.0048576099798083305, 0.00813609454780817]",0.6356315612792969,Question Answering,0.09029466658830643,False
"Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.",Machine Learning for NLP,"[0.003969230223447084, 0.002556322840973735, 0.005540612153708935, 0.0016741008730605245, 0.004154369700700045, 0.008407794870436192, 0.009442802518606186, 0.012616053223609924, 0.0749683827161789, 0.0022626055870205164, 0.8025215268135071, 0.00620829313993454, 0.002046236302703619, 0.008551269769668579, 0.00520437303930521, 0.0035155373625457287, 0.02719399333000183, 0.0026216839905828238, 0.00272776884958148, 0.0012321228859946132, 0.00920823123306036, 0.002007497940212488, 0.0013692218344658613]",0.8025215268135071,Machine Learning for NLP,0.8025215268135071,True
"When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential crossmodal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-theart image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more naturalparticularly when gaze is encoded with a dedicated recurrent component.","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.011076042428612709, 0.0061280992813408375, 0.04120120406150818, 0.007890603505074978, 0.011262052692472935, 0.05335186794400215, 0.004709179047495127, 0.005016295239329338, 0.030603328719735146, 0.6436402797698975, 0.005975507199764252, 0.004406709689646959, 0.010246625170111656, 0.005541809368878603, 0.04090398550033569, 0.003856039373204112, 0.0062817116267979145, 0.010138043202459812, 0.06618915498256683, 0.014954655431210995, 0.004614634905010462, 0.008082139305770397, 0.00393006531521678]",0.6436402797698975,"Language Grounding to Vision, Robotics and Beyond",0.011076042428612709,False
"We present OPINIONDIGEST, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained Transformer model, which verbalizes them into an opinion summary. OPINIONDIGEST can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment. Automatic evaluation on YELP data shows that our framework outperforms competitive baselines. Human studies on two corpora verify that OPINIONDIGEST produces informative summaries and shows promising customization capabilities 1 .","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0014710903633385897, 0.004222112707793713, 0.003691949648782611, 0.012187396176159382, 0.003005595412105322, 0.010035744868218899, 0.01598464697599411, 0.010510927997529507, 0.003856273600831628, 0.0032364651560783386, 0.002142320852726698, 0.003613037057220936, 0.003675189334899187, 0.0012361534172669053, 0.023579120635986328, 0.0020195969846099615, 0.0030149470549076796, 0.03357518091797829, 0.007859141565859318, 0.8355898261070251, 0.005990288686007261, 0.005353828892111778, 0.004149086307734251]",0.8355898261070251,Summarization,0.03357518091797829,False
"Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L 1 or L âˆž norm of input features. However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited. In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. In addition, we propose two methods to ensure the numerical stability of the model training. The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. The second one is re-formulation, which decomposes the exponent operation to avoid computing the realvalued powers of the input and further accelerate the pooling operation. Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.",Machine Learning for NLP,"[0.004454239271581173, 0.0028759255073964596, 0.0040351031348109245, 0.0024529879447072744, 0.005515765864402056, 0.02515767142176628, 0.015523415058851242, 0.12405522167682648, 0.08833929896354675, 0.0033249829430133104, 0.6304728984832764, 0.006208681967109442, 0.0032649878412485123, 0.008565333671867847, 0.009733844548463821, 0.005547230131924152, 0.023515693843364716, 0.007336529903113842, 0.006144998129457235, 0.005805330816656351, 0.011067132465541363, 0.004385699518024921, 0.002217119326815009]",0.6304728984832764,Machine Learning for NLP,0.6304728984832764,True
"Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.",Computational Social Science and Social Media,"[0.014437134377658367, 0.5998588800430298, 0.010709627531468868, 0.03768134489655495, 0.05628194659948349, 0.005156165920197964, 0.0036491311620920897, 0.006809894926846027, 0.00589051702991128, 0.005620280746370554, 0.01182498224079609, 0.0032568674068897963, 0.009364905767142773, 0.009736149571835995, 0.1255536824464798, 0.00834353081882, 0.009006871841847897, 0.046878691762685776, 0.008570476435124874, 0.004661560524255037, 0.0054581137374043465, 0.0042534819804131985, 0.006995726376771927]",0.5998588800430298,Computational Social Science and Social Media,0.5998588800430298,True
"We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept intersentence event mention pairs. As such, we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI, featuring discourse, syntax, and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets.",Information Extraction,"[0.001122932997532189, 0.0018683815142139792, 0.0013489358825609088, 0.004051473923027515, 0.0017879053484648466, 0.0025824676267802715, 0.8812271356582642, 0.02190348319709301, 0.0022454119753092527, 0.0021021708380430937, 0.005256422329694033, 0.0010048022959381342, 0.0015767303993925452, 0.006249291356652975, 0.0031245697755366564, 0.006192533299326897, 0.028766172006726265, 0.0039103394374251366, 0.002175265457481146, 0.0042870789766311646, 0.01116766594350338, 0.002654068637639284, 0.003394696395844221]",0.8812271356582642,Information Extraction,0.8812271356582642,True
"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT (Devlin et al., 2019)  and RoBERTa (Liu et al., 2019) in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging outof-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pretrained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5Ã— lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain. 1",Machine Learning for NLP,"[0.007407135330140591, 0.005316759459674358, 0.008509707637131214, 0.003349062753841281, 0.006713666021823883, 0.013993442989885807, 0.005089465994387865, 0.009422024711966515, 0.13898184895515442, 0.003883619559928775, 0.6032488942146301, 0.008205294609069824, 0.0030074981041252613, 0.008709088899195194, 0.014497346244752407, 0.006289094220846891, 0.12581734359264374, 0.0038341793697327375, 0.0050771841779351234, 0.00199575861915946, 0.011659973300993443, 0.002944342093542218, 0.0020472859032452106]",0.6032488942146301,Machine Learning for NLP,0.6032488942146301,True
"Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation.",Dialogue and Interactive Systems,"[0.0011903997510671616, 0.0020075931679457426, 0.9452797174453735, 0.0049284049309790134, 0.001046429038979113, 0.002898546401411295, 0.001256320858374238, 0.0013700273120775819, 0.0011094760848209262, 0.002204019343480468, 0.0057820966467261314, 0.0015172002604231238, 0.002377851866185665, 0.002538834000006318, 0.004177945200353861, 0.0011923136189579964, 0.003797063371166587, 0.004688034299761057, 0.004140185192227364, 0.001203963183797896, 0.0029668137431144714, 0.001046204473823309, 0.001280333148315549]",0.9452797174453735,Dialogue and Interactive Systems,0.9452797174453735,True
"Real-world knowledge graphs are often characterized by low-frequency relations-a challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of models derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of learning in this fewshot setting. We find that a simple zero-shot baseline-which ignores any relation-specific information-achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.",Information Extraction,"[0.008183905854821205, 0.008666676469147205, 0.00948912464082241, 0.006057939492166042, 0.014941012486815453, 0.02201414294540882, 0.06951197236776352, 0.096438929438591, 0.041091907769441605, 0.008869261480867863, 0.5485467314720154, 0.0036259039770811796, 0.005161691922694445, 0.020766180008649826, 0.01082180067896843, 0.021587800234556198, 0.052607715129852295, 0.011352328583598137, 0.003046521218493581, 0.005375073291361332, 0.020960886031389236, 0.005995933432132006, 0.004886609967797995]",0.5485467314720154,Machine Learning for NLP,0.06951197236776352,False
"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others' responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.",Dialogue and Interactive Systems,"[0.0013810568489134312, 0.0020134560763835907, 0.9216644167900085, 0.0071889241226017475, 0.0011163471499457955, 0.005524574778974056, 0.0018159787869080901, 0.00226266379468143, 0.0011012100148946047, 0.0024268983397632837, 0.01716301217675209, 0.0014968829927965999, 0.0018893856322392821, 0.007349557708948851, 0.004476361442357302, 0.001025922829285264, 0.005045547615736723, 0.003034460823982954, 0.0042691463604569435, 0.0015604342333972454, 0.00357521278783679, 0.001152633223682642, 0.0014658622676506639]",0.9216644167900085,Dialogue and Interactive Systems,0.9216644167900085,True
"We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the classic hidden Markov model with the contextual representation power of pretrained language models. Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations. We further refine CHMM with an alternate-training approach (CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERT-NER's output is regarded as an additional weak source to train the CHMM in return. Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins.",Information Extraction,"[0.0013662190176546574, 0.003728945506736636, 0.0033553324174135923, 0.0016990202711895108, 0.002467323327437043, 0.004672016482800245, 0.8217869400978088, 0.030547896400094032, 0.003129262011498213, 0.0032179856207221746, 0.036126136779785156, 0.0016232130583375692, 0.002289052354171872, 0.006057471502572298, 0.006235819309949875, 0.0034550100099295378, 0.012538894079625607, 0.005618902388960123, 0.002683540340512991, 0.0035853718873113394, 0.037763532251119614, 0.003247830318287015, 0.0028042634949088097]",0.8217869400978088,Information Extraction,0.8217869400978088,True
"Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g., named entity recognition in English) and knowledge of other languages (e.g., question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both tasks and languages. We also investigate the role of different sampling strategies used during meta-learning. We present experiments on five different tasks and six different languages from the XTREME multilingual benchmark dataset (Hu et al., 2020) . Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multitask baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed model.",Machine Translation and Multilinguality,"[0.005784962326288223, 0.005976436659693718, 0.0075009907595813274, 0.0023518495727330446, 0.006928382907062769, 0.010959737934172153, 0.005123704206198454, 0.009778364561498165, 0.03493979573249817, 0.0019121505320072174, 0.7003768086433411, 0.04009639844298363, 0.004837111569941044, 0.007027261424809694, 0.025474099442362785, 0.00632835179567337, 0.08409203588962555, 0.004593629855662584, 0.005189757328480482, 0.00224157958291471, 0.023660480976104736, 0.002708063693717122, 0.0021180843468755484]",0.7003768086433411,Machine Learning for NLP,0.04009639844298363,False
"We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.","Language Grounding to Vision, Robotics and Beyond","[0.07292889058589935, 0.15667732059955597, 0.0034707263112068176, 0.02210252173244953, 0.09966644644737244, 0.00624742079526186, 0.005708969198167324, 0.012403640896081924, 0.11229367554187775, 0.13076667487621307, 0.010223958641290665, 0.010750537738204002, 0.027503101155161858, 0.014156120829284191, 0.11078029125928879, 0.10359172523021698, 0.013565704226493835, 0.03284398466348648, 0.01699954830110073, 0.0033527084160596132, 0.007549639325588942, 0.015980860218405724, 0.010435488075017929]",0.15667732059955597,Computational Social Science and Social Media,0.13076667487621307,False
"Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semiautoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widelyused benchmark datasets show that our proposed model achieves more than 4Ã— speedup while maintaining comparable performance compared with the corresponding autoregressive model. * indicates equal contribution â€  indicates corresponding author Src. es gibt heute viele Farmer mit diesem Ansatz Feasible there are lots of farmers doing this today Trans. there are a lot of farmers doing this today Trans. 1 there are lots of of farmers doing this today Trans. 2 there are a lot farmers doing this today",Machine Translation and Multilinguality,"[0.0010542760137468576, 0.000803016300778836, 0.001547966618090868, 0.0004387352673802525, 0.0019818423315882683, 0.003010375192388892, 0.0005003912374377251, 0.0009186538518406451, 0.00327251385897398, 0.0010680918348953128, 0.002694500843062997, 0.9631543159484863, 0.0025392957031726837, 0.0006031807861290872, 0.0033350763842463493, 0.0010294686071574688, 0.0010432425187900662, 0.0013772352831438184, 0.003742000088095665, 0.002020618412643671, 0.0013705765595659614, 0.0017217780696228147, 0.0007728813216090202]",0.9631543159484863,Machine Translation and Multilinguality,0.9631543159484863,True
"The main goal behind state-of-the-art pretrained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot crosslingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pretraining. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml. Gurevych. 2020. AdapterDrop: On the Efficiency of Adapters in Transformers. arXiv preprint.",Machine Translation and Multilinguality,"[0.002650222275406122, 0.0017711370019242167, 0.002697464544326067, 0.0010205183643847704, 0.006785649340599775, 0.005952667910605669, 0.0063659558072686195, 0.0076180920004844666, 0.025310108438134193, 0.0023600717540830374, 0.052948419004678726, 0.8254733681678772, 0.003995927516371012, 0.004584867507219315, 0.010415855795145035, 0.0036823248956352472, 0.006155381910502911, 0.0029208981432020664, 0.00490691838786006, 0.0020474789198487997, 0.013046355918049812, 0.004760713316500187, 0.0025295026134699583]",0.8254733681678772,Machine Translation and Multilinguality,0.8254733681678772,True
"Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decisionmaking process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural logic reasoning for explainable recommendation (LOGER) by drawing on interpretable logical rules to guide the pathreasoning process for explanation generation. We experiment on three large-scale datasets in the e-commerce domain, demonstrating the effectiveness of our method in delivering highquality recommendations as well as ascertaining the faithfulness of the derived explanation.",Information Retrieval and Text Mining,"[0.010283096693456173, 0.015568397007882595, 0.013019265606999397, 0.01088456716388464, 0.03577698394656181, 0.0740673616528511, 0.05001094564795494, 0.16126208007335663, 0.09202411025762558, 0.0705697312951088, 0.13540765643119812, 0.009916727431118488, 0.005158912390470505, 0.11319588869810104, 0.04898367449641228, 0.008245551958680153, 0.04504087567329407, 0.02269749902188778, 0.018053675070405006, 0.03123554214835167, 0.011603233404457569, 0.008981596678495407, 0.008012703619897366]",0.16126208007335663,Information Retrieval and Text Mining,0.16126208007335663,True
"Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Entman, 1983) . Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for connotations to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a postdecoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.",Generation,"[0.03550225496292114, 0.08381512761116028, 0.008937661536037922, 0.13074351847171783, 0.03114456869661808, 0.023231729865074158, 0.006389106623828411, 0.01111309602856636, 0.07829513400793076, 0.018949635326862335, 0.00491299107670784, 0.008000364527106285, 0.040859468281269073, 0.03125840052962303, 0.1169876754283905, 0.05790980905294418, 0.023768266662955284, 0.24405424296855927, 0.006460139527916908, 0.0036101879086345434, 0.01469603180885315, 0.010350499302148819, 0.00901011098176241]",0.24405424296855927,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.023231729865074158,False
"We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. It is supplemented with 50k unlabeled sentences collected from Common Crawl and webcrawled data using intensive data-mining techniques. Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing. We believe that what we present in this paper is useful beyond the low-resource language community. This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging testbed for most recent NLP approaches.",Resources and Evaluation,"[0.04599421098828316, 0.04740949720144272, 0.013818935491144657, 0.012182043865323067, 0.024791745468974113, 0.023036440834403038, 0.003315380774438381, 0.006767585873603821, 0.05634298175573349, 0.009935957379639149, 0.018811775371432304, 0.0634080097079277, 0.171941876411438, 0.010506004095077515, 0.32677367329597473, 0.01656387932598591, 0.014666853472590446, 0.046394918113946915, 0.009034167975187302, 0.004727247171103954, 0.055222321301698685, 0.011763877235352993, 0.006590633653104305]",0.32677367329597473,Resources and Evaluation,0.32677367329597473,True
"Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded highquality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0032808140385895967, 0.0051660118624567986, 0.011249546892940998, 0.014619629830121994, 0.004047074820846319, 0.005174587015062571, 0.013105307705700397, 0.006681323517113924, 0.0037885333877056837, 0.003431417280808091, 0.011920638382434845, 0.0016803221078589559, 0.001964522758498788, 0.027134107425808907, 0.01929408498108387, 0.016301270574331284, 0.8172903060913086, 0.006960973143577576, 0.002991096116602421, 0.001810528221540153, 0.013650262728333473, 0.001788884517736733, 0.006668791640549898]",0.8172903060913086,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.8172903060913086,True
"Maintaining consistent personas is essential for dialogue agents. Although tremendous advancements have been brought, the limitedscale of annotated persona-dense data are still barriers towards training robust and consistent persona-based dialogue models. In this work, we show how the challenges can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERTover-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding. In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency.",Dialogue and Interactive Systems,"[0.0008526595775038004, 0.0012866968754678965, 0.9495646357536316, 0.003918427973985672, 0.0008750343695282936, 0.005457830615341663, 0.0014880371745675802, 0.001228040549904108, 0.0007177641964517534, 0.002083120634779334, 0.004366991110146046, 0.0016252726782113314, 0.001682705245912075, 0.0031508326064795256, 0.004750735126435757, 0.0007027715910226107, 0.002748254919424653, 0.003715177532285452, 0.003927139099687338, 0.0013222519773989916, 0.002209574216976762, 0.0010144648840650916, 0.0013115705223754048]",0.9495646357536316,Dialogue and Interactive Systems,0.9495646357536316,True
"People debate on a variety of topics on online platforms such as Reddit, or Facebook. Debates can be lengthy, with users exchanging a wealth of information and opinions. However, conversations do not always go smoothly, and users sometimes engage in unsound argumentation techniques to prove a claim. These techniques are called fallacies. Fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim. In this paper, we study the most frequent fallacies on Reddit, and we present them using the pragma-dialectical theory of argumentation. We construct a new annotated dataset of fallacies, using user comments containing fallacy mentions as noisy labels, and cleaning the data via crowdsourcing. Finally, we study the task of classifying fallacies using neural models. We find that generally the models perform better in the presence of conversational context.We have released the data and the code at github.com/sahaisaumya/informal_ fallacies.",Computational Social Science and Social Media,"[0.0034078480675816536, 0.08347150683403015, 0.007589439861476421, 0.027502065524458885, 0.012481972575187683, 0.0065855118446052074, 0.0033828874584287405, 0.004909648559987545, 0.008457347750663757, 0.004560300149023533, 0.0030252255965024233, 0.0031553141307085752, 0.006232894491404295, 0.00616378104314208, 0.09834524989128113, 0.00508163683116436, 0.005162475164979696, 0.6897527575492859, 0.004083649720996618, 0.0033127774950116873, 0.006856200285255909, 0.0033265314996242523, 0.0031530670821666718]",0.6897527575492859,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.08347150683403015,False
"Car-focused navigation services are based on turns and distances of named streets, whereas navigation instructions naturally used by humans are centered around physical objects called landmarks. We present a neural model that takes OpenStreetMap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions. Routes on the map are encoded in a location-and rotation-invariant graph representation that is decoded into natural language instructions. Our work is based on a novel dataset of 7,672 crowd-sourced instances that have been verified by human navigation in Street View. Our evaluation shows that the navigation instructions generated by our system have similar properties as human-generated instructions, and lead to successful human navigation in Street View.","Language Grounding to Vision, Robotics and Beyond","[0.017251059412956238, 0.013285841792821884, 0.01111515611410141, 0.004335398785769939, 0.017100147902965546, 0.032029904425144196, 0.019944386556744576, 0.016304047778248787, 0.026077883318066597, 0.6171901822090149, 0.011072038672864437, 0.008448858745396137, 0.014371616765856743, 0.02840067259967327, 0.0448184460401535, 0.012810326181352139, 0.015571165829896927, 0.008130044676363468, 0.020138079300522804, 0.007839556783437729, 0.02890603244304657, 0.014838281087577343, 0.010020908899605274]",0.6171901822090149,"Language Grounding to Vision, Robotics and Beyond",0.6171901822090149,True
"Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters. Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding. With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method. 1",Information Extraction,"[0.002130833687260747, 0.007421830203384161, 0.0019369920482859015, 0.003555983304977417, 0.0034371218644082546, 0.002593372482806444, 0.7968307137489319, 0.02852228470146656, 0.0033680987544357777, 0.004011139739304781, 0.010264023207128048, 0.0026102724950760603, 0.006935864686965942, 0.004110374953597784, 0.010923255234956741, 0.014197769574820995, 0.009272541850805283, 0.010292645543813705, 0.0030914414674043655, 0.0044708251953125, 0.060144610702991486, 0.005401875823736191, 0.004476151894778013]",0.7968307137489319,Information Extraction,0.7968307137489319,True
"Visual Dialog involves ""understanding"" the dialog history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to generate the correct response. In this paper, we show that co-attention models which explicitly encode dialog history outperform models that don't, achieving state-ofthe-art performance (72 % NDCG on val set). However, we also expose shortcomings of the crowd-sourcing dataset collection procedure by showing that history is indeed only required for a small amount of the data and that the current evaluation metric encourages generic replies. To that end, we propose a challenging subset (VisDialConv) of the VisDial val set and provide a benchmark of 63% NDCG.","Language Grounding to Vision, Robotics and Beyond","[0.0027965938206762075, 0.005158158019185066, 0.8030364513397217, 0.015303850173950195, 0.003680107882246375, 0.010642689652740955, 0.0024765864945948124, 0.002895988989621401, 0.0031017749570310116, 0.05967729911208153, 0.005209845490753651, 0.0018128278898075223, 0.002757301554083824, 0.012371429242193699, 0.019470982253551483, 0.0018638164037838578, 0.00481341453269124, 0.008288713172078133, 0.020515983924269676, 0.004387951921671629, 0.003296159440651536, 0.00295694125816226, 0.003485035616904497]",0.8030364513397217,Dialogue and Interactive Systems,0.05967729911208153,False
"Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labelling tasks due to their respective strength. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labelling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with the existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves the stateof-the-art results.","Syntax: Tagging, Chunking and Parsing","[0.00375473708845675, 0.00620516762137413, 0.01557709276676178, 0.005114586558192968, 0.005822929088026285, 0.030912058427929878, 0.15181763470172882, 0.04524645209312439, 0.009044746868312359, 0.004479754250496626, 0.4123792052268982, 0.009927087463438511, 0.012525898404419422, 0.007391954772174358, 0.02364671416580677, 0.010590208694338799, 0.07752898335456848, 0.008199893869459629, 0.005353074986487627, 0.007561117876321077, 0.1362685263156891, 0.006341962143778801, 0.004310282878577709]",0.4123792052268982,Machine Learning for NLP,0.1362685263156891,False
"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event predictionand, in particular, it shows clear insensitivity to the contextual impacts of negation.",Interpretability and Analysis of Models for NLP,"[0.07199212163686752, 0.0066489060409367085, 0.0026437006890773773, 0.010235510766506195, 0.011165410280227661, 0.013001021929085255, 0.0024005144368857145, 0.006254691164940596, 0.6405784487724304, 0.006228631362318993, 0.06934195756912231, 0.0073098610155284405, 0.028951693326234818, 0.012827064841985703, 0.038798216730356216, 0.022348368540406227, 0.017735697329044342, 0.007424843031913042, 0.00468176044523716, 0.002037598052993417, 0.008867752738296986, 0.0055502005852758884, 0.002975990530103445]",0.6405784487724304,Interpretability and Analysis of Models for NLP,0.6405784487724304,True
"Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the personalityrelated characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results.",Dialogue and Interactive Systems,"[0.0012301617534831166, 0.002508163917809725, 0.9338296055793762, 0.007972029969096184, 0.0014436651254072785, 0.0053739994764328, 0.0014144573360681534, 0.0012901559239253402, 0.001035789493471384, 0.0020440437365323305, 0.0067838034592568874, 0.0016061044298112392, 0.002030608244240284, 0.004214920103549957, 0.0060574510134756565, 0.0007562342216260731, 0.0022494359873235226, 0.006116227712482214, 0.005995098501443863, 0.0019220601534470916, 0.001318985247053206, 0.001210542512126267, 0.0015964052872732282]",0.9338296055793762,Dialogue and Interactive Systems,0.9338296055793762,True
"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domaininvariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initialization and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning. 1",Information Retrieval and Text Mining,"[0.002632737159729004, 0.0015881077852100134, 0.0023602379951626062, 0.0014883755939081311, 0.00332980090752244, 0.005902145989239216, 0.008763475343585014, 0.0352734699845314, 0.06902141124010086, 0.0016578072682023048, 0.8165408372879028, 0.0060629225336015224, 0.0015725604025647044, 0.004996123723685741, 0.0037046000361442566, 0.0029068700969219208, 0.015592464245855808, 0.002705681836232543, 0.0028623612597584724, 0.002347172936424613, 0.005467520095407963, 0.0019335298566147685, 0.001289714709855616]",0.8165408372879028,Machine Learning for NLP,0.0352734699845314,False
"Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans. However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching. To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model. The former two tasks could help the model in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.",Dialogue and Interactive Systems,"[0.0008369216229766607, 0.0015652792062610388, 0.9419240355491638, 0.007272372953593731, 0.0009587359381839633, 0.0028761913999915123, 0.002552504651248455, 0.0021171574480831623, 0.0006702713435515761, 0.001765872584655881, 0.00492970272898674, 0.0016367248026654124, 0.0015618980396538973, 0.005591857247054577, 0.005721871741116047, 0.0009223002125509083, 0.002642780775204301, 0.0037735465448349714, 0.004619188141077757, 0.0016666275914758444, 0.0019008342642337084, 0.0010774784022942185, 0.0014157406985759735]",0.9419240355491638,Dialogue and Interactive Systems,0.9419240355491638,True
"We introduce Self-CRItic Pretraining Transformers (SCRIPT) for representation learning of text. The popular masked language modeling (MLM) pretraining methods like BERT replace some tokens with [MASK] and an encoder is trained to recover them, while ELEC-TRA trains a discriminator to detect replaced tokens proposed by a generator. In contrast, we train a language model as in MLM and further derive a discriminator or critic on top of the encoder without using any additional parameters. That is, the model itself is a critic. SCRIPT combines MLM training and discriminative training for learning rich representations and compute-and sample-efficiency. We demonstrate improved sample-efficiency in pretraining and enhanced representations evidenced by improved downstream task performance on GLUE and SQuAD over strong baselines. Also, the self-critic scores can be directly used as pseudo-log-likelihood for efficient scoring.",Machine Learning for NLP,"[0.003751722862944007, 0.0018837961833924055, 0.004745891317725182, 0.0016142301028594375, 0.003073138650506735, 0.024485686793923378, 0.002992532216012478, 0.022587532177567482, 0.05871336907148361, 0.0025443704798817635, 0.7997258901596069, 0.004007081035524607, 0.0017701176693663, 0.0051103802397847176, 0.005623364821076393, 0.0032940246164798737, 0.0358375683426857, 0.0020543262362480164, 0.0029174175579100847, 0.0020631495863199234, 0.008253845386207104, 0.0018594164866954088, 0.00109105056617409]",0.7997258901596069,Machine Learning for NLP,0.7997258901596069,True
"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin. 1",Dialogue and Interactive Systems,"[0.001925068674609065, 0.006086395587772131, 0.8181537985801697, 0.04423933103680611, 0.003020399482920766, 0.008773880079388618, 0.004076794721186161, 0.006676936522126198, 0.002140265889465809, 0.0024790114257484674, 0.011954453773796558, 0.003700559725984931, 0.0036345762200653553, 0.010944551788270473, 0.016567964106798172, 0.0025192161556333303, 0.005883711390197277, 0.020336365327239037, 0.010749650187790394, 0.0066146510653197765, 0.0036859638057649136, 0.0024777196813374758, 0.0033587112557142973]",0.8181537985801697,Dialogue and Interactive Systems,0.8181537985801697,True
"We introduce DynaSent ('Dynamic Sentiment'), a new English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-inthe-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best models we have been able to develop; when future models solve this task, we will use them to create DynaSent version 2, continuing the dynamic evolution of this benchmark. Here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts. We also present evidence that DynaSent's Neutral category is more coherent than the comparable category in other benchmarks, and we motivate training models from scratch for each round over successive fine-tuning.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.002662023063749075, 0.03013523854315281, 0.007347954902797937, 0.009556310251355171, 0.006444063037633896, 0.014612776227295399, 0.005599545314908028, 0.007606061641126871, 0.013505995273590088, 0.003965809475630522, 0.00594436377286911, 0.008001278154551983, 0.00620627636089921, 0.0031917570158839226, 0.15411043167114258, 0.0037847429048269987, 0.006203569006174803, 0.6715402603149414, 0.007928185164928436, 0.009745139628648758, 0.013435554690659046, 0.005885145161300898, 0.002587537281215191]",0.6715402603149414,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.6715402603149414,True
"Inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly. We model the social network as an And-or Graph, named SocAoG, for the consistency of relations among a group and leveraging attributes as inference cues. Moreover, we formulate a sequential structure prediction task, and propose an Î±-Î²-Î³ strategy to incrementally parse SocAoG for the dynamic inference upon any incoming utterance: (i) an Î± process predicting attributes and relations conditioned on the semantics of dialogues, (ii) a Î² process updating the social relations based on related attributes, and (iii) a Î³ process updating individual's attributes based on interpersonal social relations. Empirical results on Di-alogRE and MovieGraph show that our model infers social relations more accurately than the state-of-the-art methods. Moreover, the ablation study shows the three processes complement each other, and the case study demonstrates the dynamic relational inference. 1 1 The code is released at https://github.com/ Liang-Qiu/SocAoG-dialogues. S1: Well then we'll-we'll see you the day after tomorrow. Mom?! Dad?! What-what. . . what you guys doing here?! S2: Well you kids talk about this place so much, we thought we'd see what all the fuss is about. S3: I certainly see what the girls like coming here.",Computational Social Science and Social Media,"[0.006381509359925985, 0.02126934938132763, 0.6755900979042053, 0.034288082271814346, 0.007147360593080521, 0.007496407721191645, 0.003085047472268343, 0.0028366660699248314, 0.007219539023935795, 0.08118173480033875, 0.0067681437358260155, 0.0016973329475149512, 0.0056470707058906555, 0.017074869945645332, 0.03133084625005722, 0.0029016046319156885, 0.013977384194731712, 0.019427437335252762, 0.03410732373595238, 0.0059059965424239635, 0.006439078599214554, 0.003960876725614071, 0.004266197327524424]",0.6755900979042053,Dialogue and Interactive Systems,0.02126934938132763,False
"Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the model. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on feature selection, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our method is more faithful and readable compared with many existing approaches. We further study and reevaluate three typical models through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness. 1",Interpretability and Analysis of Models for NLP,"[0.0028764402959495783, 0.0025526522658765316, 0.010036163963377476, 0.004095834214240313, 0.001778341713361442, 0.01200199406594038, 0.010263358242809772, 0.007402913644909859, 0.011149081401526928, 0.0032219965942204, 0.0814366564154625, 0.002938662189990282, 0.0019883604254573584, 0.010536151006817818, 0.011743595823645592, 0.0073664505034685135, 0.7809705138206482, 0.002576709957793355, 0.0034806886687874794, 0.0022016512230038643, 0.024600399658083916, 0.0017744200304150581, 0.003006927901878953]",0.7809705138206482,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.011149081401526928,False
"We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.",Interpretability and Analysis of Models for NLP,"[0.08350025862455368, 0.030322818085551262, 0.0034787687472999096, 0.009462710469961166, 0.025864671915769577, 0.014188733883202076, 0.005121692083775997, 0.018477721139788628, 0.421344131231308, 0.10341523587703705, 0.02836734615266323, 0.018762975931167603, 0.0490076020359993, 0.027266578748822212, 0.05835700035095215, 0.01624380610883236, 0.0071853450499475, 0.012651955708861351, 0.02277628891170025, 0.007149306125938892, 0.013134164735674858, 0.016500970348715782, 0.007419872563332319]",0.421344131231308,Interpretability and Analysis of Models for NLP,0.421344131231308,True
"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.",Machine Learning for NLP,"[0.005638365633785725, 0.0027104192413389683, 0.003765244735404849, 0.0025185688864439726, 0.007301025092601776, 0.007854661904275417, 0.0028717995155602694, 0.01060629915446043, 0.30786341428756714, 0.003521868959069252, 0.560125470161438, 0.020176082849502563, 0.0029326467774808407, 0.006283771246671677, 0.008891445584595203, 0.00445036543533206, 0.02118169330060482, 0.004005921073257923, 0.0029879293870180845, 0.0023878056090325117, 0.007389969192445278, 0.0029413800220936537, 0.0015939278528094292]",0.560125470161438,Machine Learning for NLP,0.560125470161438,True
"In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC. In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency). On the basis, we propose a Cooperative Graph Attention Networks (Co-GAN) approach for cooperatively learning the aspect-related sentence representation. Specifically, two graph attention networks are leveraged to model above two kinds of documentlevel sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference. Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the stateof-the-art baselines. This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0008971723727881908, 0.004257258493453264, 0.004335956182330847, 0.0037938179448246956, 0.0032907617278397083, 0.00437026796862483, 0.03282707557082176, 0.012950880452990532, 0.008555631153285503, 0.0036914353258907795, 0.004329834599047899, 0.005350725259631872, 0.0024728975258767605, 0.0019166856072843075, 0.017077039927244186, 0.005262206308543682, 0.0029332854319363832, 0.8575538396835327, 0.005848058965057135, 0.0030700108036398888, 0.00966041162610054, 0.003494205651804805, 0.0020605651661753654]",0.8575538396835327,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.8575538396835327,True
"In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes. Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.011186023242771626, 0.012465564534068108, 0.005720668006688356, 0.024263791739940643, 0.007859512232244015, 0.02118724025785923, 0.023913253098726273, 0.03493799269199371, 0.04634501785039902, 0.015001161023974419, 0.015275376848876476, 0.0010365204652771354, 0.0041175889782607555, 0.1499778926372528, 0.06319084763526917, 0.02044600062072277, 0.4732527732849121, 0.022673482075333595, 0.004724092781543732, 0.005037996917963028, 0.021347198635339737, 0.006192405242472887, 0.009847583249211311]",0.4732527732849121,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.4732527732849121,True
"In this paper, we propose a novel bipartite flatgraph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional network (GCN) are adopted to jointly learn flat entities and their inner dependencies. Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones (or outside-toinside), our model effectively captures the bidirectional interaction between them. We first use the entities recognized by the flat NER module to construct an entity graph, which is fed to the next graph module. The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions. Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models.",Information Extraction,"[0.0011650732485577464, 0.0023251716047525406, 0.002105618827044964, 0.001521105645224452, 0.0016261033015325665, 0.0033017632085829973, 0.8691520690917969, 0.024098346009850502, 0.00302104651927948, 0.0026006849948316813, 0.015068400651216507, 0.0011191301746293902, 0.0019141632365062833, 0.004370754584670067, 0.0036994165275245905, 0.0036497607361525297, 0.008560406044125557, 0.004509477410465479, 0.0018065007170662284, 0.0034986580722033978, 0.03523343801498413, 0.002862426219508052, 0.0027905721217393875]",0.8691520690917969,Information Extraction,0.8691520690917969,True
"Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA. 1 http://aqleaderboard.tomhosking.co.uk/squad",Question Answering,"[0.013107520528137684, 0.006362471263855696, 0.01757543534040451, 0.009664210490882397, 0.010216040536761284, 0.30253010988235474, 0.0050598252564668655, 0.035967957228422165, 0.017679719254374504, 0.00775870168581605, 0.0689796656370163, 0.010250610299408436, 0.009678632020950317, 0.27965047955513, 0.11080552637577057, 0.004535638727247715, 0.0371258519589901, 0.017617985606193542, 0.006018009968101978, 0.004889682866632938, 0.010492797940969467, 0.006946842186152935, 0.007086361292749643]",0.30253010988235474,Generation,0.27965047955513,False
"Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks.",Generation,"[0.0021190657280385494, 0.0014689506497234106, 0.03142555430531502, 0.0035855055321007967, 0.00259509333409369, 0.8587313294410706, 0.0030875462107360363, 0.005306106060743332, 0.0023135626688599586, 0.003323766402900219, 0.01081913523375988, 0.0035273851826786995, 0.0043266345746815205, 0.00322576891630888, 0.019830986857414246, 0.001472758362069726, 0.013235612772405148, 0.004646365065127611, 0.00466726953163743, 0.010681957937777042, 0.0039078714326024055, 0.0034446301870048046, 0.002257232554256916]",0.8587313294410706,Generation,0.8587313294410706,True
"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge in developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories 1 .",Dialogue and Interactive Systems,"[0.001786308246664703, 0.005668187979608774, 0.8665804266929626, 0.011946543119847775, 0.003827095963060856, 0.010434797033667564, 0.002991153858602047, 0.0033235372975468636, 0.0018241067882627249, 0.006951451767235994, 0.0069092935882508755, 0.0024140747264027596, 0.0026549252215772867, 0.006722930818796158, 0.024291155859827995, 0.0009963663760572672, 0.0034124720841646194, 0.00934622436761856, 0.013345647603273392, 0.006811909843236208, 0.0026683188043534756, 0.002372126094996929, 0.002720910357311368]",0.8665804266929626,Dialogue and Interactive Systems,0.8665804266929626,True
"Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the everincreasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations. 1","Syntax: Tagging, Chunking and Parsing","[0.017013531178236008, 0.013767891563475132, 0.004657169338315725, 0.007286399602890015, 0.01025425735861063, 0.005594855174422264, 0.009216041304171085, 0.019970638677477837, 0.060517847537994385, 0.003374651074409485, 0.27529728412628174, 0.01637008786201477, 0.012109920382499695, 0.010598303750157356, 0.012833081185817719, 0.43229490518569946, 0.046205807477235794, 0.012298879213631153, 0.0040826816111803055, 0.0016616475768387318, 0.01310370210558176, 0.007071420084685087, 0.0044190543703734875]",0.43229490518569946,Semantics: Lexical Semantics,0.01310370210558176,False
"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",Interpretability and Analysis of Models for NLP,"[0.008228478021919727, 0.0028397550340741873, 0.0013320189900696278, 0.0031731128692626953, 0.0066584753803908825, 0.009196380153298378, 0.004232057835906744, 0.013500790111720562, 0.7499513626098633, 0.004711661487817764, 0.12749725580215454, 0.006474446039646864, 0.002967910375446081, 0.007119218818843365, 0.010811004787683487, 0.004498182330280542, 0.01597714051604271, 0.0048169963993132114, 0.003070432459935546, 0.0032073305919766426, 0.00399438850581646, 0.0039762454107403755, 0.0017653614049777389]",0.7499513626098633,Interpretability and Analysis of Models for NLP,0.7499513626098633,True
"The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RSTstyle discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment-annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.",Discourse and Pragmatics,"[0.009499388746917248, 0.011993160471320152, 0.052767038345336914, 0.6981019973754883, 0.007005836348980665, 0.012595362029969692, 0.004701296333223581, 0.012618325650691986, 0.004875622224062681, 0.004268252290785313, 0.013692818582057953, 0.002890148898586631, 0.013756048865616322, 0.03312395140528679, 0.03189508244395256, 0.015915922820568085, 0.018576493486762047, 0.019088931381702423, 0.004530003760010004, 0.008640071377158165, 0.009063905104994774, 0.004121436271816492, 0.0062788804061710835]",0.6981019973754883,Discourse and Pragmatics,0.6981019973754883,True
"Finding codes given natural language query is beneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce the CoSQA dataset. It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed Co-CLR to enhance query-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1%, and incorporating CoCLR brings a further improvement of 10.5%. 1 .",Resources and Evaluation,"[0.007255088537931442, 0.0064882016740739346, 0.024478210136294365, 0.0062378221191465855, 0.006474214140325785, 0.026186702772974968, 0.014758462086319923, 0.028715675696730614, 0.007264418061822653, 0.007043521385639906, 0.06355880200862885, 0.008082444779574871, 0.002114449394866824, 0.06319277733564377, 0.03364991024136543, 0.012526042759418488, 0.6322150230407715, 0.005960919428616762, 0.006023922003805637, 0.0038255301769822836, 0.02453126572072506, 0.004166727419942617, 0.005249860696494579]",0.6322150230407715,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.03364991024136543,False
"Adversarial learning can learn fairer and less biased models of language than standard methods. However, current adversarial techniques only partially mitigate model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and the stability of training.",Machine Learning for NLP,"[0.004549593664705753, 0.002609310206025839, 0.007573218550533056, 0.0024732963647693396, 0.00655969837680459, 0.010368424467742443, 0.00419461727142334, 0.010483703576028347, 0.15205548703670502, 0.004902302287518978, 0.7105752229690552, 0.02029729261994362, 0.0025415909476578236, 0.004887182731181383, 0.005997051019221544, 0.003744730493053794, 0.020117919892072678, 0.0037284577265381813, 0.005891231819987297, 0.002976849675178528, 0.00879636313766241, 0.0030909914057701826, 0.0015853102086111903]",0.7105752229690552,Machine Learning for NLP,0.7105752229690552,True
"Lexical normalization, the translation of noncanonical data to standard language, has shown to improve the performance of many natural language processing tasks on social media. Yet, using multiple languages in one utterance, also called code-switching (CS), is frequently overlooked by these normalization systems, despite its common use in social media. In this paper, we propose three normalization models specifically designed to handle codeswitched data which we evaluate for two language pairs: Indonesian-English (Id-En) and Turkish-German (Tr-De). For the latter, we introduce novel normalization layers and their corresponding language ID and POS tags for the dataset, and evaluate the downstream effect of normalization on POS tagging. Results show that our CS-tailored normalization models outperform Id-En state of the art and Tr-De monolingual models, and lead to 5.4% relative performance increase for POS tagging as compared to unnormalized input. 1",Machine Translation and Multilinguality,"[0.02304907888174057, 0.19031007587909698, 0.0074614849872887135, 0.015550229698419571, 0.05793123319745064, 0.015312934294342995, 0.00963569525629282, 0.011405881494283676, 0.03710239753127098, 0.005473617929965258, 0.05720418691635132, 0.07785412669181824, 0.03879233077168465, 0.005125636700540781, 0.2950081527233124, 0.015677684918045998, 0.016141431406140327, 0.047923196107149124, 0.0142935486510396, 0.01537665817886591, 0.024385668337345123, 0.010777216404676437, 0.008207573555409908]",0.2950081527233124,Resources and Evaluation,0.07785412669181824,False
"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.",Summarization,"[0.0009604947408661246, 0.0016783849569037557, 0.0022828441578894854, 0.005820386111736298, 0.0019156025955453515, 0.00837603397667408, 0.006868655327707529, 0.006402101367712021, 0.0020147343166172504, 0.002084845444187522, 0.001975447405129671, 0.0021953468676656485, 0.002169199986383319, 0.0007765119080431759, 0.01025597658008337, 0.000936410971917212, 0.0019994359463453293, 0.006702130660414696, 0.004865937400609255, 0.9207414984703064, 0.0029285892378538847, 0.0031918217428028584, 0.0028576005715876818]",0.9207414984703064,Summarization,0.9207414984703064,True
"We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size -600 training, 310 dev, and 490 evaluation question/answer pairs -thus reflecting the cost of creating large labeled datasets with actual data. Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch. TECHQA was obtained by crawling the IB-MDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote-a technical document that addresses a specific technical issue. We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language.",Resources and Evaluation,"[0.008286694996058941, 0.016409041360020638, 0.023252947255969048, 0.01450235303491354, 0.008769241161644459, 0.029909692704677582, 0.004857428837567568, 0.016516225412487984, 0.014219017699360847, 0.010398835875093937, 0.03306370601058006, 0.007449587807059288, 0.008097895421087742, 0.5504644513130188, 0.18350034952163696, 0.0031230784952640533, 0.01736433617770672, 0.017065217718482018, 0.0077389907091856, 0.0024386525619775057, 0.011667726561427116, 0.006058332044631243, 0.004846212454140186]",0.5504644513130188,Question Answering,0.18350034952163696,False
"Natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake. People have recently taken to social media websites (e.g., Twitter) to share their sentiments and feelings with the larger community. Consequently, these platforms have become instrumental in understanding and perceiving emotions at scale. In this paper, we introduce HURRICANEEMO, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria. We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarsegrained emotion groups. Our best BERT (Devlin et al., 2019) model, even after task-guided pre-training which leverages unlabeled Twitter data, achieves only 68% accuracy (averaged across all groups). HURRICANEEMO serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains.",Computational Social Science and Social Media,"[0.0024464253801852465, 0.14417871832847595, 0.0057258615270257, 0.010646577924489975, 0.01637859083712101, 0.005204462446272373, 0.006273225415498018, 0.007684650830924511, 0.010835204273462296, 0.006067353766411543, 0.00527100870385766, 0.005367729347199202, 0.005527695640921593, 0.0035025898832827806, 0.10590694099664688, 0.0048918710090219975, 0.0038852619472891092, 0.6221229434013367, 0.010782652534544468, 0.0031253977213054895, 0.006503106560558081, 0.004145020619034767, 0.003526692744344473]",0.6221229434013367,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.14417871832847595,False
"Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.","Phonology, Morphology and Word Segmentation","[0.08144654333591461, 0.029477283358573914, 0.014086183160543442, 0.01841813698410988, 0.012335383333265781, 0.016811959445476532, 0.004613014869391918, 0.004751579370349646, 0.03156642988324165, 0.007168303709477186, 0.017403364181518555, 0.07727751135826111, 0.4773772060871124, 0.011151941493153572, 0.08383585512638092, 0.012740536592900753, 0.007991362363100052, 0.015985852107405663, 0.02936502918601036, 0.009073362685739994, 0.017618341371417046, 0.011001093313097954, 0.008503633551299572]",0.4773772060871124,"Phonology, Morphology and Word Segmentation",0.4773772060871124,True
"Conventional approaches to event detection usually require a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive. We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes. However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection. In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues. Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively. Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.",Information Extraction,"[0.0012529556406661868, 0.002541878027841449, 0.0023737645242363214, 0.0024940180592238903, 0.002750135026872158, 0.004907871130853891, 0.8224520683288574, 0.04090385138988495, 0.004403580445796251, 0.0034086843952536583, 0.027263779193162918, 0.002354780677706003, 0.0019821745809167624, 0.004955315962433815, 0.0042524198070168495, 0.0053208936005830765, 0.024580292403697968, 0.007262205705046654, 0.0034381882287561893, 0.004987258464097977, 0.019941195845603943, 0.003195131430402398, 0.0029774471186101437]",0.8224520683288574,Information Extraction,0.8224520683288574,True
"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can ever be claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of moviedomain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work 1 . * Corresponding Authors. Work done before Xiaoyu Shen joins Amazon. â€  Work done while interning at Wechat. 1 Dataset and model are available at https://github. com/chin-gyou/MovieChats. æˆ‘åˆšçœ‹å®Œã€Šæµ·ä¸Šé’¢ç´å¸ˆã€‹ï¼Œæ„Ÿè§‰â¼¼çµå¾ˆéœ‡æ’¼ I just finished watching ""The Legend of 1900"" and feel shocked DA: Inform_fact, Inform_feeling Aspects: Name Mov_Tracker: The Legend of 1900 çœŸçš„å—ï¼Œè¿™æ˜¯éƒ¨ä»€ä¹ˆç”µå½± Really, what kind of movie is this DA: Request_fact Aspects: Type Mov_Tracker: The Legend of 1900 â¼€éƒ¨æ„â¼¤åˆ©æ‹çš„è‹±è¯­ç”µå½±ï¼Œè®²â¼€ä½å¤©æ‰é’¢ç´å¸ˆâ¼€è¾ˆâ¼¦éƒ½ä½è½®èˆ¹ä¸Š An English film made in Italy, where a talented pianist lived on the ship all his life DA: Inform_fact Aspects: Region, Language, Plot Mov_Tracker: The Legend of 1900 è¿™ä¹ˆç¦»å¥‡çš„æ•…äº‹ï¼Œä¸ºä»€ä¹ˆä»–ä¸ç¦»å¼€é‚£è‰˜èˆ¹å‘¢ Such a bizarre story, why didn't he leave that boat? DA: Inform_feeling, Request_fact Aspect: Plot Mov_Tracker: The Legend of 1900 ä»–æŠŠè¿™â¾¥å½“æˆäº†ä»–çš„ç²¾ç¥žå®¶å›­ï¼Œæ‰€ä»¥èˆ¹è¦è¢«ç‚¸æŽ‰ä»–éƒ½ä¸è‚¯ç¦»å¼€ He regarded this as his spiritual home, so when the last ship was about to be blown up, he refused to leave DA: Inform_feeling, Inform_fact Aspects: Plot Mov_Tracker: The Legend of 1900 å°±åƒé˜¿â½¢æ­£ä¼ â¼€æ ·ã€‚çŽ°ä»£ç¤¾ä¼šå¾ˆå°‘æœ‰â¼ˆèƒ½å’Œä¸–ä¿—å‰²è£‚åŽ»è¿½æ±‚â¾ƒâ¼°çš„ç²¾ç¥žä¸–ç•Œ Just like in Forrest Gump. Few people in modern society can separate themselves from the world to pursue their own spiritual world DA: Inform_feeling Aspects: Name Mov_Tracker: Forrest Gump Name: The Legend of 1900 | Comment: One of the favorite movies that shocked my soul. Region: Italy | Language: English | Plot: ""a talented pianist lived on the ship all his life"" Plot: ""why didn't he leave that boat"" Plot: ""when the last ship was about to be blown up, he refused to leave"" Comment: What he cannot leave is not the boat, but the spiritual home that nurtured him.",Dialogue and Interactive Systems,"[0.0037184765096753836, 0.028476301580667496, 0.6336855888366699, 0.05562179908156395, 0.007961291819810867, 0.017939304932951927, 0.003952341619879007, 0.004955756012350321, 0.0042436132207512856, 0.016858180984854698, 0.01173133123666048, 0.0034742082934826612, 0.00788091216236353, 0.010446004569530487, 0.09041962772607803, 0.004766606260091066, 0.015634337440133095, 0.03111311048269272, 0.01927775703370571, 0.010191356763243675, 0.008732505142688751, 0.004193445667624474, 0.00472614448517561]",0.6336855888366699,Dialogue and Interactive Systems,0.6336855888366699,True
"Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously. The long and short answers can be extracted from paragraphlevel representation and token-level representation, respectively. In this way, we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.",Question Answering,"[0.004107150249183178, 0.002136144321411848, 0.003773511154577136, 0.004241633228957653, 0.0027361398097127676, 0.007602375000715256, 0.003689780365675688, 0.011640566401183605, 0.007628807798027992, 0.004521331284195185, 0.0166847612708807, 0.0013896728632971644, 0.002347280504181981, 0.8940330147743225, 0.009584030136466026, 0.002268775599077344, 0.008438990451395512, 0.0024819544050842524, 0.0015145486686378717, 0.0010960299987345934, 0.0035403536166995764, 0.0017292918637394905, 0.0028137897606939077]",0.8940330147743225,Question Answering,0.8940330147743225,True
"4 RIKEN 5 NINJAL  {kuribayashi, takumi.ito.c4,","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.02468271739780903, 0.026840578764677048, 0.050252605229616165, 0.015087754465639591, 0.021994376555085182, 0.08280275017023087, 0.028869593515992165, 0.024909263476729393, 0.10192494094371796, 0.05374452471733093, 0.08231331408023834, 0.04586174339056015, 0.04196048155426979, 0.03619622439146042, 0.057099174708127975, 0.035870958119630814, 0.07902400940656662, 0.04778541624546051, 0.03947475925087929, 0.028139017522335052, 0.05358484759926796, 0.014490124769508839, 0.007090793922543526]",0.10192494094371796,Interpretability and Analysis of Models for NLP,0.02468271739780903,False
"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically ""refills"" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-theart machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.",Machine Translation and Multilinguality,"[0.003462517634034157, 0.003301762044429779, 0.007447730749845505, 0.0013182100374251604, 0.005604949779808521, 0.009013963863253593, 0.0014554065419360995, 0.004264447838068008, 0.009455413557589054, 0.0018888835329562426, 0.022564221173524857, 0.8453199863433838, 0.012238909490406513, 0.0030085155740380287, 0.017814189195632935, 0.002394059207290411, 0.01032569445669651, 0.004582663532346487, 0.00523415207862854, 0.0029320025350898504, 0.021274423226714134, 0.003216462442651391, 0.0018815519288182259]",0.8453199863433838,Machine Translation and Multilinguality,0.8453199863433838,True
"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player textbased fantasy adventure dataset LIGHT (Urbanek et al., 2019), as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods-including the quantity of gendered words, a dialogue safety classifier, and human assessments-all of which show that our models generate less gendered, but equally engaging chit-chat responses.",Dialogue and Interactive Systems,"[0.010113230906426907, 0.024602951481938362, 0.6190693378448486, 0.10224036872386932, 0.02039407007396221, 0.013143314979970455, 0.002014999743551016, 0.0036885144654661417, 0.013474775478243828, 0.006993565708398819, 0.021028511226177216, 0.006942685693502426, 0.012565162032842636, 0.012132934294641018, 0.05757950618863106, 0.005272122099995613, 0.010741066187620163, 0.024916239082813263, 0.013685631565749645, 0.003953190520405769, 0.006254811771214008, 0.004334223922342062, 0.00485882256180048]",0.6190693378448486,Dialogue and Interactive Systems,0.6190693378448486,True
"Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. We further propose two evaluation metrics tailored towards the variable-number generation. We also introduce a new dataset (ST A C KEX) that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.",Generation,"[0.002432513050734997, 0.0015835624653846025, 0.015649015083909035, 0.004375405143946409, 0.0026987239252775908, 0.8395792841911316, 0.004883072804659605, 0.011045984923839569, 0.004140427801758051, 0.0017586825415492058, 0.013282869942486286, 0.004225121345371008, 0.004978117533028126, 0.0018413221696391702, 0.02205948531627655, 0.002013136399909854, 0.017343096435070038, 0.006459849886596203, 0.003771646413952112, 0.024662436917424202, 0.0055191065184772015, 0.0036750382278114557, 0.0020221241284161806]",0.8395792841911316,Generation,0.8395792841911316,True
"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dualencoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks. 1 * Equal contribution 1 The code and trained models have been released at https://github.com/facebookresearch/DPR.",Question Answering,"[0.0034914552234113216, 0.0027697221376001835, 0.006073908880352974, 0.005245490465313196, 0.0029448368586599827, 0.004251106642186642, 0.0029896656051278114, 0.010252430103719234, 0.0037257943768054247, 0.0026823210064321756, 0.013940159231424332, 0.0016179019585251808, 0.0018430664204061031, 0.9093017578125, 0.009205718524754047, 0.002170757157728076, 0.005042501725256443, 0.003240063786506653, 0.001558314193971455, 0.0006977667799219489, 0.002133243950083852, 0.0014397728955373168, 0.0033823049161583185]",0.9093017578125,Question Answering,0.9093017578125,True
"Concept graphs are created as universal taxonomies for text understanding in the open domain knowledge. The nodes in concept graphs include both entities and concepts. The edges are from entities to concepts, showing that an entity is an instance of a concept. In this paper, we propose the task of learning interpretable relationships from open domain facts to enrich and refine concept graphs. The Bayesian network structures are learned from open domain facts as the interpretable relationships between relations of facts and concepts of entities. We conduct extensive experiments on public English and Chinese datasets. Compared to the state-of-the-art methods, the learned network structures help improving the identification of concepts for entities based on the relations of entities on both English and Chinese datasets.",Information Extraction,"[0.0041838353499770164, 0.005868444684892893, 0.0040085697546601295, 0.007878715172410011, 0.005520987790077925, 0.009265458211302757, 0.4632945656776428, 0.07941756397485733, 0.015919269993901253, 0.00571500463411212, 0.04588210955262184, 0.0020542822312563658, 0.004305459558963776, 0.021214796230196953, 0.007820981554687023, 0.031786639243364334, 0.20383895933628082, 0.008911313489079475, 0.004069522023200989, 0.00404064916074276, 0.05181846022605896, 0.005147662945091724, 0.00803668424487114]",0.4632945656776428,Information Extraction,0.4632945656776428,True
"When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard. In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment. Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0028529572300612926, 0.002404003171250224, 0.012346497736871243, 0.0036539325956255198, 0.001483588945120573, 0.005614207126200199, 0.0040187109261751175, 0.0038088508881628513, 0.002606886439025402, 0.0024175390135496855, 0.01240783091634512, 0.002019925508648157, 0.0014063480775803328, 0.013432463631033897, 0.00550521444529295, 0.0072654723189771175, 0.8970677852630615, 0.0013271166244521737, 0.0023128928150981665, 0.0010440426412969828, 0.011086576618254185, 0.0011400798102840781, 0.002777191111817956]",0.8970677852630615,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.8970677852630615,True
"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.",Machine Learning for NLP,"[0.002816716441884637, 0.0016901507042348385, 0.00487125338986516, 0.001078647212125361, 0.0027787548024207354, 0.008574482053518295, 0.004406879656016827, 0.011421618983149529, 0.02296537347137928, 0.0017855430487543344, 0.8755013346672058, 0.0049657393246889114, 0.0011105366284027696, 0.01074477843940258, 0.004228874109685421, 0.002451484790071845, 0.026356196030974388, 0.0013885739026591182, 0.00206710840575397, 0.0010050036944448948, 0.005457475781440735, 0.0011887000873684883, 0.0011447463184595108]",0.8755013346672058,Machine Learning for NLP,0.8755013346672058,True
"We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local edits. We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA. 1",Generation,"[0.006405331660062075, 0.010926398448646069, 0.031915027648210526, 0.016401609405875206, 0.007984807714819908, 0.4911014437675476, 0.005808472633361816, 0.026512878015637398, 0.009828571230173111, 0.003515424905344844, 0.05797230079770088, 0.004659680183976889, 0.006574462167918682, 0.00908536184579134, 0.10313409566879272, 0.003942982759326696, 0.1121620163321495, 0.019424868747591972, 0.0063988021574914455, 0.04243199899792671, 0.014243843965232372, 0.00529318256303668, 0.004276462364941835]",0.4911014437675476,Generation,0.4911014437675476,True
"We release a new benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. For writing, lexical substitution systems can assist humans by suggesting words that humans cannot easily think of. However, existing benchmarks depend on human recall as the only source of data, and therefore lack coverage of the substitutes that would be most helpful to humans. Furthermore, annotators often provide substitutes of low quality, which are not actually appropriate in the given context. We collect higher-coverage and higher-quality data by framing lexical substitution as a classification problem, guided by the intuition that it is easier for humans to judge the appropriateness of candidate substitutes than conjure them from memory. To this end, we use a contextfree thesaurus to produce candidates and rely on human judgement to determine contextual appropriateness. Compared to the previous largest benchmark, our SWORDS benchmark has 3x as many substitutes per target word for the same level of quality, and its substitutes are 1.4x more appropriate (based on human judgement) for the same number of substitutes.",Resources and Evaluation,"[0.15211555361747742, 0.03516282141208649, 0.004656422883272171, 0.013319225050508976, 0.030200587585568428, 0.03294685482978821, 0.00432220846414566, 0.02352300100028515, 0.07466292381286621, 0.006070225033909082, 0.06684845685958862, 0.037951674312353134, 0.07361122220754623, 0.021988743916153908, 0.13833701610565186, 0.152089461684227, 0.031760744750499725, 0.02918258123099804, 0.005245704669505358, 0.007612413726747036, 0.030387360602617264, 0.01929902844130993, 0.008705747313797474]",0.15211555361747742,"Linguistic Theories, Cognitive Modeling and Psycholinguistics",0.13833701610565186,False
"We annotate 17,000 SNS posts with both the writer's subjective emotional intensity and the reader's objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer's subjective labels than the readers'. The large gap between the subjective and objective emotions implies the complexity of the mapping from a post to the subjective emotional intensities, which also leads to a lower performance with machine learning models. 12 Each writer provided 500 posts for the training set and 100 posts for the validation and test sets. 13 https://taku910.github.io/mecab/",Resources and Evaluation,"[0.0028483669739216566, 0.10388273000717163, 0.00661433907225728, 0.014125130139291286, 0.012545311823487282, 0.006599141284823418, 0.005667048040777445, 0.008994908072054386, 0.012452076189219952, 0.008757495321333408, 0.004696366842836142, 0.002910030074417591, 0.006040710024535656, 0.0037595450412482023, 0.15958814322948456, 0.004206154029816389, 0.005931729916483164, 0.5986788868904114, 0.011988725513219833, 0.005123216658830643, 0.00684838043525815, 0.004670287948101759, 0.0030713232699781656]",0.5986788868904114,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.15958814322948456,False
"Recent advancements in data-to-text generation largely take on the form of neural end-toend systems. Efforts have been dedicated to improving text generation systems by changing the order of training samples in a process known as curriculum learning. Past research on sequence-to-sequence learning showed that curriculum learning helps to improve both the performance and convergence speed. In this work, we delve into the same idea surrounding the training samples consisting of structured data and text pairs, where at each update, the curriculum framework selects training samples based on the model's competence. Specifically, we experiment with various difficulty metrics and put forward a soft edit distance metric for ranking training samples. Our benchmarks show faster convergence speed where training time is reduced by 38.7% and performance is boosted by 4.84 BLEU.",Generation,"[0.002273687394335866, 0.001169597264379263, 0.012698952108621597, 0.0015394920483231544, 0.0022458171006292105, 0.895927906036377, 0.001524220802821219, 0.005601461045444012, 0.002637652913108468, 0.0021040495485067368, 0.018580935895442963, 0.003967686090618372, 0.00301742902956903, 0.0026324784848839045, 0.01374018657952547, 0.001060528215020895, 0.009617747738957405, 0.003270575311034918, 0.0032327508088201284, 0.006665457971394062, 0.002902158536016941, 0.002221636939793825, 0.0013677041279152036]",0.895927906036377,Generation,0.895927906036377,True
"Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6700-6709. Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. 2020. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations.","Language Grounding to Vision, Robotics and Beyond","[0.011565038934350014, 0.007522863335907459, 0.015041189268231392, 0.008204626850783825, 0.00940959807485342, 0.01665841042995453, 0.006649320013821125, 0.005444362293928862, 0.028678039088845253, 0.6675765514373779, 0.00546591030433774, 0.004152998328208923, 0.006428741849958897, 0.10251635313034058, 0.023244507610797882, 0.00765971327200532, 0.0219159834086895, 0.006139534059911966, 0.02064572088420391, 0.0028691552579402924, 0.009053572081029415, 0.006372179836034775, 0.0067855119705200195]",0.6675765514373779,"Language Grounding to Vision, Robotics and Beyond",0.6675765514373779,True
"This paper presents a finite-state computational model of the verbal morphology of Michif. Michif, the official language of the MÃ©tis peoples, is a uniquely mixed language with Algonquian and French origins. It is spoken across the MÃ©tis homelands in what is now called Canada and the United States, but it is highly endangered with less than 100 speakers. The verbal morphology is remarkably complex, as the already polysynthetic Algonquian patterns are combined with French elements and unique morpho-phonological interactions. The model presented in this paper, LI VERB KAA-OOSHITAHK DI MICHIF handles this complexity by using a series of composed finite-state transducers to model the concatenative morphology and phonological rule alternations that are unique to Michif. Such a rulebased approach is necessary as there is insufficient language data for an approach that uses machine learning. A language model such as LI VERB KAA-OOSHITAHK DI MICHIF furthers the goals of Indigenous computational linguistics in Canada while also supporting the creation of tools for documentation, education, and revitalization that are desired by the MÃ©tis community.","Phonology, Morphology and Word Segmentation","[0.12031897157430649, 0.01984654925763607, 0.013755791820585728, 0.021202566102147102, 0.012011596001684666, 0.011541577987372875, 0.004164768848568201, 0.004913555923849344, 0.04142635315656662, 0.014722432009875774, 0.008143954910337925, 0.024129126220941544, 0.5257070064544678, 0.013412553817033768, 0.0490240640938282, 0.033029574900865555, 0.006149848457425833, 0.012725154869258404, 0.013714801520109177, 0.004993463400751352, 0.02260853722691536, 0.015100265853106976, 0.00735755218192935]",0.5257070064544678,"Phonology, Morphology and Word Segmentation",0.5257070064544678,True
"Neural lexicalized PCFGs (L-PCFGs) (Zhu et al., 2020) have been shown effective in grammar induction. However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions. Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs. Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance.","Syntax: Tagging, Chunking and Parsing","[0.04200709983706474, 0.007758743595331907, 0.01443807128816843, 0.007036938332021236, 0.008069520816206932, 0.11307741701602936, 0.007080710493028164, 0.014576579444110394, 0.0868789330124855, 0.004597678314894438, 0.10968155413866043, 0.06279265135526657, 0.08963460475206375, 0.01317878533154726, 0.04403426870703697, 0.023539511486887932, 0.09390509128570557, 0.01987544260919094, 0.009242046624422073, 0.007898054085671902, 0.20456485450267792, 0.010515093803405762, 0.005616393871605396]",0.20456485450267792,"Syntax: Tagging, Chunking and Parsing",0.20456485450267792,True
"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model named DacKGR over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embeddingbased models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https:// github.com/THU-KEG/DacKGR.",Information Extraction,"[0.003490417031571269, 0.004335327539592981, 0.01742992363870144, 0.007713881321251392, 0.006190453190356493, 0.037175148725509644, 0.3424353003501892, 0.09676346182823181, 0.017056602984666824, 0.008777590468525887, 0.22764255106449127, 0.003491788636893034, 0.0029361287597566843, 0.060310933738946915, 0.009800300002098083, 0.011415224522352219, 0.07544966042041779, 0.014174660667777061, 0.004469679202884436, 0.006056176032871008, 0.031490445137023926, 0.0046485597267746925, 0.006745778955519199]",0.3424353003501892,Information Extraction,0.3424353003501892,True
"We present INSTAMAP, an instance-based method for learning projection-based crosslingual word embeddings. Unlike prior work, it deviates from learning a single global linear projection. INSTAMAP is a non-parametric model that learns a non-linear projection by iteratively: (1) finding a globally optimal rotation of the source embedding space relying on the Kabsch algorithm, and then (2) moving each point along an instance-specific translation vector estimated from the translation vectors of the point's nearest neighbours in the training dictionary. We report performance gains with INSTAMAP over four representative state-of-the-art projection-based models on bilingual lexicon induction across a set of 28 diverse language pairs. We note prominent improvements, especially for more distant language pairs (i.e., languages with nonisomorphic monolingual spaces).",Semantics: Lexical Semantics,"[0.010986460372805595, 0.007528600748628378, 0.0024227434769272804, 0.00311272032558918, 0.012419075705111027, 0.006792720872908831, 0.0020225306507200003, 0.0050473338924348354, 0.03620067983865738, 0.0017163307638838887, 0.0635422021150589, 0.7372226715087891, 0.01831960491836071, 0.002947733271867037, 0.019834166392683983, 0.03485478460788727, 0.005998380947858095, 0.006844779010862112, 0.005461207590997219, 0.003143908455967903, 0.004774856846779585, 0.006136682815849781, 0.0026697302237153053]",0.7372226715087891,Machine Translation and Multilinguality,0.03485478460788727,False
"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce Graph-Glove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology. 1",Machine Learning for NLP,"[0.03696495294570923, 0.019288228824734688, 0.00595490587875247, 0.007364158984273672, 0.012226482853293419, 0.016029253602027893, 0.009723292663693428, 0.030585912987589836, 0.06232069060206413, 0.010998479090631008, 0.1266668736934662, 0.007744790054857731, 0.013011790812015533, 0.011804732494056225, 0.024300869554281235, 0.4473859369754791, 0.09120204299688339, 0.016202038154006004, 0.004158572293817997, 0.0031968990806490183, 0.02572915516793728, 0.011692165397107601, 0.0054478333331644535]",0.4473859369754791,Semantics: Lexical Semantics,0.1266668736934662,False
"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020) . In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.",Interpretability and Analysis of Models for NLP,"[0.0030831878539174795, 0.0022111807484179735, 0.012662171386182308, 0.0062684835866093636, 0.00174187240190804, 0.009551097638905048, 0.0039147562347352505, 0.0033532078377902508, 0.0063042533583939075, 0.0018443145090714097, 0.01992741785943508, 0.0013656169176101685, 0.0011176586849614978, 0.012908229604363441, 0.009657532908022404, 0.007422285154461861, 0.876049816608429, 0.002579726045951247, 0.0027373305056244135, 0.0013271165080368519, 0.010190431959927082, 0.0014071118785068393, 0.0023751230910420418]",0.876049816608429,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.0063042533583939075,False
"Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a languageindependent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexicalsemantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.",Machine Translation and Multilinguality,"[0.008115797303617, 0.010245152749121189, 0.0048486278392374516, 0.010073808953166008, 0.008886621333658695, 0.009580901823937893, 0.18875648081302643, 0.05953546240925789, 0.024463718757033348, 0.004248755984008312, 0.16747312247753143, 0.010317803360521793, 0.00982003565877676, 0.011157494969666004, 0.015450267121195793, 0.12166483700275421, 0.24323268234729767, 0.010869241319596767, 0.00356706022284925, 0.0028142628725618124, 0.058304980397224426, 0.007306997198611498, 0.009265842847526073]",0.24323268234729767,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.010317803360521793,False
"The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset 1 .","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.011497510597109795, 0.005830942653119564, 0.07098090648651123, 0.020123068243265152, 0.00550069659948349, 0.3529515862464905, 0.008440395817160606, 0.008192080073058605, 0.020569967105984688, 0.006431000307202339, 0.024514196440577507, 0.007280140183866024, 0.017644036561250687, 0.006147222127765417, 0.06282082945108414, 0.01874186284840107, 0.246890589594841, 0.021669631823897362, 0.015118724666535854, 0.023533565923571587, 0.026601193472743034, 0.010943042114377022, 0.0075768399983644485]",0.3529515862464905,Generation,0.021669631823897362,False
"Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context. Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn's existing content. Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.",Computational Social Science and Social Media,"[0.0169508196413517, 0.057383570820093155, 0.1158580407500267, 0.10778826475143433, 0.01659337803721428, 0.24499158561229706, 0.004414374008774757, 0.013467487879097462, 0.007292328402400017, 0.014417332597076893, 0.017260903492569923, 0.005333635024726391, 0.015542679466307163, 0.028093967586755753, 0.20344580709934235, 0.010739418677985668, 0.02725404128432274, 0.04020409658551216, 0.014347244054079056, 0.016566181555390358, 0.005455696955323219, 0.008817678317427635, 0.007781506981700659]",0.24499158561229706,Generation,0.057383570820093155,False
"Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embedding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (contextinsensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (highresource) and 5 target (low-resource) corpora. * : equal contribution Topic Topic Words Topic Label Z 1 (S 1 ) profit, growth, stocks, apple, fall, Trading consumer, buy, billion, shares Z 2 (S 2 ) smartphone, ipad, apple, app, Product Line iphone, devices, phone, tablet Z 3 (S 3 ) microsoft, mac, linux, ibm, ios, Operating System apple, xp, windows, software Z 4 (T ) apple, talk, computers, shares, ? disease, driver, electronics, profit, ios",Information Retrieval and Text Mining,"[0.00451264763250947, 0.0052115158177912235, 0.0030522309243679047, 0.011979539878666401, 0.007317017298191786, 0.020122656598687172, 0.03117002360522747, 0.6225042939186096, 0.04557288438081741, 0.004598001483827829, 0.08711046725511551, 0.01110835000872612, 0.003394014434888959, 0.01192143652588129, 0.012189713306725025, 0.02647295594215393, 0.044832341372966766, 0.01008671335875988, 0.005180653650313616, 0.011298857629299164, 0.007376940920948982, 0.007609365042299032, 0.00537736713886261]",0.6225042939186096,Information Retrieval and Text Mining,0.6225042939186096,True
"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the taskspecific networks for prediction. However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks. As a solution, we design a multitask learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification. Empirical studies on two real-world datasets confirm the superiority of the proposed model.",Information Extraction,"[0.0006204852252267301, 0.0014230151427909732, 0.0011773478472605348, 0.0010941275395452976, 0.0011231685057282448, 0.0013491165591403842, 0.9418778419494629, 0.010065281763672829, 0.0015965529019013047, 0.0016656480729579926, 0.005840167868882418, 0.0009242845117114484, 0.0011703332420438528, 0.0018167657544836402, 0.0017046811990439892, 0.0018909948412328959, 0.00516114104539156, 0.0029399427585303783, 0.0015825353330001235, 0.002365856897085905, 0.00937690120190382, 0.0017719136085361242, 0.0014618901768699288]",0.9418778419494629,Information Extraction,0.9418778419494629,True
"The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height â‰¤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a âˆ¼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0027504495810717344, 0.0017879493534564972, 0.0146596385166049, 0.003495544195175171, 0.0013329440262168646, 0.0068909176625311375, 0.004272781778126955, 0.0031352387741208076, 0.003403334179893136, 0.0029627815820276737, 0.00901937484741211, 0.0023652021773159504, 0.0016597966896370053, 0.00412595784291625, 0.004175929352641106, 0.008135084062814713, 0.8920372724533081, 0.0014326400123536587, 0.002453817054629326, 0.0018921239534392953, 0.02432922087609768, 0.0013281236169859767, 0.002353877993300557]",0.8920372724533081,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.8920372724533081,True
"Implicit discourse relation recognition (IDRR) aims to identify logical relations between two adjacent sentences in the discourse. Existing models fail to fully utilize the contextual information which plays an important role in interpreting each local sentence. In this paper, we thus propose a novel graph-based Context Tracking Network (CT-Net) to model the discourse context for IDRR. The CT-Net firstly converts the discourse into the paragraph association graph (PAG), where each sentence tracks their closely related context from the intricate discourse through different types of edges. Then, the CT-Net extracts contextual representation from the PAG through a specially designed cross-grained updating mechanism, which can effectively integrate both sentence-level and token-level contextual semantics. Experiments on PDTB 2.0 show that the CT-Net gains better performance than models that roughly model the context.",Discourse and Pragmatics,"[0.008195390924811363, 0.017466766759753227, 0.022537777200341225, 0.7153272032737732, 0.009461491368710995, 0.003529538167640567, 0.011709746904671192, 0.016578542068600655, 0.00544621841982007, 0.005314686335623264, 0.01091020368039608, 0.0017685152124613523, 0.00997871719300747, 0.031809013336896896, 0.019801460206508636, 0.03699781745672226, 0.028899529948830605, 0.015385989099740982, 0.005014258902519941, 0.006542605347931385, 0.005465394351631403, 0.0041060117073357105, 0.007753096520900726]",0.7153272032737732,Discourse and Pragmatics,0.7153272032737732,True
"Despite significant progress in neural abstractive summarization, recent studies have shown that the current models are prone to generating summaries that are unfaithful to the original context. To address the issue, we study contrast candidate generation and selection as a model-agnostic post-processing technique to correct the extrinsic hallucinations (i.e. information not present in the source text) in unfaithful summaries. We learn a discriminative correction model by generating alternative candidate summaries where named entities and quantities in the generated summary are replaced with ones with compatible semantic types from the source document. This model is then used to select the best candidate as the final output summary. Our experiments and analysis across a number of neural summarization systems show that our proposed method is effective in identifying and correcting extrinsic hallucinations. We analyze the typical hallucination phenomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction.",Summarization,"[0.0010036969324573874, 0.0014034404885023832, 0.0018444033339619637, 0.0055504185147583485, 0.0018606011290103197, 0.006105815526098013, 0.00516713410615921, 0.005383465904742479, 0.0021098710130900145, 0.0017971704946830869, 0.0016999762738123536, 0.002211917657405138, 0.002212081104516983, 0.0006556170992553234, 0.007289898581802845, 0.0008564181625843048, 0.0014258630108088255, 0.004377974662929773, 0.00394738744944334, 0.9346407055854797, 0.0027214474976062775, 0.003070445731282234, 0.002664237516000867]",0.9346407055854797,Summarization,0.9346407055854797,True
"Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast.",Machine Translation and Multilinguality,"[0.0009840208804234862, 0.0009540159371681511, 0.000835877435747534, 0.00038201489951461554, 0.0026959755923599005, 0.0009821654530242085, 0.0006712935864925385, 0.0012002894654870033, 0.0037252018228173256, 0.000777680310420692, 0.0022880611941218376, 0.9686856865882874, 0.0019459623144939542, 0.00044619152322411537, 0.0030784588307142258, 0.001102747512049973, 0.0004895989550277591, 0.0015972958644852042, 0.0023527154698967934, 0.0011982753640040755, 0.0011523646535351872, 0.0016681365668773651, 0.0007859496981836855]",0.9686856865882874,Machine Translation and Multilinguality,0.9686856865882874,True
"Most of the previous work on Event Detection (ED) has only considered the datasets with a small number of event types (i.e., up to 38 types). In this work, we present the first study on fine-grained ED (FED) where the evaluation dataset involves much more fine-grained event types (i.e., 449 types). We propose a novel method to transform the Semcor dataset for Word Sense Disambiguation into a large and high-quality dataset for FED. Extensive evaluation of the current ED methods is conducted to demonstrate the challenges of the generated datasets for FED, calling for more research effort in this area.",Information Extraction,"[0.005041324067860842, 0.010374709963798523, 0.003181673586368561, 0.011356430128216743, 0.005777173209935427, 0.005223722662776709, 0.5662051439285278, 0.044055745005607605, 0.005442399997264147, 0.0054773748852312565, 0.013718626461923122, 0.002618498867377639, 0.010960455983877182, 0.010156107135117054, 0.02040020190179348, 0.08967092633247375, 0.08026043325662613, 0.022090012207627296, 0.004420976620167494, 0.005100214388221502, 0.06092429906129837, 0.0069653186947107315, 0.010578151792287827]",0.5662051439285278,Information Extraction,0.5662051439285278,True
"Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a ""one-size-fits-all"" approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and nonnative speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach and foster further research into these aspects, we release a dataset of complex words annotated by readers with different backgrounds.","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.24545608460903168, 0.04127766191959381, 0.00288423546589911, 0.025120429694652557, 0.03339759260416031, 0.012765457853674889, 0.0029374961741268635, 0.016179464757442474, 0.09142964333295822, 0.009691343642771244, 0.02530466951429844, 0.012110079638659954, 0.12305687367916107, 0.014106662943959236, 0.08207479864358902, 0.16218237578868866, 0.016691027209162712, 0.020079120993614197, 0.0048358687199652195, 0.007532223593443632, 0.02088739722967148, 0.020059989765286446, 0.009939554147422314]",0.24545608460903168,"Linguistic Theories, Cognitive Modeling and Psycholinguistics",0.24545608460903168,True
"Given questions regarding some prototypical situation -such as Name something that people usually do before they leave the house for work? -a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a longrunning international game show -FAMILY-FEUD. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task. * Equal contribution.",Question Answering,"[0.00436553405597806, 0.00431201234459877, 0.007088133599609137, 0.005062497220933437, 0.0049341898411512375, 0.005738906096667051, 0.001510710921138525, 0.007043901365250349, 0.011971994303166866, 0.008304434828460217, 0.014779231511056423, 0.0023563657887279987, 0.0027000796981155872, 0.8664440512657166, 0.027217630296945572, 0.0020778400357812643, 0.008675801567733288, 0.003980899695307016, 0.0026841105427592993, 0.0006591718411073089, 0.003152035176753998, 0.0019012695411220193, 0.0030392080079764128]",0.8664440512657166,Question Answering,0.8664440512657166,True
"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the Super-GLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalizationthe combination of input specification, loss function, and reuse of pretrained parametersby users of the dataset, rather than improvements in the pretrained model's reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance by 2-6 points and (ii) several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model's extreme sensitivity to hyperparameters. We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.007257360965013504, 0.006954088807106018, 0.025637255981564522, 0.015178719535470009, 0.007239505648612976, 0.031051531434059143, 0.012812817469239235, 0.014755410142242908, 0.02925022691488266, 0.009866754524409771, 0.03757242113351822, 0.0019040897022932768, 0.0028044788632541895, 0.21088209748268127, 0.05542660132050514, 0.010297123342752457, 0.4644071161746979, 0.014002273790538311, 0.004509424325078726, 0.002996027935296297, 0.022861365228891373, 0.004799030255526304, 0.0075342548079788685]",0.4644071161746979,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.4644071161746979,True
"Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semisupervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F 1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context. Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task. 1",Machine Translation and Multilinguality,"[0.010370687581598759, 0.008644169196486473, 0.0029696698766201735, 0.002210987498983741, 0.01264273189008236, 0.00711399968713522, 0.002205497119575739, 0.008158204145729542, 0.017024345695972443, 0.0017320807091891766, 0.045554764568805695, 0.7812071442604065, 0.014201516285538673, 0.0029641210567206144, 0.022396693006157875, 0.024754075333476067, 0.00583363464102149, 0.007531845010817051, 0.004173353314399719, 0.002851736731827259, 0.006916311103850603, 0.005896525923162699, 0.002645844593644142]",0.7812071442604065,Machine Translation and Multilinguality,0.7812071442604065,True
"Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building Duo-RAT, a re-implementation of the state-of-theart RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema 1 . * Equal contribution, order was determined by a quantum random number draw.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.005258954130113125, 0.003465516958385706, 0.023364894092082977, 0.010105712339282036, 0.002010280266404152, 0.01660173013806343, 0.0046832868829369545, 0.006175648421049118, 0.007079568225890398, 0.004326376598328352, 0.013133983127772808, 0.0013948349514976144, 0.0022789454087615013, 0.05784254148602486, 0.01620178297162056, 0.008781627751886845, 0.7906545400619507, 0.002842620713636279, 0.0033941424917429686, 0.00185090908780694, 0.01314550545066595, 0.0021283586975187063, 0.0032782782800495625]",0.7906545400619507,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.7906545400619507,True
"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",Ethics and NLP,"[0.03711697459220886, 0.34659290313720703, 0.0028027035295963287, 0.01733161136507988, 0.24612340331077576, 0.0033787896391004324, 0.006968128960579634, 0.007648265454918146, 0.09745470434427261, 0.009785335510969162, 0.032386414706707, 0.024454213678836823, 0.015957316383719444, 0.009863350540399551, 0.05704719200730324, 0.01761416718363762, 0.011925490573048592, 0.017986932769417763, 0.010376647114753723, 0.0035387263633310795, 0.006022131536155939, 0.008481058292090893, 0.009143571369349957]",0.34659290313720703,Computational Social Science and Social Media,0.24612340331077576,False
"We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model 'accuracy' scores (Ã  la Marvin and Linzen (2018)) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter syntactic violations they fail to accurately predict the longer reaction times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations. 1. S d (was impressed) < Sc(was impressed)","Linguistic Theories, Cognitive Modeling and Psycholinguistics","[0.09562800824642181, 0.011826762929558754, 0.005035300273448229, 0.007273912895470858, 0.014108353294432163, 0.025856325402855873, 0.0035216601099818945, 0.008054421283304691, 0.43053746223449707, 0.01814326085150242, 0.03054477646946907, 0.021522607654333115, 0.06417438387870789, 0.016812052577733994, 0.0809023380279541, 0.011177719570696354, 0.029111625626683235, 0.01587572880089283, 0.008175519295036793, 0.006820071022957563, 0.07648827880620956, 0.012854142114520073, 0.005555332638323307]",0.43053746223449707,Interpretability and Analysis of Models for NLP,0.09562800824642181,False
"Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain. Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. However, this practice requires significant domainspecific data and computational resources which may not always be available. In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. We demonstrate that by explicitly incorporating the multi-granularity information of unseen and domain-specific words via the adaptation of (word based) ngrams, the performance of a generic pretrained model can be greatly improved. Specifically, we introduce a Transformer-based Domainaware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. Experimental results illustrate the effectiveness of T-DNA on eight lowresource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities. 1","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.007536990102380514, 0.006474454887211323, 0.004874183796346188, 0.005724458023905754, 0.011420279741287231, 0.01100610475987196, 0.05399299040436745, 0.05266670882701874, 0.0915502980351448, 0.0020892333704978228, 0.5616714954376221, 0.030587242916226387, 0.01587373949587345, 0.009991875849664211, 0.02779938466846943, 0.012835461646318436, 0.018816625699400902, 0.012855270877480507, 0.004258326254785061, 0.005006896331906319, 0.043665118515491486, 0.004877834115177393, 0.004425170365720987]",0.5616714954376221,Machine Learning for NLP,0.018816625699400902,False
"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.",Semantics: Lexical Semantics,"[0.13857071101665497, 0.02927396260201931, 0.006552983541041613, 0.08492307364940643, 0.01879766583442688, 0.013988792896270752, 0.007820569910109043, 0.011130481958389282, 0.06480404734611511, 0.02588164433836937, 0.023879021406173706, 0.006625885609537363, 0.09398592263460159, 0.037890397012233734, 0.05392736569046974, 0.26127180457115173, 0.016667770221829414, 0.045567091554403305, 0.011238371022045612, 0.004981433041393757, 0.011486389674246311, 0.018512099981307983, 0.012222525663673878]",0.26127180457115173,Semantics: Lexical Semantics,0.26127180457115173,True
"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.",Generation,"[0.0178399495780468, 0.036589957773685455, 0.12820099294185638, 0.04470614716410637, 0.0111282579600811, 0.3423490524291992, 0.0029119555838406086, 0.0040539326146245, 0.010250357910990715, 0.01117113046348095, 0.01935461349785328, 0.007427399046719074, 0.026759620755910873, 0.018511895090341568, 0.1752418726682663, 0.0073591917753219604, 0.053446393460035324, 0.026670332998037338, 0.013914521783590317, 0.013948405161499977, 0.013772537000477314, 0.0072470069862902164, 0.007144575007259846]",0.3423490524291992,Generation,0.3423490524291992,True
"The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.",Question Answering,"[0.0025355552788823843, 0.0020144109148532152, 0.004143688827753067, 0.003982729744166136, 0.002237757435068488, 0.003179114544764161, 0.0036324758548289537, 0.007136386353522539, 0.004393980372697115, 0.0043064565397799015, 0.010505606420338154, 0.0006354089127853513, 0.0010502495570108294, 0.9240773916244507, 0.004566154908388853, 0.0017021639505401254, 0.009881441481411457, 0.002063132356852293, 0.0014181399019435048, 0.0006145203369669616, 0.002169224200770259, 0.0009797696257010102, 0.0027743226382881403]",0.9240773916244507,Question Answering,0.9240773916244507,True
"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference. The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.",Machine Translation and Multilinguality,"[0.0008844138355925679, 0.0007398679153993726, 0.0014459023950621486, 0.0004907603142783046, 0.002309514442458749, 0.0036344071850180626, 0.000717286835424602, 0.0012339501408860087, 0.00330739701166749, 0.00075252924580127, 0.0032663356978446245, 0.9590860605239868, 0.0027648545801639557, 0.0005294462316669524, 0.004849010147154331, 0.0010641936678439379, 0.0011938801035284996, 0.002050303388386965, 0.0030285860411822796, 0.00232719536870718, 0.0016019782051444054, 0.0018108009826391935, 0.0009112823754549026]",0.9590860605239868,Machine Translation and Multilinguality,0.9590860605239868,True
"Disentanglement of latent representations into content and style spaces has been a commonly employed method for unsupervised text style transfer. These techniques aim to learn the disentangled representations and tweak them to modify the style of a sentence. In this paper, we propose a counterfactual-based method to modify the latent representation, by posing a 'what-if' scenario. This simple and disciplined approach also enables a fine-grained control on the transfer strength. We conduct experiments with the proposed methodology on multiple attribute transfer tasks like Sentiment, Formality and Excitement to support our hypothesis.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.017969276756048203, 0.008385129272937775, 0.014923492446541786, 0.016466423869132996, 0.011676404625177383, 0.5275071859359741, 0.006635596975684166, 0.01573359966278076, 0.038936253637075424, 0.029492154717445374, 0.0697086900472641, 0.0060867564752697945, 0.020685279741883278, 0.003687727963551879, 0.04792750999331474, 0.014528471976518631, 0.014510937966406345, 0.029843205586075783, 0.02441347949206829, 0.05918692424893379, 0.00522173848003149, 0.01212118286639452, 0.004352552816271782]",0.5275071859359741,Generation,0.029843205586075783,False
"Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work section generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relationaware Related work Generator (RRG), which generates an abstractive related work section from multiple scientific papers in the same research area. Concretely, we propose a relationaware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation interact and are refined iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers 1 . Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work section generation task.",Summarization,"[0.003571236040443182, 0.002064961940050125, 0.008128657937049866, 0.01648663729429245, 0.004665209446102381, 0.1191379725933075, 0.09827563911676407, 0.034133490175008774, 0.013065144419670105, 0.005979760084301233, 0.028373317793011665, 0.012466016225516796, 0.009389019571244717, 0.0034234856721013784, 0.019089918583631516, 0.009210032410919666, 0.05908684805035591, 0.014044887386262417, 0.010977697558701038, 0.4928724467754364, 0.013688984327018261, 0.012392501346766949, 0.009476150386035442]",0.4928724467754364,Summarization,0.4928724467754364,True
"Commonsense reasoning aims to incorporate sets of commonsense facts, retrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about ordinary situations. The dynamic nature of commonsense knowledge postulates models capable of performing multi-hop reasoning over new situations. This feature also results in having large-scale sparse Knowledge Graphs, where such reasoning process is needed to predict relations between new events. However, existing approaches in this area are limited by considering CKGs as a limited set of facts, thus rendering them unfit for reasoning over new unseen situations and events. In this paper, we present a neural-symbolic reasoner, which is capable of reasoning over large-scale dynamic CKGs. The logic rules for reasoning over CKGs are learned during training by our model. In addition to providing interpretable explanation, the learned logic rules help to generalise prediction to newly introduced events. Experimental results on the task of link prediction on CKGs prove the effectiveness of our model by outperforming the state-of-the-art models.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0041845208033919334, 0.005749681498855352, 0.014315414242446423, 0.011991201899945736, 0.005357302259653807, 0.009765739552676678, 0.18841251730918884, 0.029070323333144188, 0.020488442853093147, 0.015978381037712097, 0.03708164021372795, 0.001480534323491156, 0.0022555524483323097, 0.11217669397592545, 0.009518438018858433, 0.01800043322145939, 0.4444507956504822, 0.01545852329581976, 0.0038763300981372595, 0.003458853345364332, 0.03233558312058449, 0.00438388017937541, 0.010209273546934128]",0.4444507956504822,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.4444507956504822,True
"Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased by position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms, leading to more complete and diverse summaries.",Summarization,"[0.0012008914491161704, 0.002979047130793333, 0.0024319489020854235, 0.008939494378864765, 0.0021844147704541683, 0.008175298571586609, 0.005701156333088875, 0.006998821627348661, 0.002136250026524067, 0.003451165510341525, 0.002003579866141081, 0.0014888716395944357, 0.0024823204148560762, 0.0008754865848459303, 0.011751309037208557, 0.0011754805454984307, 0.0021028388291597366, 0.005825235974043608, 0.004833958111703396, 0.9135069251060486, 0.0033710200805217028, 0.0034472413826733828, 0.002937286626547575]",0.9135069251060486,Summarization,0.9135069251060486,True
"Named entity recognition (NER) is a wellstudied task in natural language processing. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. To tackle these issues, we propose a two-stage entity identifier. First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundaryadjusted span proposals with the corresponding categories. Our method effectively utilizes the boundary information of entities and partially matched spans during training. Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference. Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.",Information Extraction,"[0.0014476390788331628, 0.003505783388391137, 0.00215926137752831, 0.0021125678904354572, 0.0021927764173597097, 0.004048458766192198, 0.8268983364105225, 0.027398476377129555, 0.0028361580334603786, 0.00293309660628438, 0.012666639871895313, 0.0013418261660262942, 0.002974244300276041, 0.004384264815598726, 0.0069597028195858, 0.0056175668723881245, 0.0153310252353549, 0.0066115944646298885, 0.0020156127866357565, 0.00469488138332963, 0.05438052490353584, 0.003894989611580968, 0.003594635985791683]",0.8268983364105225,Information Extraction,0.8268983364105225,True
"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays an important role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.004176347516477108, 0.0034962480422109365, 0.011540518142282963, 0.005879433825612068, 0.003500254824757576, 0.009904731065034866, 0.014179864898324013, 0.0077195861376821995, 0.01326107606291771, 0.0031594554893672466, 0.047566451132297516, 0.0016590870218351483, 0.001575290341861546, 0.022183991968631744, 0.011445162817835808, 0.010051199235022068, 0.786003053188324, 0.00467378506436944, 0.0027833476196974516, 0.0017935930518433452, 0.02765406295657158, 0.0018627539975568652, 0.0039306990802288055]",0.786003053188324,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.786003053188324,True
"Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers' representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0027910710778087378, 0.0018045247998088598, 0.0061708418652415276, 0.0037093369755893946, 0.0010529609862715006, 0.004823083523660898, 0.005163747817277908, 0.002726141596212983, 0.004205516539514065, 0.0027086669579148293, 0.0062052407301962376, 0.0019923842046409845, 0.0016658499371260405, 0.00438428670167923, 0.004731269087642431, 0.012308560311794281, 0.9054734110832214, 0.0018779189558699727, 0.002568757627159357, 0.0013220737455412745, 0.018244078382849693, 0.0013816994614899158, 0.0026886570267379284]",0.9054734110832214,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.9054734110832214,True
"Affect preferences vary with user demographics, and tapping into demographic information provides important cues about the users' language preferences. In this paper, we utilize the user demographics, and propose EMPATH-BERT, a demographic-aware framework for empathy prediction based on BERT. Through several comparative experiments, we show that EMPATHBERT surpasses traditional machine learning and deep learning models, and illustrate the importance of user demographics to predict empathy and distress in user responses to stimulative news articles. We also highlight the importance of affect information in the responses by developing affect-aware models to predict user demographic attributes.",Computational Social Science and Social Media,"[0.019264817237854004, 0.21358519792556763, 0.01420615054666996, 0.017379367724061012, 0.04620716720819473, 0.010822475887835026, 0.014448300004005432, 0.011450718156993389, 0.10718017816543579, 0.04006955772638321, 0.020818263292312622, 0.008779280818998814, 0.014073512516915798, 0.010332497768104076, 0.08501530438661575, 0.005678005050867796, 0.007478127721697092, 0.2812650501728058, 0.03851444646716118, 0.014371044933795929, 0.005628833081573248, 0.007285541854798794, 0.006146254017949104]",0.2812650501728058,"Sentiment Analysis, Stylistic Analysis, and Argument Mining",0.21358519792556763,False
"Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. We use a standard editing model with simple task-specific reranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies. Experiments on two machine translation (MT) datasets show new state-of-art results. We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.",Generation,"[0.005694350693374872, 0.003467910923063755, 0.009193171747028828, 0.008831753395497799, 0.00818045437335968, 0.20546503365039825, 0.023432545363903046, 0.06680389493703842, 0.01574515365064144, 0.0058471872471272945, 0.1012883111834526, 0.05701883137226105, 0.01292512845247984, 0.009344524703919888, 0.061125971376895905, 0.003996241372078657, 0.024993689730763435, 0.010802027769386768, 0.015168042853474617, 0.3172682821750641, 0.018349600955843925, 0.008635401725769043, 0.006422425154596567]",0.3172682821750641,Summarization,0.20546503365039825,False
"A major obstacle to the wide-spread adoption of neural retrieval models is that they require large supervised training sets to surpass traditional term-based techniques, which are constructed from raw corpora. In this paper, we propose an approach to zero-shot learning for passage retrieval that uses synthetic question generation to close this gap. The question generation system is trained on general domain data, but is applied to documents in the targeted domain. This allows us to create arbitrarily large, yet noisy, question-passage relevance pairs that are domain specific. Furthermore, when this is coupled with a simple hybrid termneural model, first-stage retrieval performance can be improved further. Empirically, we show that this is an effective strategy for building neural passage retrieval models in the absence of large training corpora. Depending on the domain, this technique can even approach the accuracy of supervised models.",Question Answering,"[0.008656369522213936, 0.0038344424683600664, 0.007830129005014896, 0.00786660984158516, 0.010342172347009182, 0.02886001020669937, 0.02841832861304283, 0.5001785755157471, 0.021643975749611855, 0.008858122862875462, 0.08706394582986832, 0.008479926735162735, 0.004178502596914768, 0.1873789280653, 0.015280632302165031, 0.009515440091490746, 0.0244925357401371, 0.008378739468753338, 0.005781925283372402, 0.005045345984399319, 0.006323159672319889, 0.005006280727684498, 0.006585911847651005]",0.5001785755157471,Information Retrieval and Text Mining,0.1873789280653,False
"Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. We further propose a new dataset, Blended-SkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.",Dialogue and Interactive Systems,"[0.0014047697186470032, 0.002774118212983012, 0.9346243739128113, 0.009972767904400826, 0.0015551430406048894, 0.0031355381943285465, 0.0009526872891001403, 0.000988754560239613, 0.0009993741987273097, 0.004460261203348637, 0.0028955326415598392, 0.001191090326756239, 0.0016656609950587153, 0.005182458087801933, 0.007038708310574293, 0.000855004123877734, 0.002362314146012068, 0.005531163886189461, 0.005797757301479578, 0.0015192137798294425, 0.0018051323713734746, 0.0013723074225708842, 0.0019158974755555391]",0.9346243739128113,Dialogue and Interactive Systems,0.9346243739128113,True
"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.",Machine Translation and Multilinguality,"[0.0008584291790612042, 0.0006356554222293198, 0.0021771439351141453, 0.0004626357986126095, 0.0017619745340198278, 0.006486780941486359, 0.0009229251299984753, 0.0014594902750104666, 0.0031947146635502577, 0.0009211058495566249, 0.004618064034730196, 0.9541013836860657, 0.001954244449734688, 0.0007424693321809173, 0.0034762637224048376, 0.0012489600339904428, 0.0026744618080556393, 0.0016985418042168021, 0.0032853519078344107, 0.0024019295815378428, 0.001996466191485524, 0.0020116260275244713, 0.0009094491833820939]",0.9541013836860657,Machine Translation and Multilinguality,0.9541013836860657,True
"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are reflected in conventional video captioning. Unlike images, actions in videos are also inherently linked to social aspects such as intentions (why the action is taking place), effects (what changes due to the action), and attributes that describe the agent. Thus for video understanding, such as when captioning videos or when answering questions about videos, one must have an understanding of these commonsense aspects. We present the first work on generating commonsense captions directly from videos, to describe latent aspects such as intentions, effects, and attributes. We present a new dataset ""Video-to-Commonsense (V2C)"" that contains âˆ¼ 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. Both the generation task and the QA task can be used to enrich video captions.","Language Grounding to Vision, Robotics and Beyond","[0.003238758072257042, 0.014555065892636776, 0.02180979773402214, 0.007145498413592577, 0.007745207287371159, 0.01683003082871437, 0.008123827166855335, 0.006299379747360945, 0.005666860844939947, 0.7205790877342224, 0.00435641547665, 0.002500635338947177, 0.003064603777602315, 0.009309391491115093, 0.06354153156280518, 0.0028683708515018225, 0.01377233024686575, 0.008799954317510128, 0.04488386958837509, 0.014347309246659279, 0.0073155807331204414, 0.008019868284463882, 0.00522654689848423]",0.7205790877342224,"Language Grounding to Vision, Robotics and Beyond",0.7205790877342224,True
"We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We explore whether categorical perception effects follow and show that the messages are not compositional.",Interpretability and Analysis of Models for NLP,"[0.02643647789955139, 0.01792016439139843, 0.05119839310646057, 0.022901710122823715, 0.021701999008655548, 0.016261840239167213, 0.0035425073001533747, 0.005833025556057692, 0.07688920944929123, 0.6067435145378113, 0.014725886285305023, 0.004182818345725536, 0.012555188499391079, 0.008063281886279583, 0.022064318880438805, 0.009885597974061966, 0.00908817257732153, 0.007132184691727161, 0.035888154059648514, 0.007849537767469883, 0.003491446143016219, 0.009749576449394226, 0.005895021837204695]",0.6067435145378113,"Language Grounding to Vision, Robotics and Beyond",0.07688920944929123,False
"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. * Equal contribution. The work was done when the first author was an intern at ByteDance.",Machine Translation and Multilinguality,"[0.0008648562361486256, 0.0011688143713399768, 0.001336299697868526, 0.00047202236601151526, 0.0026954198256134987, 0.0016112640732899308, 0.0008463633712381124, 0.001553308917209506, 0.0031692818738520145, 0.000967351021245122, 0.0023895245976746082, 0.9589845538139343, 0.0024560773745179176, 0.0005982896545901895, 0.0063943518325686455, 0.0011603414313867688, 0.0009837332181632519, 0.002236433094367385, 0.003290190128609538, 0.0017545019509270787, 0.0020941912662237883, 0.0019751873333007097, 0.0009976492729038]",0.9589845538139343,Machine Translation and Multilinguality,0.9589845538139343,True
"Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL. 1","Language Grounding to Vision, Robotics and Beyond","[0.007596331182867289, 0.005560265388339758, 0.06310742348432541, 0.014492516405880451, 0.007450085133314133, 0.01919873245060444, 0.03320149704813957, 0.054996307939291, 0.01293671689927578, 0.21159924566745758, 0.061180226504802704, 0.004018760751932859, 0.0039095282554626465, 0.28077876567840576, 0.020202912390232086, 0.00985291600227356, 0.10851592570543289, 0.006786982528865337, 0.035251349210739136, 0.00656769098713994, 0.015184210613369942, 0.005224485415965319, 0.012387033551931381]",0.28077876567840576,Question Answering,0.21159924566745758,False
"Multi-hop question answering (QA) requires a model to retrieve and integrate information from multiple passages to answer a question. Rapid progress has been made on multi-hop QA systems with regard to standard evaluation metrics, including EM and F1. However, by simply evaluating the correctness of the answers, it is unclear to what extent these systems have learned the ability to perform multihop reasoning. In this paper, we propose an additional sub-question evaluation for the multihop QA dataset HotpotQA, in order to shed some light on explaining the reasoning process of QA systems in answering complex questions. We adopt a neural decomposition model to generate sub-questions for a multi-hop question, followed by extracting the corresponding sub-answers. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system.",Question Answering,"[0.0027498630806803703, 0.0021852464415133, 0.004684867337346077, 0.004424784332513809, 0.002672845497727394, 0.0031348601914942265, 0.0020736965816468, 0.005982720758765936, 0.0047289724461734295, 0.004577907267957926, 0.006407492328435183, 0.0014819242060184479, 0.0017157201655209064, 0.9302734732627869, 0.007129005156457424, 0.0014582431176677346, 0.003570718690752983, 0.0026379358023405075, 0.001965693198144436, 0.0005839236546307802, 0.0016527613624930382, 0.001229124958626926, 0.002678273944184184]",0.9302734732627869,Question Answering,0.9302734732627869,True
"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as ""what happened before/after [some event]?"" We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance. 1",Question Answering,"[0.01815914362668991, 0.010932519100606441, 0.004229798447340727, 0.02382039837539196, 0.008726526983082294, 0.014312916435301304, 0.02109750546514988, 0.0358637198805809, 0.02465326339006424, 0.014601240865886211, 0.01801968365907669, 0.002434301655739546, 0.0124362139031291, 0.6075721979141235, 0.06292850524187088, 0.022168371826410294, 0.05190908536314964, 0.006362557411193848, 0.004742198623716831, 0.004608849994838238, 0.011753800325095654, 0.007456635124981403, 0.011210592463612556]",0.6075721979141235,Question Answering,0.6075721979141235,True
"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",Interpretability and Analysis of Models for NLP,"[0.015651829540729523, 0.003894355846568942, 0.0024012059438973665, 0.003564225509762764, 0.008544666692614555, 0.005450059659779072, 0.0031910929828882217, 0.005770144984126091, 0.7386736869812012, 0.01007023360580206, 0.03295494616031647, 0.08857695758342743, 0.01721976138651371, 0.0029019482899457216, 0.014064443297684193, 0.007470987271517515, 0.0028338595293462276, 0.0076578594744205475, 0.010820487514138222, 0.0061952220275998116, 0.004211017396301031, 0.0058426507748663425, 0.0020383717492222786]",0.7386736869812012,Interpretability and Analysis of Models for NLP,0.7386736869812012,True
"Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that AAs learn linguistic patterns and context-dependent reasoning.",Computational Social Science and Social Media,"[0.006564298644661903, 0.016537873074412346, 0.017470428720116615, 0.050993070006370544, 0.007256259210407734, 0.015124095603823662, 0.009758643805980682, 0.009248095564544201, 0.008786067366600037, 0.003975356929004192, 0.015498464927077293, 0.0015162572963163257, 0.003574504517018795, 0.03689440339803696, 0.07951176911592484, 0.01535941381007433, 0.643156886100769, 0.021630410104990005, 0.005143046844750643, 0.0035013481974601746, 0.017128368839621544, 0.003588302992284298, 0.0077825584448874]",0.643156886100769,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.016537873074412346,False
"Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference. It provides two innovative components for better learning representations for entity alignment. It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity. It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair. Such strategies allow NMN to effectively construct matchingoriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task. Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-ofthe-art methods.",Information Extraction,"[0.002340831561014056, 0.004726361948996782, 0.002394559560343623, 0.0023938941303640604, 0.004777937661856413, 0.0054242475889623165, 0.7537317872047424, 0.060592327266931534, 0.00697705103084445, 0.0031462328042834997, 0.05403777211904526, 0.0036989697255194187, 0.003588326508179307, 0.003870629705488682, 0.00570856174454093, 0.009069561958312988, 0.015139900147914886, 0.006405925378203392, 0.002679072320461273, 0.0056816767901182175, 0.034331247210502625, 0.005201708525419235, 0.0040815481916069984]",0.7537317872047424,Information Extraction,0.7537317872047424,True
"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PLOTMACHINES, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PLOTMACHINES with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and GROVER, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",Generation,"[0.005221152678132057, 0.007168645039200783, 0.015439572744071484, 0.010236783884465694, 0.0035512454342097044, 0.795758068561554, 0.003191367955878377, 0.007467801682651043, 0.004249058663845062, 0.009570689871907234, 0.009234318509697914, 0.0015557178994640708, 0.007376035675406456, 0.005682250019162893, 0.04707682505249977, 0.003944520838558674, 0.01524523738771677, 0.013774164952337742, 0.005342013202607632, 0.014623244293034077, 0.0054646991193294525, 0.005847843363881111, 0.0029787442181259394]",0.795758068561554,Generation,0.795758068561554,True
"We introduce NLQuAD, the first data set with baseline methods for non-factoid long question answering, a task requiring documentlevel language understanding. In contrast to existing span detection question answering data sets, NLQuAD has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union (IoU), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare BERT, RoBERTa, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. NLQuAD's samples exceed the input limitation of most pretrained Transformer-based models, encouraging future research on long sequence language models. 1",Resources and Evaluation,"[0.006334770005196333, 0.003085487987846136, 0.005716883577406406, 0.0060986545868217945, 0.004469543229788542, 0.008034768514335155, 0.002731523010879755, 0.011144326068460941, 0.010935156606137753, 0.0061761485412716866, 0.026568172499537468, 0.002812844468280673, 0.003699229098856449, 0.8521044850349426, 0.01984201930463314, 0.0026134366635233164, 0.01023109070956707, 0.0033673113211989403, 0.0022288141772150993, 0.0009781512198969722, 0.004826216027140617, 0.0021821341942995787, 0.0038188796024769545]",0.8521044850349426,Question Answering,0.01984201930463314,False
"Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 outperforms existing best unified model and achieves competitive or even better performance than the pre-trained and fine-tuned model mBART on tens of WMT's translation directions. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual Transformer baseline. Code, data and trained models are available at https://github. com/PANXiao1994/mRASP2.",Machine Translation and Multilinguality,"[0.0011448005679994822, 0.0016859541647136211, 0.001389858080074191, 0.0005194401601329446, 0.003163171000778675, 0.001437749364413321, 0.0009481878369115293, 0.00166858930606395, 0.002948357490822673, 0.0009337635128758848, 0.0021203539799898863, 0.9559381604194641, 0.0029419255442917347, 0.0005862284451723099, 0.006770001258701086, 0.0015652613947167993, 0.0007603046833537519, 0.0028407596983015537, 0.003584332065656781, 0.0019498419715091586, 0.001919879810884595, 0.0021780570968985558, 0.0010050757555291057]",0.9559381604194641,Machine Translation and Multilinguality,0.9559381604194641,True
"Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-ofthe-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters. 1","Language Grounding to Vision, Robotics and Beyond","[0.006520537659525871, 0.013280579820275307, 0.37176230549812317, 0.025044407695531845, 0.008316596038639545, 0.007455798331648111, 0.011835805140435696, 0.007075049914419651, 0.023816360160708427, 0.14243444800376892, 0.01601596176624298, 0.00659222062677145, 0.00779608404263854, 0.008972433395683765, 0.027528585866093636, 0.005527094472199678, 0.012178809382021427, 0.02688569948077202, 0.21841411292552948, 0.027357660233974457, 0.0067615993320941925, 0.010085869580507278, 0.008342047221958637]",0.37176230549812317,Dialogue and Interactive Systems,0.14243444800376892,False
"Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types between the implicitly connected entity pairs can be identified with relational reasoning patterns in the real world. In this paper, we propose a unified framework to jointly extract explicit and implicit relational triples. To explore entity pairs that may be implicitly connected by relations, we propose a binary pointer network to extract overlapping relational triples relevant to each word sequentially and retain the information of previously extracted triples in an external memory. To infer the relation types of implicit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method.",Information Extraction,"[0.0009682558593340218, 0.0016038859030231833, 0.001551691209897399, 0.002171203261241317, 0.001344284974038601, 0.002009717747569084, 0.9142664074897766, 0.012339683249592781, 0.0016272637294605374, 0.0022473474964499474, 0.004833994898945093, 0.0006179713527671993, 0.0011630926746875048, 0.00459293182939291, 0.002340057399123907, 0.0032964791171252728, 0.01786680333316326, 0.004343309439718723, 0.0015934606781229377, 0.002273156773298979, 0.01300395280122757, 0.0016254220390692353, 0.0023195839021354914]",0.9142664074897766,Information Extraction,0.9142664074897766,True
"Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types' complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does. 1",Machine Learning for NLP,"[0.0017674688715487719, 0.0026267224457114935, 0.0018447190523147583, 0.0019719251431524754, 0.002385940868407488, 0.002815904561430216, 0.856817364692688, 0.02658679150044918, 0.005595730151981115, 0.0035070425365120173, 0.019151555374264717, 0.0015358614036813378, 0.002190767787396908, 0.005480083171278238, 0.0047063580714166164, 0.0044810413382947445, 0.01255886722356081, 0.004805195610970259, 0.0018340134993195534, 0.0034049563109874725, 0.02680877037346363, 0.0036582681350409985, 0.0034647532738745213]",0.856817364692688,Information Extraction,0.019151555374264717,False
"While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences. One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences. To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task.",Generation,"[0.004209131933748722, 0.004337717313319445, 0.014247131533920765, 0.007363504264503717, 0.005360622890293598, 0.6250776648521423, 0.030064813792705536, 0.04031811282038689, 0.006895344238728285, 0.006456366274505854, 0.035037025809288025, 0.003400214249268174, 0.003998585976660252, 0.028825312852859497, 0.05259071663022041, 0.005599006544798613, 0.05472475290298462, 0.031291745603084564, 0.004972826223820448, 0.015091076493263245, 0.01013815775513649, 0.005163775756955147, 0.004836323671042919]",0.6250776648521423,Generation,0.6250776648521423,True
"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to outof-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.",Machine Learning for NLP,"[0.0071122730150818825, 0.002995608374476433, 0.0027094120159745216, 0.0016981493681669235, 0.005927092395722866, 0.028029881417751312, 0.002962888916954398, 0.03262871131300926, 0.0889769196510315, 0.0012749494053423405, 0.7416582107543945, 0.011143901385366917, 0.0030633441638201475, 0.006811279337853193, 0.01597023382782936, 0.0045445566065609455, 0.020186293870210648, 0.0034208898432552814, 0.002358978148549795, 0.0027857653331011534, 0.009650268591940403, 0.0027354920748621225, 0.001354989013634622]",0.7416582107543945,Machine Learning for NLP,0.7416582107543945,True
"Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search -the de facto standard inference algorithm in NMT -and Eikema and Aziz (2020)   propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift. 1",Machine Translation and Multilinguality,"[0.0011367496335878968, 0.0010918756015598774, 0.0012968983501195908, 0.0006166364764794707, 0.0034734862856566906, 0.001866607810370624, 0.0008027260773815215, 0.001747912378050387, 0.005204560700803995, 0.0012820109259337187, 0.004833174403756857, 0.9527669548988342, 0.0024138852022588253, 0.001063535688444972, 0.006689741741865873, 0.0012559699825942516, 0.000928053050301969, 0.0018910032231360674, 0.0030090189538896084, 0.0015361319528892636, 0.0018921667942777276, 0.0020421412773430347, 0.001158745028078556]",0.9527669548988342,Machine Translation and Multilinguality,0.9527669548988342,True
"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.",Summarization,"[0.0013291083741933107, 0.002243742113932967, 0.0019275136291980743, 0.006306602619588375, 0.0028743722941726446, 0.008456984534859657, 0.006345270201563835, 0.009769473224878311, 0.003523854538798332, 0.002228847471997142, 0.0024486221373081207, 0.0026981867849826813, 0.0028278306126594543, 0.0009398674592375755, 0.015812261030077934, 0.0011044233106076717, 0.0021164005156606436, 0.007172029931098223, 0.004858155734837055, 0.904075562953949, 0.003585553029552102, 0.003908359445631504, 0.003446907037869096]",0.904075562953949,Summarization,0.904075562953949,True
"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available. 1","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.0039296881295740604, 0.003262182930484414, 0.002894398057833314, 0.0010756788542494178, 0.008977524936199188, 0.005295798182487488, 0.0019929278641939163, 0.00467293243855238, 0.043500397354364395, 0.00201887427829206, 0.053584251552820206, 0.8037779927253723, 0.008502299897372723, 0.0018549576634541154, 0.019411446526646614, 0.003145799972116947, 0.004975672345608473, 0.004981120582669973, 0.004192712251096964, 0.002002744935452938, 0.010080215521156788, 0.0040671490132808685, 0.0018033086089417338]",0.8037779927253723,Machine Translation and Multilinguality,0.004975672345608473,False
"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for nonfactoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms stateof-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.",Summarization,"[0.0014496465446427464, 0.0017375587485730648, 0.004015372134745121, 0.012597756460309029, 0.0022774168755859137, 0.015664607286453247, 0.005954383406788111, 0.009051896631717682, 0.003748031798750162, 0.00292702903971076, 0.004542408045381308, 0.0024295898620039225, 0.0025068747345358133, 0.0036507679615169764, 0.011900994926691055, 0.0011182371526956558, 0.004532397259026766, 0.004903793800622225, 0.005496523808687925, 0.8895079493522644, 0.0034466979559510946, 0.0029949445743113756, 0.003545123618096113]",0.8895079493522644,Summarization,0.8895079493522644,True
"Recent work on entity coreference resolution (CR) follows current trends in Deep Learning applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention.",Discourse and Pragmatics,"[0.007409983780235052, 0.028292691335082054, 0.020141610875725746, 0.4744194447994232, 0.013496896252036095, 0.006771471351385117, 0.10800576955080032, 0.053664155304431915, 0.006903789006173611, 0.008394449017941952, 0.017308296635746956, 0.002460665302351117, 0.008238837122917175, 0.038374122232198715, 0.038296569138765335, 0.029592378064990044, 0.045957230031490326, 0.039517227560281754, 0.006106887944042683, 0.012064271606504917, 0.016460761427879333, 0.006302442401647568, 0.011819956824183464]",0.4744194447994232,Discourse and Pragmatics,0.4744194447994232,True
"While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how sourcetarget domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with selftraining and by increasing the amount of target side monolingual data.",Machine Translation and Multilinguality,"[0.002442351309582591, 0.0031628792639821768, 0.0013452956918627024, 0.0007387172663584352, 0.0071599786169826984, 0.0018950870726257563, 0.0010596499778330326, 0.0024554647970944643, 0.009667917154729366, 0.0018072581151500344, 0.004674733150750399, 0.922915518283844, 0.004637788515537977, 0.0009086060454137623, 0.014605686068534851, 0.0021782321855425835, 0.0010297260014340281, 0.003830976551398635, 0.004385934676975012, 0.00198251917026937, 0.0025022891350090504, 0.0032145276200026274, 0.0013988008722662926]",0.922915518283844,Machine Translation and Multilinguality,0.922915518283844,True
"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever (Min et al., 2019) . 1",Information Extraction,"[0.003270813263952732, 0.0023265162017196417, 0.005585864651948214, 0.005821720231324434, 0.0025620090309530497, 0.005073218140751123, 0.03179757669568062, 0.016435319557785988, 0.005537054035812616, 0.004789839498698711, 0.025256777182221413, 0.0011010034941136837, 0.001971482764929533, 0.8416405916213989, 0.006524950265884399, 0.0027448805049061775, 0.01776966266334057, 0.003069676226004958, 0.0019292085198685527, 0.001087247976101935, 0.006694004405289888, 0.0017468909500166774, 0.00526370108127594]",0.8416405916213989,Question Answering,0.03179757669568062,False
"BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA). BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction. In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. Span Selection Pre-Training (SSPT) poses cloze-like training instances, but rather than draw the answer from the model's parameters, it is selected from a relevant passage. We find significant and consistent improvements over both BERT BASE and BERT LARGE on multiple Machine Reading Comprehension (MRC) datasets. Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT LARGE by 3 F1 points on short answer prediction. We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system. Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.",Machine Learning for NLP,"[0.010367721319198608, 0.0049607898108661175, 0.01280873455107212, 0.010603908449411392, 0.004982535727322102, 0.025786567479372025, 0.008699560537934303, 0.026341207325458527, 0.05486537516117096, 0.007789694704115391, 0.209214448928833, 0.003944204654544592, 0.00474705221131444, 0.40607157349586487, 0.042670365422964096, 0.008042851462960243, 0.11775678396224976, 0.004643880296498537, 0.005139674060046673, 0.002147969789803028, 0.02097656950354576, 0.003465439425781369, 0.003973087295889854]",0.40607157349586487,Question Answering,0.209214448928833,False
"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., latestage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of SSIL in the language-drift translation game.",Dialogue and Interactive Systems,"[0.0030603192280977964, 0.004169601947069168, 0.8799412846565247, 0.007997783832252026, 0.003310561180114746, 0.007191239856183529, 0.0009622318320907652, 0.0017376589821651578, 0.004281623754650354, 0.011634103953838348, 0.020800497382879257, 0.0030429174657911062, 0.0030201890040189028, 0.005793903488665819, 0.008889151737093925, 0.001244362792931497, 0.004136313684284687, 0.005556111689656973, 0.015245189890265465, 0.001969157950952649, 0.002399215940386057, 0.0016808857908472419, 0.0019356888951733708]",0.8799412846565247,Dialogue and Interactive Systems,0.8799412846565247,True
"We address the problem of extractive question answering using document-level distant supervision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact, and that different configurations provide complementary benefits. We demonstrate that a multiobjective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries. 1",Question Answering,"[0.010810730047523975, 0.007105574011802673, 0.009577197954058647, 0.030044587329030037, 0.009762429632246494, 0.03895607590675354, 0.032355327159166336, 0.07367148250341415, 0.02482384815812111, 0.012572214007377625, 0.060815051198005676, 0.010213853791356087, 0.013089899905025959, 0.478689044713974, 0.0590115524828434, 0.0035910699516534805, 0.014896969310939312, 0.015419145114719868, 0.01235187891870737, 0.04858935624361038, 0.01438229437917471, 0.006439824588596821, 0.012830717489123344]",0.478689044713974,Question Answering,0.478689044713974,True
"Text-to-SQL is the problem of converting a user question into an SQL query, when the question and database are given. In this article, we present a neural network approach called RYANSQL (Recursively Yielding Annotation Network for SQL) to solve complex Text-to-SQL tasks for cross-domain databases. Statement Position Code (SPC) is defined to transform a nested SQL query into a set of non-nested SELECT statements; a sketch-based slot-filling approach is proposed to synthesize each SELECT statement for its corresponding SPC. Additionally, two input manipulation methods are presented to improve generation performance further. RYANSQL achieved competitive result of 58.2% accuracy on the challenging Spider benchmark. At the time of submission (April 2020), RYANSQL v2, a variant of original RYANSQL, is positioned at 3rd place among all systems and 1st place among the systems not using database content Submission",Generation,"[0.003139934968203306, 0.0029034626204520464, 0.022645769640803337, 0.007003767415881157, 0.0015154157299548388, 0.01703670248389244, 0.007277086842805147, 0.007660300005227327, 0.002801263704895973, 0.0023739896714687347, 0.012846071273088455, 0.0013465872034430504, 0.0021374293137341738, 0.022925470024347305, 0.011265469714999199, 0.005068197380751371, 0.8399453163146973, 0.002338595688343048, 0.003043327946215868, 0.0024209243711084127, 0.017237253487110138, 0.0018339513335376978, 0.0032336667645722628]",0.8399453163146973,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.01703670248389244,False
"Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. This impedes the learning of those data-driven neural dialogue models. Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples. In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously. In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data. Note that, the proposed data manipulation framework is fully data-driven and learnable. It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples. Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.",Dialogue and Interactive Systems,"[0.0007631822372786701, 0.0012481113662943244, 0.9549245238304138, 0.004161519929766655, 0.0009183142683468759, 0.0031256170477718115, 0.0010207929881289601, 0.0011716163717210293, 0.0008348203846253455, 0.0012920884182676673, 0.005575296934694052, 0.0021709296852350235, 0.0014572994550690055, 0.0024170568212866783, 0.0034807762131094933, 0.000681328063365072, 0.001815148047171533, 0.003169033210724592, 0.0049774302169680595, 0.001293147448450327, 0.001381855458021164, 0.0009749786695465446, 0.0011450201272964478]",0.9549245238304138,Dialogue and Interactive Systems,0.9549245238304138,True
"Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.",Ethics and NLP,"[0.02739756554365158, 0.08342824876308441, 0.006429152563214302, 0.017817160114645958, 0.09320099651813507, 0.013871314004063606, 0.010570705868303776, 0.010265921242535114, 0.10196434706449509, 0.008165160194039345, 0.055168479681015015, 0.008587492629885674, 0.02309432625770569, 0.009662091732025146, 0.33271199464797974, 0.021055907011032104, 0.07160789519548416, 0.04960198700428009, 0.010221172124147415, 0.005253245122730732, 0.02417886070907116, 0.007576339412480593, 0.008169632405042648]",0.33271199464797974,Resources and Evaluation,0.09320099651813507,False
"The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. Experimental results on the WMT19 Englishâ†”German Metrics shared tasks show that our proposed method outperforms commonly-used MT metrics in terms of human correlation. In particular, our proposed method performs well even when all the MT systems are very competitive, which is when most existing metrics fail to distinguish between them. The source code is freely available at https://github.com/NLP2CT /Difficulty-Aware-MT-Evaluation.",Machine Translation and Multilinguality,"[0.002820270834490657, 0.004837905988097191, 0.0034417472779750824, 0.001674356171861291, 0.009799245744943619, 0.004827575292438269, 0.0015746515709906816, 0.004886610433459282, 0.010468973778188229, 0.0034426646307110786, 0.007998204790055752, 0.8494110703468323, 0.00600765086710453, 0.0026329727843403816, 0.04997648671269417, 0.0028395673725754023, 0.0032867230474948883, 0.006898270919919014, 0.007857078686356544, 0.0037123486399650574, 0.004385039675980806, 0.004572917241603136, 0.002647742396220565]",0.8494110703468323,Machine Translation and Multilinguality,0.8494110703468323,True
"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.",Summarization,"[0.0010798806324601173, 0.0014961169799789786, 0.0019139935029670596, 0.00577569892629981, 0.002128125401213765, 0.008095093071460724, 0.007108846679329872, 0.009174595586955547, 0.001926527009345591, 0.0019940752536058426, 0.0025348630733788013, 0.002334842225536704, 0.0024663270451128483, 0.0008020672248676419, 0.008467254228889942, 0.0011570416390895844, 0.0019142754608765244, 0.004311711061745882, 0.0041711716912686825, 0.9216044545173645, 0.0034490164835006, 0.0030383507255464792, 0.0030556651763617992]",0.9216044545173645,Summarization,0.9216044545173645,True
"Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework -NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final finetuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEE-DLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDRchem 93.74, BC5CDR-disease 90.69, NCBIdisease 92.28.",Information Extraction,"[0.003921279683709145, 0.0059006232768297195, 0.007095037493854761, 0.003069549333304167, 0.005993801634758711, 0.009800507687032223, 0.10915742814540863, 0.05838969349861145, 0.013787075877189636, 0.002656415104866028, 0.66526198387146, 0.0038372771814465523, 0.0027245711535215378, 0.015559914521872997, 0.01027015782892704, 0.006594355218112469, 0.035837769508361816, 0.006030545569956303, 0.0028437001165002584, 0.003113495884463191, 0.022785112261772156, 0.0021958083380013704, 0.003173823468387127]",0.66526198387146,Machine Learning for NLP,0.10915742814540863,False
"The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores.",Machine Translation and Multilinguality,"[0.0007725813775323331, 0.0006803914438933134, 0.0012395908124744892, 0.00032035497133620083, 0.002224031835794449, 0.0020582019351422787, 0.000766927725635469, 0.0015237336046993732, 0.002534438855946064, 0.0007219774997793138, 0.0034957779571413994, 0.9671867489814758, 0.0014239430893212557, 0.00045231738477014005, 0.0024579642340540886, 0.0009868963388726115, 0.0010952195152640343, 0.0012369845062494278, 0.0033256590832024813, 0.0017029207665473223, 0.0014596125110983849, 0.0015793105121701956, 0.0007543567335233092]",0.9671867489814758,Machine Translation and Multilinguality,0.9671867489814758,True
"Natural language processing often faces the problem of data diversity such as different domains, themes, styles and so on. Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples. To solve this problem, we firstly propose an autoencoding topic model with mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describe the data diversity. Having obtained the clustering assignment for each sample, we develop the ensemble LM (En-sLM) with the technique of weight modulation. Specifically, EnsLM contains a backbone which is adjusted by a few modulated weights to fit for different sample clusters. As a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features. EnsLM can be trained jointly with mATM with flexible LM backbone. We evaluate the effectiveness of both mATM and EnsLM on different language understanding and generative tasks.",Machine Learning for NLP,"[0.006429645698517561, 0.002924492582678795, 0.009582610800862312, 0.004094325937330723, 0.005432464182376862, 0.029019197449088097, 0.005948455072939396, 0.03245490789413452, 0.050483137369155884, 0.0029143746942281723, 0.6866171360015869, 0.0037634402979165316, 0.0036819514352828264, 0.005418333224952221, 0.013385530561208725, 0.008779479190707207, 0.10016647726297379, 0.004642721265554428, 0.003388036508113146, 0.0044624945148825645, 0.012035883963108063, 0.0025947068352252245, 0.0017801892245188355]",0.6866171360015869,Machine Learning for NLP,0.6866171360015869,True
"We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.","Language Grounding to Vision, Robotics and Beyond","[0.011795620433986187, 0.013445592485368252, 0.06024719402194023, 0.037616338580846786, 0.01163814589381218, 0.06791218370199203, 0.009495671838521957, 0.01693812571465969, 0.010273491032421589, 0.4146295189857483, 0.024764344096183777, 0.004887829534709454, 0.012333221733570099, 0.02096068300306797, 0.08473360538482666, 0.006812788546085358, 0.036746472120285034, 0.011973001062870026, 0.06861740350723267, 0.040092211216688156, 0.01582876779139042, 0.010866095311939716, 0.007391668390482664]",0.4146295189857483,"Language Grounding to Vision, Robotics and Beyond",0.4146295189857483,True
"In structured prediction problems, crosslingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.","Syntax: Tagging, Chunking and Parsing","[0.004012397490441799, 0.0023767375387251377, 0.0045483591966331005, 0.0014701569452881813, 0.005058072041720152, 0.009501461870968342, 0.012057763524353504, 0.023170417174696922, 0.06866324692964554, 0.00292745023034513, 0.6845526695251465, 0.10997159779071808, 0.0026894810143858194, 0.0061172302812337875, 0.007326224353164434, 0.003246849635615945, 0.017488591372966766, 0.003608192317187786, 0.005228713154792786, 0.003408689284697175, 0.01632511615753174, 0.004238661844283342, 0.002011977368965745]",0.6845526695251465,Machine Learning for NLP,0.01632511615753174,False
"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios. In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules. An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios. It can also utilize labeled data for training to achieve improved prediction accuracy. After training, an FA-RNN often remains interpretable and can be converted back into regular expressions. We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zeroshot and low-resource settings and remain very competitive in rich-resource settings.",Interpretability and Analysis of Models for NLP,"[0.00422760471701622, 0.0025691469199955463, 0.003401580499485135, 0.0020099675748497248, 0.003658745903521776, 0.009151755832135677, 0.01215391606092453, 0.09526927024126053, 0.08083610981702805, 0.0022455325815826654, 0.6958867311477661, 0.005373896565288305, 0.0022195016499608755, 0.00991299469023943, 0.007788575254380703, 0.004999830387532711, 0.028412705287337303, 0.0037368584889918566, 0.0032309959642589092, 0.0031622271053493023, 0.01468964759260416, 0.00315462751314044, 0.0019077162723988295]",0.6958867311477661,Machine Learning for NLP,0.08083610981702805,False
"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the traintest discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.",Dialogue and Interactive Systems,"[0.0023765238001942635, 0.005117237102240324, 0.8422082662582397, 0.028937935829162598, 0.003457134123891592, 0.006723508704453707, 0.004277922213077545, 0.006766068283468485, 0.001977781765162945, 0.00365176098421216, 0.013174787163734436, 0.0019452909473329782, 0.002893463708460331, 0.019556285813450813, 0.01476327981799841, 0.002362753264605999, 0.006598641164600849, 0.01449371688067913, 0.007068228907883167, 0.003282299730926752, 0.002920767990872264, 0.00227312627248466, 0.003173353848978877]",0.8422082662582397,Dialogue and Interactive Systems,0.8422082662582397,True
"With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for imagetext sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multichannel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.005198686849325895, 0.04343004897236824, 0.0351630300283432, 0.014526784420013428, 0.017481233924627304, 0.006174085196107626, 0.014451081864535809, 0.014081141911447048, 0.022332940250635147, 0.36978983879089355, 0.006347325164824724, 0.006611822638660669, 0.005907900631427765, 0.006715053226798773, 0.07612797617912292, 0.005960025824606419, 0.009239180013537407, 0.06720654666423798, 0.22867827117443085, 0.01760653592646122, 0.00567571772262454, 0.012699363753199577, 0.008595355786383152]",0.36978983879089355,"Language Grounding to Vision, Robotics and Beyond",0.06720654666423798,False
"Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model's limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the finetuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT's accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.","Semantics: Sentence-level Semantics, Textual Inference and Other areas","[0.010097267106175423, 0.003345089964568615, 0.009391755796968937, 0.004823098424822092, 0.0031402374152094126, 0.022333461791276932, 0.005025973077863455, 0.00506994966417551, 0.03822678327560425, 0.004566330928355455, 0.04482221603393555, 0.002878061030060053, 0.004391169175505638, 0.021083848550915718, 0.020381279289722443, 0.00796362105756998, 0.7309028506278992, 0.003572924295440316, 0.0038849730044603348, 0.001757448655553162, 0.04613839089870453, 0.002916323486715555, 0.0032869321294128895]",0.7309028506278992,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.7309028506278992,True
"We present SICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of Marelli et al. (2014) from English into Dutch. Having a parallel inference dataset allows us to compare both monolingual and multilingual NLP models for English and Dutch on the two tasks. In the paper, we motivate and detail the translation process, perform a baseline evaluation on both the original SICK dataset and its Dutch incarnation SICK-NL, taking inspiration from Dutch skipgram embeddings and contextualised embedding models. In addition, we encapsulate two phenomena encountered in the translation to formulate stress tests and verify how well the Dutch models capture syntactic restructurings that do not affect semantics. Our main finding is all models perform worse on SICK-NL than on SICK, indicating that the Dutch dataset is more challenging than the English original. Results on the stress tests show that models don't fully capture word order freedom in Dutch, warranting future systematic studies.",Resources and Evaluation,"[0.008049420081079006, 0.008133143186569214, 0.0067741782404482365, 0.007856602780520916, 0.006325652357190847, 0.01651637628674507, 0.007286598905920982, 0.0058720409870147705, 0.029606442898511887, 0.0024845453444868326, 0.05782487243413925, 0.03819698840379715, 0.010486136190593243, 0.012151309289038181, 0.05771684646606445, 0.01720546744763851, 0.6328730583190918, 0.00596828805282712, 0.0052879126742482185, 0.002958235563710332, 0.04886343702673912, 0.005407507065683603, 0.006154932081699371]",0.6328730583190918,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.05771684646606445,False
"Existing language model compression methods mostly use a simple L 2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CODIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pretraining and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods. 1  Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV.",Machine Learning for NLP,"[0.003478755010291934, 0.001831978908739984, 0.0032691427040845156, 0.0015466869808733463, 0.003923231270164251, 0.011846421286463737, 0.004884694237262011, 0.02142457105219364, 0.0734243243932724, 0.0017635911935940385, 0.815747082233429, 0.005548428278416395, 0.002277119318023324, 0.003222846891731024, 0.005005729850381613, 0.0032620965503156185, 0.01786315068602562, 0.0033226185478270054, 0.0020931323524564505, 0.0029062621761113405, 0.008305496536195278, 0.0019407813670113683, 0.0011117008980363607]",0.815747082233429,Machine Learning for NLP,0.815747082233429,True
"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phraseto-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision. We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations. By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios. Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weaklysupervised methods. With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%. We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results. 1","Language Grounding to Vision, Robotics and Beyond","[0.003908494487404823, 0.00407843291759491, 0.02085394412279129, 0.005572603549808264, 0.007459460757672787, 0.005966259632259607, 0.009361633099615574, 0.007648933213204145, 0.011202624067664146, 0.7542998194694519, 0.008881404995918274, 0.0042482041753828526, 0.0038871909491717815, 0.007763220928609371, 0.030096611008048058, 0.004678612109273672, 0.010656872764229774, 0.005845471750944853, 0.07233144342899323, 0.0071849701926112175, 0.0048441276885569096, 0.005354474298655987, 0.003875200403854251]",0.7542998194694519,"Language Grounding to Vision, Robotics and Beyond",0.7542998194694519,True
"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it reranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.",Machine Learning for NLP,"[0.009847474284470081, 0.0054390751756727695, 0.04020678624510765, 0.005738819949328899, 0.010671486146748066, 0.05146418884396553, 0.01392371580004692, 0.042242344468832016, 0.1323973536491394, 0.02995465323328972, 0.3168146014213562, 0.034636855125427246, 0.008356841281056404, 0.005523371510207653, 0.013109667226672173, 0.007027940358966589, 0.059899408370256424, 0.005895010195672512, 0.1632615178823471, 0.013717434369027615, 0.015359105542302132, 0.010489098727703094, 0.004023123532533646]",0.3168146014213562,Machine Learning for NLP,0.3168146014213562,True
"In this digital age, online users expect personalized content. To cater to diverse group of audiences across online platforms it is necessary to generate multiple variants of same content with differing degree of characteristics (sentiment, style, formality, etc.). Though text-style transfer is a well explored related area, it focuses on flipping the style attribute polarity instead of regulating a fine-grained attribute transfer. In this paper we propose a hierarchical architecture for finer control over the attribute, preserving content using attribute disentanglement. We demonstrate the effectiveness of the generative process for two different attributes with varied complexity, namely sentiment and formality. With extensive experiments and human evaluation on five real-world datasets, we show that the framework can generate natural looking sentences with finer degree of control of intensity of a given attribute.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.01422523520886898, 0.022407812997698784, 0.018865859135985374, 0.009272382594645023, 0.010908576659858227, 0.5323498845100403, 0.0038233648519963026, 0.010224579833447933, 0.01445486955344677, 0.01011557225137949, 0.036795660853385925, 0.0071279024705290794, 0.016518712043762207, 0.0034525692462921143, 0.13432903587818146, 0.008737216703593731, 0.026496797800064087, 0.03821937367320061, 0.018702838569879532, 0.04218171536922455, 0.007250252645462751, 0.009019075892865658, 0.004520571790635586]",0.5323498845100403,Generation,0.03821937367320061,False
"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F 1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018)  on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Fewshot parsing can be further improved by a simple data augmentation method and selftraining. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples. 1","Syntax: Tagging, Chunking and Parsing","[0.009480506181716919, 0.008996977470815182, 0.023110026493668556, 0.01284011360257864, 0.00782358180731535, 0.023601390421390533, 0.01416474487632513, 0.013332907110452652, 0.023719526827335358, 0.012828926555812359, 0.03164376690983772, 0.004860640503466129, 0.018949104472994804, 0.019011331722140312, 0.06797770410776138, 0.005108943674713373, 0.1029643565416336, 0.02008564956486225, 0.006992774084210396, 0.012629847973585129, 0.5464004278182983, 0.006747623439878225, 0.0067291902378201485]",0.5464004278182983,"Syntax: Tagging, Chunking and Parsing",0.5464004278182983,True
"We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Crosslingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.",Resources and Evaluation,"[0.003758172271773219, 0.005934163462370634, 0.0038871197029948235, 0.0016546495025977492, 0.009413565509021282, 0.003952613100409508, 0.001835000584833324, 0.00637479405850172, 0.017390510067343712, 0.004718137439340353, 0.010475474409759045, 0.8455706834793091, 0.005014787893742323, 0.0024360446259379387, 0.03762710466980934, 0.004152192268520594, 0.005789775867015123, 0.005763709545135498, 0.007979708723723888, 0.0031716444063931704, 0.005175748374313116, 0.005288345739245415, 0.00263610715046525]",0.8455706834793091,Machine Translation and Multilinguality,0.03762710466980934,False
"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as fewshot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset). A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.",Machine Translation and Multilinguality,"[0.005047248676419258, 0.004135855473577976, 0.0053613511845469475, 0.001842001685872674, 0.008473854511976242, 0.007979798130691051, 0.008657797239720821, 0.016308503225445747, 0.06860939413309097, 0.002693267771974206, 0.6955326199531555, 0.06828168779611588, 0.0024711869191378355, 0.007812539115548134, 0.014114351943135262, 0.004765843506902456, 0.04706653207540512, 0.0041223387233912945, 0.0045552062802016735, 0.0018803267739713192, 0.014322992414236069, 0.0038367556408047676, 0.002128442283719778]",0.6955326199531555,Machine Learning for NLP,0.06828168779611588,False
"Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction. Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities. A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations. In this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue. In each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names. Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly.",Information Retrieval and Text Mining,"[0.002175508067011833, 0.003878528717905283, 0.0036879137624055147, 0.003953868057578802, 0.004565541632473469, 0.015005460008978844, 0.6899504065513611, 0.09132099896669388, 0.004652042407542467, 0.00333578372374177, 0.04480016604065895, 0.0030218791216611862, 0.003686721669510007, 0.007254716474562883, 0.009350022301077843, 0.009863187558948994, 0.03253892809152603, 0.008864407427608967, 0.0025876988656818867, 0.008592198602855206, 0.03742671757936478, 0.00491558900102973, 0.004571756813675165]",0.6899504065513611,Information Extraction,0.09132099896669388,False
"This paper studies the task of comparative preference classification (CPC). Given two entities in a sentence, our goal is to classify whether the first (or the second) entity is preferred over the other or no comparison is expressed at all between the two entities. Existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities. Some also use traditional machine learning approaches that do not generalize well. This paper proposes a novel Entityaware Dependency-based Deep Graph Attention Network (ED-GAT) that employs a multihop graph attention over a dependency graph sentence representation to leverage both the semantic information from word embeddings and the syntactic information from the dependency graph to solve the problem. Empirical evaluation shows that the proposed model achieves the state-of-the-art performance in comparative preference classification.","Sentiment Analysis, Stylistic Analysis, and Argument Mining","[0.0017202513990923762, 0.004802923649549484, 0.003027103841304779, 0.0051050991751253605, 0.0030221252236515284, 0.005088546313345432, 0.7928218841552734, 0.02527659572660923, 0.0030930067878216505, 0.003114646067842841, 0.01302692573517561, 0.001838067197240889, 0.0034662883263081312, 0.004564141388982534, 0.008353456854820251, 0.0085642971098423, 0.03732180967926979, 0.011567453853785992, 0.003475333098322153, 0.005283475387841463, 0.04750164970755577, 0.004276955034583807, 0.003687994321808219]",0.7928218841552734,Information Extraction,0.011567453853785992,False
"Vision-language pre-training (VLP) on largescale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic crossmodal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pretraining with a unified Transformer encoderdecoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established visionlanguage downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.","Language Grounding to Vision, Robotics and Beyond","[0.0027477226685732603, 0.002969532273709774, 0.027804700657725334, 0.0034437980502843857, 0.006176315248012543, 0.021463710814714432, 0.011505591683089733, 0.00729390000924468, 0.014539103023707867, 0.7348611950874329, 0.009688762947916985, 0.008217682130634785, 0.0032071759924292564, 0.009232944808900356, 0.018207695335149765, 0.003839460900053382, 0.008674252778291702, 0.007934234105050564, 0.07565338164567947, 0.006382895167917013, 0.005940769333392382, 0.00654689222574234, 0.0036682207137346268]",0.7348611950874329,"Language Grounding to Vision, Robotics and Beyond",0.7348611950874329,True
"Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text. Recently, several ED datasets in various domains have been proposed. However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models. To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED. To prevent the noises inevitable in automatically generated data from hampering training process, we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data. The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher. Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks. We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED.",Information Extraction,"[0.0010069553973153234, 0.002855706261470914, 0.0013226051814854145, 0.0021825600415468216, 0.001990123186260462, 0.0030360142700374126, 0.8739215731620789, 0.024422626942396164, 0.002027029637247324, 0.00265299784950912, 0.0073674749583005905, 0.000972933426965028, 0.0013801847817376256, 0.0044729565270245075, 0.003974566236138344, 0.00523833092302084, 0.028163524344563484, 0.0060327616520226, 0.002087499713525176, 0.0033482022117823362, 0.015985220670700073, 0.0024677678011357784, 0.0030904237646609545]",0.8739215731620789,Information Extraction,0.8739215731620789,True
