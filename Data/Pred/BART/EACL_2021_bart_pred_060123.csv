Labels,Paper Name,abstract,Full Prediction,Max Score,Max Label,Actual Label Score
Computational Social Science and Social Media,"Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions","Computational modelling of political discourse tasks has become an increasingly important area of research in natural language processing. Populist rhetoric has risen across the political sphere in recent years; however, computational approaches to it have been scarce due to its complex nature. In this paper, we present the new Us vs. Them dataset, consisting of 6861 Reddit comments annotated for populist attitudes and the first large-scale computational models of this phenomenon. We investigate the relationship between populist mindsets and social groups, as well as a range of emotions typically associated with these. We set a baseline for two tasks related to populist attitudes and present a set of multi-task learning models that leverage and demonstrate the importance of emotion and group identification as auxiliary tasks.","{'sequence': 'Computational modelling of political discourse tasks has become an increasingly important area of research in natural language processing. Populist rhetoric has risen across the political sphere in recent years; however, computational approaches to it have been scarce due to its complex nature. In this paper, we present the new Us vs. Them dataset, consisting of 6861 Reddit comments annotated for populist attitudes and the first large-scale computational models of this phenomenon. We investigate the relationship between populist mindsets and social groups, as well as a range of emotions typically associated with these. We set a baseline for two tasks related to populist attitudes and present a set of multi-task learning models that leverage and demonstrate the importance of emotion and group identification as auxiliary tasks.', 'labels': ['Computational Social Science and Social Media', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Semantics: Lexical Semantics', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Information Extraction', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10749633610248566, 0.08495649695396423, 0.07960036396980286, 0.07492668926715851, 0.0743669867515564, 0.05373750999569893, 0.05148068070411682, 0.04339122027158737, 0.04081001877784729, 0.03921904414892197, 0.038106728345155716, 0.037969134747982025, 0.028754640370607376, 0.028353910893201828, 0.027121124789118767, 0.026292743161320686, 0.02620675228536129, 0.026108698919415474, 0.025850027799606323, 0.02515336684882641, 0.024250561371445656, 0.019429869949817657, 0.01641702651977539]}",0.10749633610248566,Computational Social Science and Social Media,0.10749633610248566
Computational Social Science and Social Media,Gender and Racial Fairness in Depression Research using Social Media,"Multiple studies have demonstrated that behavior on internet-based social media platforms can be indicative of an individual's mental health status. The widespread availability of such data has spurred interest in mental health research from a computational lens. While previous research has raised concerns about possible biases in models produced from this data, no study has quantified how these biases actually manifest themselves with respect to different demographic groups, such as gender and racial/ethnic groups. Here, we analyze the fairness of depression classifiers trained on Twitter data with respect to gender and racial demographic groups. We find that model performance systematically differs for underrepresented groups and that these discrepancies cannot be fully explained by trivial data representation issues. Our study concludes with recommendations on how to avoid these biases in future research.","{'sequence': ""Multiple studies have demonstrated that behavior on internet-based social media platforms can be indicative of an individual's mental health status. The widespread availability of such data has spurred interest in mental health research from a computational lens. While previous research has raised concerns about possible biases in models produced from this data, no study has quantified how these biases actually manifest themselves with respect to different demographic groups, such as gender and racial/ethnic groups. Here, we analyze the fairness of depression classifiers trained on Twitter data with respect to gender and racial demographic groups. We find that model performance systematically differs for underrepresented groups and that these discrepancies cannot be fully explained by trivial data representation issues. Our study concludes with recommendations on how to avoid these biases in future research."", 'labels': ['Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Discourse and Pragmatics', 'Information Extraction', 'Speech and Multimodality', 'Summarization', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP'], 'scores': [0.10576242953538895, 0.07988319545984268, 0.0656333789229393, 0.06239502504467964, 0.06052454933524132, 0.059657882899045944, 0.05637914687395096, 0.05036664381623268, 0.043261975049972534, 0.04197325557470322, 0.041376855224370956, 0.03985672816634178, 0.03935291990637779, 0.035604238510131836, 0.03540193289518356, 0.03178784251213074, 0.026034384965896606, 0.02593405917286873, 0.024362051859498024, 0.02353948913514614, 0.020451419055461884, 0.016328345984220505, 0.014132223092019558]}",0.10576242953538895,Computational Social Science and Social Media,0.10576242953538895
Computational Social Science and Social Media,I Beg to Differ: A study of constructive disagreement in online conversations,"Disagreements are pervasive in human communication. In this paper we investigate what makes disagreement constructive. To this end, we construct WikiDisputes, a corpus of 7 425 Wikipedia Talk page conversations that contain content disputes, and define the task of predicting whether disagreements will be escalated to mediation by a moderator. We evaluate feature-based models with linguistic markers from previous work, and demonstrate that their performance is improved by using features that capture changes in linguistic markers throughout the conversations, as opposed to averaged values. We develop a variety of neural models and show that taking into account the structure of the conversation improves predictive accuracy, exceeding that of feature-based models. We assess our best neural model in terms of both predictive accuracy and uncertainty by evaluating its behaviour when it is only exposed to the beginning of the conversation, finding that model accuracy improves and uncertainty reduces as models are exposed to more information.","{'sequence': 'Disagreements are pervasive in human communication. In this paper we investigate what makes disagreement constructive. To this end, we construct WikiDisputes, a corpus of 7 425 Wikipedia Talk page conversations that contain content disputes, and define the task of predicting whether disagreements will be escalated to mediation by a moderator. We evaluate feature-based models with linguistic markers from previous work, and demonstrate that their performance is improved by using features that capture changes in linguistic markers throughout the conversations, as opposed to averaged values. We develop a variety of neural models and show that taking into account the structure of the conversation improves predictive accuracy, exceeding that of feature-based models. We assess our best neural model in terms of both predictive accuracy and uncertainty by evaluating its behaviour when it is only exposed to the beginning of the conversation, finding that model accuracy improves and uncertainty reduces as models are exposed to more information.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Ethics and NLP', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Generation'], 'scores': [0.09301074594259262, 0.08880956470966339, 0.08564555644989014, 0.07893145829439163, 0.0655612125992775, 0.048038169741630554, 0.04729954153299332, 0.04500288888812065, 0.03949004039168358, 0.03876276686787605, 0.03810795024037361, 0.03651046007871628, 0.03477269038558006, 0.033436451107263565, 0.032658424228429794, 0.030895480886101723, 0.028017153963446617, 0.027431439608335495, 0.02471296489238739, 0.022119974717497826, 0.021800968796014786, 0.019970333203673363, 0.019013691693544388]}",0.09301074594259262,Resources and Evaluation,0.0655612125992775
Computational Social Science and Social Media,A Few Topical Tweets are Enough for Effective User Stance Detection,"User stance detection entails ascertaining the position of a user towards a target, such as an entity, topic, or claim. Recent work that employs unsupervised classification has shown that performing stance detection on vocal Twitter users, who have many tweets on a target, can be highly accurate (+98%). However, such methods perform poorly or fail completely for less vocal users, who may have authored only a few tweets about a target. In this paper, we tackle stance detection for such users using two approaches. In the first approach, we improve user-level stance detection by representing tweets using contextualized embeddings, which capture latent meanings of words in context. We show that this approach outperforms two strong baselines and achieves 89.6% accuracy and 91.3% macro F-measure on eight controversial topics. In the second approach, we expand the tweets of a given user using their Twitter timeline tweets, which may not be topically relevant, and then we perform unsupervised classification of the user, which entails clustering a user with other users in the training set. This approach achieves 95.6% accuracy and 93.1% macro F-measure.","{'sequence': 'User stance detection entails ascertaining the position of a user towards a target, such as an entity, topic, or claim. Recent work that employs unsupervised classification has shown that performing stance detection on vocal Twitter users, who have many tweets on a target, can be highly accurate (+98%). However, such methods perform poorly or fail completely for less vocal users, who may have authored only a few tweets about a target. In this paper, we tackle stance detection for such users using two approaches. In the first approach, we improve user-level stance detection by representing tweets using contextualized embeddings, which capture latent meanings of words in context. We show that this approach outperforms two strong baselines and achieves 89.6% accuracy and 91.3% macro F-measure on eight controversial topics. In the second approach, we expand the tweets of a given user using their Twitter timeline tweets, which may not be topically relevant, and then we perform unsupervised classification of the user, which entails clustering a user with other users in the training set. This approach achieves 95.6% accuracy and 93.1% macro F-measure.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Information Extraction', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Information Retrieval and Text Mining', 'Generation', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07980730384588242, 0.07364033162593842, 0.059737853705883026, 0.059630900621414185, 0.05649728327989578, 0.05592888593673706, 0.05529119819402695, 0.05163857340812683, 0.048861581832170486, 0.04583718255162239, 0.0411188080906868, 0.04023119807243347, 0.03825486823916435, 0.03795382007956505, 0.03731609880924225, 0.034832023084163666, 0.03451691195368767, 0.03228122368454933, 0.029658038169145584, 0.028619784861803055, 0.023145237937569618, 0.019558850675821304, 0.015642037615180016]}",0.07980730384588242,Speech and Multimodality,0.05529119819402695
Computational Social Science and Social Media,EmpathBERT: A BERT-based Framework for Demographic-aware Empathy Prediction,"Affect preferences vary with user demographics, and tapping into demographic information provides important cues about the users' language preferences. In this paper, we utilize the user demographics, and propose EMPATH-BERT, a demographic-aware framework for empathy prediction based on BERT. Through several comparative experiments, we show that EMPATHBERT surpasses traditional machine learning and deep learning models, and illustrate the importance of user demographics to predict empathy and distress in user responses to stimulative news articles. We also highlight the importance of affect information in the responses by developing affect-aware models to predict user demographic attributes.","{'sequence': ""Affect preferences vary with user demographics, and tapping into demographic information provides important cues about the users' language preferences. In this paper, we utilize the user demographics, and propose EMPATH-BERT, a demographic-aware framework for empathy prediction based on BERT. Through several comparative experiments, we show that EMPATHBERT surpasses traditional machine learning and deep learning models, and illustrate the importance of user demographics to predict empathy and distress in user responses to stimulative news articles. We also highlight the importance of affect information in the responses by developing affect-aware models to predict user demographic attributes."", 'labels': ['Computational Social Science and Social Media', 'Information Extraction', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Question Answering', 'Ethics and NLP', 'Generation', 'Summarization', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09582269191741943, 0.09238264709711075, 0.06473495811223984, 0.06129981577396393, 0.05984128639101982, 0.05511191114783287, 0.052802346646785736, 0.04922620579600334, 0.04406015947461128, 0.042914651334285736, 0.04177752882242203, 0.03908301144838333, 0.03887724131345749, 0.036779191344976425, 0.03525806963443756, 0.031855687499046326, 0.031752560287714005, 0.028676392510533333, 0.027452940121293068, 0.02332855574786663, 0.02201034687459469, 0.013273299671709538, 0.011678473092615604]}",0.09582269191741943,Computational Social Science and Social Media,0.09582269191741943
Computational Social Science and Social Media,Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection,"The widespread of fake news and misinformation in various domains ranging from politics, economics to public health has posed an urgent need to automatically fact-check information. A recent trend in fake news detection is to utilize evidence from external sources. However, existing evidence-aware fake news detection methods focused on either only word-level attention or evidence-level attention, which may result in suboptimal performance. In this paper, we propose a Hierarchical Multihead Attentive Network to fact-check textual claims. Our model jointly combines multi-head word-level attention and multihead document-level attention, which aid explanation in both word-level and evidencelevel. Experiments on two real-word datasets show that our model outperforms seven stateof-the-art baselines. Improvements over baselines are from 6% to 18%. Our source code and datasets are released at https:// github.com/nguyenvo09/EACL2021.","{'sequence': 'The widespread of fake news and misinformation in various domains ranging from politics, economics to public health has posed an urgent need to automatically fact-check information. A recent trend in fake news detection is to utilize evidence from external sources. However, existing evidence-aware fake news detection methods focused on either only word-level attention or evidence-level attention, which may result in suboptimal performance. In this paper, we propose a Hierarchical Multihead Attentive Network to fact-check textual claims. Our model jointly combines multi-head word-level attention and multihead document-level attention, which aid explanation in both word-level and evidencelevel. Experiments on two real-word datasets show that our model outperforms seven stateof-the-art baselines. Improvements over baselines are from 6% to 18%. Our source code and datasets are released at https:// github.com/nguyenvo09/EACL2021.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08488156646490097, 0.08188237994909286, 0.07943685352802277, 0.07344281673431396, 0.06929884850978851, 0.06719698756933212, 0.0550370067358017, 0.05367690324783325, 0.05321381986141205, 0.043897006660699844, 0.03520561382174492, 0.03418266773223877, 0.03189001977443695, 0.03173001483082771, 0.027354015037417412, 0.0262766070663929, 0.026033267378807068, 0.025434574112296104, 0.023449519649147987, 0.021709337830543518, 0.02128264494240284, 0.020888378843665123, 0.012599129229784012]}",0.08488156646490097,Information Extraction,0.06929884850978851
Computational Social Science and Social Media,Semantic Oppositeness Assisted Deep Contextual Modeling for Automatic Rumor Detection in Social Networks,"Social networks face a major challenge in the form of rumors and fake news, due to their intrinsic nature of connecting users to millions of others, and of giving any individual the power to post anything. Given the rapid, widespread dissemination of information in social networks, manually detecting suspicious news is sub-optimal. Thus, research on automatic rumor detection has become a necessity. Previous works in the domain have utilized the reply relations between posts, as well as the semantic similarity between the main post and its context, consisting of replies, in order to obtain state-of-the-art performance. In this work, we demonstrate that semantic oppositeness can improve the performance on the task of rumor detection. We show that semantic oppositeness captures elements of discord, which are not properly covered by previous efforts, which only utilize semantic similarity or reply structure. Our proposed model learns both explicit and implicit relations between the main tweet and its replies, by utilizing both semantic similarity and semantic oppositeness. Both of these employ the self-attention mechanism in neural text modeling, with semantic oppositeness utilizing word-level self-attention, and with semantic similarity utilizing post-level self-attention. We show, with extensive experiments on recent data sets for this problem, that our proposed model achieves state-of-theart performance. Further, we show that our model is more resistant to the variances in performance introduced by randomness.","{'sequence': 'Social networks face a major challenge in the form of rumors and fake news, due to their intrinsic nature of connecting users to millions of others, and of giving any individual the power to post anything. Given the rapid, widespread dissemination of information in social networks, manually detecting suspicious news is sub-optimal. Thus, research on automatic rumor detection has become a necessity. Previous works in the domain have utilized the reply relations between posts, as well as the semantic similarity between the main post and its context, consisting of replies, in order to obtain state-of-the-art performance. In this work, we demonstrate that semantic oppositeness can improve the performance on the task of rumor detection. We show that semantic oppositeness captures elements of discord, which are not properly covered by previous efforts, which only utilize semantic similarity or reply structure. Our proposed model learns both explicit and implicit relations between the main tweet and its replies, by utilizing both semantic similarity and semantic oppositeness. Both of these employ the self-attention mechanism in neural text modeling, with semantic oppositeness utilizing word-level self-attention, and with semantic similarity utilizing post-level self-attention. We show, with extensive experiments on recent data sets for this problem, that our proposed model achieves state-of-theart performance. Further, we show that our model is more resistant to the variances in performance introduced by randomness.', 'labels': ['Computational Social Science and Social Media', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Discourse and Pragmatics', 'Summarization', 'Information Extraction', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Generation', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1067504733800888, 0.07213719934225082, 0.06570950895547867, 0.05531768128275871, 0.05390368774533272, 0.053898703306913376, 0.04876088351011276, 0.045370474457740784, 0.044597871601581573, 0.0444648303091526, 0.043444350361824036, 0.04299075901508331, 0.04049300774931908, 0.034965142607688904, 0.0348854698240757, 0.03320864588022232, 0.030313177034258842, 0.0297226645052433, 0.02872493863105774, 0.027057595551013947, 0.025257820263504982, 0.019241422414779663, 0.01878373883664608]}",0.1067504733800888,Computational Social Science and Social Media,0.1067504733800888
Computational Social Science and Social Media,PHASE: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media,"Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Contextualizing the buildup of such ideation is critical for the identification of users at risk. In this work, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users' historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user's historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users' historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming stateof-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations. 1","{'sequence': ""Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Contextualizing the buildup of such ideation is critical for the identification of users at risk. In this work, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users' historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user's historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users' historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming stateof-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations. 1"", 'labels': ['Ethics and NLP', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Information Extraction', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Generation', 'NLP Applications', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08774440735578537, 0.07985614240169525, 0.06816422939300537, 0.06343008577823639, 0.06239462271332741, 0.05363301560282707, 0.05096150189638138, 0.05082103610038757, 0.04992186278104782, 0.04484627768397331, 0.043644264340400696, 0.04361682012677193, 0.04144715145230293, 0.04107176512479782, 0.03260717913508415, 0.03026682883501053, 0.027928808704018593, 0.027525413781404495, 0.027407923713326454, 0.026749713346362114, 0.022954992949962616, 0.014535356312990189, 0.008470666594803333]}",0.08774440735578537,Ethics and NLP,0.06239462271332741
Computational Social Science and Social Media,From the Stage to the Audience: Propaganda on Reddit,"Political discussions revolve around ideological conflicts that often split the audience into two opposing parties. Both parties try to win the argument by bringing forward information. However, often this information is misleading, and its dissemination employs propaganda techniques. In this work, we analyze the impact of propaganda on six major political forums on Reddit that target a diverse audience in two countries, the US and the UK. We focus on three research questions: i) who is posting propaganda? ii) how does propaganda differ across the political spectrum? and iii) how is propaganda received on political forums?","{'sequence': 'Political discussions revolve around ideological conflicts that often split the audience into two opposing parties. Both parties try to win the argument by bringing forward information. However, often this information is misleading, and its dissemination employs propaganda techniques. In this work, we analyze the impact of propaganda on six major political forums on Reddit that target a diverse audience in two countries, the US and the UK. We focus on three research questions: i) who is posting propaganda? ii) how does propaganda differ across the political spectrum? and iii) how is propaganda received on political forums?', 'labels': ['Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Generation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09797726571559906, 0.08051473647356033, 0.07396344840526581, 0.06578277051448822, 0.05869926139712334, 0.05855217203497887, 0.050861019641160965, 0.05061153322458267, 0.04603498801589012, 0.04459383338689804, 0.04118220508098602, 0.038719840347766876, 0.038569673895835876, 0.03639952093362808, 0.034732136875391006, 0.033616457134485245, 0.025304552167654037, 0.025079403072595596, 0.023022068664431572, 0.02062440663576126, 0.01922803744673729, 0.018469981849193573, 0.017460644245147705]}",0.09797726571559906,Speech and Multimodality,0.038569673895835876
Computational Social Science and Social Media,Cross-Topic Rumor Detection using Topic-Mixtures,"There has been much interest in rumor detection using deep learning models in recent years. A well-known limitation of deep learning models is that they tend to learn superficial patterns, which restricts their generalization ability. We find that this is also true for cross-topic rumor detection. In this paper, we propose a method inspired by the ""mixture of experts"" paradigm. We assume that the prediction of the rumor class label given an instance is dependent on the topic distribution of the instance. After deriving a vector representation for each topic, given an instance, we derive a ""topic mixture"" vector for the instance based on its topic distribution. This topic mixture is combined with the vector representation of the instance itself to make rumor predictions. Our experiments show that our proposed method can outperform two baseline debiasing methods in a cross-topic setting. In a synthetic setting when we removed topic-specific words, our method also works better than the baselines, showing that our method does not rely on superficial features.","{'sequence': 'There has been much interest in rumor detection using deep learning models in recent years. A well-known limitation of deep learning models is that they tend to learn superficial patterns, which restricts their generalization ability. We find that this is also true for cross-topic rumor detection. In this paper, we propose a method inspired by the ""mixture of experts"" paradigm. We assume that the prediction of the rumor class label given an instance is dependent on the topic distribution of the instance. After deriving a vector representation for each topic, given an instance, we derive a ""topic mixture"" vector for the instance based on its topic distribution. This topic mixture is combined with the vector representation of the instance itself to make rumor predictions. Our experiments show that our proposed method can outperform two baseline debiasing methods in a cross-topic setting. In a synthetic setting when we removed topic-specific words, our method also works better than the baselines, showing that our method does not rely on superficial features.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Semantics: Lexical Semantics', 'Generation', 'Computational Social Science and Social Media', 'Summarization', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Ethics and NLP', 'NLP Applications', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08072587847709656, 0.07923479378223419, 0.069427490234375, 0.06440052390098572, 0.06309721618890762, 0.06264754384756088, 0.05758345127105713, 0.05164537578821182, 0.04443882405757904, 0.040209561586380005, 0.039606958627700806, 0.03801858425140381, 0.0354473814368248, 0.03413238376379013, 0.0310825128108263, 0.03081117756664753, 0.030515315011143684, 0.028766507282853127, 0.02798238955438137, 0.027816295623779297, 0.025265302509069443, 0.021925967186689377, 0.015218627639114857]}",0.08072587847709656,Dialogue and Interactive Systems,0.05164537578821182
Computational Social Science and Social Media,From Toxicity in Online Comments to Incivility in American News: Proceed with Caution,"The ability to quantify incivility online, in news and in congressional debates, is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.","{'sequence': 'The ability to quantify incivility online, in news and in congressional debates, is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Information Extraction', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0858006477355957, 0.08042572438716888, 0.07465583086013794, 0.06860340386629105, 0.06698127090930939, 0.06257887929677963, 0.059865552932024, 0.05479474365711212, 0.05071214586496353, 0.043865181505680084, 0.04020410403609276, 0.038944050669670105, 0.03596505895256996, 0.0354822538793087, 0.031773049384355545, 0.030271094292402267, 0.02868197299540043, 0.024413591250777245, 0.022824589163064957, 0.021096836775541306, 0.018511248752474785, 0.01647222600877285, 0.007076509762555361]}",0.0858006477355957,Dialogue and Interactive Systems,0.07465583086013794
Computational Social Science and Social Media,An Expert Annotated Dataset for the Detection of Online Misogyny,"Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women. We present a new hierarchical taxonomy for online misogyny, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The dataset consists of 6,567 labels for Reddit posts and comments. As previous research has found untrained crowdsourced annotators struggle with identifying misogyny, we hired and trained annotators and provided them with robust annotation guidelines. We report baseline classification performance on the binary classification task, achieving accuracy of 0.93 and F1 of 0.43. The codebook and datasets are made freely available for future researchers.","{'sequence': 'Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women. We present a new hierarchical taxonomy for online misogyny, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The dataset consists of 6,567 labels for Reddit posts and comments. As previous research has found untrained crowdsourced annotators struggle with identifying misogyny, we hired and trained annotators and provided them with robust annotation guidelines. We report baseline classification performance on the binary classification task, achieving accuracy of 0.93 and F1 of 0.43. The codebook and datasets are made freely available for future researchers.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Question Answering', 'Ethics and NLP', 'Information Extraction', 'Summarization', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.08894114941358566, 0.07578985393047333, 0.07177496701478958, 0.07020455598831177, 0.0594710148870945, 0.054425790905952454, 0.05315740033984184, 0.052208684384822845, 0.0453537218272686, 0.042766496539115906, 0.04195267707109451, 0.041114822030067444, 0.04092050716280937, 0.0367465540766716, 0.03509977087378502, 0.03131886571645737, 0.025764837861061096, 0.02525290660560131, 0.025076812133193016, 0.024514032527804375, 0.0219308752566576, 0.01892010122537613, 0.01729351095855236]}",0.08894114941358566,Dialogue and Interactive Systems,0.07020455598831177
Dialogue and Interactive Systems,Zero-shot Generalization in Dialog State Tracking through Generative Question Answering,"Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in taskoriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports natural language queries for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative questionanswering using a conditional language model pre-trained on substantive English sentences. Our model improves joint goal accuracy in zero-shot domain adaptation settings by up to 9% (absolute) over the previous state-of-theart on the MultiWOZ 2.1 dataset.","{'sequence': 'Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in taskoriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports natural language queries for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative questionanswering using a conditional language model pre-trained on substantive English sentences. Our model improves joint goal accuracy in zero-shot domain adaptation settings by up to 9% (absolute) over the previous state-of-theart on the MultiWOZ 2.1 dataset.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Generation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'NLP Applications', 'Ethics and NLP', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Resources and Evaluation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.3451991677284241, 0.0838964506983757, 0.052175894379615784, 0.04586319252848625, 0.04535900801420212, 0.03382749855518341, 0.03265327960252762, 0.030586816370487213, 0.02760191820561886, 0.027576768770813942, 0.026906156912446022, 0.02680542692542076, 0.026019735261797905, 0.02557252161204815, 0.023886462673544884, 0.021927179768681526, 0.02180534414947033, 0.020145012065768242, 0.01987924799323082, 0.01962161995470524, 0.01649612747132778, 0.015136933885514736, 0.011058193631470203]}",0.3451991677284241,Question Answering,0.0838964506983757
Dialogue and Interactive Systems,MONAH: Multi-Modal Narratives for Humans to analyze conversations,"In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of videorecorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote interpretability. Our feature engineering contributions are two-fold: firstly, we identify the range of multimodal features relevant to detect rapport-building; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building. * Corresponding author 1 Please visit www.universitytranscriptions.co.uk/jeffersontranscription-example/ for an audio example.","{'sequence': 'In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of videorecorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote interpretability. Our feature engineering contributions are two-fold: firstly, we identify the range of multimodal features relevant to detect rapport-building; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building. * Corresponding author 1 Please visit www.universitytranscriptions.co.uk/jeffersontranscription-example/ for an audio example.', 'labels': ['Generation', 'Information Extraction', 'Summarization', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Ethics and NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1408032923936844, 0.07797844707965851, 0.06953410804271698, 0.0690683051943779, 0.05422629415988922, 0.05053070932626724, 0.05020427703857422, 0.04940858483314514, 0.04329647123813629, 0.04110901430249214, 0.03716560825705528, 0.035980504006147385, 0.033346034586429596, 0.0316208079457283, 0.03003532625734806, 0.029991276562213898, 0.026823928579688072, 0.02468319982290268, 0.024217968806624413, 0.024172496050596237, 0.021253453567624092, 0.019960202276706696, 0.014589904807507992]}",0.1408032923936844,Generation,0.05422629415988922
Dialogue and Interactive Systems,MIDAS: A Dialog Act Annotation Scheme for Open Domain HumanMachine Spoken Conversations,"Dialog act prediction in open-domain conversations is an essential language comprehension task for both dialog system building and discourse analysis. Previous dialog act schemes, such as SWBD-DAMSL, are designed mainly for discourse analysis in humanhuman conversations. In this paper, we present a dialog act annotation scheme, MIDAS (Machine Interaction Dialog Act Scheme), targeted at open-domain human-machine conversations. MIDAS is designed to assist machines to improve their ability to understand human partners. MIDAS has a hierarchical structure and supports multi-label annotations. We collected and annotated a large open-domain human-machine spoken conversation dataset (consisting of 24K utterances). To validate our scheme, we leveraged transfer learning methods to train a multi-label dialog act prediction model and reached an F1 score of 0.79. 1 MIDAS 23","{'sequence': 'Dialog act prediction in open-domain conversations is an essential language comprehension task for both dialog system building and discourse analysis. Previous dialog act schemes, such as SWBD-DAMSL, are designed mainly for discourse analysis in humanhuman conversations. In this paper, we present a dialog act annotation scheme, MIDAS (Machine Interaction Dialog Act Scheme), targeted at open-domain human-machine conversations. MIDAS is designed to assist machines to improve their ability to understand human partners. MIDAS has a hierarchical structure and supports multi-label annotations. We collected and annotated a large open-domain human-machine spoken conversation dataset (consisting of 24K utterances). To validate our scheme, we leveraged transfer learning methods to train a multi-label dialog act prediction model and reached an F1 score of 0.79. 1 MIDAS 23', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Summarization', 'Resources and Evaluation', 'NLP Applications', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11091040074825287, 0.07799187302589417, 0.06386884301900864, 0.05299140140414238, 0.05230613052845001, 0.047585953027009964, 0.046698879450559616, 0.04493192210793495, 0.04466445744037628, 0.043347135186195374, 0.04163776710629463, 0.038696594536304474, 0.03826605901122093, 0.036720093339681625, 0.033963657915592194, 0.03368223458528519, 0.032092947512865067, 0.03105420246720314, 0.029361214488744736, 0.027216440066695213, 0.02597174420952797, 0.025079045444726944, 0.020961012691259384]}",0.11091040074825287,Dialogue and Interactive Systems,0.11091040074825287
Dialogue and Interactive Systems,Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models,"In this work, we study how the finetuning stage in the pretrain-finetune framework changes the behavior of a pretrained neural language generator. We focus on the transformer encoderdecoder model for the open-domain dialogue response generation task. Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named ""mix-review"". We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.","{'sequence': 'In this work, we study how the finetuning stage in the pretrain-finetune framework changes the behavior of a pretrained neural language generator. We focus on the transformer encoderdecoder model for the open-domain dialogue response generation task. Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named ""mix-review"". We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Question Answering', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Summarization', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.23017188906669617, 0.06049297749996185, 0.057506807148456573, 0.05560273677110672, 0.04821702092885971, 0.047202762216329575, 0.04598312824964523, 0.04270927608013153, 0.041126083582639694, 0.03672907501459122, 0.035239048302173615, 0.033145420253276825, 0.03252653405070305, 0.028606407344341278, 0.02860158309340477, 0.02755563333630562, 0.02578950673341751, 0.025646723806858063, 0.024161718785762787, 0.021947085857391357, 0.021063968539237976, 0.016679953783750534, 0.013294602744281292]}",0.23017188906669617,Generation,0.06049297749996185
Dialogue and Interactive Systems,Grounding as a Collaborative Process,"Collaborative grounding is a fundamental aspect of human-human dialog which allows people to negotiate meaning. In this paper we argue that it is missing from current deep learning approaches to dialog and interactive systems. Our central point is that making mistakes and being able to recover from them collaboratively is a key ingredient in grounding meaning. We illustrate the pitfalls of being unable to ground collaboratively, discuss what can be learned from the language acquisition and dialog systems literature, and reflect on how to move forward.","{'sequence': 'Collaborative grounding is a fundamental aspect of human-human dialog which allows people to negotiate meaning. In this paper we argue that it is missing from current deep learning approaches to dialog and interactive systems. Our central point is that making mistakes and being able to recover from them collaboratively is a key ingredient in grounding meaning. We illustrate the pitfalls of being unable to ground collaboratively, discuss what can be learned from the language acquisition and dialog systems literature, and reflect on how to move forward.', 'labels': ['Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'NLP Applications', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Information Extraction', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.792985737323761, 0.016836566850543022, 0.016237985342741013, 0.015990031883120537, 0.01479168888181448, 0.012166742235422134, 0.01149621419608593, 0.01075586024671793, 0.0106939896941185, 0.010367212817072868, 0.010270720347762108, 0.009508068673312664, 0.008942099288105965, 0.008200118318200111, 0.007916451431810856, 0.0077080982737243176, 0.007619824726134539, 0.005007954780012369, 0.0048597450368106365, 0.004826599266380072, 0.004803055431693792, 0.004110769368708134, 0.00390438549220562]}",0.792985737323761,Dialogue and Interactive Systems,0.792985737323761
Dialogue and Interactive Systems,Dialogue Act-based Breakdown Detection in Negotiation Dialogues,"Thanks to the success of goal-oriented negotiation dialogue systems, studies of negotiation dialogue have gained momentum in terms of both human-human negotiation support and dialogue systems. However, the field suffers from a paucity of available negotiation corpora, which hinders further development and makes it difficult to test new methodologies in novel negotiation settings. Here, we share a human-human negotiation dialogue dataset in a job interview scenario that features increased complexities in terms of the number of possible solutions and a utility function. We test the proposed corpus using a breakdown detection task for human-human negotiation support. We also introduce a dialogue act-based breakdown detection method, focusing on dialogue flow that is applicable to various corpora. Our results show that our proposed method features comparable detection performance to text-based approaches in existing corpora and better results in the proposed dataset.","{'sequence': 'Thanks to the success of goal-oriented negotiation dialogue systems, studies of negotiation dialogue have gained momentum in terms of both human-human negotiation support and dialogue systems. However, the field suffers from a paucity of available negotiation corpora, which hinders further development and makes it difficult to test new methodologies in novel negotiation settings. Here, we share a human-human negotiation dialogue dataset in a job interview scenario that features increased complexities in terms of the number of possible solutions and a utility function. We test the proposed corpus using a breakdown detection task for human-human negotiation support. We also introduce a dialogue act-based breakdown detection method, focusing on dialogue flow that is applicable to various corpora. Our results show that our proposed method features comparable detection performance to text-based approaches in existing corpora and better results in the proposed dataset.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Information Extraction', 'Generation', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.15385237336158752, 0.11121466755867004, 0.06262214481830597, 0.056848760694265366, 0.051316037774086, 0.04407060891389847, 0.042537689208984375, 0.041828226298093796, 0.037584006786346436, 0.03399943187832832, 0.03391141816973686, 0.03341934084892273, 0.03241785988211632, 0.03194332867860794, 0.03140101954340935, 0.03095230646431446, 0.02846113219857216, 0.025466956198215485, 0.025429975241422653, 0.025106409564614296, 0.02380412630736828, 0.02176876924932003, 0.02004348486661911]}",0.15385237336158752,Dialogue and Interactive Systems,0.15385237336158752
Dialogue and Interactive Systems,Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation,"A recent topic of research in natural language generation has been the development of automatic response generation modules that can automatically respond to a user's utterance in an empathetic manner. Previous research has tackled this task using neural generative methods by augmenting emotion classes with the input sequences. However, the outputs by these models may be inconsistent. We employ multitask learning to predict the emotion label and to generate a viable response for a given utterance using a common encoder with multiple decoders. Our proposed encoder-decoder model consists of a self-attention based encoder and a decoder with dot product attention mechanism to generate response with a specified emotion. We use the focal loss to handle imbalanced data distribution, and utilize the consistency loss to allow coherent decoding by the decoders. Human evaluation reveals that our model produces more emotionally pertinent responses. In addition, our model outperforms multiple strong baselines on automatic evaluation measures such as F1 and BLEU scores, thus resulting in more fluent and adequate responses.","{'sequence': ""A recent topic of research in natural language generation has been the development of automatic response generation modules that can automatically respond to a user's utterance in an empathetic manner. Previous research has tackled this task using neural generative methods by augmenting emotion classes with the input sequences. However, the outputs by these models may be inconsistent. We employ multitask learning to predict the emotion label and to generate a viable response for a given utterance using a common encoder with multiple decoders. Our proposed encoder-decoder model consists of a self-attention based encoder and a decoder with dot product attention mechanism to generate response with a specified emotion. We use the focal loss to handle imbalanced data distribution, and utilize the consistency loss to allow coherent decoding by the decoders. Human evaluation reveals that our model produces more emotionally pertinent responses. In addition, our model outperforms multiple strong baselines on automatic evaluation measures such as F1 and BLEU scores, thus resulting in more fluent and adequate responses."", 'labels': ['Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13736951351165771, 0.11553245782852173, 0.07964508980512619, 0.06638991087675095, 0.06352416425943375, 0.06134158372879028, 0.0527220256626606, 0.04025359824299812, 0.03781730681657791, 0.03486369177699089, 0.034619301557540894, 0.03146391734480858, 0.0310920812189579, 0.029327301308512688, 0.02781631052494049, 0.024139469489455223, 0.02298557572066784, 0.022001896053552628, 0.021428551524877548, 0.018348122015595436, 0.016680100932717323, 0.01661967858672142, 0.014018392190337181]}",0.13736951351165771,Generation,0.0527220256626606
Dialogue and Interactive Systems,The Gutenberg Dialogue Dataset,"Large datasets are essential for neural modeling of many NLP tasks. Current publicly available open-domain dialogue datasets offer a trade-off between quality (e.g., DailyDialog  (Li et al., 2017b)) and size (e.g., Opensubtitles (Tiedemann, 2012)). We narrow this gap by building a high-quality dataset of 14.8M utterances in English, and smaller datasets in German, Dutch, Spanish, Portuguese, Italian, and Hungarian. We extract and process dialogues from public-domain books made available by Project Gutenberg 1 . We describe our dialogue extraction pipeline, analyze the effects of the various heuristics used, and present an error analysis of extracted dialogues. Finally, we conduct experiments showing that better response quality can be achieved in zero-shot and finetuning settings by training on our data than on the larger but much noisier Opensubtitles dataset. Our open-source pipeline 2 can be extended to further languages with little additional effort. Researchers can also build their versions of existing datasets by adjusting various trade-off parameters.","{'sequence': 'Large datasets are essential for neural modeling of many NLP tasks. Current publicly available open-domain dialogue datasets offer a trade-off between quality (e.g., DailyDialog  (Li et al., 2017b)) and size (e.g., Opensubtitles (Tiedemann, 2012)). We narrow this gap by building a high-quality dataset of 14.8M utterances in English, and smaller datasets in German, Dutch, Spanish, Portuguese, Italian, and Hungarian. We extract and process dialogues from public-domain books made available by Project Gutenberg 1 . We describe our dialogue extraction pipeline, analyze the effects of the various heuristics used, and present an error analysis of extracted dialogues. Finally, we conduct experiments showing that better response quality can be achieved in zero-shot and finetuning settings by training on our data than on the larger but much noisier Opensubtitles dataset. Our open-source pipeline 2 can be extended to further languages with little additional effort. Researchers can also build their versions of existing datasets by adjusting various trade-off parameters.', 'labels': ['Machine Learning for NLP', 'Resources and Evaluation', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'NLP Applications', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Generation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Question Answering', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.1516014039516449, 0.07370947301387787, 0.0665777400135994, 0.06549759954214096, 0.06493901461362839, 0.06388957798480988, 0.058559004217386246, 0.04477608576416969, 0.04138607159256935, 0.03670969232916832, 0.03571869805455208, 0.03297479823231697, 0.030905406922101974, 0.030327774584293365, 0.029553936794400215, 0.029302218928933144, 0.02490871772170067, 0.024596745148301125, 0.02316475287079811, 0.02017812803387642, 0.01714540459215641, 0.016816673800349236, 0.01676112972199917]}",0.1516014039516449,Machine Learning for NLP,0.058559004217386246
Dialogue and Interactive Systems,Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks,"This paper addresses the task of (complex) conversational question answering over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic parSing with trAnsformer and Graph atteNtion nEtworks). It is the first approach, which employs a transformer architecture extended with Graph Attention Networks for multi-task neural semantic parsing. LASAGNE uses a transformer model for generating the base logical forms, while the Graph Attention model is used to exploit correlations between (entity) types and predicates to produce node representations. LASAGNE also includes a novel entity recognition module which detects, links, and ranks all relevant entities in the question context. We evaluate LASAGNE on a standard dataset for complex sequential question answering, on which it outperforms existing baseline averages on all question types. Specifically, we show that LASAGNE improves the F1-score on eight out of ten question types; in some cases, the increase in F1-score is more than 20% compared to the state of the art.","{'sequence': 'This paper addresses the task of (complex) conversational question answering over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic parSing with trAnsformer and Graph atteNtion nEtworks). It is the first approach, which employs a transformer architecture extended with Graph Attention Networks for multi-task neural semantic parsing. LASAGNE uses a transformer model for generating the base logical forms, while the Graph Attention model is used to exploit correlations between (entity) types and predicates to produce node representations. LASAGNE also includes a novel entity recognition module which detects, links, and ranks all relevant entities in the question context. We evaluate LASAGNE on a standard dataset for complex sequential question answering, on which it outperforms existing baseline averages on all question types. Specifically, we show that LASAGNE improves the F1-score on eight out of ten question types; in some cases, the increase in F1-score is more than 20% compared to the state of the art.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Generation', 'Machine Learning for NLP', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.6718172430992126, 0.026822274550795555, 0.026190077885985374, 0.02437092922627926, 0.024076880887150764, 0.02334597148001194, 0.021751768887043, 0.01973307877779007, 0.019568851217627525, 0.016740543767809868, 0.01613166369497776, 0.015673773363232613, 0.015503152273595333, 0.013837609440088272, 0.012403247877955437, 0.009147138334810734, 0.008811531588435173, 0.007331245578825474, 0.007294819690287113, 0.005634020548313856, 0.005460844840854406, 0.004545789211988449, 0.0038074972108006477]}",0.6718172430992126,Question Answering,0.026190077885985374
Dialogue and Interactive Systems,Recipes for Building an Open-Domain Chatbot,"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.","{'sequence': 'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.', 'labels': ['Resources and Evaluation', 'Generation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2053176462650299, 0.11892247200012207, 0.08405216038227081, 0.07068730890750885, 0.05853840708732605, 0.03864182159304619, 0.036548860371112823, 0.035781972110271454, 0.030737467110157013, 0.029457440599799156, 0.029250703752040863, 0.028899041935801506, 0.0288385059684515, 0.02541183866560459, 0.02512492798268795, 0.02420741692185402, 0.023350117728114128, 0.022353557869791985, 0.021984191611409187, 0.01728793978691101, 0.016216879710555077, 0.014650548808276653, 0.01373879425227642]}",0.2053176462650299,Resources and Evaluation,0.07068730890750885
Dialogue and Interactive Systems,Jointly Improving Language Understanding and Generation with Quality-Weighted Weak Supervision of Automatic Labeling,"Neural natural language generation (NLG) and understanding (NLU) models are data-hungry and require massive amounts of annotated data to be competitive. Recent frameworks address this bottleneck with generative models that synthesize weak labels at scale, where a small amount of training labels are expertcurated and the rest of the data is automatically annotated. We follow that approach, by automatically constructing a large-scale weaklylabeled data with a fine-tuned GPT-2, and employ a semi-supervised framework to jointly train the NLG and NLU models. The proposed framework adapts the parameter updates to the models according to the estimated labelquality. On both the E2E and Weather benchmarks, we show that this weakly supervised training paradigm is an effective approach under low resource scenarios with as little as 10 data instances, and outperforming benchmark systems on both datasets when 100% of training data is used.","{'sequence': 'Neural natural language generation (NLG) and understanding (NLU) models are data-hungry and require massive amounts of annotated data to be competitive. Recent frameworks address this bottleneck with generative models that synthesize weak labels at scale, where a small amount of training labels are expertcurated and the rest of the data is automatically annotated. We follow that approach, by automatically constructing a large-scale weaklylabeled data with a fine-tuned GPT-2, and employ a semi-supervised framework to jointly train the NLG and NLU models. The proposed framework adapts the parameter updates to the models according to the estimated labelquality. On both the E2E and Weather benchmarks, we show that this weakly supervised training paradigm is an effective approach under low resource scenarios with as little as 10 data instances, and outperforming benchmark systems on both datasets when 100% of training data is used.', 'labels': ['Machine Learning for NLP', 'Generation', 'Resources and Evaluation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1535462886095047, 0.15306155383586884, 0.06356791406869888, 0.06168246641755104, 0.057383399456739426, 0.04833203926682472, 0.042553339153528214, 0.041276246309280396, 0.03991293907165527, 0.03937691077589989, 0.03503450006246567, 0.03197884187102318, 0.029697423800826073, 0.02579537406563759, 0.0257154181599617, 0.025359055027365685, 0.022344963625073433, 0.021498240530490875, 0.02137281373143196, 0.019438154995441437, 0.017202775925397873, 0.013915866613388062, 0.009953469038009644]}",0.1535462886095047,Machine Learning for NLP,0.03991293907165527
Dialogue and Interactive Systems,Modeling Coreference Relations in Visual Dialog,"Visual dialog is a vision-language task where an agent needs to answer a series of questions grounded in an image based on the understanding of the dialog history and the image. The occurrences of coreference relations in the dialog makes it a more challenging task than visual question-answering. Most previous works have focused on learning better multi-modal representations or on exploring different ways of fusing visual and language features, while the coreferences in the dialog are mainly ignored. In this paper, based on linguistic knowledge and discourse features of human dialog we propose two soft constraints that can improve the model's ability of resolving coreferences in dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset shows that our model, which integrates two novel and linguistically inspired soft constraints in a deep transformer neural architecture, obtains new state-of-the-art performance in terms of recall at 1 and other evaluation metrics compared to current existing models and this without pretraining on other visionlanguage datasets. Our qualitative results also demonstrate the effectiveness of the method that we propose. 1","{'sequence': ""Visual dialog is a vision-language task where an agent needs to answer a series of questions grounded in an image based on the understanding of the dialog history and the image. The occurrences of coreference relations in the dialog makes it a more challenging task than visual question-answering. Most previous works have focused on learning better multi-modal representations or on exploring different ways of fusing visual and language features, while the coreferences in the dialog are mainly ignored. In this paper, based on linguistic knowledge and discourse features of human dialog we propose two soft constraints that can improve the model's ability of resolving coreferences in dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset shows that our model, which integrates two novel and linguistically inspired soft constraints in a deep transformer neural architecture, obtains new state-of-the-art performance in terms of recall at 1 and other evaluation metrics compared to current existing models and this without pretraining on other visionlanguage datasets. Our qualitative results also demonstrate the effectiveness of the method that we propose. 1"", 'labels': ['Resources and Evaluation', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Extraction', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Speech and Multimodality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13152335584163666, 0.08248986303806305, 0.07378923892974854, 0.05563483014702797, 0.049848031252622604, 0.04775817692279816, 0.04699325188994408, 0.046726640313863754, 0.04518459737300873, 0.04041440784931183, 0.037261564284563065, 0.03713815659284592, 0.03704741224646568, 0.035280369222164154, 0.03523578867316246, 0.03408360481262207, 0.027970314025878906, 0.027782026678323746, 0.024861829355359077, 0.024791955947875977, 0.021228522062301636, 0.020986614748835564, 0.015969403088092804]}",0.13152335584163666,Resources and Evaluation,0.07378923892974854
Dialogue and Interactive Systems,Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models,"Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models has suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM) 1 . ARDM models each speaker separately and takes advantage of large pre-trained language models. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with the state-of-theart methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In the PersuasionForGood task, ARDM is capable of generating human-like responses to persuade people to donate to a charity.","{'sequence': 'Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models has suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM) 1 . ARDM models each speaker separately and takes advantage of large pre-trained language models. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with the state-of-theart methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In the PersuasionForGood task, ARDM is capable of generating human-like responses to persuade people to donate to a charity.', 'labels': ['Dialogue and Interactive Systems', 'Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Resources and Evaluation', 'Generation', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Information Extraction', 'Question Answering', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10414338856935501, 0.0937841609120369, 0.08726377040147781, 0.07512318342924118, 0.0718136876821518, 0.05881757661700249, 0.048924412578344345, 0.04856196790933609, 0.03946656733751297, 0.03638076409697533, 0.033309608697891235, 0.03249775990843773, 0.03241342306137085, 0.030766256153583527, 0.029767917469143867, 0.02945026382803917, 0.028007518500089645, 0.027184590697288513, 0.023384351283311844, 0.020742110908031464, 0.020541882142424583, 0.016172638162970543, 0.01148209348320961]}",0.10414338856935501,Dialogue and Interactive Systems,0.10414338856935501
Dialogue and Interactive Systems,Few Shot Dialogue State Tracking using Meta-learning,"Dialogue State Tracking (DST) forms a core component of automated chatbot based systems designed for specific goals like hotel, taxi reservation, tourist information etc. With the increasing need to deploy such systems in new domains, solving the problem of zero/fewshot DST has become necessary. There has been a rising trend for learning to transfer knowledge from resource-rich domains to unknown domains with minimal need for additional data. In this work, we explore the merits of meta-learning algorithms for this transfer and hence, propose a meta-learner D-REPTILE specific to the DST problem. With extensive experimentation, we provide clear evidence of benefits over conventional approaches across different domains, methods, base models and datasets with significant (5-25%) improvement over the baseline in lowdata setting. Our proposed meta-learner is agnostic of the underlying model and hence any existing state-of-the-art DST system can improve its performance on unknown domains using our training strategy.","{'sequence': 'Dialogue State Tracking (DST) forms a core component of automated chatbot based systems designed for specific goals like hotel, taxi reservation, tourist information etc. With the increasing need to deploy such systems in new domains, solving the problem of zero/fewshot DST has become necessary. There has been a rising trend for learning to transfer knowledge from resource-rich domains to unknown domains with minimal need for additional data. In this work, we explore the merits of meta-learning algorithms for this transfer and hence, propose a meta-learner D-REPTILE specific to the DST problem. With extensive experimentation, we provide clear evidence of benefits over conventional approaches across different domains, methods, base models and datasets with significant (5-25%) improvement over the baseline in lowdata setting. Our proposed meta-learner is agnostic of the underlying model and hence any existing state-of-the-art DST system can improve its performance on unknown domains using our training strategy.', 'labels': ['Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1889858841896057, 0.0669436827301979, 0.06641227006912231, 0.06478314101696014, 0.057749610394239426, 0.056594137102365494, 0.05051253363490105, 0.04471541941165924, 0.04063260555267334, 0.03947388008236885, 0.0389321930706501, 0.03489825502038002, 0.032667044550180435, 0.02965061366558075, 0.02946041338145733, 0.028668582439422607, 0.026805894449353218, 0.02661171555519104, 0.02383689396083355, 0.016675055027008057, 0.014377868734300137, 0.011435221880674362, 0.00917698722332716]}",0.1889858841896057,Dialogue and Interactive Systems,0.1889858841896057
Discourse and Pragmatics,ResPer: Computationally Modelling Resisting Strategies in Persuasive Conversations,"Modelling persuasion strategies as predictors of task outcome has several real-world applications and has received considerable attention from the computational linguistics community. However, previous research has failed to account for the resisting strategies employed by an individual to foil such persuasion attempts. Grounded in prior literature in cognitive and social psychology, we propose a generalised framework for identifying resisting strategies in persuasive conversations. We instantiate our framework on two distinct datasets comprising persuasion and negotiation conversations. We also leverage a hierarchical sequence-labelling neural architecture to infer the aforementioned resisting strategies automatically. Our experiments reveal the asymmetry of power roles in non-collaborative goal-directed conversations and the benefits accrued from incorporating resisting strategies on the final conversation outcome. We also investigate the role of different resisting strategies on the conversation outcome and glean insights that corroborate with past findings. We also make the code and the dataset of this work publicly available at https://github.com/americast/ resper.","{'sequence': 'Modelling persuasion strategies as predictors of task outcome has several real-world applications and has received considerable attention from the computational linguistics community. However, previous research has failed to account for the resisting strategies employed by an individual to foil such persuasion attempts. Grounded in prior literature in cognitive and social psychology, we propose a generalised framework for identifying resisting strategies in persuasive conversations. We instantiate our framework on two distinct datasets comprising persuasion and negotiation conversations. We also leverage a hierarchical sequence-labelling neural architecture to infer the aforementioned resisting strategies automatically. Our experiments reveal the asymmetry of power roles in non-collaborative goal-directed conversations and the benefits accrued from incorporating resisting strategies on the final conversation outcome. We also investigate the role of different resisting strategies on the conversation outcome and glean insights that corroborate with past findings. We also make the code and the dataset of this work publicly available at https://github.com/americast/ resper.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Information Extraction', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Generation', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07975029945373535, 0.062350064516067505, 0.059011466801166534, 0.050113532692193985, 0.04881090670824051, 0.04861055687069893, 0.04807618260383606, 0.046853456646203995, 0.04570138826966286, 0.04521700367331505, 0.044564034789800644, 0.043643251061439514, 0.04340364784002304, 0.04189826548099518, 0.03979916125535965, 0.03891247138381004, 0.0383235327899456, 0.03649662807583809, 0.036293432116508484, 0.030596135184168816, 0.028034454211592674, 0.023362061008810997, 0.020178040489554405]}",0.07975029945373535,Speech and Multimodality,0.04570138826966286
Discourse and Pragmatics,Scientific Discourse Tagging for Evidence Extraction,"Evidence plays a crucial role in any biomedical research narrative, providing justification for some claims and refutation for others. We seek to build models of scientific argument using information extraction methods from fulltext papers. We present the capability of automatically extracting text fragments from primary research papers that describe the evidence presented in that paper's figures, which arguably provides the raw material of any scientific argument made within the paper. We apply richly contextualized deep representation learning pre-trained on biomedical domain corpus to the analysis of scientific discourse structures and the extraction of ""evidence fragments"" (i.e., the text in the results section describing data presented in a specified subfigure) from a set of biomedical experimental research articles. We first demonstrate our state-of-the-art scientific discourse tagger on two scientific discourse tagging datasets and its transferability to new datasets. We then show the benefit of leveraging scientific discourse tags for downstream tasks such as claim-extraction and evidence fragment detection. Our work demonstrates the potential of using evidence fragments derived from figure spans for improving the quality of scientific claims by cataloging, indexing and reusing evidence fragments as independent documents.","{'sequence': 'Evidence plays a crucial role in any biomedical research narrative, providing justification for some claims and refutation for others. We seek to build models of scientific argument using information extraction methods from fulltext papers. We present the capability of automatically extracting text fragments from primary research papers that describe the evidence presented in that paper\'s figures, which arguably provides the raw material of any scientific argument made within the paper. We apply richly contextualized deep representation learning pre-trained on biomedical domain corpus to the analysis of scientific discourse structures and the extraction of ""evidence fragments"" (i.e., the text in the results section describing data presented in a specified subfigure) from a set of biomedical experimental research articles. We first demonstrate our state-of-the-art scientific discourse tagger on two scientific discourse tagging datasets and its transferability to new datasets. We then show the benefit of leveraging scientific discourse tags for downstream tasks such as claim-extraction and evidence fragment detection. Our work demonstrates the potential of using evidence fragments derived from figure spans for improving the quality of scientific claims by cataloging, indexing and reusing evidence fragments as independent documents.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Question Answering', 'Summarization', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.4604637324810028, 0.04947636276483536, 0.04598835110664368, 0.04473396763205528, 0.03753697872161865, 0.03377823159098625, 0.03143807128071785, 0.028347717598080635, 0.02540973387658596, 0.023426281288266182, 0.023110421374440193, 0.022638898342847824, 0.022495180368423462, 0.022483358159661293, 0.018601637333631516, 0.017785316333174706, 0.016364214941859245, 0.01536734588444233, 0.01492849737405777, 0.013810442760586739, 0.01166505366563797, 0.011199722066521645, 0.008950450457632542]}",0.4604637324810028,Information Extraction,0.03753697872161865
Discourse and Pragmatics,Automatic Data Acquisition for Event Coreference Resolution,"We propose to leverage lexical paraphrases and high precision rules informed by news discourse structure to automatically collect coreferential and non-coreferential event pairs from unlabeled English news articles. We perform both manual validation and empirical evaluation on multiple evaluation datasets with different event domains and text genres to assess the quality of our acquired event pairs. We found that a model trained on our acquired event pairs performs comparably as the supervised model when applied to new data out of the training data domains. Further, augmenting human-annotated data with the acquired event pairs provides empirical performance gains on both in-domain and out-of-domain evaluation datasets.","{'sequence': 'We propose to leverage lexical paraphrases and high precision rules informed by news discourse structure to automatically collect coreferential and non-coreferential event pairs from unlabeled English news articles. We perform both manual validation and empirical evaluation on multiple evaluation datasets with different event domains and text genres to assess the quality of our acquired event pairs. We found that a model trained on our acquired event pairs performs comparably as the supervised model when applied to new data out of the training data domains. Further, augmenting human-annotated data with the acquired event pairs provides empirical performance gains on both in-domain and out-of-domain evaluation datasets.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Question Answering', 'NLP Applications', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.08551318198442459, 0.06560152769088745, 0.0643300712108612, 0.06427424401044846, 0.05709498003125191, 0.05607493966817856, 0.050633594393730164, 0.04971117526292801, 0.048519667237997055, 0.04585494473576546, 0.04558010399341583, 0.04449325427412987, 0.04067707434296608, 0.04027947410941124, 0.03959076851606369, 0.03897735849022865, 0.03812539577484131, 0.031142190098762512, 0.025410175323486328, 0.02147180587053299, 0.015902826562523842, 0.015409921295940876, 0.015331233851611614]}",0.08551318198442459,Resources and Evaluation,0.050633594393730164
Discourse and Pragmatics,Content-based Models of Quotation,"We explore the task of quotability identification, in which, given a document, we aim to identify which of its passages are the most quotable, i.e. the most likely to be directly quoted by later derived documents. We approach quotability identification as a passage ranking problem and evaluate how well both feature-based and BERT-based (Devlin et al., 2019) models rank the passages in a given document by their predicted quotability. We explore this problem through evaluations on five datasets that span multiple languages (English, Latin) and genres of literature (e.g. poetry, plays, novels) and whose corresponding derived documents are of multiple types (news, journal articles). Our experiments confirm the relatively strong performance of BERT-based models on this task, with the best model, a RoBERTA sequential sentence tagger, achieving an average ρ of 0.35 and NDCG@1, 5, 50 of 0.26, 0.31 and 0.40, respectively, across all five datasets.","{'sequence': 'We explore the task of quotability identification, in which, given a document, we aim to identify which of its passages are the most quotable, i.e. the most likely to be directly quoted by later derived documents. We approach quotability identification as a passage ranking problem and evaluate how well both feature-based and BERT-based (Devlin et al., 2019) models rank the passages in a given document by their predicted quotability. We explore this problem through evaluations on five datasets that span multiple languages (English, Latin) and genres of literature (e.g. poetry, plays, novels) and whose corresponding derived documents are of multiple types (news, journal articles). Our experiments confirm the relatively strong performance of BERT-based models on this task, with the best model, a RoBERTA sequential sentence tagger, achieving an average ρ of 0.35 and NDCG@1, 5, 50 of 0.26, 0.31 and 0.40, respectively, across all five datasets.', 'labels': ['Resources and Evaluation', 'Information Extraction', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'NLP Applications', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Machine Translation and Multilinguality', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11771654337644577, 0.07953949272632599, 0.07742223888635635, 0.06029858440160751, 0.05375529080629349, 0.05217941477894783, 0.04765108972787857, 0.04527963325381279, 0.044318173080682755, 0.04265296086668968, 0.04018993303179741, 0.038283348083496094, 0.037771619856357574, 0.0351872518658638, 0.03411673754453659, 0.03392088785767555, 0.03001224622130394, 0.025430399924516678, 0.024341532960534096, 0.024227293208241463, 0.02416747808456421, 0.015799425542354584, 0.015738358721137047]}",0.11771654337644577,Resources and Evaluation,0.03001224622130394
Discourse and Pragmatics,Joint Coreference Resolution and Character Linking for Multiparty Conversation,"Character linking, the task of linking mentioned people in conversations to the real world, is crucial for understanding the conversations. For the efficiency of communication, humans often choose to use pronouns (e.g., ""she"") or normal phrases (e.g., ""that girl"") rather than named entities (e.g., ""Rachel"") in the spoken language, which makes linking those mentions to real people a much more challenging than a regular entity linking task. To address this challenge, we propose to incorporate the richer context from the coreference relations among different mentions to help the linking. On the other hand, considering that finding coreference clusters itself is not a trivial task and could benefit from the global character information, we propose to jointly solve these two tasks. Specifically, we propose C 2 , the joint learning model of Coreference resolution and Character linking. The experimental results demonstrate that C 2 can significantly outperform previous works on both tasks. Further analyses are conducted to analyze the contribution of all modules in the proposed model and the effect of all hyper-parameters.","{'sequence': 'Character linking, the task of linking mentioned people in conversations to the real world, is crucial for understanding the conversations. For the efficiency of communication, humans often choose to use pronouns (e.g., ""she"") or normal phrases (e.g., ""that girl"") rather than named entities (e.g., ""Rachel"") in the spoken language, which makes linking those mentions to real people a much more challenging than a regular entity linking task. To address this challenge, we propose to incorporate the richer context from the coreference relations among different mentions to help the linking. On the other hand, considering that finding coreference clusters itself is not a trivial task and could benefit from the global character information, we propose to jointly solve these two tasks. Specifically, we propose C 2 , the joint learning model of Coreference resolution and Character linking. The experimental results demonstrate that C 2 can significantly outperform previous works on both tasks. Further analyses are conducted to analyze the contribution of all modules in the proposed model and the effect of all hyper-parameters.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Question Answering', 'Generation', 'NLP Applications', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Summarization', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07745956629514694, 0.07459054887294769, 0.07416249066591263, 0.07332708686590195, 0.06629347801208496, 0.06441784650087357, 0.058082833886146545, 0.04827915504574776, 0.04548279941082001, 0.04534951597452164, 0.043679676949977875, 0.037594858556985855, 0.03548508137464523, 0.034861400723457336, 0.03389152139425278, 0.03295235335826874, 0.030626565217971802, 0.029832914471626282, 0.029226943850517273, 0.019155684858560562, 0.01909670978784561, 0.01448680367320776, 0.01166420429944992]}",0.07745956629514694,Information Extraction,0.037594858556985855
Discourse and Pragmatics,Ellipsis Resolution as Question Answering: An Evaluation,"Most, if not all forms of ellipsis (e.g., 'so does Mary') are similar to reading comprehension questions ('what does Mary do'), in that in order to resolve them, we need to identify an appropriate text span in the preceding discourse. Following this observation, we present an alternative approach for English ellipsis resolution relying on architectures developed for question answering (QA). We present both single-task models, and joint models trained on auxiliary QA and coreference resolution datasets, clearly outperforming the current state of the art for Sluice Ellipsis (from 70.00 to 86.01 F 1 ) and Verb Phrase Ellipsis (from 72.89 to 78.66 F 1 ).","{'sequence': ""Most, if not all forms of ellipsis (e.g., 'so does Mary') are similar to reading comprehension questions ('what does Mary do'), in that in order to resolve them, we need to identify an appropriate text span in the preceding discourse. Following this observation, we present an alternative approach for English ellipsis resolution relying on architectures developed for question answering (QA). We present both single-task models, and joint models trained on auxiliary QA and coreference resolution datasets, clearly outperforming the current state of the art for Sluice Ellipsis (from 70.00 to 86.01 F 1 ) and Verb Phrase Ellipsis (from 72.89 to 78.66 F 1 )."", 'labels': ['Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Information Retrieval and Text Mining', 'Generation', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.20663277804851532, 0.07723911106586456, 0.07354845106601715, 0.05545542761683464, 0.05401981621980667, 0.048587411642074585, 0.04612688347697258, 0.04142304137349129, 0.04130910709500313, 0.04013989865779877, 0.03645288944244385, 0.03411103039979935, 0.034058425575494766, 0.033466823399066925, 0.02833249792456627, 0.026265669614076614, 0.0242626890540123, 0.019675660878419876, 0.019050899893045425, 0.016890862956643105, 0.015659010037779808, 0.01437351480126381, 0.012918121181428432]}",0.20663277804851532,Question Answering,0.03411103039979935
Discourse and Pragmatics,Improving Factual Consistency Between a Response and Persona Facts,Neural models for response generation produce responses that are semantically plausible but not necessarily factually consistent with facts describing the speaker's persona. These models are trained with fully supervised learning where the objective function barely captures factual consistency. We propose to finetune these models by reinforcement learning and an efficient reward function that explicitly captures the consistency between a response and persona facts as well as semantic plausibility 1 . Our automatic and human evaluations on the PersonaChat corpus confirm that our approach increases the rate of responses that are factually consistent with persona facts over its supervised counterpart while retaining the language quality of responses.,"{'sequence': ""Neural models for response generation produce responses that are semantically plausible but not necessarily factually consistent with facts describing the speaker's persona. These models are trained with fully supervised learning where the objective function barely captures factual consistency. We propose to finetune these models by reinforcement learning and an efficient reward function that explicitly captures the consistency between a response and persona facts as well as semantic plausibility 1 . Our automatic and human evaluations on the PersonaChat corpus confirm that our approach increases the rate of responses that are factually consistent with persona facts over its supervised counterpart while retaining the language quality of responses."", 'labels': ['Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Summarization', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Question Answering', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.25220149755477905, 0.11271845549345016, 0.06951092928647995, 0.05588877201080322, 0.052070241421461105, 0.04701636731624603, 0.04207029938697815, 0.03999238833785057, 0.032373636960983276, 0.03195340558886528, 0.03064858913421631, 0.03037036769092083, 0.028300946578383446, 0.02360784262418747, 0.022724248468875885, 0.021766839548945427, 0.019005335867404938, 0.018380094319581985, 0.017452700063586235, 0.015945248305797577, 0.015170051716268063, 0.013618250377476215, 0.007213429547846317]}",0.25220149755477905,Generation,0.03195340558886528
Discourse and Pragmatics,Top-down Discourse Parsing via Sequence Labelling,"We introduce a top-down approach to discourse parsing that is conceptually simpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020) . By framing the task as a sequence labelling problem where the goal is to iteratively segment a document into individual discourse units, we are able to eliminate the decoder and reduce the search space for splitting points. We explore both traditional recurrent models and modern pre-trained transformer models for the task, and additionally introduce a novel dynamic oracle for top-down parsing. Based on the Full metric, our proposed LSTM model sets a new state-of-the-art for RST parsing. 1","{'sequence': 'We introduce a top-down approach to discourse parsing that is conceptually simpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020) . By framing the task as a sequence labelling problem where the goal is to iteratively segment a document into individual discourse units, we are able to eliminate the decoder and reduce the search space for splitting points. We explore both traditional recurrent models and modern pre-trained transformer models for the task, and additionally introduce a novel dynamic oracle for top-down parsing. Based on the Full metric, our proposed LSTM model sets a new state-of-the-art for RST parsing. 1', 'labels': ['Discourse and Pragmatics', 'Machine Learning for NLP', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Information Extraction', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Resources and Evaluation', 'Question Answering', 'Summarization', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11204653233289719, 0.07240784168243408, 0.0705084353685379, 0.0652008056640625, 0.06514374911785126, 0.06087584048509598, 0.05085267499089241, 0.045827269554138184, 0.04562438279390335, 0.04046725854277611, 0.034949421882629395, 0.034340936690568924, 0.03412453085184097, 0.031947068870067596, 0.031865932047367096, 0.031010380014777184, 0.030839277431368828, 0.026913540437817574, 0.024944517761468887, 0.024943923577666283, 0.024037955328822136, 0.022962432354688644, 0.018165327608585358]}",0.11204653233289719,Discourse and Pragmatics,0.11204653233289719
Discourse and Pragmatics,Discourse-Aware Unsupervised Summarization for Long Scientific Documents,"We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach 1 outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles. * Equal contribution. 1 Link to our code: https://github.com/ mirandrom/HipoRank. Introduction anxiety affects quality of life in those living with parkinson's disease (pd) more so than overall cognitive status, motor deficits, apathy, and depression.","{'sequence': ""We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach 1 outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles. * Equal contribution. 1 Link to our code: https://github.com/ mirandrom/HipoRank. Introduction anxiety affects quality of life in those living with parkinson's disease (pd) more so than overall cognitive status, motor deficits, apathy, and depression."", 'labels': ['Information Extraction', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Generation', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09433089941740036, 0.07202620804309845, 0.06252048909664154, 0.055489491671323776, 0.05469205230474472, 0.05380481109023094, 0.051680319011211395, 0.05059165135025978, 0.047967880964279175, 0.04591395705938339, 0.04570511728525162, 0.0435297004878521, 0.04055732488632202, 0.03682716190814972, 0.03523732349276543, 0.031353726983070374, 0.030845174565911293, 0.030289705842733383, 0.03019643761217594, 0.02699536457657814, 0.02265443466603756, 0.020432379096746445, 0.01635836251080036]}",0.09433089941740036,Information Extraction,0.03523732349276543
Discourse and Pragmatics,Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks,"Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications for which they are purportedly developed has largely been neglected. With the advancements made by neural approaches in applications such as machine translation (MT), summarization and dialog systems, the need for coherence evaluation of these tasks is now more crucial than ever. However, coherence models are typically evaluated only on synthetic tasks, which may not be representative of their performance in downstream applications. To investigate how representative the synthetic tasks are of downstream use cases, we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks, and contrast this with their performance on three downstream applications: coherence evaluation for MT and summarization, and next utterance prediction in retrieval-based dialog. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, motivating alternate training and evaluation methods for coherence models. 1","{'sequence': 'Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications for which they are purportedly developed has largely been neglected. With the advancements made by neural approaches in applications such as machine translation (MT), summarization and dialog systems, the need for coherence evaluation of these tasks is now more crucial than ever. However, coherence models are typically evaluated only on synthetic tasks, which may not be representative of their performance in downstream applications. To investigate how representative the synthetic tasks are of downstream use cases, we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks, and contrast this with their performance on three downstream applications: coherence evaluation for MT and summarization, and next utterance prediction in retrieval-based dialog. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, motivating alternate training and evaluation methods for coherence models. 1', 'labels': ['Summarization', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Machine Learning for NLP', 'Question Answering', 'Generation', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Information Extraction', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10416191071271896, 0.10165960341691971, 0.10025636106729507, 0.06587838381528854, 0.06537970155477524, 0.058025211095809937, 0.05262790247797966, 0.0399390384554863, 0.037946198135614395, 0.03770938888192177, 0.03755718097090721, 0.035357777029275894, 0.034123387187719345, 0.03108683042228222, 0.03049691580235958, 0.029125487431883812, 0.024614891037344933, 0.024568339809775352, 0.023931460455060005, 0.020542306825518608, 0.018594898283481598, 0.01715993881225586, 0.009256880730390549]}",0.10416191071271896,Summarization,0.03049691580235958
Interpretability and Analysis of Models for NLP,Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration,"Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pretraining to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data. PRESENT ILLNESS: 58yo man w/ hx of hypertension, AFib on coumadin and NIDDM presented to ED with the worst headache of his life. He had a syncopal episode and was intubated by EMS. Medication on admission: 1mg IV ativan x 1.","{'sequence': 'Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pretraining to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data. PRESENT ILLNESS: 58yo man w/ hx of hypertension, AFib on coumadin and NIDDM presented to ED with the worst headache of his life. He had a syncopal episode and was intubated by EMS. Medication on admission: 1mg IV ativan x 1.', 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Summarization', 'Generation', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Semantics: Lexical Semantics', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.09449584782123566, 0.08864105492830276, 0.0806741863489151, 0.07777415215969086, 0.06884988397359848, 0.05860540643334389, 0.047163646668195724, 0.0465252511203289, 0.045330896973609924, 0.0432090125977993, 0.03840287774801254, 0.03414447233080864, 0.033985573798418045, 0.03318607062101364, 0.03309685364365578, 0.029122628271579742, 0.027252687141299248, 0.02594783529639244, 0.02209913171827793, 0.02035064995288849, 0.01781003549695015, 0.01718740351498127, 0.016144538298249245]}",0.09449584782123566,Speech and Multimodality,0.02594783529639244
Interpretability and Analysis of Models for NLP,Predicting Treatment Outcome from Patient Texts:The Case of Internet-Based Cognitive Behavioural Therapy,"We investigate the feasibility of applying standard text categorisation methods to patient text in order to predict treatment outcome in Internet-based cognitive behavioural therapy. The data set is unique in its detail and size for regular care for depression, social anxiety, and panic disorder. Our results indicate that there is a signal in the depression data, albeit a weak one. We also perform terminological and sentiment analysis, which confirm those results.","{'sequence': 'We investigate the feasibility of applying standard text categorisation methods to patient text in order to predict treatment outcome in Internet-based cognitive behavioural therapy. The data set is unique in its detail and size for regular care for depression, social anxiety, and panic disorder. Our results indicate that there is a signal in the depression data, albeit a weak one. We also perform terminological and sentiment analysis, which confirm those results.', 'labels': ['Information Retrieval and Text Mining', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'NLP Applications', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09933268278837204, 0.08713127672672272, 0.08099309355020523, 0.06963454931974411, 0.06252112984657288, 0.049588751047849655, 0.048805419355630875, 0.048520829528570175, 0.047224849462509155, 0.03991914540529251, 0.039830487221479416, 0.038771986961364746, 0.03605393320322037, 0.034966032952070236, 0.033778440207242966, 0.030742736533284187, 0.029130619019269943, 0.028450068086385727, 0.028043832629919052, 0.02358156070113182, 0.017041807994246483, 0.013117716647684574, 0.012819060124456882]}",0.09933268278837204,Information Retrieval and Text Mining,0.029130619019269943
Interpretability and Analysis of Models for NLP,Cross-lingual Contextualized Topic Models with Zero-shot Learning,"Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions. Lang Sentence Predicted Topic EN Blackmore's Night is a British/American traditional folk rock duo [...] rock, band, bass, formed IT I Blackmore's Night sono la band fondatrice del renaissance rock [...] rock, band, bass, formed PT Blackmore's Night é uma banda de folk rock de estilo renascentista [...] rock, band, bass, formed EN Langton's ant is a two-dimensional Turing machine with [..","{'sequence': ""Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions. Lang Sentence Predicted Topic EN Blackmore's Night is a British/American traditional folk rock duo [...] rock, band, bass, formed IT I Blackmore's Night sono la band fondatrice del renaissance rock [...] rock, band, bass, formed PT Blackmore's Night é uma banda de folk rock de estilo renascentista [...] rock, band, bass, formed EN Langton's ant is a two-dimensional Turing machine with [.."", 'labels': ['Speech and Multimodality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'NLP Applications', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Ethics and NLP', 'Question Answering', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10597877949476242, 0.09745807200670242, 0.07148131728172302, 0.06075844168663025, 0.05012798681855202, 0.04798557609319687, 0.046125900000333786, 0.0428432933986187, 0.04208117350935936, 0.040801458060741425, 0.039129748940467834, 0.037198569625616074, 0.03708131983876228, 0.03575233370065689, 0.033292703330516815, 0.033228568732738495, 0.03187880292534828, 0.027825884521007538, 0.027069777250289917, 0.02565642073750496, 0.025317609310150146, 0.02115458995103836, 0.01977168396115303]}",0.10597877949476242,Speech and Multimodality,0.025317609310150146
Interpretability and Analysis of Models for NLP,Adv-OLM: Generating Textual Adversaries via OLM,"Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in NLP can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.","{'sequence': 'Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in NLP can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Question Answering', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Generation', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining'], 'scores': [0.22522054612636566, 0.21966378390789032, 0.15563824772834778, 0.03824392706155777, 0.037880755960941315, 0.037175457924604416, 0.0313672199845314, 0.030515756458044052, 0.019182100892066956, 0.018904779106378555, 0.018592381849884987, 0.018429815769195557, 0.017947254702448845, 0.01738465391099453, 0.017049157992005348, 0.016839327290654182, 0.016206905245780945, 0.015171010047197342, 0.012907754629850388, 0.010462991893291473, 0.009303192608058453, 0.008293231949210167, 0.007619631011039019]}",0.22522054612636566,Interpretability and Analysis of Models for NLP,0.22522054612636566
Interpretability and Analysis of Models for NLP,On-Device Text Representations Robust To Misspellings via Projections,"Recently, there has been a strong interest in developing natural language applications that live on personal devices such as mobile phones, watches and IoT with the objective to preserve user privacy and have low memory. Advances in Locality-Sensitive Hashing (LSH)-based projection networks have demonstrated state-of-the-art performance in various classification tasks without explicit word (or word-piece) embedding lookup tables by computing on-the-fly text representations. In this paper, we show that the projection based neural classifiers are inherently robust to misspellings and perturbations of the input text. We empirically demonstrate that the LSH projection based classifiers are more robust to common misspellings compared to BiL-STMs (with both word-piece & word-only tokenization) and fine-tuned BERT based methods. When subject to misspelling attacks, LSH projection based classifiers had a small average accuracy drop of 2.94% across multiple classifications tasks, while the fine-tuned BERT model accuracy had a significant drop of 11.44%. 69.46 ±1.1 74.96 ±0.7 85.43 ±0.3 81.95 ±2.2 80.15 ±2.0 91.05 ±0.2 65.05 ±4.2 64.95 ±4.1","{'sequence': 'Recently, there has been a strong interest in developing natural language applications that live on personal devices such as mobile phones, watches and IoT with the objective to preserve user privacy and have low memory. Advances in Locality-Sensitive Hashing (LSH)-based projection networks have demonstrated state-of-the-art performance in various classification tasks without explicit word (or word-piece) embedding lookup tables by computing on-the-fly text representations. In this paper, we show that the projection based neural classifiers are inherently robust to misspellings and perturbations of the input text. We empirically demonstrate that the LSH projection based classifiers are more robust to common misspellings compared to BiL-STMs (with both word-piece & word-only tokenization) and fine-tuned BERT based methods. When subject to misspelling attacks, LSH projection based classifiers had a small average accuracy drop of 2.94% across multiple classifications tasks, while the fine-tuned BERT model accuracy had a significant drop of 11.44%. 69.46 ±1.1 74.96 ±0.7 85.43 ±0.3 81.95 ±2.2 80.15 ±2.0 91.05 ±0.2 65.05 ±4.2 64.95 ±4.1', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Question Answering', 'Speech and Multimodality', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09752823412418365, 0.09389792382717133, 0.07265113294124603, 0.05758365988731384, 0.05668092519044876, 0.05324554070830345, 0.04946628212928772, 0.048494961112737656, 0.04762711375951767, 0.04666999727487564, 0.04616927728056908, 0.04311639070510864, 0.0369081124663353, 0.036902617663145065, 0.03495606407523155, 0.03149838373064995, 0.027960998937487602, 0.02427401952445507, 0.023831989616155624, 0.02311370149254799, 0.022151367738842964, 0.014921500347554684, 0.010349899530410767]}",0.09752823412418365,NLP Applications,0.05668092519044876
Interpretability and Analysis of Models for NLP,Adversarial Learning of Poisson Factorisation Model for Gauging Brand Sentiment in User Reviews,"In this paper, we propose the Brand-Topic Model (BTM) which aims to detect brandassociated polarity-bearing topics from product reviews. Different from existing models for sentiment-topic extraction which assume topics are grouped under discrete sentiment categories such as 'positive', 'negative' and 'neural', BTM is able to automatically infer real-valued brand-associated sentiment scores and generate fine-grained sentiment-topics in which we can observe continuous changes of words under a certain topic (e.g., 'shaver' or 'cream') while its associated sentiment gradually varies from negative to positive. BTM is built on the Poisson factorisation model with the incorporation of adversarial learning. It has been evaluated on a dataset constructed from Amazon reviews. Experimental results show that BTM outperforms a number of competitive baselines in brand ranking, achieving a better balance of topic coherence and uniqueness, and extracting better-separated polaritybearing topics.","{'sequence': ""In this paper, we propose the Brand-Topic Model (BTM) which aims to detect brandassociated polarity-bearing topics from product reviews. Different from existing models for sentiment-topic extraction which assume topics are grouped under discrete sentiment categories such as 'positive', 'negative' and 'neural', BTM is able to automatically infer real-valued brand-associated sentiment scores and generate fine-grained sentiment-topics in which we can observe continuous changes of words under a certain topic (e.g., 'shaver' or 'cream') while its associated sentiment gradually varies from negative to positive. BTM is built on the Poisson factorisation model with the incorporation of adversarial learning. It has been evaluated on a dataset constructed from Amazon reviews. Experimental results show that BTM outperforms a number of competitive baselines in brand ranking, achieving a better balance of topic coherence and uniqueness, and extracting better-separated polaritybearing topics."", 'labels': ['Information Extraction', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Question Answering', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09485834836959839, 0.09078169614076614, 0.07774432748556137, 0.068557009100914, 0.059258416295051575, 0.05753990635275841, 0.05436289682984352, 0.04106329008936882, 0.038754902780056, 0.03716067224740982, 0.037079308182001114, 0.034993309527635574, 0.03430460765957832, 0.03265108913183212, 0.032635319977998734, 0.03122406080365181, 0.03025098517537117, 0.029606740921735764, 0.028694147244095802, 0.02607298456132412, 0.02561832219362259, 0.021736403927206993, 0.015051285736262798]}",0.09485834836959839,Information Extraction,0.03122406080365181
Interpretability and Analysis of Models for NLP,Hidden Biases in Unreliable News Detection Datasets,"Automatic unreliable news detection is a research problem with great potential impact. Recently, several papers have shown promising results on large-scale news datasets with models that only use the article itself without resorting to any fact-checking mechanism or retrieving any supporting evidence. In this work, we take a closer look at these datasets. While they all provide valuable resources for future research, we observe a number of problems that may lead to results that do not generalize in more realistic settings. Specifically, we show that selection bias during data collection leads to undesired artifacts in the datasets. In addition, while most systems train and predict at the level of individual articles, overlapping article sources in the training and evaluation data can provide a strong confounding factor that models can exploit. In the presence of this confounding factor, the models can achieve good performance by directly memorizing the site-label mapping instead of modeling the real task of unreliable news detection. We observed a significant drop (>10%) in accuracy for all models tested in a clean split with no train/test source overlap. Using the observations and experimental results, we provide practical suggestions on how to create more reliable datasets for the unreliable news detection task. We suggest future dataset creation include a simple model as a difficulty/bias probe and future model development use a clean non-overlapping site and date split. 1","{'sequence': 'Automatic unreliable news detection is a research problem with great potential impact. Recently, several papers have shown promising results on large-scale news datasets with models that only use the article itself without resorting to any fact-checking mechanism or retrieving any supporting evidence. In this work, we take a closer look at these datasets. While they all provide valuable resources for future research, we observe a number of problems that may lead to results that do not generalize in more realistic settings. Specifically, we show that selection bias during data collection leads to undesired artifacts in the datasets. In addition, while most systems train and predict at the level of individual articles, overlapping article sources in the training and evaluation data can provide a strong confounding factor that models can exploit. In the presence of this confounding factor, the models can achieve good performance by directly memorizing the site-label mapping instead of modeling the real task of unreliable news detection. We observed a significant drop (>10%) in accuracy for all models tested in a clean split with no train/test source overlap. Using the observations and experimental results, we provide practical suggestions on how to create more reliable datasets for the unreliable news detection task. We suggest future dataset creation include a simple model as a difficulty/bias probe and future model development use a clean non-overlapping site and date split. 1', 'labels': ['Question Answering', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Information Extraction', 'Resources and Evaluation', 'Generation', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Summarization', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0739157497882843, 0.06941340118646622, 0.06908242404460907, 0.06595783680677414, 0.06520131230354309, 0.054288871586322784, 0.05151788145303726, 0.04986795783042908, 0.04862361401319504, 0.04659365862607956, 0.04163656756281853, 0.039817314594984055, 0.037677399814128876, 0.0376218780875206, 0.035634975880384445, 0.03388945385813713, 0.0324743278324604, 0.032071422785520554, 0.028052331879734993, 0.025126777589321136, 0.02402060106396675, 0.020084965974092484, 0.017429249361157417]}",0.0739157497882843,Question Answering,0.032071422785520554
Interpretability and Analysis of Models for NLP,Adaptive Mixed Component LDA for Low Resource Topic Modeling,"Probabilistic topic models in low data resource scenarios are faced with less reliable estimates due to sparsity of discrete word cooccurrence counts, and do not have the luxury of retraining word or topic embeddings using neural methods. In this challenging resource constrained setting, we introduce an automatic trade-off between the discrete and continuous representations via an adaptive mixture coefficient, which places greater weight on the discrete representation when the corpus statistics are more reliable. The adaptive mixture coefficient takes into account global corpus statistics, and the uncertainty in each topic's continuous distribution. Our approach outperforms the fully discrete, fully continuous, and static mixture model on topic coherence in low resource monolingual and multilingual settings.","{'sequence': ""Probabilistic topic models in low data resource scenarios are faced with less reliable estimates due to sparsity of discrete word cooccurrence counts, and do not have the luxury of retraining word or topic embeddings using neural methods. In this challenging resource constrained setting, we introduce an automatic trade-off between the discrete and continuous representations via an adaptive mixture coefficient, which places greater weight on the discrete representation when the corpus statistics are more reliable. The adaptive mixture coefficient takes into account global corpus statistics, and the uncertainty in each topic's continuous distribution. Our approach outperforms the fully discrete, fully continuous, and static mixture model on topic coherence in low resource monolingual and multilingual settings."", 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Generation', 'Semantics: Lexical Semantics', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Summarization', 'Ethics and NLP', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07223150879144669, 0.07159540057182312, 0.06855803728103638, 0.0614376924932003, 0.057974882423877716, 0.056380435824394226, 0.054710883647203445, 0.053648777306079865, 0.05124159902334213, 0.04524868726730347, 0.04251371696591377, 0.04109417274594307, 0.0386967770755291, 0.03613000735640526, 0.03482880815863609, 0.034302931278944016, 0.030736222863197327, 0.027622075751423836, 0.025568481534719467, 0.025345785543322563, 0.024795914068818092, 0.023301459848880768, 0.02203565277159214]}",0.07223150879144669,Speech and Multimodality,0.04524868726730347
Interpretability and Analysis of Models for NLP,Graph-based Fake News Detection using a Summarization Technique,"Nowadays, fake news is spreading in various ways, and this fake information is causing a lot of social damages. Thus the need to detect fake information is increasing to prevent the damages caused by fake news. In this paper, we propose a novel graph-based fake news detection method using a summarization technique that uses only the document internal information. Our proposed method represents the relationship between all sentences using a graph and the reflection rate of contextual information among sentences is computed by using an attention mechanism. In addition, we improve the performance of fake news detection by utilizing summary information as an important subject of the document.The experimental results demonstrate that our method achieves high accuracy, 91.04%, that is 8.85%p better than the previous method.","{'sequence': 'Nowadays, fake news is spreading in various ways, and this fake information is causing a lot of social damages. Thus the need to detect fake information is increasing to prevent the damages caused by fake news. In this paper, we propose a novel graph-based fake news detection method using a summarization technique that uses only the document internal information. Our proposed method represents the relationship between all sentences using a graph and the reflection rate of contextual information among sentences is computed by using an attention mechanism. In addition, we improve the performance of fake news detection by utilizing summary information as an important subject of the document.The experimental results demonstrate that our method achieves high accuracy, 91.04%, that is 8.85%p better than the previous method.', 'labels': ['Summarization', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Computational Social Science and Social Media', 'Ethics and NLP', 'NLP Applications', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Question Answering', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.3497544825077057, 0.054901763796806335, 0.047618500888347626, 0.046750761568546295, 0.04575154185295105, 0.03998879715800285, 0.03448965772986412, 0.034315671771764755, 0.03409303352236748, 0.03239503875374794, 0.030994340777397156, 0.03068353235721588, 0.030546804890036583, 0.02941715717315674, 0.025635186582803726, 0.0246394332498312, 0.022673320025205612, 0.019064605236053467, 0.018351109698414803, 0.014278837479650974, 0.012678965926170349, 0.012596715241670609, 0.008380689658224583]}",0.3497544825077057,Summarization,0.014278837479650974
Interpretability and Analysis of Models for NLP,ProFormer: Towards On-Device LSH Projection Based Transformers,"At the heart of text based neural models lay word representations, which are powerful but occupy a lot of memory making it challenging to deploy to devices with memory constraints such as mobile phones, watches and IoT. To surmount these challenges, we introduce ProFormer -a projection based transformer architecture that is faster and lighter making it suitable to deploy to memory constraint devices and preserve user privacy. We use LSH projection layer to dynamically generate word representations on-the-fly without embedding lookup tables leading to significant memory footprint reduction from O(V.d) to O(T ), where V is the vocabulary size, d is the embedding dimension size and T is the dimension of the LSH projection representation. We also propose a local projection attention (LPA) layer, which uses self-attention to transform the input sequence of N LSH word projections into a sequence of N/K representations reducing the computations quadratically by O(K 2 ). We evaluate ProFormer on multiple text classification tasks and observed improvements over prior state-of-the-art on-device approaches for short text classification and comparable performance for long text classification tasks. Pro-Former is also competitive with other popular but highly resource-intensive approaches like BERT and even outperforms small-sized BERT variants with significant resource savings -reduces the embedding memory footprint from 92.16 MB to 1.7 KB and requires 16× less computation overhead, which is very impressive making it the fastest and smallest on-device model.","{'sequence': 'At the heart of text based neural models lay word representations, which are powerful but occupy a lot of memory making it challenging to deploy to devices with memory constraints such as mobile phones, watches and IoT. To surmount these challenges, we introduce ProFormer -a projection based transformer architecture that is faster and lighter making it suitable to deploy to memory constraint devices and preserve user privacy. We use LSH projection layer to dynamically generate word representations on-the-fly without embedding lookup tables leading to significant memory footprint reduction from O(V.d) to O(T ), where V is the vocabulary size, d is the embedding dimension size and T is the dimension of the LSH projection representation. We also propose a local projection attention (LPA) layer, which uses self-attention to transform the input sequence of N LSH word projections into a sequence of N/K representations reducing the computations quadratically by O(K 2 ). We evaluate ProFormer on multiple text classification tasks and observed improvements over prior state-of-the-art on-device approaches for short text classification and comparable performance for long text classification tasks. Pro-Former is also competitive with other popular but highly resource-intensive approaches like BERT and even outperforms small-sized BERT variants with significant resource savings -reduces the embedding memory footprint from 92.16 MB to 1.7 KB and requires 16× less computation overhead, which is very impressive making it the fastest and smallest on-device model.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Generation', 'Information Extraction', 'Question Answering', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Ethics and NLP', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24312295019626617, 0.0783921629190445, 0.07460560649633408, 0.05944276228547096, 0.054562464356422424, 0.04163398593664169, 0.03710763528943062, 0.03707043454051018, 0.03612823039293289, 0.03200128301978111, 0.030892206355929375, 0.030599698424339294, 0.03014419972896576, 0.029618965461850166, 0.028797797858715057, 0.02505810372531414, 0.022785376757383347, 0.020783664658665657, 0.020633330568671227, 0.020577281713485718, 0.018264757469296455, 0.016428226605057716, 0.01134887058287859]}",0.24312295019626617,Resources and Evaluation,0.03014419972896576
Interpretability and Analysis of Models for NLP,Detecting Extraneous Content in Podcasts,"Podcast episodes often contain material extraneous to the main content, such as advertisements, interleaved within the audio and the written descriptions. We present classifiers that leverage both textual and listening patterns in order to detect such content in podcast descriptions and audio transcripts. We demonstrate that our models are effective by evaluating them on the downstream task of podcast summarization and show that we can substantively improve ROUGE scores and reduce the extraneous content generated in the summaries.","{'sequence': 'Podcast episodes often contain material extraneous to the main content, such as advertisements, interleaved within the audio and the written descriptions. We present classifiers that leverage both textual and listening patterns in order to detect such content in podcast descriptions and audio transcripts. We demonstrate that our models are effective by evaluating them on the downstream task of podcast summarization and show that we can substantively improve ROUGE scores and reduce the extraneous content generated in the summaries.', 'labels': ['Summarization', 'Speech and Multimodality', 'Resources and Evaluation', 'Information Extraction', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Generation', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.35517778992652893, 0.06997372210025787, 0.0663939118385315, 0.048100851476192474, 0.04033038020133972, 0.039769496768713, 0.033619027584791183, 0.032810576260089874, 0.03085271455347538, 0.030799679458141327, 0.029200846329331398, 0.026760686188936234, 0.025013932958245277, 0.022486601024866104, 0.02179565280675888, 0.02120685763657093, 0.02036881446838379, 0.017881495878100395, 0.01767450012266636, 0.015714183449745178, 0.01379617303609848, 0.01219919417053461, 0.008072912693023682]}",0.35517778992652893,Summarization,0.02036881446838379
Interpretability and Analysis of Models for NLP,Combining Deep Generative Models and Multi-lingual Pretraining for Semi-supervised Document Classification,"Semi-supervised learning through deep generative models and multi-lingual pretraining techniques have orchestrated tremendous success across different areas of NLP. Nonetheless, their development has happened in isolation, while the combination of both could potentially be effective for tackling task-specific labelled data shortage. To bridge this gap, we combine semi-supervised deep generative models and multi-lingual pretraining to form a pipeline for document classification task. Compared to strong supervised learning baselines, our semi-supervised classification framework is highly competitive and outperforms the state-of-the-art counterparts in lowresource settings across several languages. 1","{'sequence': 'Semi-supervised learning through deep generative models and multi-lingual pretraining techniques have orchestrated tremendous success across different areas of NLP. Nonetheless, their development has happened in isolation, while the combination of both could potentially be effective for tackling task-specific labelled data shortage. To bridge this gap, we combine semi-supervised deep generative models and multi-lingual pretraining to form a pipeline for document classification task. Compared to strong supervised learning baselines, our semi-supervised classification framework is highly competitive and outperforms the state-of-the-art counterparts in lowresource settings across several languages. 1', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Summarization', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.35167622566223145, 0.08152738958597183, 0.07600575685501099, 0.05103263631463051, 0.04143155738711357, 0.035762298852205276, 0.033391788601875305, 0.03307025879621506, 0.032399460673332214, 0.026152627542614937, 0.025208907201886177, 0.02366585284471512, 0.023470483720302582, 0.02125883661210537, 0.019667882472276688, 0.019393712282180786, 0.01851200871169567, 0.018349071964621544, 0.01608702540397644, 0.014478122815489769, 0.014430206269025803, 0.012861392460763454, 0.010166408494114876]}",0.35167622566223145,Machine Learning for NLP,0.05103263631463051
Interpretability and Analysis of Models for NLP,Event-Driven News Stream Clustering using Entity-Aware Contextual Embeddings,"We propose a method for online news stream clustering that is a variant of the nonparametric streaming K-means algorithm. Our model uses a combination of sparse and dense document representations, aggregates document-cluster similarity along these multiple representations and makes the clustering decision using a neural classifier. The weighted document-cluster similarity model is learned using a novel adaptation of the triplet loss into a linear classification objective. We show that the use of a suitable fine-tuning objective and external knowledge in pre-trained transformer models yields significant improvements in the effectiveness of contextual embeddings for clustering. Our model achieves a new state-of-the-art on a standard stream clustering dataset of English documents.","{'sequence': 'We propose a method for online news stream clustering that is a variant of the nonparametric streaming K-means algorithm. Our model uses a combination of sparse and dense document representations, aggregates document-cluster similarity along these multiple representations and makes the clustering decision using a neural classifier. The weighted document-cluster similarity model is learned using a novel adaptation of the triplet loss into a linear classification objective. We show that the use of a suitable fine-tuning objective and external knowledge in pre-trained transformer models yields significant improvements in the effectiveness of contextual embeddings for clustering. Our model achieves a new state-of-the-art on a standard stream clustering dataset of English documents.', 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Computational Social Science and Social Media', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Machine Learning for NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Generation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'NLP Applications', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08255743235349655, 0.07682274281978607, 0.06892530620098114, 0.06492019444704056, 0.057026542723178864, 0.056405991315841675, 0.04949966445565224, 0.048846807330846786, 0.04771340638399124, 0.040223632007837296, 0.03976065292954445, 0.03910454735159874, 0.03864942491054535, 0.038130711764097214, 0.0372154526412487, 0.03682325780391693, 0.03482898324728012, 0.03344837948679924, 0.03103525936603546, 0.027703292667865753, 0.018235711380839348, 0.01773749105632305, 0.014385107904672623]}",0.08255743235349655,Dialogue and Interactive Systems,0.04949966445565224
Interpretability and Analysis of Models for NLP,One-class Text Classification with Multi-modal Deep Support Vector Data Description,"This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated.","{'sequence': 'This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'NLP Applications', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11831173300743103, 0.07947085797786713, 0.06355220079421997, 0.06018446013331413, 0.05770739167928696, 0.0547499805688858, 0.047283004969358444, 0.046736735850572586, 0.0464136116206646, 0.045134592801332474, 0.038821544498205185, 0.0352218858897686, 0.0342024490237236, 0.03409675508737564, 0.03291384503245354, 0.03261937201023102, 0.030682813376188278, 0.02684103325009346, 0.026677049696445465, 0.026470312848687172, 0.02495255321264267, 0.018586356192827225, 0.01836954429745674]}",0.11831173300743103,Speech and Multimodality,0.0464136116206646
Summarization,Civil Rephrases Of Toxic Texts With Self-Supervised Transformers,"Platforms that support online commentary, from social networks to news sites, are increasingly leveraging machine learning to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for human moderators to do, and computational approaches are still nascent. This work focuses on models that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-tosequence tasks, a self-supervised learning model is introduced, called CAE-T5 1 . CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.","{'sequence': 'Platforms that support online commentary, from social networks to news sites, are increasingly leveraging machine learning to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for human moderators to do, and computational approaches are still nascent. This work focuses on models that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-tosequence tasks, a self-supervised learning model is introduced, called CAE-T5 1 . CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.', 'labels': ['Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Information Extraction', 'Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Question Answering', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07728476077318192, 0.07551652938127518, 0.06822304427623749, 0.06661195307970047, 0.06454743444919586, 0.06435069441795349, 0.04989289492368698, 0.04909275472164154, 0.04737097769975662, 0.040676798671483994, 0.0395120270550251, 0.03886426240205765, 0.038285333663225174, 0.03690240532159805, 0.0342281311750412, 0.0332024022936821, 0.030604839324951172, 0.029833970591425896, 0.029390672221779823, 0.025235753506422043, 0.023946603760123253, 0.020588217303156853, 0.015837624669075012]}",0.07728476077318192,Computational Social Science and Social Media,0.03886426240205765
Summarization,Entity-level Factual Consistency of Abstractive Text Summarization,"A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-ofthe-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.","{'sequence': 'A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-ofthe-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.', 'labels': ['Summarization', 'Generation', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.5477301478385925, 0.06486469507217407, 0.03455822914838791, 0.0327591709792614, 0.02924884483218193, 0.024935578927397728, 0.022198669612407684, 0.021971089765429497, 0.021066511049866676, 0.020876025781035423, 0.018927721306681633, 0.018768522888422012, 0.018107129260897636, 0.017330829054117203, 0.014991245232522488, 0.01380791887640953, 0.01335469912737608, 0.013277042657136917, 0.012873899191617966, 0.0125391511246562, 0.00986899621784687, 0.00881245918571949, 0.007131420541554689]}",0.5477301478385925,Summarization,0.5477301478385925
Summarization,Unsupervised Abstractive Summarization of Bengali Text Documents,"Abstractive summarization systems generally rely on large collections of documentsummary pairs. However, the performance of abstractive systems remains a challenge due to the unavailability of the parallel data for low-resource languages like Bengali. To overcome this problem, we propose a graph-based unsupervised abstractive summarization system in the single-document setting for Bengali text documents, which requires only a Part-Of-Speech (POS) tagger and a pre-trained language model trained on Bengali texts. We also provide a human-annotated dataset with document-summary pairs to evaluate our abstractive model and to support the comparison of future abstractive summarization systems of the Bengali Language. We conduct experiments on this dataset and compare our system with several well-established unsupervised extractive summarization systems. Our unsupervised abstractive summarization model outperforms the baselines without being exposed to any human-annotated reference summaries. 1 * Equal contribution. Listed by alphabetical order. 1 We make our code & dataset publicly available at https://github.com/tafseer-nayeem/ BengaliSummarization for reproduciblity.","{'sequence': 'Abstractive summarization systems generally rely on large collections of documentsummary pairs. However, the performance of abstractive systems remains a challenge due to the unavailability of the parallel data for low-resource languages like Bengali. To overcome this problem, we propose a graph-based unsupervised abstractive summarization system in the single-document setting for Bengali text documents, which requires only a Part-Of-Speech (POS) tagger and a pre-trained language model trained on Bengali texts. We also provide a human-annotated dataset with document-summary pairs to evaluate our abstractive model and to support the comparison of future abstractive summarization systems of the Bengali Language. We conduct experiments on this dataset and compare our system with several well-established unsupervised extractive summarization systems. Our unsupervised abstractive summarization model outperforms the baselines without being exposed to any human-annotated reference summaries. 1 * Equal contribution. Listed by alphabetical order. 1 We make our code & dataset publicly available at https://github.com/tafseer-nayeem/ BengaliSummarization for reproduciblity.', 'labels': ['Summarization', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Information Extraction', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.23793263733386993, 0.10352182388305664, 0.06522729992866516, 0.061319269239902496, 0.051618028432130814, 0.040002185851335526, 0.03930274024605751, 0.03621324524283409, 0.03521047160029411, 0.03273998945951462, 0.031890951097011566, 0.030623313039541245, 0.030524766072630882, 0.03008544072508812, 0.024310225620865822, 0.02262103371322155, 0.021744446828961372, 0.0205118078738451, 0.019394047558307648, 0.017741326242685318, 0.01688716746866703, 0.015517388470470905, 0.015060354955494404]}",0.23793263733386993,Summarization,0.23793263733386993
Summarization,StructSum: Summarization via Structured Representations,"Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoderdecoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines. 1","{'sequence': 'Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoderdecoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines. 1', 'labels': ['Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Generation', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Ethics and NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.139714315533638, 0.07463020831346512, 0.07043657451868057, 0.054627519100904465, 0.0536176972091198, 0.04799898341298103, 0.04649128019809723, 0.045132193714380264, 0.04460074007511139, 0.044455643743276596, 0.04262973368167877, 0.038857802748680115, 0.03531922399997711, 0.031632207334041595, 0.031229516491293907, 0.030598241835832596, 0.028797060251235962, 0.02805117703974247, 0.024380072951316833, 0.023167235776782036, 0.021881364285945892, 0.021106645464897156, 0.020644567906856537]}",0.139714315533638,Summarization,0.139714315533638
Generation,Contrastive Multi-document Question Generation,"Multi-document question generation focuses on generating a question that covers the common aspect of multiple documents. Such a model is useful in generating clarifying options. However, a naive model trained only using the targeted (""positive"") document set may generate too generic questions that cover a larger scope than delineated by the document set. To address this challenge, we introduce the contrastive learning strategy where given ""positive"" and ""negative"" sets of documents, we generate a question that is closely related to the ""positive"" set but is far away from the ""negative"" set. This setting allows generated questions to be more specific and related to the target document set. To generate such specific questions, we propose Multi-Source Coordinated Question Generator (MSCQG), a novel framework that includes a supervised learning (SL) stage and a reinforcement learning (RL) stage. In the SL stage, a singledocument question generator is trained. In the RL stage, a coordinator model is trained to find optimal attention weights to align multiple single-document generators, by optimizing a reward designed to promote specificity of generated questions. We also develop an effective auxiliary objective, named Setinduced Contrastive Regularization (SCR) that improves the coordinator's contrastive learning during the RL stage. We show that our model significantly outperforms several strong baselines, as measured by automatic metrics and human evaluation. The source repository is publicly available at www.github.com/ woonsangcho/contrast_qgen.","{'sequence': 'Multi-document question generation focuses on generating a question that covers the common aspect of multiple documents. Such a model is useful in generating clarifying options. However, a naive model trained only using the targeted (""positive"") document set may generate too generic questions that cover a larger scope than delineated by the document set. To address this challenge, we introduce the contrastive learning strategy where given ""positive"" and ""negative"" sets of documents, we generate a question that is closely related to the ""positive"" set but is far away from the ""negative"" set. This setting allows generated questions to be more specific and related to the target document set. To generate such specific questions, we propose Multi-Source Coordinated Question Generator (MSCQG), a novel framework that includes a supervised learning (SL) stage and a reinforcement learning (RL) stage. In the SL stage, a singledocument question generator is trained. In the RL stage, a coordinator model is trained to find optimal attention weights to align multiple single-document generators, by optimizing a reward designed to promote specificity of generated questions. We also develop an effective auxiliary objective, named Setinduced Contrastive Regularization (SCR) that improves the coordinator\'s contrastive learning during the RL stage. We show that our model significantly outperforms several strong baselines, as measured by automatic metrics and human evaluation. The source repository is publicly available at www.github.com/ woonsangcho/contrast_qgen.', 'labels': ['Generation', 'Speech and Multimodality', 'Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.18617980182170868, 0.06948694586753845, 0.06169991195201874, 0.06071142107248306, 0.05831340700387955, 0.05756474286317825, 0.0437958762049675, 0.04029100760817528, 0.03804390877485275, 0.036177653819322586, 0.035503655672073364, 0.033410992473363876, 0.032273970544338226, 0.03162187337875366, 0.03130469098687172, 0.028677692636847496, 0.027130981907248497, 0.025889037176966667, 0.023528477177023888, 0.022833438590168953, 0.020165808498859406, 0.019015705212950706, 0.016379015520215034]}",0.18617980182170868,Generation,0.18617980182170868
Summarization,Extractive Summarization Considering Discourse and Coreference Relations based on Heterogeneous Graph,"Modeling the relations between text spans in a document is a crucial yet challenging problem for extractive summarization. Various kinds of relations exist among text spans of different granularity, such as discourse relations between elementary discourse units and coreference relations between phrase mentions. In this paper, we propose a heterogeneous graph based model for extractive summarization that incorporates both discourse and coreference relations. The heterogeneous graph contains three types of nodes, each corresponds to text spans of different granularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method.","{'sequence': 'Modeling the relations between text spans in a document is a crucial yet challenging problem for extractive summarization. Various kinds of relations exist among text spans of different granularity, such as discourse relations between elementary discourse units and coreference relations between phrase mentions. In this paper, we propose a heterogeneous graph based model for extractive summarization that incorporates both discourse and coreference relations. The heterogeneous graph contains three types of nodes, each corresponds to text spans of different granularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method.', 'labels': ['Summarization', 'Speech and Multimodality', 'Information Extraction', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Generation', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13428154587745667, 0.0888306051492691, 0.0807485282421112, 0.06596209853887558, 0.05931306257843971, 0.05407121032476425, 0.05321532487869263, 0.04902775213122368, 0.0415070615708828, 0.04003138467669487, 0.03914099931716919, 0.03687534108757973, 0.031928256154060364, 0.029282789677381516, 0.028341177850961685, 0.026938026770949364, 0.02601240575313568, 0.024884918704628944, 0.02452186681330204, 0.024204827845096588, 0.01563323102891445, 0.015535824000835419, 0.00971178524196148]}",0.13428154587745667,Summarization,0.13428154587745667
Generation,Generating Weather Comments from Meteorological Simulations,"The task of generating weather-forecast comments from meteorological simulations has the following requirements: (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a datato-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing weather information, such as sunny and rain, for our model to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our model performed best against baselines in terms of informativeness. We make our code and data publicly available 1 .","{'sequence': 'The task of generating weather-forecast comments from meteorological simulations has the following requirements: (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a datato-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing weather information, such as sunny and rain, for our model to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our model performed best against baselines in terms of informativeness. We make our code and data publicly available 1 .', 'labels': ['Resources and Evaluation', 'Generation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Summarization', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.13452275097370148, 0.08850574493408203, 0.08750934153795242, 0.0782235860824585, 0.07237128913402557, 0.06647194921970367, 0.061718281358480453, 0.05815922096371651, 0.043942976742982864, 0.0353846475481987, 0.03538253903388977, 0.0330461710691452, 0.031858623027801514, 0.028963422402739525, 0.02556910738348961, 0.019920717924833298, 0.018805690109729767, 0.018250223249197006, 0.014734123833477497, 0.014471432194113731, 0.011747647076845169, 0.011615931987762451, 0.008824465796351433]}",0.13452275097370148,Resources and Evaluation,0.08850574493408203
Summarization,Discourse Understanding and Factual Consistency in Abstractive Summarization,"We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator -Discriminator Networks (Co-opNet), a novel transformerbased framework where a generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold longform scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.","{'sequence': 'We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator -Discriminator Networks (Co-opNet), a novel transformerbased framework where a generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold longform scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.', 'labels': ['Summarization', 'Generation', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.20723502337932587, 0.0971483439207077, 0.07344217598438263, 0.06938082724809647, 0.060462836176157, 0.055487629026174545, 0.043574392795562744, 0.039338380098342896, 0.03881504759192467, 0.03355131298303604, 0.03270114213228226, 0.030207442119717598, 0.0298050194978714, 0.029467687010765076, 0.027506409212946892, 0.026710564270615578, 0.02528379298746586, 0.018726974725723267, 0.014662466943264008, 0.013390748761594296, 0.013283266685903072, 0.011068897321820259, 0.008749582804739475]}",0.20723502337932587,Summarization,0.20723502337932587
Generation,Neural-Driven Search-Based Paraphrase Generation,"We study a search-based paraphrase generation scheme where candidate paraphrases are generated by iterated transformations from the original sentence and evaluated in terms of syntax quality, semantic distance, and lexical distance. The semantic distance is derived from BERT, and the lexical quality is based on GPT2 perplexity. To solve this multi-objective search problem, we propose two algorithms: Monte-Carlo Tree Search For Paraphrase Generation (MCPG) and Pareto Tree Search (PTS). We provide an extensive set of experiments on 5 datasets with a rigorous reproduction and validation for several state-of-the-art paraphrase generation algorithms. These experiments show that, although being non explicitly supervised, our algorithms perform well against these baselines.","{'sequence': 'We study a search-based paraphrase generation scheme where candidate paraphrases are generated by iterated transformations from the original sentence and evaluated in terms of syntax quality, semantic distance, and lexical distance. The semantic distance is derived from BERT, and the lexical quality is based on GPT2 perplexity. To solve this multi-objective search problem, we propose two algorithms: Monte-Carlo Tree Search For Paraphrase Generation (MCPG) and Pareto Tree Search (PTS). We provide an extensive set of experiments on 5 datasets with a rigorous reproduction and validation for several state-of-the-art paraphrase generation algorithms. These experiments show that, although being non explicitly supervised, our algorithms perform well against these baselines.', 'labels': ['Generation', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Dialogue and Interactive Systems', 'Summarization', 'NLP Applications', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2614145278930664, 0.09000372141599655, 0.06926315277814865, 0.06497432291507721, 0.049224309623241425, 0.04531349241733551, 0.04272399842739105, 0.04117334261536598, 0.03744721785187721, 0.030924150720238686, 0.0279909186065197, 0.027904488146305084, 0.026925012469291687, 0.02539561316370964, 0.02466757409274578, 0.023438261821866035, 0.023239824920892715, 0.01886332407593727, 0.01677021197974682, 0.01584044098854065, 0.014984971843659878, 0.01379221212118864, 0.007724822033196688]}",0.2614145278930664,Generation,0.2614145278930664
Generation,Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning,"Recent advancements in data-to-text generation largely take on the form of neural end-toend systems. Efforts have been dedicated to improving text generation systems by changing the order of training samples in a process known as curriculum learning. Past research on sequence-to-sequence learning showed that curriculum learning helps to improve both the performance and convergence speed. In this work, we delve into the same idea surrounding the training samples consisting of structured data and text pairs, where at each update, the curriculum framework selects training samples based on the model's competence. Specifically, we experiment with various difficulty metrics and put forward a soft edit distance metric for ranking training samples. Our benchmarks show faster convergence speed where training time is reduced by 38.7% and performance is boosted by 4.84 BLEU.","{'sequence': ""Recent advancements in data-to-text generation largely take on the form of neural end-toend systems. Efforts have been dedicated to improving text generation systems by changing the order of training samples in a process known as curriculum learning. Past research on sequence-to-sequence learning showed that curriculum learning helps to improve both the performance and convergence speed. In this work, we delve into the same idea surrounding the training samples consisting of structured data and text pairs, where at each update, the curriculum framework selects training samples based on the model's competence. Specifically, we experiment with various difficulty metrics and put forward a soft edit distance metric for ranking training samples. Our benchmarks show faster convergence speed where training time is reduced by 38.7% and performance is boosted by 4.84 BLEU."", 'labels': ['Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Summarization'], 'scores': [0.2575712502002716, 0.07211264967918396, 0.06358566880226135, 0.04622114822268486, 0.04385516047477722, 0.041376929730176926, 0.03603648766875267, 0.035869937390089035, 0.0355587936937809, 0.03454500436782837, 0.03304101899266243, 0.031681135296821594, 0.03005189262330532, 0.028516333550214767, 0.0280854981392622, 0.02792864292860031, 0.02606840617954731, 0.024804437533020973, 0.02459845505654812, 0.021063540130853653, 0.020768823102116585, 0.01936609484255314, 0.017292656004428864]}",0.2575712502002716,Generation,0.2575712502002716
Summarization,Unsupervised Extractive Summarization using Pointwise Mutual Information,"Unsupervised approaches to extractive summarization usually rely on a notion of sentence importance defined by the semantic similarity between a sentence and the document. We propose new metrics of relevance and redundancy using pointwise mutual information (PMI) between sentences, which can be easily computed by a pre-trained language model. Intuitively, a relevant sentence allows readers to infer the document content (high PMI with the document), and a redundant sentence can be inferred from the summary (high PMI with the summary). We then develop a greedy sentence selection algorithm to maximize relevance and minimize redundancy of extracted sentences. We show that our method outperforms similarity-based methods on datasets in a range of domains including news, medical journal articles, and personal anecdotes.","{'sequence': 'Unsupervised approaches to extractive summarization usually rely on a notion of sentence importance defined by the semantic similarity between a sentence and the document. We propose new metrics of relevance and redundancy using pointwise mutual information (PMI) between sentences, which can be easily computed by a pre-trained language model. Intuitively, a relevant sentence allows readers to infer the document content (high PMI with the document), and a redundant sentence can be inferred from the summary (high PMI with the summary). We then develop a greedy sentence selection algorithm to maximize relevance and minimize redundancy of extracted sentences. We show that our method outperforms similarity-based methods on datasets in a range of domains including news, medical journal articles, and personal anecdotes.', 'labels': ['Summarization', 'Information Extraction', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Question Answering', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Generation', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.18638408184051514, 0.12353087961673737, 0.06859638541936874, 0.06607148796319962, 0.0642305463552475, 0.043810807168483734, 0.0398433580994606, 0.03881232440471649, 0.038448575884103775, 0.03695131465792656, 0.035169679671525955, 0.03344067558646202, 0.03235812857747078, 0.031278468668460846, 0.02683878503739834, 0.02133330889046192, 0.020460378378629684, 0.02011919766664505, 0.019940460100769997, 0.017266416922211647, 0.016670262441039085, 0.00953121017664671, 0.008913276717066765]}",0.18638408184051514,Summarization,0.18638408184051514
Generation,Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs,"Paraphrase generation plays an essential role in natural language process (NLP), and it has many downstream applications. However, training supervised paraphrase models requires many annotated paraphrase pairs, which are usually costly to obtain. On the other hand, the paraphrases generated by existing unsupervised approaches are usually syntactically similar to the source sentences and are limited in diversity. In this paper, we demonstrate that it is possible to generate syntactically various paraphrases without the need for annotated paraphrase pairs. We propose Syntactically controlled Paraphrase Generator (SynPG), an encoder-decoder based model that learns to disentangle the semantics and the syntax of a sentence from a collection of unannotated texts. The disentanglement enables SynPG to control the syntax of output paraphrases by manipulating the embedding in the syntactic space. Extensive experiments using automatic metrics and human evaluation show that SynPG performs better syntactic control than unsupervised baselines, while the quality of the generated paraphrases is competitive. We also demonstrate that the performance of SynPG is competitive or even better than supervised models when the unannotated data is large. Finally, we show that the syntactically controlled paraphrases generated by SynPG can be utilized for data augmentation to improve the robustness of NLP models.","{'sequence': 'Paraphrase generation plays an essential role in natural language process (NLP), and it has many downstream applications. However, training supervised paraphrase models requires many annotated paraphrase pairs, which are usually costly to obtain. On the other hand, the paraphrases generated by existing unsupervised approaches are usually syntactically similar to the source sentences and are limited in diversity. In this paper, we demonstrate that it is possible to generate syntactically various paraphrases without the need for annotated paraphrase pairs. We propose Syntactically controlled Paraphrase Generator (SynPG), an encoder-decoder based model that learns to disentangle the semantics and the syntax of a sentence from a collection of unannotated texts. The disentanglement enables SynPG to control the syntax of output paraphrases by manipulating the embedding in the syntactic space. Extensive experiments using automatic metrics and human evaluation show that SynPG performs better syntactic control than unsupervised baselines, while the quality of the generated paraphrases is competitive. We also demonstrate that the performance of SynPG is competitive or even better than supervised models when the unannotated data is large. Finally, we show that the syntactically controlled paraphrases generated by SynPG can be utilized for data augmentation to improve the robustness of NLP models.', 'labels': ['NLP Applications', 'Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Information Extraction', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24679213762283325, 0.11753484606742859, 0.05647196248173714, 0.04735875874757767, 0.04595339670777321, 0.0425330214202404, 0.042206212878227234, 0.039558589458465576, 0.03713752701878548, 0.03687865287065506, 0.03301829844713211, 0.027846921235322952, 0.0249797273427248, 0.024662507697939873, 0.02220783568918705, 0.02183086983859539, 0.021498823538422585, 0.02138635516166687, 0.0211948212236166, 0.01899663917720318, 0.01891021803021431, 0.01786436140537262, 0.013177327811717987]}",0.24679213762283325,NLP Applications,0.11753484606742859
Summarization,With Measured Words: Simple Sentence Selection for Black-Box Optimization of Sentence Compression Algorithms,"Sentence Compression is the task of generating a shorter, yet grammatical version of a given sentence, preserving the essence of the original sentence. This paper proposes a Black-Box Optimizer for Compression (B-BOC): given a black-box compression algorithm and assuming not all sentences need be compressed -find the best candidates for compression in order to maximize both compression rate and quality. Given a required compression ratio, we consider two scenarios: (i) single-sentence compression, and (ii) sentences-sequence compression. In the first scenario, our optimizer is trained to predict how well each sentence could be compressed while meeting the specified ratio requirement. In the latter, the desired compression ratio is applied to a sequence of sentences (e.g., a paragraph) as a whole, rather than on each individual sentence. To achieve that, we use B-BOC to assign an optimal compression ratio to each sentence, then cast it as a Knapsack problem, which we solve using bounded dynamic programming. We evaluate B-BOC on both scenarios on three datasets, demonstrating that our optimizer improves both accuracy and Rouge-F1-score compared to direct application of other compression algorithms.","{'sequence': 'Sentence Compression is the task of generating a shorter, yet grammatical version of a given sentence, preserving the essence of the original sentence. This paper proposes a Black-Box Optimizer for Compression (B-BOC): given a black-box compression algorithm and assuming not all sentences need be compressed -find the best candidates for compression in order to maximize both compression rate and quality. Given a required compression ratio, we consider two scenarios: (i) single-sentence compression, and (ii) sentences-sequence compression. In the first scenario, our optimizer is trained to predict how well each sentence could be compressed while meeting the specified ratio requirement. In the latter, the desired compression ratio is applied to a sequence of sentences (e.g., a paragraph) as a whole, rather than on each individual sentence. To achieve that, we use B-BOC to assign an optimal compression ratio to each sentence, then cast it as a Knapsack problem, which we solve using bounded dynamic programming. We evaluate B-BOC on both scenarios on three datasets, demonstrating that our optimizer improves both accuracy and Rouge-F1-score compared to direct application of other compression algorithms.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Summarization', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09827529639005661, 0.08789162337779999, 0.07097700983285904, 0.06376363337039948, 0.06352506577968597, 0.06275838613510132, 0.04613637924194336, 0.042572855949401855, 0.0418759360909462, 0.04154246672987938, 0.04040592908859253, 0.038884107023477554, 0.035947054624557495, 0.03349216654896736, 0.03332340717315674, 0.03284832462668419, 0.030694490298628807, 0.026673177257180214, 0.02501070313155651, 0.02479439042508602, 0.022920023649930954, 0.021295370534062386, 0.014392297714948654]}",0.09827529639005661,Dialogue and Interactive Systems,0.038884107023477554
Generation,Neural Data-to-Text Generation with LM-based Text Augmentation,"For many new application domains for datato-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel fewshot approach for this setting. Our approach automatically augments the data available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with data samples. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised seq2seq models with less than 10% annotations. By utilizing all annotated data, our model can boost the performance of a standard seq2seq model by over 5 BLEU points, establishing a new state-of-the-art on both datasets.","{'sequence': 'For many new application domains for datato-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel fewshot approach for this setting. Our approach automatically augments the data available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with data samples. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised seq2seq models with less than 10% annotations. By utilizing all annotated data, our model can boost the performance of a standard seq2seq model by over 5 BLEU points, establishing a new state-of-the-art on both datasets.', 'labels': ['Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Information Extraction', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'NLP Applications', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.19496659934520721, 0.10561946779489517, 0.09564880281686783, 0.0752289667725563, 0.05820939689874649, 0.042447227984666824, 0.038891036063432693, 0.037088844925165176, 0.0343179851770401, 0.032846178859472275, 0.030281011015176773, 0.028256313875317574, 0.027813497930765152, 0.02606469951570034, 0.025472601875662804, 0.02405695617198944, 0.020886244252324104, 0.020755408331751823, 0.020097928121685982, 0.017740219831466675, 0.015774916857481003, 0.015017040073871613, 0.012518640607595444]}",0.19496659934520721,Generation,0.19496659934520721
Summarization,Globalizing BERT-based Transformer Architectures for Long Document Summarization,"Fine-tuning a large language model on downstream tasks has become a commonly adopted process in the Natural Language Processing (NLP) (Wang et al., 2019). However, such a process, when associated with the current transformer-based (Vaswani et al., 2017) architectures, shows several limitations when the target task requires to reason with long documents. In this work, we introduce a novel hierarchical propagation layer that spreads information between multiple transformer windows. We adopt a hierarchical approach where the input is divided in multiple blocks independently processed by the scaled dot-attentions and combined between the successive layers. We validate the effectiveness of our approach on three extractive summarization corpora of long scientific papers and news articles. We compare our approach to standard and pre-trained language-model-based summarizers and report state-of-the-art results for long document summarization and comparable results for smaller document summarization.","{'sequence': 'Fine-tuning a large language model on downstream tasks has become a commonly adopted process in the Natural Language Processing (NLP) (Wang et al., 2019). However, such a process, when associated with the current transformer-based (Vaswani et al., 2017) architectures, shows several limitations when the target task requires to reason with long documents. In this work, we introduce a novel hierarchical propagation layer that spreads information between multiple transformer windows. We adopt a hierarchical approach where the input is divided in multiple blocks independently processed by the scaled dot-attentions and combined between the successive layers. We validate the effectiveness of our approach on three extractive summarization corpora of long scientific papers and news articles. We compare our approach to standard and pre-trained language-model-based summarizers and report state-of-the-art results for long document summarization and comparable results for smaller document summarization.', 'labels': ['Summarization', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Resources and Evaluation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Generation', 'Discourse and Pragmatics', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Computational Social Science and Social Media'], 'scores': [0.24013447761535645, 0.11363740265369415, 0.08078685402870178, 0.07640974968671799, 0.05977664142847061, 0.04907751455903053, 0.04772741720080376, 0.0312739834189415, 0.027589233592152596, 0.02733200415968895, 0.026201052591204643, 0.02512291632592678, 0.024648170918226242, 0.024482741951942444, 0.02232120931148529, 0.019093522801995277, 0.01895715482532978, 0.018408505246043205, 0.01763113960623741, 0.015780014917254448, 0.013081693090498447, 0.011115322820842266, 0.009411252103745937]}",0.24013447761535645,Summarization,0.24013447761535645
Generation,Changing the Mind of Transformers for Topically-Controllable Language Generation,"Large Transformer-based language models can aid human authors by suggesting plausible continuations of text written so far. However, current interactive writing assistants do not allow authors to guide text generation in desired topical directions. To address this limitation, we design a framework that displays multiple candidate upcoming topics, of which a user can select a subset to guide the generation. Our framework consists of two components: (1) a method that produces a set of candidate topics by predicting the centers of word clusters in the possible continuations, and ( 2 ) a text generation model whose output adheres to the chosen topics. The training of both components is self-supervised, using only unlabeled text. Our experiments demonstrate that our topic options are better than those of standard clustering approaches, and our framework often generates fluent sentences related to the chosen topics, as judged by automated metrics and crowdsourced workers.","{'sequence': 'Large Transformer-based language models can aid human authors by suggesting plausible continuations of text written so far. However, current interactive writing assistants do not allow authors to guide text generation in desired topical directions. To address this limitation, we design a framework that displays multiple candidate upcoming topics, of which a user can select a subset to guide the generation. Our framework consists of two components: (1) a method that produces a set of candidate topics by predicting the centers of word clusters in the possible continuations, and ( 2 ) a text generation model whose output adheres to the chosen topics. The training of both components is self-supervised, using only unlabeled text. Our experiments demonstrate that our topic options are better than those of standard clustering approaches, and our framework often generates fluent sentences related to the chosen topics, as judged by automated metrics and crowdsourced workers.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Question Answering', 'NLP Applications', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Summarization', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Resources and Evaluation', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14708708226680756, 0.11770246922969818, 0.09970365464687347, 0.06296083331108093, 0.053431373089551926, 0.05109453201293945, 0.04657135158777237, 0.040094949305057526, 0.03906826674938202, 0.03887424245476723, 0.03761518746614456, 0.0363648422062397, 0.030433980748057365, 0.029210994020104408, 0.025783898308873177, 0.022687092423439026, 0.02109234407544136, 0.019453633576631546, 0.018870733678340912, 0.01883968524634838, 0.01844249852001667, 0.013367855921387672, 0.011248494498431683]}",0.14708708226680756,Generation,0.14708708226680756
Summarization,How to Evaluate a Summarizer: Study Design and Statistical Analysis for Manual Linguistic Quality Evaluation,"Manual evaluation is essential to judge progress on automatic text summarization. However, we conduct a survey on recent summarization system papers that reveals little agreement on how to perform such evaluation studies. We conduct two evaluation experiments on two aspects of summaries' linguistic quality (coherence and repetitiveness) to compare Likert-type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another. In our survey, we also find that study parameters such as the overall number of annotators and distribution of annotators to annotation items are often not fully reported and that subsequent statistical analysis ignores grouping factors arising from one annotator judging multiple summaries. Using our evaluation experiments, we show that the total number of annotators can have a strong impact on study power and that current statistical analysis methods can inflate type I error rates up to eight-fold. In addition, we highlight that for the purpose of system comparison the current practice of eliciting multiple judgements per summary leads to less powerful and reliable annotations given a fixed study budget.","{'sequence': ""Manual evaluation is essential to judge progress on automatic text summarization. However, we conduct a survey on recent summarization system papers that reveals little agreement on how to perform such evaluation studies. We conduct two evaluation experiments on two aspects of summaries' linguistic quality (coherence and repetitiveness) to compare Likert-type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another. In our survey, we also find that study parameters such as the overall number of annotators and distribution of annotators to annotation items are often not fully reported and that subsequent statistical analysis ignores grouping factors arising from one annotator judging multiple summaries. Using our evaluation experiments, we show that the total number of annotators can have a strong impact on study power and that current statistical analysis methods can inflate type I error rates up to eight-fold. In addition, we highlight that for the purpose of system comparison the current practice of eliciting multiple judgements per summary leads to less powerful and reliable annotations given a fixed study budget."", 'labels': ['Summarization', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Generation', 'Speech and Multimodality', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.2294720858335495, 0.0967901200056076, 0.07714877277612686, 0.047082461416721344, 0.0469621978700161, 0.04469563066959381, 0.043610166758298874, 0.04291447624564171, 0.03729653358459473, 0.036841899156570435, 0.033191874623298645, 0.03092179074883461, 0.02981104888021946, 0.028169864788651466, 0.027667010203003883, 0.023296087980270386, 0.02131340652704239, 0.020022712647914886, 0.019384874030947685, 0.018335387110710144, 0.016871312633156776, 0.014102170243859291, 0.014098189771175385]}",0.2294720858335495,Summarization,0.2294720858335495
Generation,Non-Autoregressive Text Generation with Pre-trained Language Models,"Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component. 1","{'sequence': 'Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component. 1', 'labels': ['Generation', 'Summarization', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Question Answering', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24158553779125214, 0.1553940623998642, 0.10889183729887009, 0.04971536621451378, 0.0466492734849453, 0.04096768796443939, 0.03379077464342117, 0.0283416248857975, 0.028069712221622467, 0.02788318134844303, 0.026252105832099915, 0.02608003094792366, 0.02594216912984848, 0.024959063157439232, 0.02190585434436798, 0.021469878032803535, 0.019824394956231117, 0.019526470452547073, 0.014878246933221817, 0.013858998194336891, 0.008680648170411587, 0.007902244105935097, 0.007430745754390955]}",0.24158553779125214,Generation,0.24158553779125214
Summarization,Quantifying Appropriateness of Summarization Data for Curriculum Learning,"Much research has reported the training data of summarization models are noisy; summaries often do not reflect what is written in the source texts. We propose an effective method of curriculum learning to train summarization models from such noisy data. Curriculum learning is used to train sequence-to-sequence models with noisy data. In translation tasks, previous research quantified noise of the training data using two models trained with noisy and clean corpora. Because such corpora do not exist in summarization fields, we propose a model that can quantify noise from a single noisy corpus. We conduct experiments on three summarization models; one pretrained model and two non-pretrained models, and verify our method improves the performance. Furthermore, we analyze how different curricula affect the performance of pretrained and nonpretrained summarization models. Our result on human evaluation also shows our method improves the performance of summarization models.","{'sequence': 'Much research has reported the training data of summarization models are noisy; summaries often do not reflect what is written in the source texts. We propose an effective method of curriculum learning to train summarization models from such noisy data. Curriculum learning is used to train sequence-to-sequence models with noisy data. In translation tasks, previous research quantified noise of the training data using two models trained with noisy and clean corpora. Because such corpora do not exist in summarization fields, we propose a model that can quantify noise from a single noisy corpus. We conduct experiments on three summarization models; one pretrained model and two non-pretrained models, and verify our method improves the performance. Furthermore, we analyze how different curricula affect the performance of pretrained and nonpretrained summarization models. Our result on human evaluation also shows our method improves the performance of summarization models.', 'labels': ['Summarization', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Question Answering', 'Computational Social Science and Social Media', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11097722500562668, 0.08958393335342407, 0.08087195456027985, 0.06940464675426483, 0.05156312510371208, 0.050513725727796555, 0.050042103976011276, 0.048845075070858, 0.04842839017510414, 0.0478835366666317, 0.039023254066705704, 0.03723395615816116, 0.03613251820206642, 0.03234673663973808, 0.028980465605854988, 0.027100905776023865, 0.0268869549036026, 0.025861026719212532, 0.02448040060698986, 0.021014858037233353, 0.020683282986283302, 0.018070058897137642, 0.014071875251829624]}",0.11097722500562668,Summarization,0.11097722500562668
Generation,"Expanding, Retrieving and Infilling: Diversifying Cross-Domain Question Generation with Flexible Templates","Sequence-to-sequence based models have recently shown promising results in generating high-quality questions. However, these models are also known to have main drawbacks such as lack of diversity and bad sentence structures. In this paper, we focus on question generation over SQL database and propose a novel framework by expanding, retrieving, and infilling that first incorporates flexible templates with a neural-based model to generate diverse expressions of questions with guidance of sentence structure. Furthermore, a new activation/deactivation mechanism is proposed for template-based sequenceto-sequence generation, which learns to discriminate template patterns and content patterns, thus further improves generation quality. We conduct experiments on two largescale cross-domain datasets. The experiments show that the superiority of our question generation method in producing more diverse questions while maintaining high quality and consistency under both automatic evaluation and human evaluation.","{'sequence': 'Sequence-to-sequence based models have recently shown promising results in generating high-quality questions. However, these models are also known to have main drawbacks such as lack of diversity and bad sentence structures. In this paper, we focus on question generation over SQL database and propose a novel framework by expanding, retrieving, and infilling that first incorporates flexible templates with a neural-based model to generate diverse expressions of questions with guidance of sentence structure. Furthermore, a new activation/deactivation mechanism is proposed for template-based sequenceto-sequence generation, which learns to discriminate template patterns and content patterns, thus further improves generation quality. We conduct experiments on two largescale cross-domain datasets. The experiments show that the superiority of our question generation method in producing more diverse questions while maintaining high quality and consistency under both automatic evaluation and human evaluation.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Summarization', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24106699228286743, 0.07335364073514938, 0.06238876283168793, 0.05497719347476959, 0.05467032268643379, 0.044817965477705, 0.04467722773551941, 0.04360346496105194, 0.04335666447877884, 0.03916076943278313, 0.037126313894987106, 0.029226863756775856, 0.02883755788207054, 0.028423836454749107, 0.02497958391904831, 0.02478730119764805, 0.023250268772244453, 0.021481437608599663, 0.02012953907251358, 0.016385335475206375, 0.016377480700612068, 0.014604403637349606, 0.012317095883190632]}",0.24106699228286743,Generation,0.24106699228286743
Generation,ChainCQG: Flow-Aware Conversational Question Generation,"Conversational systems enable numerous valuable applications, and question-answering is an important component underlying many of these. However, conversational questionanswering remains challenging due to the lack of realistic, domain-specific training data. Inspired by this bottleneck, we focus on conversational question generation as a means to generate synthetic conversations for training and evaluation purposes. We present a number of novel strategies to improve conversational flow and accommodate varying question types and overall fluidity. Specifically, we design ChainCQG as a two-stage architecture that learns question-answer representations across multiple dialogue turns using a flow propagation training strategy. ChainCQG significantly outperforms both answer-aware and answer-unaware SOTA baselines (e.g., up to 48% BLEU-1 improvement). Additionally, our model is able to generate different types of questions, with improved fluidity and coreference alignment.","{'sequence': 'Conversational systems enable numerous valuable applications, and question-answering is an important component underlying many of these. However, conversational questionanswering remains challenging due to the lack of realistic, domain-specific training data. Inspired by this bottleneck, we focus on conversational question generation as a means to generate synthetic conversations for training and evaluation purposes. We present a number of novel strategies to improve conversational flow and accommodate varying question types and overall fluidity. Specifically, we design ChainCQG as a two-stage architecture that learns question-answer representations across multiple dialogue turns using a flow propagation training strategy. ChainCQG significantly outperforms both answer-aware and answer-unaware SOTA baselines (e.g., up to 48% BLEU-1 improvement). Additionally, our model is able to generate different types of questions, with improved fluidity and coreference alignment.', 'labels': ['Question Answering', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'NLP Applications', 'Information Retrieval and Text Mining', 'Information Extraction', 'Ethics and NLP', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.34362882375717163, 0.1579941213130951, 0.06827230006456375, 0.060008276253938675, 0.042977169156074524, 0.030233466997742653, 0.02597157657146454, 0.023357566446065903, 0.022638874128460884, 0.02051449753344059, 0.02014235593378544, 0.019922632724046707, 0.01860806904733181, 0.018204649910330772, 0.017519157379865646, 0.017458859831094742, 0.014800265431404114, 0.014439132995903492, 0.014039545319974422, 0.013460378162562847, 0.013137825764715672, 0.012131351977586746, 0.010539047420024872]}",0.34362882375717163,Question Answering,0.1579941213130951
Generation,Polarized-VAE: Proximity Based Disentangled Representation Learning for Text Generation,"Learning disentangled representations of realworld data is a challenging open problem. Most previous methods have focused on either supervised approaches which use attribute labels or unsupervised approaches that manipulate the factorization in the latent space of models such as the variational autoencoder (VAE) by training with task-specific losses. In this work, we propose polarized-VAE, an approach that disentangles select attributes in the latent space based on proximity measures reflecting the similarity between data points with respect to these attributes. We apply our method to disentangle the semantics and syntax of sentences and carry out transfer experiments. Polarized-VAE outperforms the VAE baseline and is competitive with state-of-the-art approaches, while being more a general framework that is applicable to other attribute disentanglement tasks.","{'sequence': 'Learning disentangled representations of realworld data is a challenging open problem. Most previous methods have focused on either supervised approaches which use attribute labels or unsupervised approaches that manipulate the factorization in the latent space of models such as the variational autoencoder (VAE) by training with task-specific losses. In this work, we propose polarized-VAE, an approach that disentangles select attributes in the latent space based on proximity measures reflecting the similarity between data points with respect to these attributes. We apply our method to disentangle the semantics and syntax of sentences and carry out transfer experiments. Polarized-VAE outperforms the VAE baseline and is competitive with state-of-the-art approaches, while being more a general framework that is applicable to other attribute disentanglement tasks.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Computational Social Science and Social Media', 'Generation', 'Ethics and NLP', 'Question Answering', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Discourse and Pragmatics', 'NLP Applications', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.17948691546916962, 0.08102183043956757, 0.07672262191772461, 0.06206338107585907, 0.060480996966362, 0.04989858344197273, 0.04794057086110115, 0.04542204365134239, 0.04021942988038063, 0.03552766144275665, 0.03472156822681427, 0.030758004635572433, 0.030678624287247658, 0.02978847734630108, 0.02709987945854664, 0.025590693578124046, 0.024498237296938896, 0.024476606398820877, 0.021562978625297546, 0.01967860944569111, 0.019215496256947517, 0.01749017834663391, 0.015656612813472748]}",0.17948691546916962,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.04989858344197273
Summarization,Informative and Controllable Opinion Summarization,"Opinion summarization is the task of automatically generating summaries for a set of reviews about a specific target (e.g., a movie or a product). Since the number of reviews for each target can be prohibitively large, neural network-based methods follow a two-stage approach where an extractive step first pre-selects a subset of salient opinions and an abstractive step creates the summary while conditioning on the extracted subset. However, the extractive model leads to loss of information which may be useful depending on user needs. In this paper we propose a summarization framework that eliminates the need to rely only on pre-selected content and waste possibly useful information, especially when customizing summaries. The framework enables the use of all input reviews by first condensing them into multiple dense vectors which serve as input to an abstractive model. We showcase an effective instantiation of our framework which produces more informative summaries and also allows to take user preferences into account using our zero-shot customization technique. Experimental results demonstrate that our model improves the state of the art on the Rotten Tomatoes dataset and generates customized summaries effectively. ""Coach Carter"" Reviews • Samuel L. Jackson plays the real-life coach of a high school basketball team in this solid sports drama ... • Great performance by Samuel Jackson but predictable as a slam dunk ... • ... excellent basketball choreography, Coach Carter is fun, hopeful, occasionally silly and, what can I say, inspiring. Consensus Summary Even though it's based on a true story, Coach Carter is pretty formulaic stuff, but it's effective and energetic, thanks to a strong central performance from Samuel L. Jackson.","{'sequence': 'Opinion summarization is the task of automatically generating summaries for a set of reviews about a specific target (e.g., a movie or a product). Since the number of reviews for each target can be prohibitively large, neural network-based methods follow a two-stage approach where an extractive step first pre-selects a subset of salient opinions and an abstractive step creates the summary while conditioning on the extracted subset. However, the extractive model leads to loss of information which may be useful depending on user needs. In this paper we propose a summarization framework that eliminates the need to rely only on pre-selected content and waste possibly useful information, especially when customizing summaries. The framework enables the use of all input reviews by first condensing them into multiple dense vectors which serve as input to an abstractive model. We showcase an effective instantiation of our framework which produces more informative summaries and also allows to take user preferences into account using our zero-shot customization technique. Experimental results demonstrate that our model improves the state of the art on the Rotten Tomatoes dataset and generates customized summaries effectively. ""Coach Carter"" Reviews • Samuel L. Jackson plays the real-life coach of a high school basketball team in this solid sports drama ... • Great performance by Samuel Jackson but predictable as a slam dunk ... • ... excellent basketball choreography, Coach Carter is fun, hopeful, occasionally silly and, what can I say, inspiring. Consensus Summary Even though it\'s based on a true story, Coach Carter is pretty formulaic stuff, but it\'s effective and energetic, thanks to a strong central performance from Samuel L. Jackson.', 'labels': ['Summarization', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Information Extraction', 'Question Answering', 'Generation', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14215828478336334, 0.0648660659790039, 0.064503513276577, 0.06330787390470505, 0.059730492532253265, 0.058756642043590546, 0.05772789940237999, 0.056210558861494064, 0.05547075346112251, 0.04097767919301987, 0.04060790315270424, 0.03848791867494583, 0.03656523674726486, 0.029237622395157814, 0.02833992801606655, 0.027678564190864563, 0.025448990985751152, 0.022739462554454803, 0.021502114832401276, 0.02089395746588707, 0.019793733954429626, 0.01565687358379364, 0.009337956085801125]}",0.14215828478336334,Summarization,0.14215828478336334
Generation,Enconter: Entity Constrained Progressive Sequence Generation via Insertion-based Transformer,"Pretrained using large amount of data, autoregressive language models are able to generate high quality sequences. However, these models do not perform well under hard lexical constraints as they lack fine control of content generation process. Progressive insertion-based transformers can overcome the above limitation and efficiently generate a sequence in parallel given some input tokens as constraint. These transformers however may fail to support hard lexical constraints as their generation process is more likely to terminate prematurely. The paper analyses such early termination problems and proposes the ENtity-CONstrained insertion TransformER (ENCONTER), a new insertion transformer that addresses the above pitfall without compromising much generation efficiency. We introduce a new training strategy that considers predefined hard lexical constraints (e.g., entities to be included in the generated sequence). Our experiments show that ENCONTER outperforms other baseline models in several performance metrics rendering it more suitable in practical applications. 1","{'sequence': 'Pretrained using large amount of data, autoregressive language models are able to generate high quality sequences. However, these models do not perform well under hard lexical constraints as they lack fine control of content generation process. Progressive insertion-based transformers can overcome the above limitation and efficiently generate a sequence in parallel given some input tokens as constraint. These transformers however may fail to support hard lexical constraints as their generation process is more likely to terminate prematurely. The paper analyses such early termination problems and proposes the ENtity-CONstrained insertion TransformER (ENCONTER), a new insertion transformer that addresses the above pitfall without compromising much generation efficiency. We introduce a new training strategy that considers predefined hard lexical constraints (e.g., entities to be included in the generated sequence). Our experiments show that ENCONTER outperforms other baseline models in several performance metrics rendering it more suitable in practical applications. 1', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Question Answering', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Summarization', 'NLP Applications', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09188925474882126, 0.0867350623011589, 0.06537297368049622, 0.05843331664800644, 0.05546582117676735, 0.05316804349422455, 0.05304430052638054, 0.05007509887218475, 0.048970818519592285, 0.04512652009725571, 0.044210419058799744, 0.042585235089063644, 0.04048032686114311, 0.04012511298060417, 0.038943298161029816, 0.03655918687582016, 0.027042770758271217, 0.025852659717202187, 0.025493737310171127, 0.0196468997746706, 0.01912347599864006, 0.01659291982650757, 0.015062650665640831]}",0.09188925474882126,Generation,0.09188925474882126
Generation,DRAG: Director-Generator Language Modelling Framework for Non-Parallel Author Stylized Rewriting,"Author stylized rewriting is the task of rewriting an input text in a particular author's style. Recent works in this area have leveraged Transformer-based language models in a denoising autoencoder setup to generate author stylized text without relying on a parallel corpus of data. However, these approaches are limited by the lack of explicit control of target attributes and being entirely data-driven. In this paper, we propose a Director-Generator framework to rewrite content in the target author's style, specifically focusing on certain target attributes. We show that our proposed framework works well even with a limitedsized target author corpus. Our experiments on corpora consisting of relatively small-sized text authored by three distinct authors show significant improvements upon existing works to rewrite input texts in target author's style. Our quantitative and qualitative analyses further show that our model has better meaning retention and results in more fluent generations.","{'sequence': ""Author stylized rewriting is the task of rewriting an input text in a particular author's style. Recent works in this area have leveraged Transformer-based language models in a denoising autoencoder setup to generate author stylized text without relying on a parallel corpus of data. However, these approaches are limited by the lack of explicit control of target attributes and being entirely data-driven. In this paper, we propose a Director-Generator framework to rewrite content in the target author's style, specifically focusing on certain target attributes. We show that our proposed framework works well even with a limitedsized target author corpus. Our experiments on corpora consisting of relatively small-sized text authored by three distinct authors show significant improvements upon existing works to rewrite input texts in target author's style. Our quantitative and qualitative analyses further show that our model has better meaning retention and results in more fluent generations."", 'labels': ['Generation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Question Answering', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.13221125304698944, 0.0735156387090683, 0.059421539306640625, 0.05422956123948097, 0.04967062547802925, 0.04916493594646454, 0.04681088775396347, 0.04676980525255203, 0.04446326196193695, 0.044287070631980896, 0.041319847106933594, 0.04003077372908592, 0.039057038724422455, 0.035961031913757324, 0.03196018561720848, 0.031106499955058098, 0.027482111006975174, 0.02733418717980385, 0.027287395671010017, 0.026575107127428055, 0.02493470348417759, 0.024573754519224167, 0.02183281071484089]}",0.13221125304698944,Generation,0.13221125304698944
Generation,Incremental Beam Manipulation for Natural Language Generation,"The performance of natural language generation systems has improved substantially with modern neural networks. At test time they typically employ beam search to avoid locally optimal but globally suboptimal predictions. However, due to model errors, a larger beam size can lead to deteriorating performance according to the evaluation metric. For this reason, it is common to rerank the output of beam search, but this relies on beam search to produce a good set of hypotheses, which limits the potential gains. Other alternatives to beam search require changes to the training of the model, which restricts their applicability compared to beam search. This paper proposes incremental beam manipulation, i.e. reranking the hypotheses in the beam during decoding instead of only at the end. This way, hypotheses that are unlikely to lead to a good final output are discarded, and in their place hypotheses that would have been ignored will be considered instead. Applying incremental beam manipulation leads to an improvement of 1.93 and 5.82 BLEU points over vanilla beam search for the test sets of the E2E and WebNLG challenges respectively. The proposed method also outperformed a strong reranker by 1.04 BLEU points on the E2E challenge, while being on par with it on the WebNLG dataset.","{'sequence': 'The performance of natural language generation systems has improved substantially with modern neural networks. At test time they typically employ beam search to avoid locally optimal but globally suboptimal predictions. However, due to model errors, a larger beam size can lead to deteriorating performance according to the evaluation metric. For this reason, it is common to rerank the output of beam search, but this relies on beam search to produce a good set of hypotheses, which limits the potential gains. Other alternatives to beam search require changes to the training of the model, which restricts their applicability compared to beam search. This paper proposes incremental beam manipulation, i.e. reranking the hypotheses in the beam during decoding instead of only at the end. This way, hypotheses that are unlikely to lead to a good final output are discarded, and in their place hypotheses that would have been ignored will be considered instead. Applying incremental beam manipulation leads to an improvement of 1.93 and 5.82 BLEU points over vanilla beam search for the test sets of the E2E and WebNLG challenges respectively. The proposed method also outperformed a strong reranker by 1.04 BLEU points on the E2E challenge, while being on par with it on the WebNLG dataset.', 'labels': ['Generation', 'Resources and Evaluation', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Speech and Multimodality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.16444151103496552, 0.10838115960359573, 0.07823488861322403, 0.072198286652565, 0.05107657611370087, 0.045527733862400055, 0.0431048758327961, 0.04174215346574783, 0.04171820357441902, 0.0402546226978302, 0.03662484139204025, 0.032563310116529465, 0.03195051848888397, 0.025459175929427147, 0.024496925994753838, 0.023770149797201157, 0.023649848997592926, 0.022198807448148727, 0.021706854924559593, 0.02058294229209423, 0.018563810735940933, 0.017339518293738365, 0.014413359574973583]}",0.16444151103496552,Generation,0.16444151103496552
Summarization,Self-Supervised and Controlled Multi-Document Opinion Summarization,"We address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard log-likelihood loss and mainstream models. We address the problem of hallucinations through the use of control codes, to steer the generation towards more coherent and relevant summaries. Our benchmarks on two English datasets against graph-based and recent neural abstractive unsupervised models show that our proposed method generates summaries with a superior quality and relevance, as well as a high sentiment and topic alignment with the input reviews. This is confirmed in our human evaluation which focuses explicitly on the faithfulness of generated summaries. We also provide an ablation study showing the importance of the control setup in controlling hallucinations.","{'sequence': 'We address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard log-likelihood loss and mainstream models. We address the problem of hallucinations through the use of control codes, to steer the generation towards more coherent and relevant summaries. Our benchmarks on two English datasets against graph-based and recent neural abstractive unsupervised models show that our proposed method generates summaries with a superior quality and relevance, as well as a high sentiment and topic alignment with the input reviews. This is confirmed in our human evaluation which focuses explicitly on the faithfulness of generated summaries. We also provide an ablation study showing the importance of the control setup in controlling hallucinations.', 'labels': ['Summarization', 'Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.22087404131889343, 0.11730273813009262, 0.059455543756484985, 0.0549234077334404, 0.05113082751631737, 0.0490124486386776, 0.04268169403076172, 0.039897989481687546, 0.033145464956760406, 0.03211238980293274, 0.030239515006542206, 0.029427161440253258, 0.027234893292188644, 0.025991803035140038, 0.025238728150725365, 0.024463381618261337, 0.022441308945417404, 0.021916141733527184, 0.01992364227771759, 0.019866574555635452, 0.01919248327612877, 0.01860176958143711, 0.014926018193364143]}",0.22087404131889343,Summarization,0.22087404131889343
Generation,Don't Change Me! User-Controllable Selective Paraphrase Generation,"In the paraphrase generation task, source sentences often contain phrases that should not be altered. Which phrases, however, can be context dependent and can vary by application. Our solution to this challenge is to provide the user with explicit tags that can be placed around any arbitrary segment of text to mean ""don't change me!"" when generating a paraphrase; the model learns to explicitly copy these phrases to the output. The contribution of this work is a novel data generation technique using distant supervision that allows us to start with a pretrained sequenceto-sequence model and fine-tune a paraphrase generator that exhibits this behavior, allowing user-controllable paraphrase generation. Additionally, we modify the loss during fine-tuning to explicitly encourage diversity in model output. Our technique is language agnostic, and we report experiments in English and Chinese.","{'sequence': 'In the paraphrase generation task, source sentences often contain phrases that should not be altered. Which phrases, however, can be context dependent and can vary by application. Our solution to this challenge is to provide the user with explicit tags that can be placed around any arbitrary segment of text to mean ""don\'t change me!"" when generating a paraphrase; the model learns to explicitly copy these phrases to the output. The contribution of this work is a novel data generation technique using distant supervision that allows us to start with a pretrained sequenceto-sequence model and fine-tune a paraphrase generator that exhibits this behavior, allowing user-controllable paraphrase generation. Additionally, we modify the loss during fine-tuning to explicitly encourage diversity in model output. Our technique is language agnostic, and we report experiments in English and Chinese.', 'labels': ['Generation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Ethics and NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13635697960853577, 0.07530297338962555, 0.06919283419847488, 0.06417638063430786, 0.05852464959025383, 0.05048266425728798, 0.04923562705516815, 0.043824564665555954, 0.04260279983282089, 0.038663774728775024, 0.03751103952527046, 0.036843184381723404, 0.036334190517663956, 0.035505421459674835, 0.03242996335029602, 0.030541766434907913, 0.02984878607094288, 0.02858435921370983, 0.02757999673485756, 0.02268282324075699, 0.022302471101284027, 0.017324846237897873, 0.014147907495498657]}",0.13635697960853577,Generation,0.13635697960853577
Generation,Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning,"Likelihood training and maximization-based decoding result in dull and repetitive generated texts even when using powerful language models (Holtzman et al., 2019) . Adding a loss function for regularization was shown to improve text generation output by helping avoid unwanted properties, such as contradiction or repetition (Li et al., 2020) . In this work, we propose fine-tuning a language model by using policy gradient reinforcement learning, directly optimizing for better generation. We apply this approach to minimizing repetition in generated text, and show that, when combined with unlikelihood training (Welleck et al., 2020), our method further reduces repetition without impacting the language model quality. We also evaluate other methods for improving generation at training and decoding time, and compare them using various metrics aimed at control for better text generation output.","{'sequence': 'Likelihood training and maximization-based decoding result in dull and repetitive generated texts even when using powerful language models (Holtzman et al., 2019) . Adding a loss function for regularization was shown to improve text generation output by helping avoid unwanted properties, such as contradiction or repetition (Li et al., 2020) . In this work, we propose fine-tuning a language model by using policy gradient reinforcement learning, directly optimizing for better generation. We apply this approach to minimizing repetition in generated text, and show that, when combined with unlikelihood training (Welleck et al., 2020), our method further reduces repetition without impacting the language model quality. We also evaluate other methods for improving generation at training and decoding time, and compare them using various metrics aimed at control for better text generation output.', 'labels': ['Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'NLP Applications', 'Discourse and Pragmatics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.32011330127716064, 0.06387142091989517, 0.0484943650662899, 0.04149514064192772, 0.04103910177946091, 0.04047380015254021, 0.036101989448070526, 0.03560565412044525, 0.03284337744116783, 0.031034862622618675, 0.030855437740683556, 0.03050886280834675, 0.02980421856045723, 0.029677581042051315, 0.02879473939538002, 0.023394379764795303, 0.022503787651658058, 0.02236756682395935, 0.02062257193028927, 0.020254991948604584, 0.018990157172083855, 0.01848156563937664, 0.012671073898673058]}",0.32011330127716064,Generation,0.32011330127716064
Information Extraction,Exploiting Position and Contextual Word Embeddings for Keyphrase Extraction from Scientific Papers,"Keyphrases associated with research papers provide an effective way to find useful information in the large and growing scholarly digital collections. In this paper, we present KPRank, an unsupervised graph-based algorithm for keyphrase extraction that exploits both positional information and contextual word embeddings into a biased PageRank. Our experimental results on five benchmark datasets show that KPRank that uses contextual word embeddings with additional position signal outperforms previous approaches and strong baselines for this task.","{'sequence': 'Keyphrases associated with research papers provide an effective way to find useful information in the large and growing scholarly digital collections. In this paper, we present KPRank, an unsupervised graph-based algorithm for keyphrase extraction that exploits both positional information and contextual word embeddings into a biased PageRank. Our experimental results on five benchmark datasets show that KPRank that uses contextual word embeddings with additional position signal outperforms previous approaches and strong baselines for this task.', 'labels': ['Information Extraction', 'Information Retrieval and Text Mining', 'Generation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Question Answering', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24192336201667786, 0.0678291916847229, 0.05431712418794632, 0.05317411199212074, 0.05095013976097107, 0.047746334224939346, 0.043964315205812454, 0.04160147160291672, 0.03888504207134247, 0.03663520887494087, 0.03555983304977417, 0.035434942692518234, 0.034010350704193115, 0.028887122869491577, 0.028817586600780487, 0.02848733402788639, 0.0265896487981081, 0.024289684370160103, 0.021823376417160034, 0.017883233726024628, 0.017318537458777428, 0.01603773422539234, 0.007834325544536114]}",0.24192336201667786,Information Extraction,0.24192336201667786
Information Extraction,ENPAR:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction,"Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019)  usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. EN-PAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pretrain an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.","{'sequence': 'Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019)  usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. EN-PAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pretrain an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Generation', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'NLP Applications', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09563793241977692, 0.07929281145334244, 0.053950682282447815, 0.053091030567884445, 0.05287959426641464, 0.05092679709196091, 0.05048021674156189, 0.046153247356414795, 0.04588540643453598, 0.0454091802239418, 0.043134719133377075, 0.041754014790058136, 0.04125101864337921, 0.03553629294037819, 0.03427673876285553, 0.03297867998480797, 0.032322484999895096, 0.03217839077115059, 0.030781693756580353, 0.030686546117067337, 0.025606762617826462, 0.023279840126633644, 0.0225058700889349]}",0.09563793241977692,Information Extraction,0.09563793241977692
Information Extraction,Dynamic Graph Transformer for Implicit Tag Recognition,"Textual information extraction is a typical research topic in the NLP community. Several NLP tasks such as named entity recognition and relation extraction between entities have been well-studied in previous work. However, few works pay their attention to the implicit information. For example, a financial news article mentioned ""Apple Inc."" may be also related to Samsung, even though Samsung is not explicitly mentioned in this article. This work presents a novel dynamic graph transformer that distills the textual information and the entity relations on the fly. Experimental results confirm the effectiveness of our approach to implicit tag recognition.","{'sequence': 'Textual information extraction is a typical research topic in the NLP community. Several NLP tasks such as named entity recognition and relation extraction between entities have been well-studied in previous work. However, few works pay their attention to the implicit information. For example, a financial news article mentioned ""Apple Inc."" may be also related to Samsung, even though Samsung is not explicitly mentioned in this article. This work presents a novel dynamic graph transformer that distills the textual information and the entity relations on the fly. Experimental results confirm the effectiveness of our approach to implicit tag recognition.', 'labels': ['Information Extraction', 'Information Retrieval and Text Mining', 'NLP Applications', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Generation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Summarization', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Question Answering', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.43796825408935547, 0.052460309118032455, 0.0484708808362484, 0.03740311414003372, 0.03533313050866127, 0.03531496599316597, 0.03288299962878227, 0.03265885263681412, 0.029777202755212784, 0.029024051502346992, 0.02827608212828636, 0.026110028848052025, 0.02493346482515335, 0.024588795378804207, 0.024204418063163757, 0.01827191561460495, 0.01709643006324768, 0.01502255443483591, 0.013392342254519463, 0.011465519666671753, 0.009206568822264671, 0.008399991318583488, 0.007738262414932251]}",0.43796825408935547,Information Extraction,0.43796825408935547
Information Extraction,Multilingual Entity and Relation Extraction Dataset and Model,"We present a novel dataset and model for a multilingual setting to approach the task of Joint Entity and Relation Extraction. The SMi-LER dataset consists of 1.1 M annotated sentences, representing 36 relations, and 14 languages. To the best of our knowledge, this is currently both the largest and the most comprehensive dataset of this type. We introduce HERBERTa, a pipeline that combines two independent BERT models: one for sequence classification, and the other for entity tagging. The model achieves micro F 1 81.49 for English on this dataset, which is close to the current SOTA on CoNLL, SpERT.","{'sequence': 'We present a novel dataset and model for a multilingual setting to approach the task of Joint Entity and Relation Extraction. The SMi-LER dataset consists of 1.1 M annotated sentences, representing 36 relations, and 14 languages. To the best of our knowledge, this is currently both the largest and the most comprehensive dataset of this type. We introduce HERBERTa, a pipeline that combines two independent BERT models: one for sequence classification, and the other for entity tagging. The model achieves micro F 1 81.49 for English on this dataset, which is close to the current SOTA on CoNLL, SpERT.', 'labels': ['Speech and Multimodality', 'Generation', 'Summarization', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Semantics: Lexical Semantics', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Discourse and Pragmatics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.12380735576152802, 0.09982074052095413, 0.07818058878183365, 0.06538312882184982, 0.052706439048051834, 0.05033266171813011, 0.04847533628344536, 0.04821722209453583, 0.045665934681892395, 0.045188333839178085, 0.04456262290477753, 0.04038584604859352, 0.0369880273938179, 0.03347352519631386, 0.03161495923995972, 0.029757997021079063, 0.02742377482354641, 0.02485264092683792, 0.017788242548704147, 0.016170958057045937, 0.01582743413746357, 0.013453095220029354, 0.009923196397721767]}",0.12380735576152802,Speech and Multimodality,0.03161495923995972
Information Extraction,Bootstrapping Relation Extractors using Syntactic Search by Examples,"The advent of neural-networks in NLP brought with it substantial improvements in supervised relation extraction. However, obtaining a sufficient quantity of training data remains a key challenge. In this work we propose a process for bootstrapping training datasets which can be performed quickly by non-NLP-experts. We take advantage of search engines over syntactic-graphs (Such as Shlain et al. ( 2020 )) which expose a friendly by-example syntax. We use these to obtain positive examples by searching for sentences that are syntactically similar to user input examples. We apply this technique to relations from TACRED and Do-cRED and show that the resulting models are competitive with models trained on manually annotated data and on data obtained from distant supervision. The models also outperform models trained using NLG data augmentation techniques. Extending the search-based approach with the NLG method further improves the results.","{'sequence': 'The advent of neural-networks in NLP brought with it substantial improvements in supervised relation extraction. However, obtaining a sufficient quantity of training data remains a key challenge. In this work we propose a process for bootstrapping training datasets which can be performed quickly by non-NLP-experts. We take advantage of search engines over syntactic-graphs (Such as Shlain et al. ( 2020 )) which expose a friendly by-example syntax. We use these to obtain positive examples by searching for sentences that are syntactically similar to user input examples. We apply this technique to relations from TACRED and Do-cRED and show that the resulting models are competitive with models trained on manually annotated data and on data obtained from distant supervision. The models also outperform models trained using NLG data augmentation techniques. Extending the search-based approach with the NLG method further improves the results.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Information Extraction', 'Question Answering', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Generation', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Resources and Evaluation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.19356966018676758, 0.06927791237831116, 0.061156049370765686, 0.05163530260324478, 0.048097096383571625, 0.04413776099681854, 0.04270318150520325, 0.04262534901499748, 0.04108692705631256, 0.04024415835738182, 0.036175478249788284, 0.034813042730093, 0.03367266803979874, 0.03297141566872597, 0.03114468790590763, 0.029915278777480125, 0.029255960136651993, 0.026557734236121178, 0.024903064593672752, 0.023165589198470116, 0.022359076887369156, 0.020527660846710205, 0.020004894584417343]}",0.19356966018676758,Machine Learning for NLP,0.061156049370765686
Information Extraction,How Certain is Your Transformer?,"In this work, we consider the problem of uncertainty estimation for Transformer-based models. We investigate the applicability of uncertainty estimates based on dropout usage at the inference stage (Monte Carlo dropout). The series of experiments on natural language understanding tasks shows that the resulting uncertainty estimates improve the quality of detection of error-prone instances. Special attention is paid to the construction of computationally inexpensive estimates via Monte Carlo dropout and Determinantal Point Processes.","{'sequence': 'In this work, we consider the problem of uncertainty estimation for Transformer-based models. We investigate the applicability of uncertainty estimates based on dropout usage at the inference stage (Monte Carlo dropout). The series of experiments on natural language understanding tasks shows that the resulting uncertainty estimates improve the quality of detection of error-prone instances. Special attention is paid to the construction of computationally inexpensive estimates via Monte Carlo dropout and Determinantal Point Processes.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Ethics and NLP', 'Information Extraction', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13593924045562744, 0.0990433320403099, 0.07140292972326279, 0.05847719684243202, 0.058468047529459, 0.050024084746837616, 0.047742001712322235, 0.0445098914206028, 0.04389713704586029, 0.040840618312358856, 0.03994649648666382, 0.03973323479294777, 0.033730290830135345, 0.03279486671090126, 0.032738663256168365, 0.032616566866636276, 0.028495049104094505, 0.021899903193116188, 0.021725265309214592, 0.020599789917469025, 0.017278339713811874, 0.016431955620646477, 0.011665062047541142]}",0.13593924045562744,NLP Applications,0.0445098914206028
Information Extraction,Adapting Event Extractors to Medical Data: Bridging the Covariate Shift,"We tackle the task of adapting event extractors to new domains without labeled data, by aligning the marginal distributions of source and target domains. As a testbed, we create two new event extraction datasets using English texts from two medical domains: (i) clinical notes, and (ii) doctor-patient conversations. We test the efficacy of three marginal alignment techniques: (i) adversarial domain adaptation (ADA), (ii) domain adaptive fine-tuning (DAFT), and (iii) a new instance weighting technique based on language model likelihood scores (LIW). LIW and DAFT improve over a no-transfer BERT baseline on both domains, but ADA only improves on notes. Deeper analysis of performance under different types of shifts (e.g., lexical shift, semantic shift) explains some of the variations among models. Our best-performing models reach F1 scores of 70.0 and 72.9 on notes and conversations respectively, using no labeled target data.","{'sequence': 'We tackle the task of adapting event extractors to new domains without labeled data, by aligning the marginal distributions of source and target domains. As a testbed, we create two new event extraction datasets using English texts from two medical domains: (i) clinical notes, and (ii) doctor-patient conversations. We test the efficacy of three marginal alignment techniques: (i) adversarial domain adaptation (ADA), (ii) domain adaptive fine-tuning (DAFT), and (iii) a new instance weighting technique based on language model likelihood scores (LIW). LIW and DAFT improve over a no-transfer BERT baseline on both domains, but ADA only improves on notes. Deeper analysis of performance under different types of shifts (e.g., lexical shift, semantic shift) explains some of the variations among models. Our best-performing models reach F1 scores of 70.0 and 72.9 on notes and conversations respectively, using no labeled target data.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'NLP Applications', 'Summarization', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.1141241043806076, 0.08298639953136444, 0.07106789201498032, 0.05863026902079582, 0.05698370561003685, 0.054161008447408676, 0.05146591737866402, 0.04795881733298302, 0.04674118384718895, 0.041698139160871506, 0.04045816510915756, 0.03906601667404175, 0.03891582041978836, 0.03516953065991402, 0.0348532572388649, 0.03322816267609596, 0.03047974221408367, 0.026423607021570206, 0.02579537034034729, 0.02255363203585148, 0.022213846445083618, 0.014566170983016491, 0.01045932061970234]}",0.1141241043806076,Question Answering,0.04795881733298302
Information Extraction,An End-to-end Model for Entity-level Relation Extraction using Multi-instance Learning,"We present a joint model for entity-level relation extraction from documents. In contrast to other approaches -which focus on local intra-sentence mention pairs and thus require annotations on mention level -our model operates on entity level. To do so, a multi-task approach is followed that builds upon coreference resolution and gathers relevant signals via multi-instance learning with multi-level representations combining global entity and local mention information. We achieve state-of-theart relation extraction results on the DocRED dataset and report the first entity-level end-toend relation extraction results for future reference. Finally, our experimental results suggest that a joint approach is on par with taskspecific learning, though more efficient due to shared parameters and training steps.","{'sequence': 'We present a joint model for entity-level relation extraction from documents. In contrast to other approaches -which focus on local intra-sentence mention pairs and thus require annotations on mention level -our model operates on entity level. To do so, a multi-task approach is followed that builds upon coreference resolution and gathers relevant signals via multi-instance learning with multi-level representations combining global entity and local mention information. We achieve state-of-theart relation extraction results on the DocRED dataset and report the first entity-level end-toend relation extraction results for future reference. Finally, our experimental results suggest that a joint approach is on par with taskspecific learning, though more efficient due to shared parameters and training steps.', 'labels': ['Information Extraction', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'NLP Applications', 'Information Retrieval and Text Mining', 'Generation', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.10142546892166138, 0.0776684358716011, 0.05743475630879402, 0.05734757333993912, 0.052623242139816284, 0.05194476991891861, 0.04934442415833473, 0.04868582636117935, 0.04698721319437027, 0.04650673270225525, 0.04603481665253639, 0.04578718543052673, 0.039751216769218445, 0.039313819259405136, 0.038373176008462906, 0.036089006811380386, 0.034997571259737015, 0.029960323125123978, 0.022616194561123848, 0.022520573809742928, 0.02085634134709835, 0.017433704808354378, 0.016297506168484688]}",0.10142546892166138,Information Extraction,0.10142546892166138
Information Extraction,FAST: Financial News and Tweet Based Time Aware Network for Stock Trading,"Designing profitable trading strategies is complex as stock movements are highly stochastic; the market is influenced by large volumes of noisy data across diverse information sources like news and social media. Prior work mostly treats stock movement prediction as a regression or classification task and is not directly optimized towards profit-making. Further, they do not model the fine-grain temporal irregularities in the release of vast volumes of text that the market responds to quickly. Building on these limitations, we propose a novel hierarchical, learning to rank approach that uses textual data to make time-aware predictions for ranking stocks based on expected profit. Our approach outperforms state-of-the-art methods by over 8% in terms of cumulative profit and risk-adjusted returns in trading simulations on two benchmarks: English tweets and Chinese financial news spanning two major stock indexes and four global markets. Through ablative and qualitative analyses, we build the case for our method as a tool for daily stock trading.","{'sequence': 'Designing profitable trading strategies is complex as stock movements are highly stochastic; the market is influenced by large volumes of noisy data across diverse information sources like news and social media. Prior work mostly treats stock movement prediction as a regression or classification task and is not directly optimized towards profit-making. Further, they do not model the fine-grain temporal irregularities in the release of vast volumes of text that the market responds to quickly. Building on these limitations, we propose a novel hierarchical, learning to rank approach that uses textual data to make time-aware predictions for ranking stocks based on expected profit. Our approach outperforms state-of-the-art methods by over 8% in terms of cumulative profit and risk-adjusted returns in trading simulations on two benchmarks: English tweets and Chinese financial news spanning two major stock indexes and four global markets. Through ablative and qualitative analyses, we build the case for our method as a tool for daily stock trading.', 'labels': ['Speech and Multimodality', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Information Retrieval and Text Mining', 'Information Extraction', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.08310075849294662, 0.0671619102358818, 0.06681136786937714, 0.06554649025201797, 0.06531079858541489, 0.06102077290415764, 0.06052358075976372, 0.055641114711761475, 0.043882593512535095, 0.04188817739486694, 0.04154687747359276, 0.040900904685258865, 0.039855025708675385, 0.03582851588726044, 0.03265007957816124, 0.03231726959347725, 0.031899161636829376, 0.03081393428146839, 0.02396552264690399, 0.023736348375678062, 0.021027741953730583, 0.017695188522338867, 0.016875844448804855]}",0.08310075849294662,Speech and Multimodality,0.06102077290415764
Information Extraction,AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization,"Redundancy-aware extractive summarization systems score the redundancy of the sentences to be included in a summary either jointly with their salience information or separately as an additional sentence scoring step. Previous work shows the efficacy of jointly scoring and selecting sentences with neural sequence generation models. It is, however, not well-understood if the gain is due to better encoding techniques or better redundancy reduction approaches. Similarly, the contribution of salience versus diversity components on the created summary is not studied well. Building on the state-of-the-art encoding methods for summarization, we present two adaptive learning models: AREDSUM-SEQ that jointly considers salience and novelty during sentence selection; and a two-step AREDSUM-CTX that scores salience first, then learns to balance salience and redundancy, enabling the measurement of the impact of each aspect. Empirical results on CNN/DailyMail and NYT50 datasets show that by modeling diversity explicitly in a separate step, AREDSUM-CTX achieves significantly better performance than AREDSUM-SEQ as well as state-of-the-art extractive summarization baselines.","{'sequence': 'Redundancy-aware extractive summarization systems score the redundancy of the sentences to be included in a summary either jointly with their salience information or separately as an additional sentence scoring step. Previous work shows the efficacy of jointly scoring and selecting sentences with neural sequence generation models. It is, however, not well-understood if the gain is due to better encoding techniques or better redundancy reduction approaches. Similarly, the contribution of salience versus diversity components on the created summary is not studied well. Building on the state-of-the-art encoding methods for summarization, we present two adaptive learning models: AREDSUM-SEQ that jointly considers salience and novelty during sentence selection; and a two-step AREDSUM-CTX that scores salience first, then learns to balance salience and redundancy, enabling the measurement of the impact of each aspect. Empirical results on CNN/DailyMail and NYT50 datasets show that by modeling diversity explicitly in a separate step, AREDSUM-CTX achieves significantly better performance than AREDSUM-SEQ as well as state-of-the-art extractive summarization baselines.', 'labels': ['Summarization', 'Information Extraction', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15724748373031616, 0.09947072714567184, 0.07336722314357758, 0.06302031129598618, 0.055301863700151443, 0.05361710861325264, 0.05181247368454933, 0.04119249805808067, 0.039820827543735504, 0.03593798726797104, 0.03565959632396698, 0.032762207090854645, 0.03239916265010834, 0.029523268342018127, 0.027641363441944122, 0.02536698989570141, 0.025342248380184174, 0.022328609600663185, 0.02209876850247383, 0.02204766310751438, 0.019238153472542763, 0.018181920051574707, 0.016621645539999008]}",0.15724748373031616,Summarization,0.09947072714567184
Information Extraction,Two Training Strategies for Improving Relation Extraction over Universal Graph,"This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies: (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-ofthe-art result on the NYT10 dataset. The code and datasets used in this paper are available at https://github.com/baodaiqin/ UGDSRE.","{'sequence': 'This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies: (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-ofthe-art result on the NYT10 dataset. The code and datasets used in this paper are available at https://github.com/baodaiqin/ UGDSRE.', 'labels': ['Dialogue and Interactive Systems', 'Generation', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Ethics and NLP', 'NLP Applications', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.08244064450263977, 0.06777171790599823, 0.06775906682014465, 0.06720452755689621, 0.06048920750617981, 0.057645104825496674, 0.05588984116911888, 0.0531676821410656, 0.05017547309398651, 0.04706021770834923, 0.045205358415842056, 0.038047440350055695, 0.03740018978714943, 0.03675101697444916, 0.032345812767744064, 0.03055175580084324, 0.028749659657478333, 0.028429629281163216, 0.02690172754228115, 0.02505660615861416, 0.022509219124913216, 0.02018386870622635, 0.018264232203364372]}",0.08244064450263977,Dialogue and Interactive Systems,0.03675101697444916
Information Extraction,GLaRA: Graph-based Labeling Rule Augmentation for Weakly Supervised Named Entity Recognition,"Instead of using expensive manual annotations, researchers have proposed to train named entity recognition (NER) systems using heuristic labeling rules. However, devising labeling rules is challenging because it often requires a considerable amount of manual effort and domain expertise. To alleviate this problem, we propose GLARA, a graph-based labeling rule augmentation framework, to learn new labeling rules from unlabeled data. We first create a graph with nodes representing candidate rules extracted from unlabeled data. Then, we design a new graph neural network to augment labeling rules by exploring the semantic relations between rules. We finally apply the augmented rules on unlabeled data to generate weak labels and train a NER model using the weakly labeled data. We evaluate our method on three NER datasets and find that we can achieve an average improvement of +20% F1 score over the best baseline when given a small set of seed rules. ⋮ Labeling Rule Applier *noma -> Disease *athy -> Disease *tion -> Other *lity -> Other Seeding Rules *noma-> Disease *athy -> Disease *homa -> Disease *kemias-> Disease *ndrome-> Disease Rank and Select New Rules ... *noma *kemias *ation *tion *lity *ility *athy *ndrome *homa *itoyl *ation *ency *trophy *onia *noma *tonia *kemias *tity *itity *tion *ndrome *homa *ndrome Candidate Rules","{'sequence': 'Instead of using expensive manual annotations, researchers have proposed to train named entity recognition (NER) systems using heuristic labeling rules. However, devising labeling rules is challenging because it often requires a considerable amount of manual effort and domain expertise. To alleviate this problem, we propose GLARA, a graph-based labeling rule augmentation framework, to learn new labeling rules from unlabeled data. We first create a graph with nodes representing candidate rules extracted from unlabeled data. Then, we design a new graph neural network to augment labeling rules by exploring the semantic relations between rules. We finally apply the augmented rules on unlabeled data to generate weak labels and train a NER model using the weakly labeled data. We evaluate our method on three NER datasets and find that we can achieve an average improvement of +20% F1 score over the best baseline when given a small set of seed rules. ⋮ Labeling Rule Applier *noma -> Disease *athy -> Disease *tion -> Other *lity -> Other Seeding Rules *noma-> Disease *athy -> Disease *homa -> Disease *kemias-> Disease *ndrome-> Disease Rank and Select New Rules ... *noma *kemias *ation *tion *lity *ility *athy *ndrome *homa *itoyl *ation *ency *trophy *onia *noma *tonia *kemias *tity *itity *tion *ndrome *homa *ndrome Candidate Rules', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Generation', 'Question Answering', 'Semantics: Lexical Semantics', 'Summarization', 'NLP Applications', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07493988424539566, 0.0699339210987091, 0.06971114873886108, 0.06087014079093933, 0.05777371674776077, 0.054412730038166046, 0.0523114949464798, 0.05205085501074791, 0.04965030029416084, 0.049441028386354446, 0.044618383049964905, 0.040802694857120514, 0.038617733865976334, 0.03766339644789696, 0.036513593047857285, 0.03231208026409149, 0.03057120367884636, 0.029389766976237297, 0.027174176648259163, 0.027035631239414215, 0.026132961735129356, 0.020256368443369865, 0.01781683787703514]}",0.07493988424539566,Dialogue and Interactive Systems,0.0699339210987091
Information Extraction,ChEMU-Ref: A Corpus for Modeling Anaphora Resolution in the Chemical Domain,"Chemical patents contain rich coreference and bridging links, which are the target of this research. Specially, we introduce a novel annotation scheme, based on which we create the ChEMU-Ref dataset from reaction description snippets in English-language chemical patents. We propose a neural approach to anaphora resolution, which we show to achieve strong results, especially when jointly trained over coreference and bridging links.","{'sequence': 'Chemical patents contain rich coreference and bridging links, which are the target of this research. Specially, we introduce a novel annotation scheme, based on which we create the ChEMU-Ref dataset from reaction description snippets in English-language chemical patents. We propose a neural approach to anaphora resolution, which we show to achieve strong results, especially when jointly trained over coreference and bridging links.', 'labels': ['Question Answering', 'Summarization', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Information Retrieval and Text Mining', 'Generation', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'NLP Applications', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.08579045534133911, 0.08383116126060486, 0.08358778804540634, 0.07613168656826019, 0.07586710155010223, 0.06740591675043106, 0.06410910189151764, 0.05254760757088661, 0.04897405207157135, 0.044512663036584854, 0.040887802839279175, 0.03769410774111748, 0.028093615546822548, 0.026733199134469032, 0.026450680568814278, 0.02508646249771118, 0.02307966537773609, 0.021808002144098282, 0.0201603751629591, 0.01971607469022274, 0.018732793629169464, 0.0179941114038229, 0.01080554723739624]}",0.08579045534133911,Question Answering,0.07586710155010223
Information Extraction,Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs,"Real-world knowledge graphs are often characterized by low-frequency relations-a challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of models derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of learning in this fewshot setting. We find that a simple zero-shot baseline-which ignores any relation-specific information-achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.","{'sequence': 'Real-world knowledge graphs are often characterized by low-frequency relations-a challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of models derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of learning in this fewshot setting. We find that a simple zero-shot baseline-which ignores any relation-specific information-achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Summarization', 'Resources and Evaluation', 'Generation', 'NLP Applications', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08105554431676865, 0.07767793536186218, 0.06998077034950256, 0.06116251274943352, 0.05758363753557205, 0.05243772268295288, 0.04941801726818085, 0.04936223477125168, 0.047353845089673996, 0.04617946594953537, 0.04598502442240715, 0.043460533022880554, 0.04030535742640495, 0.03907676786184311, 0.032434456050395966, 0.03191792592406273, 0.03176572546362877, 0.031186193227767944, 0.0284274909645319, 0.027248051017522812, 0.023137258365750313, 0.02104710415005684, 0.011796444654464722]}",0.08105554431676865,Question Answering,0.05758363753557205
Information Extraction,GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction,"We revisit the classic problem of documentlevel role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoderdecoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.","{'sequence': 'We revisit the classic problem of documentlevel role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoderdecoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.', 'labels': ['Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Question Answering', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1975211501121521, 0.16887305676937103, 0.06349135935306549, 0.057216156274080276, 0.053405653685331345, 0.03768821433186531, 0.03446987271308899, 0.034197598695755005, 0.03189102187752724, 0.02994624339044094, 0.02981826849281788, 0.029580190777778625, 0.028617942705750465, 0.02854945883154869, 0.0273627620190382, 0.02685784548521042, 0.02678128331899643, 0.019977375864982605, 0.017413552850484848, 0.01734987273812294, 0.016423311084508896, 0.01576884835958481, 0.006799004971981049]}",0.1975211501121521,Generation,0.057216156274080276
Information Extraction,Learning Relatedness between Types with Prototypes for Relation Extraction,"Relation schemas are often pre-defined for each relation dataset. Relation types can be related from different datasets and have overlapping semantics. We hypothesize we can combine these datasets according to the semantic relatedness between the relation types to overcome the problem of lack of training data. It is often easy to discover the connection between relation types based on relation names or annotation guides, but hard to measure the exact similarity and take advantage of the connection between the relation types from different datasets. We propose to use prototypical examples to represent each relation type and use these examples to augment related types from a different dataset. We obtain further improvement (ACE05) with this type augmentation over a strong baseline which uses multi-task learning between datasets to obtain better feature representation for relations. We make our implementation publicly available: https://github. com/fufrank5/relatedness * This is the work that the author has done before joining Amazon Alexa AI.","{'sequence': 'Relation schemas are often pre-defined for each relation dataset. Relation types can be related from different datasets and have overlapping semantics. We hypothesize we can combine these datasets according to the semantic relatedness between the relation types to overcome the problem of lack of training data. It is often easy to discover the connection between relation types based on relation names or annotation guides, but hard to measure the exact similarity and take advantage of the connection between the relation types from different datasets. We propose to use prototypical examples to represent each relation type and use these examples to augment related types from a different dataset. We obtain further improvement (ACE05) with this type augmentation over a strong baseline which uses multi-task learning between datasets to obtain better feature representation for relations. We make our implementation publicly available: https://github. com/fufrank5/relatedness * This is the work that the author has done before joining Amazon Alexa AI.', 'labels': ['Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Speech and Multimodality', 'NLP Applications', 'Question Answering', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Discourse and Pragmatics', 'Summarization', 'Information Extraction', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07650764286518097, 0.07631181925535202, 0.07002580910921097, 0.06074700132012367, 0.057257987558841705, 0.05623430386185646, 0.0511649064719677, 0.050034694373607635, 0.049269407987594604, 0.04508085176348686, 0.04386274144053459, 0.0399375781416893, 0.03526349738240242, 0.03465317562222481, 0.03351736068725586, 0.03313850611448288, 0.03308013081550598, 0.030942661687731743, 0.03062383085489273, 0.025885408744215965, 0.02287239581346512, 0.022005483508110046, 0.021582700312137604]}",0.07650764286518097,Dialogue and Interactive Systems,0.022005483508110046
Information Extraction,"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks","Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KGto-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration. Empirically, we investigate KG matches for the SocialIQA (SIQA) (Sap et al., 2019b) , Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0 (Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), Con-ceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHowbased KGs are the best matches for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation. 1 Knowledge Coverage Analysis Knowledge Alignment Analysis Knowledge Integration Analysis knowledge insertion question answer Triple: Context: Tracy brought the kids to their dentist appointment, but it was scheduled during the school day. What does Tracy need to do before this? Correct Answer: Keep the kids home from school until after their appointments ConceptNet Knowledge Subgraph: atLocation atLocation Pair: Context: Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards. How would you describe Sydney?","{'sequence': ""Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KGto-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration. Empirically, we investigate KG matches for the SocialIQA (SIQA) (Sap et al., 2019b) , Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0 (Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), Con-ceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHowbased KGs are the best matches for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation. 1 Knowledge Coverage Analysis Knowledge Alignment Analysis Knowledge Integration Analysis knowledge insertion question answer Triple: Context: Tracy brought the kids to their dentist appointment, but it was scheduled during the school day. What does Tracy need to do before this? Correct Answer: Keep the kids home from school until after their appointments ConceptNet Knowledge Subgraph: atLocation atLocation Pair: Context: Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards. How would you describe Sydney?"", 'labels': ['Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Summarization', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Interpretability and Analysis of Models for NLP'], 'scores': [0.14037233591079712, 0.06313909590244293, 0.058217376470565796, 0.05635269358754158, 0.054156020283699036, 0.049130432307720184, 0.048829250037670135, 0.045296330004930496, 0.04104352742433548, 0.04070277884602547, 0.04039020836353302, 0.039600878953933716, 0.03691139072179794, 0.03556355461478233, 0.034855183213949203, 0.03393394872546196, 0.031868670135736465, 0.031485382467508316, 0.025722844526171684, 0.02507842145860195, 0.024144919589161873, 0.02220207266509533, 0.021002721041440964]}",0.14037233591079712,Question Answering,0.039600878953933716
Information Extraction,CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata,"In this paper, we propose CHOLAN, a modular approach to target end-to-end entity linking (EL) over knowledge bases. CHOLAN consists of a pipeline of two transformerbased models integrated sequentially to accomplish the EL task. The first transformer model identifies surface forms (entity mentions) in a given text. For each mention, a second transformer model is employed to classify the target entity among a predefined candidates list. The latter transformer is fed by an enriched context captured from the sentence (i.e. local context), and entity description gained from Wikipedia. Such external contexts have not been used in state of the art EL approaches. Our empirical study was conducted on two well-known knowledge bases (i.e., Wikidata and Wikipedia). The empirical results suggest that CHOLAN outperforms state-of-the-art approaches on standard datasets such as CoNLL-AIDA, MSNBC, AQUAINT, ACE2004, and T-REx.","{'sequence': 'In this paper, we propose CHOLAN, a modular approach to target end-to-end entity linking (EL) over knowledge bases. CHOLAN consists of a pipeline of two transformerbased models integrated sequentially to accomplish the EL task. The first transformer model identifies surface forms (entity mentions) in a given text. For each mention, a second transformer model is employed to classify the target entity among a predefined candidates list. The latter transformer is fed by an enriched context captured from the sentence (i.e. local context), and entity description gained from Wikipedia. Such external contexts have not been used in state of the art EL approaches. Our empirical study was conducted on two well-known knowledge bases (i.e., Wikidata and Wikipedia). The empirical results suggest that CHOLAN outperforms state-of-the-art approaches on standard datasets such as CoNLL-AIDA, MSNBC, AQUAINT, ACE2004, and T-REx.', 'labels': ['Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Generation', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics'], 'scores': [0.1024441123008728, 0.07605265825986862, 0.06179897114634514, 0.061331018805503845, 0.05746295303106308, 0.050019923597574234, 0.047828979790210724, 0.046883489936590195, 0.04627065360546112, 0.0460088737308979, 0.04479921981692314, 0.04429147392511368, 0.043240249156951904, 0.0363137423992157, 0.03173467889428139, 0.0306259673088789, 0.029314659535884857, 0.029177416115999222, 0.026229476556181908, 0.025268761441111565, 0.025010371580719948, 0.020450470969080925, 0.01744188368320465]}",0.1024441123008728,Speech and Multimodality,0.05746295303106308
Information Extraction,Probing into the Root: A Dataset for Reason Extraction of Structural Events from Financial Documents,"This paper proposes a new task regarding event reason extraction from document-level texts. Unlike the previous causality detection task, we do not assign target events in the text but only provide structural event descriptions, and such settings accord more with practice scenarios. Moreover, we annotate a large dataset FinReason for evaluation, which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events, multiple-reasons, and implicit-reasons are included. In total, FinReason contains 8,794 documents, 12,861 financial events and 11,006 reason spans. We also provide the performance of existing canonical methods in event extraction and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best model and human performance, and existing methods are far from resolving this problem.","{'sequence': 'This paper proposes a new task regarding event reason extraction from document-level texts. Unlike the previous causality detection task, we do not assign target events in the text but only provide structural event descriptions, and such settings accord more with practice scenarios. Moreover, we annotate a large dataset FinReason for evaluation, which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events, multiple-reasons, and implicit-reasons are included. In total, FinReason contains 8,794 documents, 12,861 financial events and 11,006 reason spans. We also provide the performance of existing canonical methods in event extraction and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best model and human performance, and existing methods are far from resolving this problem.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Summarization', 'NLP Applications', 'Discourse and Pragmatics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1473395824432373, 0.06334174424409866, 0.06255681812763214, 0.05970136821269989, 0.056056909263134, 0.052800532430410385, 0.05210376903414726, 0.047226861119270325, 0.04530731216073036, 0.042318109422922134, 0.041582394391298294, 0.039850201457738876, 0.039069127291440964, 0.037614140659570694, 0.03246864676475525, 0.028613358736038208, 0.025065310299396515, 0.023657023906707764, 0.022797448560595512, 0.022628268226981163, 0.0209795031696558, 0.01921667344868183, 0.017704851925373077]}",0.1473395824432373,Resources and Evaluation,0.05970136821269989
Information Extraction,A Simple Three-Step Approach for the Automatic Detection of Exaggerated Statements in Health Science News,"There is a huge difference between a scientific journal reporting 'wine consumption might be correlated to cancer', and a media outlet publishing 'wine causes cancer' citing the journal's results. The above example is a typical case of a scientific statement being exaggerated as an outcome of the rising problem of media manipulation. Given a pair of statements (say one from the source journal article and the other from the news article covering the results published in the journal), is it possible to ascertain with some confidence whether one is an exaggerated version of the other? This paper presents a surprisingly simple yet rational three-step approach that performs best for this task. We solve the task by breaking it into three sub-tasks as follows -(a) given a statement from a scientific paper or press release, we first extract relation phrases (e.g., 'causes' versus 'might be correlated to') connecting the dependent (e.g., 'cancer') and the independent ('wine') variable, (b) classify the strength of the relationship phrase extracted and (c) compare the strengths of the relation phrases extracted from the statements to identify whether one statement contains an exaggerated version of the other, and to what extent. Through rigorous experiments, we demonstrate that our simple approach by far outperforms baseline models that compare state-of-the-art embedding of the statement pairs through a binary classifier or recast the problem as a textual entailment task, which appears to be a very natural choice in this settings.","{'sequence': ""There is a huge difference between a scientific journal reporting 'wine consumption might be correlated to cancer', and a media outlet publishing 'wine causes cancer' citing the journal's results. The above example is a typical case of a scientific statement being exaggerated as an outcome of the rising problem of media manipulation. Given a pair of statements (say one from the source journal article and the other from the news article covering the results published in the journal), is it possible to ascertain with some confidence whether one is an exaggerated version of the other? This paper presents a surprisingly simple yet rational three-step approach that performs best for this task. We solve the task by breaking it into three sub-tasks as follows -(a) given a statement from a scientific paper or press release, we first extract relation phrases (e.g., 'causes' versus 'might be correlated to') connecting the dependent (e.g., 'cancer') and the independent ('wine') variable, (b) classify the strength of the relationship phrase extracted and (c) compare the strengths of the relation phrases extracted from the statements to identify whether one statement contains an exaggerated version of the other, and to what extent. Through rigorous experiments, we demonstrate that our simple approach by far outperforms baseline models that compare state-of-the-art embedding of the statement pairs through a binary classifier or recast the problem as a textual entailment task, which appears to be a very natural choice in this settings."", 'labels': ['Question Answering', 'Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'NLP Applications', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11157190799713135, 0.08859417587518692, 0.08347078412771225, 0.08082467317581177, 0.06102273240685463, 0.06095737963914871, 0.04923718795180321, 0.04715839773416519, 0.04124772176146507, 0.03912590816617012, 0.036679256707429886, 0.03648955374956131, 0.03420311585068703, 0.03183276951313019, 0.029673045501112938, 0.02916448935866356, 0.027315497398376465, 0.022889813408255577, 0.022419214248657227, 0.017908742651343346, 0.017415380105376244, 0.017015283927321434, 0.013782885856926441]}",0.11157190799713135,Question Answering,0.08859417587518692
Information Extraction,Fine-Grained Event Trigger Detection,"Most of the previous work on Event Detection (ED) has only considered the datasets with a small number of event types (i.e., up to 38 types). In this work, we present the first study on fine-grained ED (FED) where the evaluation dataset involves much more fine-grained event types (i.e., 449 types). We propose a novel method to transform the Semcor dataset for Word Sense Disambiguation into a large and high-quality dataset for FED. Extensive evaluation of the current ED methods is conducted to demonstrate the challenges of the generated datasets for FED, calling for more research effort in this area.","{'sequence': 'Most of the previous work on Event Detection (ED) has only considered the datasets with a small number of event types (i.e., up to 38 types). In this work, we present the first study on fine-grained ED (FED) where the evaluation dataset involves much more fine-grained event types (i.e., 449 types). We propose a novel method to transform the Semcor dataset for Word Sense Disambiguation into a large and high-quality dataset for FED. Extensive evaluation of the current ED methods is conducted to demonstrate the challenges of the generated datasets for FED, calling for more research effort in this area.', 'labels': ['Generation', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Information Extraction', 'NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Question Answering', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.16836023330688477, 0.09003674983978271, 0.08470809459686279, 0.07454416155815125, 0.053352274000644684, 0.04857330769300461, 0.04786508530378342, 0.0400785394012928, 0.03780185431241989, 0.03679554536938667, 0.036209218204021454, 0.03529100865125656, 0.03164561465382576, 0.031567059457302094, 0.027849916368722916, 0.02596258372068405, 0.02422703057527542, 0.02213113009929657, 0.02026485837996006, 0.019918611273169518, 0.016525033861398697, 0.014564968645572662, 0.011727158911526203]}",0.16836023330688477,Generation,0.07454416155815125
Information Extraction,Multi-facet Universal Schema,"Universal schema (USchema) assumes that two sentence patterns that share the same entity pairs are similar to each other. This assumption is widely adopted for solving various types of relation extraction (RE) tasks. Nevertheless, each sentence pattern could contain multiple facets, and not every facet is similar to all the facets of another sentence pattern cooccurring with the same entity pair. To address the violation of the USchema assumption, we propose multi-facet universal schema that uses a neural model to represent each sentence pattern as multiple facet embeddings and encourage one of these facet embeddings to be close to that of another sentence pattern if they cooccur with the same entity pair. In our experiments, we demonstrate that multi-facet embeddings significantly outperform their singlefacet embedding counterpart, compositional universal schema (CUSchema) (Verga et al.,  2016), in distantly supervised relation extraction tasks. Moreover, we can also use multiple embeddings to detect the entailment relation between two sentence patterns when no manual label is available.","{'sequence': 'Universal schema (USchema) assumes that two sentence patterns that share the same entity pairs are similar to each other. This assumption is widely adopted for solving various types of relation extraction (RE) tasks. Nevertheless, each sentence pattern could contain multiple facets, and not every facet is similar to all the facets of another sentence pattern cooccurring with the same entity pair. To address the violation of the USchema assumption, we propose multi-facet universal schema that uses a neural model to represent each sentence pattern as multiple facet embeddings and encourage one of these facet embeddings to be close to that of another sentence pattern if they cooccur with the same entity pair. In our experiments, we demonstrate that multi-facet embeddings significantly outperform their singlefacet embedding counterpart, compositional universal schema (CUSchema) (Verga et al.,  2016), in distantly supervised relation extraction tasks. Moreover, we can also use multiple embeddings to detect the entailment relation between two sentence patterns when no manual label is available.', 'labels': ['Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Question Answering', 'Computational Social Science and Social Media', 'Generation', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08774321526288986, 0.0738777369260788, 0.06967947632074356, 0.06703562289476395, 0.06024637818336487, 0.05348709225654602, 0.05183444917201996, 0.051466986536979675, 0.044145893305540085, 0.04404407739639282, 0.0425591766834259, 0.04013745114207268, 0.0370282307267189, 0.03346692770719528, 0.03320034220814705, 0.033028002828359604, 0.03230559453368187, 0.030050182715058327, 0.028878100216388702, 0.025939375162124634, 0.024086149409413338, 0.02110583521425724, 0.014653678052127361]}",0.08774321526288986,Dialogue and Interactive Systems,0.024086149409413338
Information Extraction,Identifying Named Entities as they are Typed,"Identifying named entities in written text is an essential component of the text processing pipeline used in applications such as text editors to gain a better understanding of the semantics of the text. However, the typical experimental setup for evaluating Named Entity Recognition (NER) systems is not directly applicable to systems that process text in real time as the text is being typed. Evaluation is performed on a sentence level assuming the end-user is willing to wait until the entire sentence is typed for entities to be identified and further linked to identifiers or coreferenced. We introduce a novel experimental setup for NER systems for applications where decisions about named entity boundaries need to be performed in an online fashion. We study how state-of-the-art methods perform under this setup in multiple languages and propose adaptations to these models to suit this new experimental setup. Experimental results show that the best systems that are evaluated on each token after its typed, reach performance within 1-5 F 1 points of systems that are evaluated at the end of the sentence. These show that entity recognition can be performed in this setup and open up the development of other NLP tools in a similar setup.","{'sequence': 'Identifying named entities in written text is an essential component of the text processing pipeline used in applications such as text editors to gain a better understanding of the semantics of the text. However, the typical experimental setup for evaluating Named Entity Recognition (NER) systems is not directly applicable to systems that process text in real time as the text is being typed. Evaluation is performed on a sentence level assuming the end-user is willing to wait until the entire sentence is typed for entities to be identified and further linked to identifiers or coreferenced. We introduce a novel experimental setup for NER systems for applications where decisions about named entity boundaries need to be performed in an online fashion. We study how state-of-the-art methods perform under this setup in multiple languages and propose adaptations to these models to suit this new experimental setup. Experimental results show that the best systems that are evaluated on each token after its typed, reach performance within 1-5 F 1 points of systems that are evaluated at the end of the sentence. These show that entity recognition can be performed in this setup and open up the development of other NLP tools in a similar setup.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'NLP Applications', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Extraction', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Summarization', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0937393382191658, 0.0880790650844574, 0.06675474345684052, 0.0604509674012661, 0.057462818920612335, 0.05682522803544998, 0.0561872161924839, 0.05617043748497963, 0.04659493640065193, 0.044685665518045425, 0.041587091982364655, 0.040270961821079254, 0.03873428329825401, 0.036139246076345444, 0.034723035991191864, 0.032001279294490814, 0.028178159147500992, 0.026553094387054443, 0.022916940972208977, 0.021065812557935715, 0.02065429463982582, 0.018066614866256714, 0.012158733792603016]}",0.0937393382191658,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.044685665518045425
Information Extraction,Do Syntax Trees Help Pre-trained Transformers Extract Information?,"Much recent work suggests that incorporating syntax information from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these models implicitly encode syntax. In this work, we systematically study the utility of incorporating dependency trees into pretrained transformers on three representative information extraction tasks: semantic role labeling (SRL), named entity recognition, and relation extraction. We propose and investigate two distinct strategies for incorporating dependency structure: a late fusion approach, which applies a graph neural network on the output of a transformer, and a joint fusion approach, which infuses syntax structure into the transformer attention layers. These strategies are representative of prior work, but we introduce additional model design elements that are necessary for obtaining improved performance. Our empirical analysis demonstrates that these syntax-infused transformers obtain state-of-the-art results on SRL and relation extraction tasks. However, our analysis also reveals a critical shortcoming of these models: we find that their performance gains are highly contingent on the availability of human-annotated dependency parses, which raises important questions regarding the viability of syntax-augmented transformers in real-world applications. 1","{'sequence': 'Much recent work suggests that incorporating syntax information from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these models implicitly encode syntax. In this work, we systematically study the utility of incorporating dependency trees into pretrained transformers on three representative information extraction tasks: semantic role labeling (SRL), named entity recognition, and relation extraction. We propose and investigate two distinct strategies for incorporating dependency structure: a late fusion approach, which applies a graph neural network on the output of a transformer, and a joint fusion approach, which infuses syntax structure into the transformer attention layers. These strategies are representative of prior work, but we introduce additional model design elements that are necessary for obtaining improved performance. Our empirical analysis demonstrates that these syntax-infused transformers obtain state-of-the-art results on SRL and relation extraction tasks. However, our analysis also reveals a critical shortcoming of these models: we find that their performance gains are highly contingent on the availability of human-annotated dependency parses, which raises important questions regarding the viability of syntax-augmented transformers in real-world applications. 1', 'labels': ['Information Extraction', 'Question Answering', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.39043644070625305, 0.05790456011891365, 0.05095573514699936, 0.035043224692344666, 0.034968990832567215, 0.03264918923377991, 0.030865058302879333, 0.03079885058104992, 0.030544502660632133, 0.030525382608175278, 0.02984069474041462, 0.028553327545523643, 0.02779981680214405, 0.02744262106716633, 0.026118174195289612, 0.02254924178123474, 0.022106405347585678, 0.019967354834079742, 0.018075639382004738, 0.01745699718594551, 0.014398511499166489, 0.013603704050183296, 0.007395564112812281]}",0.39043644070625305,Information Extraction,0.39043644070625305
Information Extraction,"Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries","Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major benefit of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.","{'sequence': 'Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major benefit of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'NLP Applications', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Information Extraction', 'Machine Learning for NLP', 'Resources and Evaluation', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10031160712242126, 0.08172405511140823, 0.05717265605926514, 0.05603844299912453, 0.055247485637664795, 0.05144057422876358, 0.05047890916466713, 0.05047517269849777, 0.04955395311117172, 0.04764352738857269, 0.04615255817770958, 0.04104987531900406, 0.0391019806265831, 0.03779960423707962, 0.035400379449129105, 0.034093983471393585, 0.0332244448363781, 0.02948044054210186, 0.02478734590113163, 0.023755868896842003, 0.02116653323173523, 0.01782386377453804, 0.016076698899269104]}",0.10031160712242126,Question Answering,0.03779960423707962
Information Extraction,LSOIE: A Large-Scale Dataset for Supervised Open Information Extraction,"Open Information Extraction (OIE) systems seek to compress the factual propositions of a sentence into a series of n-ary tuples. These tuples are useful for downstream tasks in natural language processing like knowledge base creation, textual entailment, and natural language understanding. However, current OIE datasets are limited in both size and diversity. We introduce a new dataset by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset (LSOIE). Our LSOIE dataset is 20 times larger than the next largest human-annotated OIE dataset. We construct and evaluate several benchmark OIE models on LSOIE, providing baselines for future improvements on the task. Our LSOIE data, models, and code are made publicly available. 1","{'sequence': 'Open Information Extraction (OIE) systems seek to compress the factual propositions of a sentence into a series of n-ary tuples. These tuples are useful for downstream tasks in natural language processing like knowledge base creation, textual entailment, and natural language understanding. However, current OIE datasets are limited in both size and diversity. We introduce a new dataset by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset (LSOIE). Our LSOIE dataset is 20 times larger than the next largest human-annotated OIE dataset. We construct and evaluate several benchmark OIE models on LSOIE, providing baselines for future improvements on the task. Our LSOIE data, models, and code are made publicly available. 1', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Speech and Multimodality', 'NLP Applications', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.4069415032863617, 0.05298539623618126, 0.048745252192020416, 0.044484786689281464, 0.03760415315628052, 0.03649933263659477, 0.03202874958515167, 0.03175792098045349, 0.028252605348825455, 0.027430204674601555, 0.027242187410593033, 0.02520553208887577, 0.023673061281442642, 0.02279582805931568, 0.02187076397240162, 0.0195082426071167, 0.019098171964287758, 0.01815643720328808, 0.01781001314520836, 0.017261164262890816, 0.013808894902467728, 0.013673143461346626, 0.013166680932044983]}",0.4069415032863617,Information Extraction,0.4069415032863617
Information Extraction,Metric-Type Identification for Multi-Level Header Numerical Tables in Scientific Papers,"Numerical tables are widely used to present experimental results in scientific papers. For table understanding, a metric-type is essential to discriminate numbers in the tables. We introduce a new information extraction task, metrictype identification from multi-level header numerical tables, and provide a dataset extracted from scientific papers consisting of header tables, captions, and metric-types. We then propose two joint-learning neural classification and generation schemes featuring pointergenerator-based and BERT-based models. Our results show that the joint models can handle both in-header and out-of-header metric-type identification problems.","{'sequence': 'Numerical tables are widely used to present experimental results in scientific papers. For table understanding, a metric-type is essential to discriminate numbers in the tables. We introduce a new information extraction task, metrictype identification from multi-level header numerical tables, and provide a dataset extracted from scientific papers consisting of header tables, captions, and metric-types. We then propose two joint-learning neural classification and generation schemes featuring pointergenerator-based and BERT-based models. Our results show that the joint models can handle both in-header and out-of-header metric-type identification problems.', 'labels': ['Information Extraction', 'Generation', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Summarization', 'Question Answering', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.5534114241600037, 0.13059942424297333, 0.04241044819355011, 0.026909688487648964, 0.02679101936519146, 0.02222084254026413, 0.019770167768001556, 0.018449710682034492, 0.01670994609594345, 0.016464922577142715, 0.016365811228752136, 0.014396343380212784, 0.013872223906219006, 0.013450069352984428, 0.0111512066796422, 0.01071559265255928, 0.009131744503974915, 0.007217972073704004, 0.0066038197837769985, 0.006398987025022507, 0.006354119628667831, 0.005749334115535021, 0.004855166655033827]}",0.5534114241600037,Information Extraction,0.5534114241600037
Information Extraction,BERT Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection,"Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these models has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 models, tested on two standard benchmarks. SpanBERT and PubMedBERT emerged as the best models in our evaluation: this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch.","{'sequence': 'Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these models has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 models, tested on two standard benchmarks. SpanBERT and PubMedBERT emerged as the best models in our evaluation: this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Information Extraction', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Summarization', 'Speech and Multimodality', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality'], 'scores': [0.2776414752006531, 0.07614319026470184, 0.06911972165107727, 0.06439851224422455, 0.04641842097043991, 0.04300664737820625, 0.0426635779440403, 0.039281606674194336, 0.03567709028720856, 0.032057762145996094, 0.030423998832702637, 0.029230376705527306, 0.025131387636065483, 0.023616038262844086, 0.022945646196603775, 0.021535499021410942, 0.020603323355317116, 0.018027810379862785, 0.01796887256205082, 0.01686074584722519, 0.01668524369597435, 0.015598893165588379, 0.014964116737246513]}",0.2776414752006531,Machine Learning for NLP,0.025131387636065483
Information Extraction,TrNews: Heterogeneous User-Interest Transfer Learning for News Recommendation,"We investigate how to solve the cross-corpus news recommendation for unseen users in the future. This is a problem where traditional content-based recommendation techniques often fail. Luckily, in real-world recommendation services, some publisher (e.g., Daily news) may have accumulated a large corpus with lots of consumers which can be used for a newly deployed publisher (e.g., Political news). To take advantage of the existing corpus, we propose a transfer learning model (dubbed as TrNews) for news recommendation to transfer the knowledge from a source corpus to a target corpus. To tackle the heterogeneity of different user interests and of different word distributions across corpora, we design a translator-based transfer-learning strategy to learn a representation mapping between source and target corpora. The learned translator can be used to generate representations for unseen users in the future. We show through experiments on real-world datasets that TrNews is better than various baselines in terms of four metrics. We also show that our translator is effective among existing transfer strategies.","{'sequence': 'We investigate how to solve the cross-corpus news recommendation for unseen users in the future. This is a problem where traditional content-based recommendation techniques often fail. Luckily, in real-world recommendation services, some publisher (e.g., Daily news) may have accumulated a large corpus with lots of consumers which can be used for a newly deployed publisher (e.g., Political news). To take advantage of the existing corpus, we propose a transfer learning model (dubbed as TrNews) for news recommendation to transfer the knowledge from a source corpus to a target corpus. To tackle the heterogeneity of different user interests and of different word distributions across corpora, we design a translator-based transfer-learning strategy to learn a representation mapping between source and target corpora. The learned translator can be used to generate representations for unseen users in the future. We show through experiments on real-world datasets that TrNews is better than various baselines in terms of four metrics. We also show that our translator is effective among existing transfer strategies.', 'labels': ['Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Generation', 'Machine Translation and Multilinguality', 'Question Answering', 'NLP Applications', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10282345861196518, 0.08530226349830627, 0.06317614018917084, 0.06232369318604469, 0.0623137429356575, 0.06022489070892334, 0.05599803104996681, 0.04226994886994362, 0.041992805898189545, 0.03988910838961601, 0.038869790732860565, 0.03840035945177078, 0.03760288283228874, 0.032746605575084686, 0.03246277943253517, 0.031905706971883774, 0.031191231682896614, 0.030531927943229675, 0.026538243517279625, 0.024577070027589798, 0.024399030953645706, 0.021523989737033844, 0.012936409562826157]}",0.10282345861196518,Information Extraction,0.10282345861196518
Question Answering,NoiseQA: Challenge Set Evaluation for User-Centric Question Answering,"When Question-Answering (QA) systems are deployed in the real world, users query them through a variety of interfaces, such as speaking to voice assistants, typing questions into a search engine, or even translating questions to languages supported by the QA system. While there has been significant community attention devoted to identifying correct answers in passages assuming a perfectly formed question, we show that components in the pipeline that precede an answering engine can introduce varied and considerable sources of error, and performance can degrade substantially based on these upstream noise sources even for powerful pre-trained QA models. We conclude that there is substantial room for progress before QA systems can be effectively deployed, highlight the need for QA evaluation to expand to consider real-world use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans. 1 XQuAD EN ASR MT Keyboard Model EM F1 EM F1 EM F1 EM F1","{'sequence': 'When Question-Answering (QA) systems are deployed in the real world, users query them through a variety of interfaces, such as speaking to voice assistants, typing questions into a search engine, or even translating questions to languages supported by the QA system. While there has been significant community attention devoted to identifying correct answers in passages assuming a perfectly formed question, we show that components in the pipeline that precede an answering engine can introduce varied and considerable sources of error, and performance can degrade substantially based on these upstream noise sources even for powerful pre-trained QA models. We conclude that there is substantial room for progress before QA systems can be effectively deployed, highlight the need for QA evaluation to expand to consider real-world use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans. 1 XQuAD EN ASR MT Keyboard Model EM F1 EM F1 EM F1 EM F1', 'labels': ['Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.5805844068527222, 0.0720963329076767, 0.04894062504172325, 0.041673675179481506, 0.02385309338569641, 0.02319468930363655, 0.018411438912153244, 0.01715632900595665, 0.015233459882438183, 0.014516166411340237, 0.014214937575161457, 0.013011238537728786, 0.01288189273327589, 0.012847287580370903, 0.01233036071062088, 0.012273358181118965, 0.01076869573444128, 0.010765785351395607, 0.01074735727161169, 0.010323585942387581, 0.009640681557357311, 0.007349619176238775, 0.007184915244579315]}",0.5805844068527222,Question Answering,0.5805844068527222
Question Answering,"Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering","Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large number of entities and relations inside knowledge bases (KB), previous work usually utilized sophisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieveand-rerank framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework. Experiments show that: (1) Our IRbased retrieval method is able to collect highquality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multitask learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.","{'sequence': 'Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large number of entities and relations inside knowledge bases (KB), previous work usually utilized sophisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieveand-rerank framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework. Experiments show that: (1) Our IRbased retrieval method is able to collect highquality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multitask learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.', 'labels': ['Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Summarization', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.6818028688430786, 0.03375973179936409, 0.028494758531451225, 0.02644542045891285, 0.022826356813311577, 0.02074291557073593, 0.01626064069569111, 0.016205953434109688, 0.013661791570484638, 0.013660792261362076, 0.01226513646543026, 0.012225673533976078, 0.011942671611905098, 0.011290114372968674, 0.010855816304683685, 0.0108171496540308, 0.010150806978344917, 0.009753577411174774, 0.009124389849603176, 0.007467034738510847, 0.007271849550306797, 0.007035358808934689, 0.005939215887337923]}",0.6818028688430786,Question Answering,0.6818028688430786
Question Answering,Unification-based Reconstruction of Multi-hop Explanations for Science Questions,"This paper presents a novel framework for reconstructing multi-hop explanations in science Question Answering (QA). While existing approaches for multi-hop reasoning build explanations considering each question in isolation, we propose a method to leverage explanatory patterns emerging in a corpus of scientific explanations. Specifically, the framework ranks a set of atomic facts by integrating lexical relevance with the notion of unification power, estimated analysing explanations for similar questions in the corpus. An extensive evaluation is performed on the Worldtree corpus, integrating k-NN clustering and Information Retrieval (IR) techniques. We present the following conclusions: (1) The proposed method achieves results competitive with Transformers, yet being orders of magnitude faster, a feature that makes it scalable to large explanatory corpora (2) The unificationbased mechanism has a key role in reducing semantic drift, contributing to the reconstruction of many hops explanations (6 or more facts) and the ranking of complex inference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed explanations can support downstream QA models, improving the accuracy of BERT by up to 10% overall.","{'sequence': 'This paper presents a novel framework for reconstructing multi-hop explanations in science Question Answering (QA). While existing approaches for multi-hop reasoning build explanations considering each question in isolation, we propose a method to leverage explanatory patterns emerging in a corpus of scientific explanations. Specifically, the framework ranks a set of atomic facts by integrating lexical relevance with the notion of unification power, estimated analysing explanations for similar questions in the corpus. An extensive evaluation is performed on the Worldtree corpus, integrating k-NN clustering and Information Retrieval (IR) techniques. We present the following conclusions: (1) The proposed method achieves results competitive with Transformers, yet being orders of magnitude faster, a feature that makes it scalable to large explanatory corpora (2) The unificationbased mechanism has a key role in reducing semantic drift, contributing to the reconstruction of many hops explanations (6 or more facts) and the ranking of complex inference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed explanations can support downstream QA models, improving the accuracy of BERT by up to 10% overall.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Summarization', 'Generation', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.7592862844467163, 0.01994987204670906, 0.01974444091320038, 0.01919838786125183, 0.018100392073392868, 0.016616754233837128, 0.015645461156964302, 0.013195672072470188, 0.012278775684535503, 0.010563022457063198, 0.010440900921821594, 0.009967847727239132, 0.009939623065292835, 0.009021751582622528, 0.00829455628991127, 0.007391628809273243, 0.006982604507356882, 0.006547784432768822, 0.006194651126861572, 0.005886791739612818, 0.0058656176552176476, 0.00494362460449338, 0.003943473566323519]}",0.7592862844467163,Question Answering,0.7592862844467163
Information Retrieval and Text Mining,Regulatory Compliance through Doc2Doc Information Retrieval: A case study in EU/UK legislation where text similarity has limitations,The contribution of Ms. Eva Katakalou was restricted to the creation and the validation of the datasets as well as to the authoring of the corresponding parts of the manuscript. Figure 1: Number of legislative acts issued by the EU per year. The gold color of the bars indicates how many of the published acts are amendments to older ones. 6 legislation.gov.uk 7 See Appendix A for details on the dataset curation.,"{'sequence': 'The contribution of Ms. Eva Katakalou was restricted to the creation and the validation of the datasets as well as to the authoring of the corresponding parts of the manuscript. Figure 1: Number of legislative acts issued by the EU per year. The gold color of the bars indicates how many of the published acts are amendments to older ones. 6 legislation.gov.uk 7 See Appendix A for details on the dataset curation.', 'labels': ['Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Question Answering', 'Summarization', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP'], 'scores': [0.10322711616754532, 0.07627890259027481, 0.06984193623065948, 0.05584646761417389, 0.051820673048496246, 0.05162189528346062, 0.047780442982912064, 0.04741112142801285, 0.04648921266198158, 0.04523397237062454, 0.043843258172273636, 0.041667595505714417, 0.037421952933073044, 0.03643457964062691, 0.033573850989341736, 0.03192608803510666, 0.03160587698221207, 0.03138565272092819, 0.02957765944302082, 0.028028294444084167, 0.023622071370482445, 0.01824202574789524, 0.01711941510438919]}",0.10322711616754532,Generation,0.041667595505714417
Question Answering,Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets,"Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding train sets. In addition, we find that 60-70% of answers in the test sets are also present in the train sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can generalize, and what drives their overall performance. We find that all models perform substantially worse on questions that cannot be memorized from train sets, with a mean absolute performance difference of 61% between repeated and nonrepeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that train set memorization plays in these benchmarks.","{'sequence': 'Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding train sets. In addition, we find that 60-70% of answers in the test sets are also present in the train sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can generalize, and what drives their overall performance. We find that all models perform substantially worse on questions that cannot be memorized from train sets, with a mean absolute performance difference of 61% between repeated and nonrepeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that train set memorization plays in these benchmarks.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Summarization', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.8696533441543579, 0.01659543626010418, 0.008794748224318027, 0.008683751337230206, 0.00781988725066185, 0.0077078756876289845, 0.007105768192559481, 0.006586543750017881, 0.006566907279193401, 0.006506999488919973, 0.006352629046887159, 0.005517110228538513, 0.005371622275561094, 0.004965831991285086, 0.004567204974591732, 0.004478963557630777, 0.004090459086000919, 0.003962509334087372, 0.0037935744039714336, 0.002974236151203513, 0.0028762235306203365, 0.002639264101162553, 0.0023891415912657976]}",0.8696533441543579,Question Answering,0.8696533441543579
Information Retrieval and Text Mining,Modeling Context in Answer Sentence Selection Systems on a Latency Budget,"Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low latency, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive models designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better accuracy. In this work, we present an approach to efficiently incorporate contextual information in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode context, improves 6% to 11% over noncontextual state of the art in AS2 with minimal impact on system latency. All experiments in this work were conducted in English.","{'sequence': 'Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low latency, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive models designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better accuracy. In this work, we present an approach to efficiently incorporate contextual information in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode context, improves 6% to 11% over noncontextual state of the art in AS2 with minimal impact on system latency. All experiments in this work were conducted in English.', 'labels': ['Question Answering', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.8217114806175232, 0.029576674103736877, 0.014346715994179249, 0.012326410040259361, 0.012034187093377113, 0.01141017209738493, 0.009560707025229931, 0.009356793016195297, 0.009139638394117355, 0.007652639877051115, 0.007385595235973597, 0.0073190005496144295, 0.006126213353127241, 0.005419163964688778, 0.004723037593066692, 0.004662486724555492, 0.00409913482144475, 0.004059868399053812, 0.003948048688471317, 0.003932452294975519, 0.003911023959517479, 0.003753263270482421, 0.003545209998264909]}",0.8217114806175232,Question Answering,0.012034187093377113
Information Retrieval and Text Mining,On the Calibration and Uncertainty of Neural Learning to Rank Models for Conversational Search,"According to the Probability Ranking Principle (PRP), ranking documents in decreasing order of their probability of relevance leads to an optimal document ranking for ad-hoc retrieval. The PRP holds when two conditions are met: [C1] the models are well calibrated, and, [C2] the probabilities of relevance are reported with certainty. We know however that deep neural networks (DNNs) are often not well calibrated and have several sources of uncertainty, and thus [C1] and [C2] might not be satisfied by neural rankers. Given the success of neural Learning to Rank (L2R) approaches-and here, especially BERT-based approaches-we first analyze under which circumstances deterministic neural rankers are calibrated for conversational search problems. Then, motivated by our findings we use two techniques to model the uncertainty of neural rankers leading to the proposed stochastic rankers, which output a predictive distribution of relevance as opposed to point estimates. Our experimental results on the ad-hoc retrieval task of conversation response ranking 1 reveal that (i) BERTbased rankers are not robustly calibrated and that stochastic BERT-based rankers yield better calibration; and (ii) uncertainty estimation is beneficial for both risk-aware neural ranking, i.e. taking into account the uncertainty when ranking documents, and for predicting unanswerable conversational contexts.","{'sequence': 'According to the Probability Ranking Principle (PRP), ranking documents in decreasing order of their probability of relevance leads to an optimal document ranking for ad-hoc retrieval. The PRP holds when two conditions are met: [C1] the models are well calibrated, and, [C2] the probabilities of relevance are reported with certainty. We know however that deep neural networks (DNNs) are often not well calibrated and have several sources of uncertainty, and thus [C1] and [C2] might not be satisfied by neural rankers. Given the success of neural Learning to Rank (L2R) approaches-and here, especially BERT-based approaches-we first analyze under which circumstances deterministic neural rankers are calibrated for conversational search problems. Then, motivated by our findings we use two techniques to model the uncertainty of neural rankers leading to the proposed stochastic rankers, which output a predictive distribution of relevance as opposed to point estimates. Our experimental results on the ad-hoc retrieval task of conversation response ranking 1 reveal that (i) BERTbased rankers are not robustly calibrated and that stochastic BERT-based rankers yield better calibration; and (ii) uncertainty estimation is beneficial for both risk-aware neural ranking, i.e. taking into account the uncertainty when ranking documents, and for predicting unanswerable conversational contexts.', 'labels': ['Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Computational Social Science and Social Media', 'Generation', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Ethics and NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0823495164513588, 0.07775178551673889, 0.06985144317150116, 0.06586302816867828, 0.05509527400135994, 0.04759699106216431, 0.04569762945175171, 0.04312393441796303, 0.04219285398721695, 0.04201614484190941, 0.04182429984211922, 0.041385866701602936, 0.03854319825768471, 0.0383293591439724, 0.035407908260822296, 0.032912444323301315, 0.03239383548498154, 0.03108568675816059, 0.030248891562223434, 0.02947327494621277, 0.026986779645085335, 0.026880742982029915, 0.022989142686128616]}",0.0823495164513588,Discourse and Pragmatics,0.026880742982029915
Question Answering,Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering,"Commonly used information retrieval methods such as TF-IDF in open-domain question answering (QA) systems are insufficient to capture deep semantic matching that goes beyond lexical overlaps. Some recent studies consider the retrieval process as maximum inner product search (MIPS) using dense question and paragraph representations, achieving promising results on several informationseeking QA datasets. However, the pretraining of the dense vector representations is highly resource-demanding, e.g., requires a very large batch size and lots of training steps. In this work, we propose a sample-efficient method to pretrain the paragraph encoder. First, instead of using heuristically created pseudo questionparagraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three opendomain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the datasets, our method achieves more than 4-point absolute improvement in terms of answer exact match.","{'sequence': 'Commonly used information retrieval methods such as TF-IDF in open-domain question answering (QA) systems are insufficient to capture deep semantic matching that goes beyond lexical overlaps. Some recent studies consider the retrieval process as maximum inner product search (MIPS) using dense question and paragraph representations, achieving promising results on several informationseeking QA datasets. However, the pretraining of the dense vector representations is highly resource-demanding, e.g., requires a very large batch size and lots of training steps. In this work, we propose a sample-efficient method to pretrain the paragraph encoder. First, instead of using heuristically created pseudo questionparagraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three opendomain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the datasets, our method achieves more than 4-point absolute improvement in terms of answer exact match.', 'labels': ['Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.20462621748447418, 0.08326902240514755, 0.07141213864088058, 0.06481925398111343, 0.05836954340338707, 0.0572916604578495, 0.045791178941726685, 0.044573742896318436, 0.041530173271894455, 0.03797076269984245, 0.03263857588171959, 0.031358473002910614, 0.030487561598420143, 0.02822892740368843, 0.027739267796278, 0.025086041539907455, 0.021607499569654465, 0.020065760239958763, 0.01886899583041668, 0.018720807507634163, 0.01737828366458416, 0.009568487294018269, 0.008597603999078274]}",0.20462621748447418,Question Answering,0.20462621748447418
Question Answering,Representations for Question Answering from Documents with Tables and Text,"Tables in Web documents are pervasive and can be directly used to answer many of the queries searched on the Web, motivating their integration in question answering. Very often information presented in tables is succinct and hard to interpret with standard language representations. On the other hand, tables often appear within textual context, such as an article describing the table. Using the information from an article as additional context can potentially enrich table representations. In this work we aim to improve question answering from tables by refining table representations based on information from surrounding text. We also present an effective method to combine text and table-based predictions for question answering from full documents, obtaining significant improvements on the Natural Questions dataset.","{'sequence': 'Tables in Web documents are pervasive and can be directly used to answer many of the queries searched on the Web, motivating their integration in question answering. Very often information presented in tables is succinct and hard to interpret with standard language representations. On the other hand, tables often appear within textual context, such as an article describing the table. Using the information from an article as additional context can potentially enrich table representations. In this work we aim to improve question answering from tables by refining table representations based on information from surrounding text. We also present an effective method to combine text and table-based predictions for question answering from full documents, obtaining significant improvements on the Natural Questions dataset.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Machine Translation and Multilinguality', 'Summarization', 'Generation', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.2972726821899414, 0.061822861433029175, 0.05803094431757927, 0.05304500460624695, 0.04616948589682579, 0.044193316251039505, 0.04029437527060509, 0.03791644424200058, 0.03759419918060303, 0.035348374396562576, 0.03531309589743614, 0.03081970475614071, 0.030788632109761238, 0.02588043361902237, 0.02240517921745777, 0.02222241461277008, 0.01981906034052372, 0.01975966803729534, 0.019257791340351105, 0.01908044144511223, 0.01762230508029461, 0.013498206622898579, 0.01184542290866375]}",0.2972726821899414,Question Answering,0.2972726821899414
Information Retrieval and Text Mining,Is the Understanding of Explicit Discourse Relations Required in Machine Reading Comprehension?,"An in-depth analysis of the level of language understanding required by existing Machine Reading Comprehension (MRC) benchmarks can provide insight into the reading capabilities of machines. In this paper, we propose an ablation-based methodology to assess the extent to which MRC datasets evaluate the understanding of explicit discourse relations. We define seven MRC skills which require the understanding of different discourse relations. We then introduce ablation methods that verify whether these skills are required to succeed on a dataset. By observing the drop in performance of neural MRC models evaluated on the original and the modified dataset, we can measure to what degree the dataset requires these skills, in order to be understood correctly. Experiments on three large-scale datasets with the BERT-base and ALBERT-xxlarge model show that the relative changes for all skills are small (less than 6%). These results imply that most of the answered questions in the examined datasets do not require understanding the discourse structure of the text. To specifically probe for natural language understanding, there is a need to design more challenging benchmarks that can correctly evaluate the intended skills 1 .","{'sequence': 'An in-depth analysis of the level of language understanding required by existing Machine Reading Comprehension (MRC) benchmarks can provide insight into the reading capabilities of machines. In this paper, we propose an ablation-based methodology to assess the extent to which MRC datasets evaluate the understanding of explicit discourse relations. We define seven MRC skills which require the understanding of different discourse relations. We then introduce ablation methods that verify whether these skills are required to succeed on a dataset. By observing the drop in performance of neural MRC models evaluated on the original and the modified dataset, we can measure to what degree the dataset requires these skills, in order to be understood correctly. Experiments on three large-scale datasets with the BERT-base and ALBERT-xxlarge model show that the relative changes for all skills are small (less than 6%). These results imply that most of the answered questions in the examined datasets do not require understanding the discourse structure of the text. To specifically probe for natural language understanding, there is a need to design more challenging benchmarks that can correctly evaluate the intended skills 1 .', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Ethics and NLP', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Information Extraction', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1023654118180275, 0.07547000050544739, 0.06756100803613663, 0.059586651623249054, 0.05777091532945633, 0.057194557040929794, 0.057138532400131226, 0.04806094244122505, 0.045519474893808365, 0.043946847319602966, 0.0412408672273159, 0.0394112728536129, 0.036684032529592514, 0.03602605313062668, 0.03352225944399834, 0.030329851433634758, 0.029562044888734818, 0.028528962284326553, 0.028517110273241997, 0.026223279535770416, 0.02324308641254902, 0.022007983177900314, 0.010088878683745861]}",0.1023654118180275,Resources and Evaluation,0.029562044888734818
Question Answering,Query Generation for Multimodal Documents,"This paper studies the problem of generating likely queries for multimodal documents with images. Our application scenario is enabling efficient ""first-stage retrieval"" of relevant documents, by attaching generated queries to documents before indexing. We can then index this expanded text to efficiently narrow down to candidate matches using inverted index, so that expensive reranking can follow. Our evaluation results show that our proposed multimodal representation meaningfully improves relevance ranking. More importantly, our framework can achieve the state of the art in the first-stage retrieval scenarios.","{'sequence': 'This paper studies the problem of generating likely queries for multimodal documents with images. Our application scenario is enabling efficient ""first-stage retrieval"" of relevant documents, by attaching generated queries to documents before indexing. We can then index this expanded text to efficiently narrow down to candidate matches using inverted index, so that expensive reranking can follow. Our evaluation results show that our proposed multimodal representation meaningfully improves relevance ranking. More importantly, our framework can achieve the state of the art in the first-stage retrieval scenarios.', 'labels': ['Generation', 'Resources and Evaluation', 'Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'NLP Applications', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.329752117395401, 0.1255994439125061, 0.05198720842599869, 0.04878108948469162, 0.04801645502448082, 0.0421128012239933, 0.038120366632938385, 0.03343943879008293, 0.03273392468690872, 0.030295997858047485, 0.02396984025835991, 0.023023514077067375, 0.02258921228349209, 0.021747931838035583, 0.02057449147105217, 0.01952022686600685, 0.018950961530208588, 0.013686489313840866, 0.01245369203388691, 0.012306460179388523, 0.010580351576209068, 0.010389311239123344, 0.009368502534925938]}",0.329752117395401,Generation,0.05198720842599869
Information Retrieval and Text Mining,Benchmarking Machine Reading Comprehension: A Psychological Perspective,"Machine reading comprehension (MRC) has received considerable attention as a benchmark for natural language understanding. However, the conventional task design of MRC lacks explainability beyond the model interpretation, i.e., reading comprehension by a model cannot be explained in human terms. To this end, this position paper provides a theoretical basis for the design of MRC datasets based on psychology as well as psychometrics, and summarizes it in terms of the prerequisites for benchmarking MRC. We conclude that future datasets should (i) evaluate the capability of the model for constructing a coherent and grounded representation to understand contextdependent situations and (ii) ensure substantive validity by shortcut-proof questions and explanation as a part of the task design.","{'sequence': 'Machine reading comprehension (MRC) has received considerable attention as a benchmark for natural language understanding. However, the conventional task design of MRC lacks explainability beyond the model interpretation, i.e., reading comprehension by a model cannot be explained in human terms. To this end, this position paper provides a theoretical basis for the design of MRC datasets based on psychology as well as psychometrics, and summarizes it in terms of the prerequisites for benchmarking MRC. We conclude that future datasets should (i) evaluate the capability of the model for constructing a coherent and grounded representation to understand contextdependent situations and (ii) ensure substantive validity by shortcut-proof questions and explanation as a part of the task design.', 'labels': ['Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Ethics and NLP', 'NLP Applications', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Question Answering', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12583652138710022, 0.10319487005472183, 0.06610028445720673, 0.06155001372098923, 0.05752502754330635, 0.05458448827266693, 0.053435198962688446, 0.05320404842495918, 0.04816002771258354, 0.04501502960920334, 0.03996983543038368, 0.03781991824507713, 0.03514610603451729, 0.034609172493219376, 0.033521868288517, 0.033205095678567886, 0.02492380701005459, 0.020043745636940002, 0.017820438370108604, 0.01699424535036087, 0.014815832488238811, 0.013599387370049953, 0.008925044909119606]}",0.12583652138710022,Resources and Evaluation,0.013599387370049953
Question Answering,Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?,"Multi-hop question answering (QA) requires a model to retrieve and integrate information from multiple passages to answer a question. Rapid progress has been made on multi-hop QA systems with regard to standard evaluation metrics, including EM and F1. However, by simply evaluating the correctness of the answers, it is unclear to what extent these systems have learned the ability to perform multihop reasoning. In this paper, we propose an additional sub-question evaluation for the multihop QA dataset HotpotQA, in order to shed some light on explaining the reasoning process of QA systems in answering complex questions. We adopt a neural decomposition model to generate sub-questions for a multi-hop question, followed by extracting the corresponding sub-answers. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system.","{'sequence': 'Multi-hop question answering (QA) requires a model to retrieve and integrate information from multiple passages to answer a question. Rapid progress has been made on multi-hop QA systems with regard to standard evaluation metrics, including EM and F1. However, by simply evaluating the correctness of the answers, it is unclear to what extent these systems have learned the ability to perform multihop reasoning. In this paper, we propose an additional sub-question evaluation for the multihop QA dataset HotpotQA, in order to shed some light on explaining the reasoning process of QA systems in answering complex questions. We adopt a neural decomposition model to generate sub-questions for a multi-hop question, followed by extracting the corresponding sub-answers. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system.', 'labels': ['Question Answering', 'Information Extraction', 'Generation', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'NLP Applications', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.1769132763147354, 0.11555414646863937, 0.08523618429899216, 0.06712508201599121, 0.06571370363235474, 0.05113668367266655, 0.04701872169971466, 0.03388715907931328, 0.03384062275290489, 0.03261800855398178, 0.032405316829681396, 0.03134326636791229, 0.02765006572008133, 0.027627360075712204, 0.02444078028202057, 0.02282877080142498, 0.021279798820614815, 0.021036382764577866, 0.018124444410204887, 0.01756693422794342, 0.01637076400220394, 0.016065942123532295, 0.01421660091727972]}",0.1769132763147354,Question Answering,0.1769132763147354
Question Answering,Complementary Evidence Identification in Open-Domain Question Answering,"This paper proposes a new problem of complementary evidence identification for opendomain question answering (QA). The problem aims to efficiently find a small set of passages that covers full evidence from multiple aspects as to answer a complex question. To this end, we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set, in addition to the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain.","{'sequence': 'This paper proposes a new problem of complementary evidence identification for opendomain question answering (QA). The problem aims to efficiently find a small set of passages that covers full evidence from multiple aspects as to answer a complex question. To this end, we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set, in addition to the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Resources and Evaluation', 'Summarization', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'NLP Applications', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.6347662806510925, 0.03738168254494667, 0.027675559744238853, 0.02463333122432232, 0.022526029497385025, 0.022456998005509377, 0.01909107342362404, 0.01908326894044876, 0.019072089344263077, 0.018883606418967247, 0.018216751515865326, 0.017243245616555214, 0.016677288338541985, 0.015723198652267456, 0.01523573324084282, 0.010696569457650185, 0.010099760256707668, 0.009817907586693764, 0.009531369432806969, 0.00919975247234106, 0.009140503592789173, 0.006474833935499191, 0.006373226642608643]}",0.6347662806510925,Question Answering,0.6347662806510925
Question Answering,Complex Question Answering on knowledge graphs using machine translation and multi-task learning,"Question answering (QA) over a knowledge graph (KG) is a task of answering a natural language (NL) query using the information stored in KG. In a real-world industrial setting, this involves addressing multiple challenges including entity linking, multi-hop reasoning over KG, etc. Traditional approaches handle these challenges in a modularized sequential manner where errors in one module lead to the accumulation of errors in downstream modules. Often these challenges are inter-related and the solutions to them can reinforce each other when handled simultaneously in an end-to-end learning setup. To this end, we propose a multi-task BERT based Neural Machine Translation (NMT) model to address these challenges. Through experimental analysis, we demonstrate the efficacy of our proposed approach on one publicly available and one proprietary dataset.","{'sequence': 'Question answering (QA) over a knowledge graph (KG) is a task of answering a natural language (NL) query using the information stored in KG. In a real-world industrial setting, this involves addressing multiple challenges including entity linking, multi-hop reasoning over KG, etc. Traditional approaches handle these challenges in a modularized sequential manner where errors in one module lead to the accumulation of errors in downstream modules. Often these challenges are inter-related and the solutions to them can reinforce each other when handled simultaneously in an end-to-end learning setup. To this end, we propose a multi-task BERT based Neural Machine Translation (NMT) model to address these challenges. Through experimental analysis, we demonstrate the efficacy of our proposed approach on one publicly available and one proprietary dataset.', 'labels': ['Question Answering', 'NLP Applications', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Summarization', 'Generation', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.6878100633621216, 0.030581539496779442, 0.0300575140863657, 0.02915448509156704, 0.021403145045042038, 0.019842403009533882, 0.01842934638261795, 0.017061971127986908, 0.01705952361226082, 0.016667498275637627, 0.0130417849868536, 0.012808387167751789, 0.01239717099815607, 0.011181876994669437, 0.009137850254774094, 0.008256152272224426, 0.007455578073859215, 0.007048984058201313, 0.0067848325707018375, 0.006428590510040522, 0.006183812394738197, 0.006066425237804651, 0.005140987224876881]}",0.6878100633621216,Question Answering,0.6878100633621216
Question Answering,Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering,"Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-theart results on the Natural Questions and Triv-iaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.","{'sequence': 'Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-theart results on the Natural Questions and Triv-iaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.', 'labels': ['Generation', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'NLP Applications', 'Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Learning for NLP', 'Ethics and NLP', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.40257346630096436, 0.2857462763786316, 0.03916056081652641, 0.025155572220683098, 0.025067895650863647, 0.024788761511445045, 0.023107638582587242, 0.019574560225009918, 0.015331736765801907, 0.013934562914073467, 0.013840984553098679, 0.013085337355732918, 0.01297326572239399, 0.012388437055051327, 0.01044832356274128, 0.010144459083676338, 0.010114481672644615, 0.008966642431914806, 0.007823683321475983, 0.007304414641112089, 0.007262277416884899, 0.006147142965346575, 0.0050595286302268505]}",0.40257346630096436,Generation,0.2857462763786316
Question Answering,Knowledge Base Question Answering through Recursive Hypergraphs,"Knowledge Base Question Answering (KBQA) is the problem of predicting an answer for a factoid question over a given knowledge base (KB). Answering questions typically requires reasoning over multiple links in the given KB. Humans tend to answer questions by grouping different objects to perform reasoning over acquired knowledge. Hypergraphs provide a natural tool to model group relationships. In this work, inspired by typical human intelligence, we propose a new method for KBQA based on hypergraphs. Existing methods for KBQA, though effective, do not explicitly incorporate the recursive relational group structure in the given KB. Our method, which we name RecHyperNet (Recursive Hypergraph Network), exploits a new way of modelling KBs through recursive hypergraphs to organise such group relationships in KBs. Experiments on multiple KBQA benchmarks demonstrate the effectiveness of the proposed RecHyperNet. We have released the code.","{'sequence': 'Knowledge Base Question Answering (KBQA) is the problem of predicting an answer for a factoid question over a given knowledge base (KB). Answering questions typically requires reasoning over multiple links in the given KB. Humans tend to answer questions by grouping different objects to perform reasoning over acquired knowledge. Hypergraphs provide a natural tool to model group relationships. In this work, inspired by typical human intelligence, we propose a new method for KBQA based on hypergraphs. Existing methods for KBQA, though effective, do not explicitly incorporate the recursive relational group structure in the given KB. Our method, which we name RecHyperNet (Recursive Hypergraph Network), exploits a new way of modelling KBs through recursive hypergraphs to organise such group relationships in KBs. Experiments on multiple KBQA benchmarks demonstrate the effectiveness of the proposed RecHyperNet. We have released the code.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Extraction', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'NLP Applications', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.6022784113883972, 0.0432414673268795, 0.030696414411067963, 0.028328118845820427, 0.022749291732907295, 0.0211601871997118, 0.02101878821849823, 0.020733509212732315, 0.020500052720308304, 0.018739590421319008, 0.01764070801436901, 0.01744806207716465, 0.017389612272381783, 0.01660715602338314, 0.015577916987240314, 0.012793105095624924, 0.011967317201197147, 0.01191496942192316, 0.011878189630806446, 0.011478057131171227, 0.008993016555905342, 0.008960084989666939, 0.007905983366072178]}",0.6022784113883972,Question Answering,0.6022784113883972
Question Answering,Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation,"A major obstacle to the wide-spread adoption of neural retrieval models is that they require large supervised training sets to surpass traditional term-based techniques, which are constructed from raw corpora. In this paper, we propose an approach to zero-shot learning for passage retrieval that uses synthetic question generation to close this gap. The question generation system is trained on general domain data, but is applied to documents in the targeted domain. This allows us to create arbitrarily large, yet noisy, question-passage relevance pairs that are domain specific. Furthermore, when this is coupled with a simple hybrid termneural model, first-stage retrieval performance can be improved further. Empirically, we show that this is an effective strategy for building neural passage retrieval models in the absence of large training corpora. Depending on the domain, this technique can even approach the accuracy of supervised models.","{'sequence': 'A major obstacle to the wide-spread adoption of neural retrieval models is that they require large supervised training sets to surpass traditional term-based techniques, which are constructed from raw corpora. In this paper, we propose an approach to zero-shot learning for passage retrieval that uses synthetic question generation to close this gap. The question generation system is trained on general domain data, but is applied to documents in the targeted domain. This allows us to create arbitrarily large, yet noisy, question-passage relevance pairs that are domain specific. Furthermore, when this is coupled with a simple hybrid termneural model, first-stage retrieval performance can be improved further. Empirically, we show that this is an effective strategy for building neural passage retrieval models in the absence of large training corpora. Depending on the domain, this technique can even approach the accuracy of supervised models.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Question Answering', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Information Extraction', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.22959883511066437, 0.07367279380559921, 0.06746847182512283, 0.054825689643621445, 0.05171511694788933, 0.05075571686029434, 0.04446502402424812, 0.04177982360124588, 0.036149900406599045, 0.03417639434337616, 0.031378693878650665, 0.03068563900887966, 0.027600692585110664, 0.026076139882206917, 0.025945106521248817, 0.025860842317342758, 0.025546224787831306, 0.024659350514411926, 0.02346222847700119, 0.020708823576569557, 0.019689396023750305, 0.017404239624738693, 0.016374865546822548]}",0.22959883511066437,Generation,0.06746847182512283
Interpretability and Analysis of Models for NLP,Measuring and Improving Faithfulness of Attention in Neural Machine Translation,"While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model's true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases.","{'sequence': ""While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model's true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases."", 'labels': ['Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Information Extraction', 'Machine Learning for NLP', 'Generation', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09846029430627823, 0.06765839457511902, 0.059664901345968246, 0.05388612672686577, 0.05333281680941582, 0.05047815293073654, 0.047750018537044525, 0.04450899735093117, 0.04325730726122856, 0.04314905032515526, 0.043079592287540436, 0.040632449090480804, 0.04056607931852341, 0.038426488637924194, 0.037615805864334106, 0.03507574647665024, 0.03434907644987106, 0.03121883049607277, 0.030823811888694763, 0.028985710814595222, 0.028865346685051918, 0.024716151878237724, 0.023498721420764923]}",0.09846029430627823,Machine Translation and Multilinguality,0.05047815293073654
Interpretability and Analysis of Models for NLP,Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?,"Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of 'probing' tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks. 1 * Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion.","{'sequence': ""Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of 'probing' tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks. 1 * Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion."", 'labels': ['Interpretability and Analysis of Models for NLP', 'Question Answering', 'Ethics and NLP', 'Generation', 'Machine Learning for NLP', 'Resources and Evaluation', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Summarization', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.1179644912481308, 0.1006077378988266, 0.08373337984085083, 0.07028587907552719, 0.06525256484746933, 0.062199149280786514, 0.06139826774597168, 0.04564061015844345, 0.04023783653974533, 0.040010467171669006, 0.039278414100408554, 0.03841949999332428, 0.031431250274181366, 0.027425937354564667, 0.026458395645022392, 0.026195254176855087, 0.02239559032022953, 0.022113407030701637, 0.02066918835043907, 0.016203969717025757, 0.01546725444495678, 0.013858425430953503, 0.012752991169691086]}",0.1179644912481308,Interpretability and Analysis of Models for NLP,0.1179644912481308
Interpretability and Analysis of Models for NLP,On the evolution of syntactic information encoded by BERT's contextualized representations,"The adaptation of pretrained language models to solve supervised tasks has become a baseline in NLP, and many recent works have focused on studying how linguistic information is encoded in the pretrained sentence representations. Among other information, it has been shown that entire syntax trees are implicitly embedded in the geometry of such models. As these models are often fine-tuned, it becomes increasingly important to understand how the encoded knowledge evolves along the fine-tuning. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semanticsrelated tasks) in different ways along the finetuning process depending on the task.","{'sequence': 'The adaptation of pretrained language models to solve supervised tasks has become a baseline in NLP, and many recent works have focused on studying how linguistic information is encoded in the pretrained sentence representations. Among other information, it has been shown that entire syntax trees are implicitly embedded in the geometry of such models. As these models are often fine-tuned, it becomes increasingly important to understand how the encoded knowledge evolves along the fine-tuning. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semanticsrelated tasks) in different ways along the finetuning process depending on the task.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Dialogue and Interactive Systems', 'Information Extraction', 'Question Answering', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Generation', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Speech and Multimodality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Summarization', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.16298972070217133, 0.13986735045909882, 0.09024374932050705, 0.05070602521300316, 0.04942307248711586, 0.047848913818597794, 0.04744876176118851, 0.045302864164114, 0.04170123487710953, 0.039519671350717545, 0.0366690419614315, 0.03275499492883682, 0.03029942698776722, 0.02713075466454029, 0.025291763246059418, 0.025103086605668068, 0.02158224768936634, 0.018578287214040756, 0.017727838829159737, 0.0133433248847723, 0.012956061400473118, 0.01274765096604824, 0.01076423842459917]}",0.16298972070217133,Interpretability and Analysis of Models for NLP,0.16298972070217133
Interpretability and Analysis of Models for NLP,Are Neural Networks Extracting Linguistic Properties or Memorizing Training Data? An Observation with a Multilingual Probe for Predicting Tense,"We evaluate the ability of Bert embeddings to represent tense information, taking French and Chinese as a case study. In French, the tense information is expressed by verb morphology and can be captured by simple surface information. On the contrary, tense interpretation in Chinese is driven by abstract, lexical, syntactic and even pragmatic information. We show that while French tenses can easily be predicted from sentence representations, results drop sharply for Chinese, which suggests that Bert is more likely to memorize shallow patterns from the training data rather than uncover abstract properties.","{'sequence': 'We evaluate the ability of Bert embeddings to represent tense information, taking French and Chinese as a case study. In French, the tense information is expressed by verb morphology and can be captured by simple surface information. On the contrary, tense interpretation in Chinese is driven by abstract, lexical, syntactic and even pragmatic information. We show that while French tenses can easily be predicted from sentence representations, results drop sharply for Chinese, which suggests that Bert is more likely to memorize shallow patterns from the training data rather than uncover abstract properties.', 'labels': ['Information Extraction', 'Discourse and Pragmatics', 'Question Answering', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Generation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Machine Learning for NLP', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1359945684671402, 0.08147995918989182, 0.07440165430307388, 0.06811629980802536, 0.060804128646850586, 0.05858867987990379, 0.052884116768836975, 0.05021285638213158, 0.04848417267203331, 0.046968188136816025, 0.041945330798625946, 0.03831377252936363, 0.035920366644859314, 0.03460045903921127, 0.03299323096871376, 0.03130858764052391, 0.027723196893930435, 0.017553173005580902, 0.015328090637922287, 0.013718764297664165, 0.013524276204407215, 0.010252845473587513, 0.008883316069841385]}",0.1359945684671402,Information Extraction,0.03130858764052391
Interpretability and Analysis of Models for NLP,Language Modelling as a Multi-Task Problem,"In this paper, we propose to study language modelling as a multi-task problem, bringing together three strands of research: multitask learning, linguistics, and interpretability. Based on hypotheses derived from linguistic theory, we investigate whether language models adhere to learning principles of multi-task learning during training. To showcase the idea, we analyse the generalisation behaviour of language models as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of language modelling. We argue that this insight is valuable for multitask learning, linguistics and interpretability research and can lead to exciting new findings in all three domains.","{'sequence': 'In this paper, we propose to study language modelling as a multi-task problem, bringing together three strands of research: multitask learning, linguistics, and interpretability. Based on hypotheses derived from linguistic theory, we investigate whether language models adhere to learning principles of multi-task learning during training. To showcase the idea, we analyse the generalisation behaviour of language models as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of language modelling. We argue that this insight is valuable for multitask learning, linguistics and interpretability research and can lead to exciting new findings in all three domains.', 'labels': ['Speech and Multimodality', 'Computational Social Science and Social Media', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Ethics and NLP', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Question Answering', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12550437450408936, 0.07143548876047134, 0.06825743615627289, 0.06541034579277039, 0.06156091392040253, 0.057771068066358566, 0.0563947893679142, 0.046901047229766846, 0.046510253101587296, 0.04335092008113861, 0.040585510432720184, 0.03975515812635422, 0.035267431288957596, 0.03434739634394646, 0.03165656328201294, 0.03128618001937866, 0.028125761076807976, 0.025641651824116707, 0.021963419392704964, 0.020877692848443985, 0.02071315050125122, 0.01598230190575123, 0.01070112269371748]}",0.12550437450408936,Speech and Multimodality,0.06541034579277039
Interpretability and Analysis of Models for NLP,Challenges in Automated Debiasing for Toxic Language Detection,"Warning: this paper contains content that may be offensive or upsetting. Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept study. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.","{'sequence': 'Warning: this paper contains content that may be offensive or upsetting. Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept study. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Information Extraction', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'NLP Applications', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07458875328302383, 0.07268524914979935, 0.06904397904872894, 0.06545969843864441, 0.06518252938985825, 0.0567750446498394, 0.05331087484955788, 0.04616120085120201, 0.04365255683660507, 0.04351811856031418, 0.04351581633090973, 0.0432865247130394, 0.04202160984277725, 0.03950527682900429, 0.0359262190759182, 0.03409992903470993, 0.03328299522399902, 0.027967795729637146, 0.02727864682674408, 0.02521214634180069, 0.023599587380886078, 0.019660620018839836, 0.01426488533616066]}",0.07458875328302383,Question Answering,0.027967795729637146
Interpretability and Analysis of Models for NLP,Paraphrases do not explain word analogies,"Many types of distributional word embeddings (weakly) encode linguistic regularities as directions (the difference between jump and jumped will be in a similar direction to that of walk and walked, and so on). Several attempts have been made to explain this fact. We respond to Allen and Hospedales' recent (ICML, 2019) theoretical explanation, which claims that word2vec and GloVe will encode linguistic regularities whenever a specific relation of paraphrase holds between the four words involved in the regularity. We demonstrate that the explanation does not go through: the paraphrase relations needed under this explanation do not hold empirically.","{'sequence': ""Many types of distributional word embeddings (weakly) encode linguistic regularities as directions (the difference between jump and jumped will be in a similar direction to that of walk and walked, and so on). Several attempts have been made to explain this fact. We respond to Allen and Hospedales' recent (ICML, 2019) theoretical explanation, which claims that word2vec and GloVe will encode linguistic regularities whenever a specific relation of paraphrase holds between the four words involved in the regularity. We demonstrate that the explanation does not go through: the paraphrase relations needed under this explanation do not hold empirically."", 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Generation', 'Discourse and Pragmatics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Resources and Evaluation', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08340359479188919, 0.08257494121789932, 0.07692165672779083, 0.06653409451246262, 0.06309761852025986, 0.05586538463830948, 0.053788937628269196, 0.05276567116379738, 0.051617007702589035, 0.051084041595458984, 0.04648752510547638, 0.042892105877399445, 0.03280520439147949, 0.029433192685246468, 0.02900228463113308, 0.026718322187662125, 0.02614131197333336, 0.024776984006166458, 0.02332186885178089, 0.022763824090361595, 0.02205871418118477, 0.020439019426703453, 0.015506773255765438]}",0.08340359479188919,Dialogue and Interactive Systems,0.06309761852025986
Interpretability and Analysis of Models for NLP,Evaluating Neural Model Robustness for Machine Comprehension,"We evaluate neural model robustness to adversarial attacks using different types of linguistic unit perturbations -character and word, and propose a new method for strategic sentencelevel perturbations. We experiment with different amounts of perturbations to examine model confidence and misclassification rate, and contrast model performance with different embeddings BERT and ELMo on two benchmark datasets SQuAD and TriviaQA. We demonstrate how to improve model performance during an adversarial attack by using ensembles. Finally, we analyze factors that affect model behavior under adversarial attack, and develop a new model to predict errors during attacks. Our novel findings reveal that (a) unlike BERT, models that use ELMo embeddings are more susceptible to adversarial attacks, (b) unlike word and paraphrase, character perturbations affect the model the most but are most easily compensated for by adversarial training, (c) word perturbations lead to more high-confidence misclassifications compared to sentence-and character-level perturbations, (d) the type of question and model answer length (the longer the answer the more likely it is to be incorrect) is the most predictive of model errors in adversarial setting, and (e) conclusions about model behavior are dataset-specific.","{'sequence': 'We evaluate neural model robustness to adversarial attacks using different types of linguistic unit perturbations -character and word, and propose a new method for strategic sentencelevel perturbations. We experiment with different amounts of perturbations to examine model confidence and misclassification rate, and contrast model performance with different embeddings BERT and ELMo on two benchmark datasets SQuAD and TriviaQA. We demonstrate how to improve model performance during an adversarial attack by using ensembles. Finally, we analyze factors that affect model behavior under adversarial attack, and develop a new model to predict errors during attacks. Our novel findings reveal that (a) unlike BERT, models that use ELMo embeddings are more susceptible to adversarial attacks, (b) unlike word and paraphrase, character perturbations affect the model the most but are most easily compensated for by adversarial training, (c) word perturbations lead to more high-confidence misclassifications compared to sentence-and character-level perturbations, (d) the type of question and model answer length (the longer the answer the more likely it is to be incorrect) is the most predictive of model errors in adversarial setting, and (e) conclusions about model behavior are dataset-specific.', 'labels': ['Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Information Extraction', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining'], 'scores': [0.09411244094371796, 0.07133310288190842, 0.06483878940343857, 0.06303752213716507, 0.0606108196079731, 0.05035930871963501, 0.04776081442832947, 0.04661368951201439, 0.04379306733608246, 0.042355526238679886, 0.04165506362915039, 0.037889741361141205, 0.035945404320955276, 0.03516146168112755, 0.035004206001758575, 0.034012965857982635, 0.0339181125164032, 0.03338177874684334, 0.029745927080512047, 0.027600927278399467, 0.02597607858479023, 0.025266174226999283, 0.019627122208476067]}",0.09411244094371796,Resources and Evaluation,0.0606108196079731
Interpretability and Analysis of Models for NLP,On Hallucination and Predictive Uncertainty in Conditional Language Generation,"Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant.","{'sequence': 'Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant.', 'labels': ['Generation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Question Answering', 'Information Extraction', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.254239022731781, 0.07903411984443665, 0.0746380090713501, 0.04920968785881996, 0.044428516179323196, 0.040244486182928085, 0.03915875777602196, 0.038063302636146545, 0.03547276556491852, 0.03542058542370796, 0.030893931165337563, 0.030871834605932236, 0.029314937070012093, 0.02872110716998577, 0.027763376012444496, 0.02725128084421158, 0.026741599664092064, 0.02537258341908455, 0.0225501898676157, 0.017618728801608086, 0.016144974157214165, 0.013706158846616745, 0.013140087947249413]}",0.254239022731781,Generation,0.0746380090713501
Interpretability and Analysis of Models for NLP,DISK-CSV: Distilling Interpretable Semantic Knowledge with a Class Semantic Vector,"Neural networks (NN) applied to natural language processing (NLP) are becoming deeper and more complex, making them increasingly difficult to understand and interpret. Even in applications of limited scope on fixed data, the creation of these complex ""black-boxes"" creates substantial challenges for debugging, understanding, and generalization. But rapid development in this field has now lead to building more straightforward and interpretable models. We propose a new technique (DISK-CSV) to distill knowledge concurrently from any neural network architecture for text classification, captured as a lightweight interpretable/explainable classifier. Across multiple datasets, our approach achieves better performance than the target black-box. In addition, our approach provides better explanations than existing techniques.","{'sequence': 'Neural networks (NN) applied to natural language processing (NLP) are becoming deeper and more complex, making them increasingly difficult to understand and interpret. Even in applications of limited scope on fixed data, the creation of these complex ""black-boxes"" creates substantial challenges for debugging, understanding, and generalization. But rapid development in this field has now lead to building more straightforward and interpretable models. We propose a new technique (DISK-CSV) to distill knowledge concurrently from any neural network architecture for text classification, captured as a lightweight interpretable/explainable classifier. Across multiple datasets, our approach achieves better performance than the target black-box. In addition, our approach provides better explanations than existing techniques.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Speech and Multimodality', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.20208482444286346, 0.13678181171417236, 0.10569769889116287, 0.06671222299337387, 0.05030965059995651, 0.03926868364214897, 0.03719082474708557, 0.03427128121256828, 0.03356817737221718, 0.03244226798415184, 0.032038453966379166, 0.031268056482076645, 0.02864031121134758, 0.023480631411075592, 0.022648457437753677, 0.01996486634016037, 0.019652515649795532, 0.01796392910182476, 0.015820642933249474, 0.015363171696662903, 0.013272732496261597, 0.011470199562609196, 0.010088598355650902]}",0.20208482444286346,NLP Applications,0.13678181171417236
Interpretability and Analysis of Models for NLP,Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings,"In contrast to their word-or sentence-level counterparts, character embeddings are still poorly understood. We aim at closing this gap with an in-depth study of English character embeddings. For this, we use resources from research on grapheme-color synesthesia -a neuropsychological phenomenon where letters are associated with colors -, which give us insight into which characters are similar for synesthetes and how characters are organized in color space. Comparing 10 different character embeddings, we ask: How similar are character embeddings to a synesthete's perception of characters? And how similar are character embeddings extracted from different models? We find that LSTMs agree with humans more than transformers. Comparing across tasks, grapheme-to-phoneme conversion results in the most human-like character embeddings. Finally, ELMo embeddings differ from both humans and other models. * Equal contribution. 1 This fact has led to the establishment of a workshop with the same name: https://blackboxnlp.github.io","{'sequence': ""In contrast to their word-or sentence-level counterparts, character embeddings are still poorly understood. We aim at closing this gap with an in-depth study of English character embeddings. For this, we use resources from research on grapheme-color synesthesia -a neuropsychological phenomenon where letters are associated with colors -, which give us insight into which characters are similar for synesthetes and how characters are organized in color space. Comparing 10 different character embeddings, we ask: How similar are character embeddings to a synesthete's perception of characters? And how similar are character embeddings extracted from different models? We find that LSTMs agree with humans more than transformers. Comparing across tasks, grapheme-to-phoneme conversion results in the most human-like character embeddings. Finally, ELMo embeddings differ from both humans and other models. * Equal contribution. 1 This fact has led to the establishment of a workshop with the same name: https://blackboxnlp.github.io"", 'labels': ['Question Answering', 'Resources and Evaluation', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'NLP Applications', 'Machine Learning for NLP', 'Generation', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11789457499980927, 0.10537360608577728, 0.07030816376209259, 0.06062785163521767, 0.05902569741010666, 0.0548887774348259, 0.04342033714056015, 0.03889238461852074, 0.03772812336683273, 0.03747260570526123, 0.0362626388669014, 0.03597324714064598, 0.030677076429128647, 0.03046470694243908, 0.02990589290857315, 0.029676955193281174, 0.0292345080524683, 0.028391562402248383, 0.027214128524065018, 0.02527453750371933, 0.02480144612491131, 0.02368328534066677, 0.022807834669947624]}",0.11789457499980927,Question Answering,0.029676955193281174
Interpretability and Analysis of Models for NLP,BERTese: Learning to Speak to BERT,"Large pre-trained language models have been shown to encode large amounts of world and commonsense knowledge in their parameters, leading to substantial interest in methods for extracting that knowledge. In past work, knowledge was extracted by taking manuallyauthored queries and gathering paraphrases for them using a separate pipeline. In this work, we propose a method for automatically rewriting queries into ""BERTese"", a paraphrase query that is directly optimized towards better knowledge extraction. To encourage meaningful rewrites, we add auxiliary loss functions that encourage the query to correspond to actual language tokens. We empirically show our approach outperforms competing baselines, obviating the need for complex pipelines. Moreover, BERTese provides some insight into the type of language that helps language models perform knowledge extraction.","{'sequence': 'Large pre-trained language models have been shown to encode large amounts of world and commonsense knowledge in their parameters, leading to substantial interest in methods for extracting that knowledge. In past work, knowledge was extracted by taking manuallyauthored queries and gathering paraphrases for them using a separate pipeline. In this work, we propose a method for automatically rewriting queries into ""BERTese"", a paraphrase query that is directly optimized towards better knowledge extraction. To encourage meaningful rewrites, we add auxiliary loss functions that encourage the query to correspond to actual language tokens. We empirically show our approach outperforms competing baselines, obviating the need for complex pipelines. Moreover, BERTese provides some insight into the type of language that helps language models perform knowledge extraction.', 'labels': ['Information Extraction', 'Speech and Multimodality', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Resources and Evaluation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10576865822076797, 0.059034936130046844, 0.057955726981163025, 0.053515784442424774, 0.052079856395721436, 0.05131542682647705, 0.050634779036045074, 0.05012071132659912, 0.04858282580971718, 0.04836377128958702, 0.047561582177877426, 0.046893857419490814, 0.04281416907906532, 0.03418341279029846, 0.03262750059366226, 0.030036235228180885, 0.02990388497710228, 0.028939461335539818, 0.02873712219297886, 0.02820202335715294, 0.027667619287967682, 0.02615891583263874, 0.018901774659752846]}",0.10576865822076797,Information Extraction,0.053515784442424774
Interpretability and Analysis of Models for NLP,Interpretability for Morphological Inflection: from Character-level Predictions to Subword-level Rules,"Neural models for morphological inflection have recently attained very high results. However, their interpretation remains challenging. Towards this goal, we propose a simple linguistically-motivated variant to the encoderdecoder model with attention. In our model, character-level cross-attention mechanism is complemented with a self-attention module over substrings of the input. We design a novel approach for pattern extraction from attention weights to interpret what the model learn. We apply our methodology to analyze the model's decisions on three typologicallydifferent languages and find that a) our pattern extraction method applied to cross-attention weights uncovers variation in form of inflection morphemes, b) pattern extraction from self-attention shows triggers for such variation, c) both types of patterns are closely aligned with grammar inflection classes and class assignment criteria, for all three languages. Additionally, we find that the proposed encoder attention component leads to consistent performance improvements over a strong baseline. Query No. of/Acc Patterns gold target=*scono & MSD=msd it 23/1.00 *re: 9/1.0 (in| z| o| ti| chi| re) *ire: 7/1.0 (s| col| or| ire) *ir: 6/1.0 (re| in| ser| ir| si) *cir*: 1/1.0 (in| fer| o| cir |si) gold target=*ano & MSD=msd it 189/1.00 *are: 149/1.0 (z| am| pic| are) *arsi: 26//1.0 (im| pa| per| arsi) *car*:3/1.0 (ri|mb|ec|car|si) *izzarsi:2/1.0 (dest|abil|izz|arsi) *iarsi:2/1.0 (di|lan|i|arsi) *par*:2/1.0 (dis|col|par|si) *ciarsi:1/1.0 (au|to|den|un|ci|arsi) *mar*:1/1.0 (in|for|mar|si) *rarsi:1/1.0 (gi|ost|r|arsi) *itarsi:1/1.0 (ri|abil|it|arsi) *quar*:1/1.0 (sci|ac|quar|si) gold target=*ono & !(*scono) & MSD=msd it 41/0.95 *ere: 18/0.95 (ri| otten| ere) *dere: 10/1.0 (te| le| ve| dere) *ger*: 3/1.0 (cos| par| ger| si) *re:3/1.0 (servi|re) *e:1/0.0 (ri|ro|m|per|e) *ir*:1/1.0 (1908:s|ent|ir|si) *si:1/1.0 (es|p|or|si) *ire:1/1.0 (ri|di|ven|ire) *er*:1/1.0 (r|aggi|ung|er|si) *mer*:1/1.0 (ass|u|mer|si) Table 6: Italian Self-Att sub Patterns. MSD query msd it is V;IND;PRS;3;PL. Number of examples (No of ) and accuracy (Acc) are shown per selection with query and per group pattern. For each query, we list all extracted lemma patterns (sorted by frequency in a decreasing order) along with one segmented lemma example (in parentheses) mapped to the pattern. Query No. of/Acc Patterns gold target=*koot & MSD=msd fin 37/0.97 *aa:8/1.0 (kar|sa |st |aa) *ua:5/1.0 (ku |or |ett |ua) *id*:5/1.0 (pro |mo |vo |id |a) *a:4/1.0 (pu |r |je |hti |a) *ta:4/1.0 (sk |r |uud |a |ta) *taa:4/1.0 (jo |kel |taa) *illa:2/1.0 (aal |to |illa) *ella:2/0.5 (n |ar |a |hd |ella) *ttaa:1/1.0 (ha |h |mo |ttaa) *sia:1/1.0 (har |sia), *ista:1/1.0 (li |i |pa |ista) gold target=*kööt & MSD=msd fin 9/1.00 *ä:3/1.0 (v |et |ele |hti |ä) *ää:3/1.0 (jä |n |ist |ää) *tä:2/1.0 (kä |pä |tä) *tää:1/1.0 (hy |mä |h |ää)","{'sequence': ""Neural models for morphological inflection have recently attained very high results. However, their interpretation remains challenging. Towards this goal, we propose a simple linguistically-motivated variant to the encoderdecoder model with attention. In our model, character-level cross-attention mechanism is complemented with a self-attention module over substrings of the input. We design a novel approach for pattern extraction from attention weights to interpret what the model learn. We apply our methodology to analyze the model's decisions on three typologicallydifferent languages and find that a) our pattern extraction method applied to cross-attention weights uncovers variation in form of inflection morphemes, b) pattern extraction from self-attention shows triggers for such variation, c) both types of patterns are closely aligned with grammar inflection classes and class assignment criteria, for all three languages. Additionally, we find that the proposed encoder attention component leads to consistent performance improvements over a strong baseline. Query No. of/Acc Patterns gold target=*scono & MSD=msd it 23/1.00 *re: 9/1.0 (in| z| o| ti| chi| re) *ire: 7/1.0 (s| col| or| ire) *ir: 6/1.0 (re| in| ser| ir| si) *cir*: 1/1.0 (in| fer| o| cir |si) gold target=*ano & MSD=msd it 189/1.00 *are: 149/1.0 (z| am| pic| are) *arsi: 26//1.0 (im| pa| per| arsi) *car*:3/1.0 (ri|mb|ec|car|si) *izzarsi:2/1.0 (dest|abil|izz|arsi) *iarsi:2/1.0 (di|lan|i|arsi) *par*:2/1.0 (dis|col|par|si) *ciarsi:1/1.0 (au|to|den|un|ci|arsi) *mar*:1/1.0 (in|for|mar|si) *rarsi:1/1.0 (gi|ost|r|arsi) *itarsi:1/1.0 (ri|abil|it|arsi) *quar*:1/1.0 (sci|ac|quar|si) gold target=*ono & !(*scono) & MSD=msd it 41/0.95 *ere: 18/0.95 (ri| otten| ere) *dere: 10/1.0 (te| le| ve| dere) *ger*: 3/1.0 (cos| par| ger| si) *re:3/1.0 (servi|re) *e:1/0.0 (ri|ro|m|per|e) *ir*:1/1.0 (1908:s|ent|ir|si) *si:1/1.0 (es|p|or|si) *ire:1/1.0 (ri|di|ven|ire) *er*:1/1.0 (r|aggi|ung|er|si) *mer*:1/1.0 (ass|u|mer|si) Table 6: Italian Self-Att sub Patterns. MSD query msd it is V;IND;PRS;3;PL. Number of examples (No of ) and accuracy (Acc) are shown per selection with query and per group pattern. For each query, we list all extracted lemma patterns (sorted by frequency in a decreasing order) along with one segmented lemma example (in parentheses) mapped to the pattern. Query No. of/Acc Patterns gold target=*koot & MSD=msd fin 37/0.97 *aa:8/1.0 (kar|sa |st |aa) *ua:5/1.0 (ku |or |ett |ua) *id*:5/1.0 (pro |mo |vo |id |a) *a:4/1.0 (pu |r |je |hti |a) *ta:4/1.0 (sk |r |uud |a |ta) *taa:4/1.0 (jo |kel |taa) *illa:2/1.0 (aal |to |illa) *ella:2/0.5 (n |ar |a |hd |ella) *ttaa:1/1.0 (ha |h |mo |ttaa) *sia:1/1.0 (har |sia), *ista:1/1.0 (li |i |pa |ista) gold target=*kööt & MSD=msd fin 9/1.00 *ä:3/1.0 (v |et |ele |hti |ä) *ää:3/1.0 (jä |n |ist |ää) *tä:2/1.0 (kä |pä |tä) *tää:1/1.0 (hy |mä |h |ää)"", 'labels': ['Speech and Multimodality', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Information Extraction', 'Generation', 'Resources and Evaluation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Summarization', 'Discourse and Pragmatics', 'Ethics and NLP', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media'], 'scores': [0.06616119295358658, 0.06412195414304733, 0.06259129196405411, 0.06248936057090759, 0.06215936690568924, 0.054874204099178314, 0.05380367860198021, 0.04584481567144394, 0.044938016682863235, 0.043745432049036026, 0.043396174907684326, 0.042532891035079956, 0.04245546832680702, 0.042047660797834396, 0.03888620063662529, 0.037362415343523026, 0.0346066989004612, 0.033358149230480194, 0.03200313448905945, 0.02972221188247204, 0.028160428628325462, 0.02085896208882332, 0.013880319893360138]}",0.06616119295358658,Speech and Multimodality,0.06248936057090759
Interpretability and Analysis of Models for NLP,"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT","Multilingual pretrained language models have demonstrated remarkable zero-shot crosslingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model's internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a taskspecific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during finetuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.","{'sequence': ""Multilingual pretrained language models have demonstrated remarkable zero-shot crosslingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model's internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a taskspecific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during finetuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis."", 'labels': ['Question Answering', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Information Extraction', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Summarization', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining'], 'scores': [0.0907997265458107, 0.08527922630310059, 0.06861721724271774, 0.06710854917764664, 0.062368135899305344, 0.055005352944135666, 0.044321175664663315, 0.04389028623700142, 0.04317789524793625, 0.042467325925827026, 0.04076410084962845, 0.03983784466981888, 0.039049915969371796, 0.036879461258649826, 0.034633077681064606, 0.0340840220451355, 0.02835414744913578, 0.026347685605287552, 0.026077792048454285, 0.022982580587267876, 0.022855881601572037, 0.022848889231681824, 0.022249726578593254]}",0.0907997265458107,Question Answering,0.06861721724271774
Interpretability and Analysis of Models for NLP,Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples,"Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F 1 detection scores of up to 91.4% against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0% F 1 .","{'sequence': 'Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F 1 detection scores of up to 91.4% against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0% F 1 .', 'labels': ['Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Generation', 'Machine Translation and Multilinguality', 'NLP Applications', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.08729544281959534, 0.0704934373497963, 0.06342772394418716, 0.0633864551782608, 0.06016647443175316, 0.05424509570002556, 0.04849342256784439, 0.04843279346823692, 0.04642946645617485, 0.04559371620416641, 0.04350689798593521, 0.043011073023080826, 0.042976874858140945, 0.04265747219324112, 0.04216616973280907, 0.03529724106192589, 0.03376447409391403, 0.032335322350263596, 0.0271877683699131, 0.024187792092561722, 0.019265754148364067, 0.013235248625278473, 0.012443945743143559]}",0.08729544281959534,Question Answering,0.03376447409391403
Interpretability and Analysis of Models for NLP,Telling BERT's Full Story: from Local Attention to Global Aggregation,"We take a deep look into the behaviour of selfattention heads in the transformer architecture. In light of recent work discouraging the use of attention distributions for explaining a model's behaviour, we show that attention distributions can nevertheless provide insights into the local behaviour of attention heads. This way, we propose a distinction between local patterns revealed by attention and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant mismatch between attention and attribution distributions, caused by the mixing of context inside the model. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing.","{'sequence': ""We take a deep look into the behaviour of selfattention heads in the transformer architecture. In light of recent work discouraging the use of attention distributions for explaining a model's behaviour, we show that attention distributions can nevertheless provide insights into the local behaviour of attention heads. This way, we propose a distinction between local patterns revealed by attention and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant mismatch between attention and attribution distributions, caused by the mixing of context inside the model. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing."", 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Summarization', 'Resources and Evaluation', 'Ethics and NLP', 'Generation', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.07222167402505875, 0.06972388178110123, 0.06905143707990646, 0.06831105798482895, 0.05879092216491699, 0.05321182683110237, 0.05229933559894562, 0.05058054253458977, 0.04554011672735214, 0.0443376861512661, 0.04251569136977196, 0.03993460163474083, 0.03884235396981239, 0.03709360212087631, 0.03564595803618431, 0.03247074410319328, 0.0319695770740509, 0.03066219575703144, 0.029875153675675392, 0.02822527475655079, 0.02756459265947342, 0.022256677970290184, 0.018875161185860634]}",0.07222167402505875,Information Extraction,0.0319695770740509
Interpretability and Analysis of Models for NLP,Attention Can Reflect Syntactic Structure (If You Let It),"Since the popularization of the Transformer as a general-purpose feature encoder for NLP, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on Englisha language with rigid word order and a lack of inflectional morphology. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of attention as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen. * Equal contribution. Order was decided by a coin toss.","{'sequence': 'Since the popularization of the Transformer as a general-purpose feature encoder for NLP, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on Englisha language with rigid word order and a lack of inflectional morphology. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of attention as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen. * Equal contribution. Order was decided by a coin toss.', 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'NLP Applications', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Generation', 'Summarization', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0803120881319046, 0.06426385045051575, 0.06191016361117363, 0.06111450865864754, 0.06105274707078934, 0.0600726380944252, 0.058145321905612946, 0.052288856357336044, 0.04867482930421829, 0.04862187057733536, 0.04629141092300415, 0.045655615627765656, 0.037322163581848145, 0.03660259768366814, 0.03522668778896332, 0.03418947756290436, 0.03068453259766102, 0.02869395539164543, 0.02667611464858055, 0.023904949426651, 0.022911181673407555, 0.018987560644745827, 0.016396842896938324]}",0.0803120881319046,Speech and Multimodality,0.045655615627765656
"Language Grounding to Vision, Robotics and Beyond",L2C: Describing Visual Differences Needs Semantic Understanding of Individuals,"Recent advances in language and vision push forward the research of captioning a single image to describing visual differences between image pairs. Suppose there are two images, I 1 and I 2 , and the task is to generate a description W 1,2 comparing them, existing methods directly model ⟨I 1 , I 2 ⟩ → W 1,2 mapping without the semantic understanding of individuals. In this paper, we introduce a Learningto-Compare (L2C) model, which learns to understand the semantic structures of these two images and compare them while learning to describe each one. We demonstrate that L2C benefits from a comparison between explicit semantic representations and singleimage captions, and generalizes better on the new testing image pairs. It outperforms the baseline on both automatic evaluation and human evaluation for the Birds-to-Words dataset.","{'sequence': 'Recent advances in language and vision push forward the research of captioning a single image to describing visual differences between image pairs. Suppose there are two images, I 1 and I 2 , and the task is to generate a description W 1,2 comparing them, existing methods directly model ⟨I 1 , I 2 ⟩ → W 1,2 mapping without the semantic understanding of individuals. In this paper, we introduce a Learningto-Compare (L2C) model, which learns to understand the semantic structures of these two images and compare them while learning to describe each one. We demonstrate that L2C benefits from a comparison between explicit semantic representations and singleimage captions, and generalizes better on the new testing image pairs. It outperforms the baseline on both automatic evaluation and human evaluation for the Birds-to-Words dataset.', 'labels': ['Resources and Evaluation', 'Generation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Summarization', 'NLP Applications', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09121643751859665, 0.08964081853628159, 0.07441232353448868, 0.06411828100681305, 0.06073567643761635, 0.054470472037792206, 0.04788936302065849, 0.04630330950021744, 0.04039790481328964, 0.039680786430835724, 0.039089687168598175, 0.0385177806019783, 0.03776146098971367, 0.03627127408981323, 0.03584407642483711, 0.035503044724464417, 0.03278318792581558, 0.032425470650196075, 0.02806706540286541, 0.025238770991563797, 0.02366751804947853, 0.01871662028133869, 0.007248628884553909]}",0.09121643751859665,Resources and Evaluation,0.04039790481328964
"Language Grounding to Vision, Robotics and Beyond",Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation,"This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low.","{'sequence': 'This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Information Extraction', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'NLP Applications', 'Generation', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14948216080665588, 0.11756414920091629, 0.05844894424080849, 0.05219701677560806, 0.05173466354608536, 0.046938393265008926, 0.043078165501356125, 0.04020017012953758, 0.03947744518518448, 0.03732764348387718, 0.03534800931811333, 0.034584008157253265, 0.03439432755112648, 0.03262191638350487, 0.03195252642035484, 0.03084239363670349, 0.030626269057393074, 0.025959905236959457, 0.02591228112578392, 0.025504272431135178, 0.02211884967982769, 0.01787489838898182, 0.015811655670404434]}",0.14948216080665588,Machine Translation and Multilinguality,0.04020017012953758
"Language Grounding to Vision, Robotics and Beyond",An Empirical Study on the Generalization Power of Neural Representations Learned via Visual Guessing Games,"Guessing games are a prototypical instance of the ""learning by interacting"" paradigm. This work investigates how well an artificial agent can benefit from playing guessing games when later asked to perform on novel NLP downstream tasks such as Visual Question Answering (VQA). We propose two ways to exploit playing guessing games: 1) a supervised learning scenario in which the agent learns to mimic successful guessing games and 2) a novel way for an agent to play by itself, called Self-play via Iterated Experience Learning (SPIEL). We evaluate the ability of both procedures to generalise: an in-domain evaluation shows an increased accuracy (+7.79) compared with competitors on the evaluation suite CompGuessWhat?!; a transfer evaluation shows improved performance for VQA on the TDIUC dataset in terms of harmonic average accuracy (+5.31) thanks to more fine-grained object representations learned via SPIEL.","{'sequence': 'Guessing games are a prototypical instance of the ""learning by interacting"" paradigm. This work investigates how well an artificial agent can benefit from playing guessing games when later asked to perform on novel NLP downstream tasks such as Visual Question Answering (VQA). We propose two ways to exploit playing guessing games: 1) a supervised learning scenario in which the agent learns to mimic successful guessing games and 2) a novel way for an agent to play by itself, called Self-play via Iterated Experience Learning (SPIEL). We evaluate the ability of both procedures to generalise: an in-domain evaluation shows an increased accuracy (+7.79) compared with competitors on the evaluation suite CompGuessWhat?!; a transfer evaluation shows improved performance for VQA on the TDIUC dataset in terms of harmonic average accuracy (+5.31) thanks to more fine-grained object representations learned via SPIEL.', 'labels': ['Question Answering', 'NLP Applications', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Discourse and Pragmatics', 'Generation', 'Speech and Multimodality', 'Information Extraction', 'Computational Social Science and Social Media', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.8796864151954651, 0.01587647758424282, 0.011336085386574268, 0.01110379584133625, 0.010917861014604568, 0.007017088122665882, 0.0068021249026060104, 0.005767414811998606, 0.005429963115602732, 0.0046427687630057335, 0.004544394090771675, 0.00453854538500309, 0.004189677536487579, 0.0037196653429418802, 0.003564902115613222, 0.0032116323709487915, 0.0031629090663045645, 0.0029705450870096684, 0.0027504938188940287, 0.0026403749361634254, 0.002615139354020357, 0.00214013340882957, 0.0013716445537284017]}",0.8796864151954651,Question Answering,0.0026403749361634254
"Language Grounding to Vision, Robotics and Beyond",Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning,"Unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs, but only with images and sentences drawn from different sources and object labels detected from the images. In previous work, pseudo-captions, i.e., sentences that contain the detected object labels, were assigned to a given image. The focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. However, pseudo-captions contain many words that are irrelevant to a given image. In this work, we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. We propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. The experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. Combined with the sentence-level alignment method of previous work, our method further improves its performance. These results confirm the importance of careful alignment in word-level details. 1","{'sequence': 'Unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs, but only with images and sentences drawn from different sources and object labels detected from the images. In previous work, pseudo-captions, i.e., sentences that contain the detected object labels, were assigned to a given image. The focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. However, pseudo-captions contain many words that are irrelevant to a given image. In this work, we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. We propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. The experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. Combined with the sentence-level alignment method of previous work, our method further improves its performance. These results confirm the importance of careful alignment in word-level details. 1', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Question Answering', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07308325916528702, 0.07128271460533142, 0.06941917538642883, 0.05701611936092377, 0.055032625794410706, 0.05004139617085457, 0.04861736670136452, 0.04817119613289833, 0.04575171321630478, 0.04304075241088867, 0.04283567890524864, 0.041960131376981735, 0.039454136043787, 0.0391552671790123, 0.036668457090854645, 0.03530092537403107, 0.03472711518406868, 0.03390732407569885, 0.03273726627230644, 0.03220537304878235, 0.02558467537164688, 0.024825267493724823, 0.019182002171874046]}",0.07308325916528702,Dialogue and Interactive Systems,0.03472711518406868
"Language Grounding to Vision, Robotics and Beyond",SANDI: Story-and-Images Alignment,"The Internet contains a multitude of social media posts and other stories where text is interspersed with images. In these contexts, images are not simply used for general illustration, but are judiciously placed in certain spots of a story for multimodal descriptions and narration. In this work we analyze the problem of text-image alignment, and present SANDI, a methodology for automatically selecting images from an image collection and aligning them with text paragraphs of a story. SANDI combines visual tags, user-provided tags and background knowledge, and uses an Integer Linear Program to compute alignments that are semantically meaningful. Experiments show that SANDI can select and align images with texts with high quality of semantic fit.","{'sequence': 'The Internet contains a multitude of social media posts and other stories where text is interspersed with images. In these contexts, images are not simply used for general illustration, but are judiciously placed in certain spots of a story for multimodal descriptions and narration. In this work we analyze the problem of text-image alignment, and present SANDI, a methodology for automatically selecting images from an image collection and aligning them with text paragraphs of a story. SANDI combines visual tags, user-provided tags and background knowledge, and uses an Integer Linear Program to compute alignments that are semantically meaningful. Experiments show that SANDI can select and align images with texts with high quality of semantic fit.', 'labels': ['Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'NLP Applications', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Generation', 'Ethics and NLP', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.08498729765415192, 0.08201784640550613, 0.08176586776971817, 0.0793347954750061, 0.07814252376556396, 0.06448664516210556, 0.0585518442094326, 0.052309803664684296, 0.04974346607923508, 0.045476749539375305, 0.041140682995319366, 0.04033241048455238, 0.03495500981807709, 0.02963916026055813, 0.027401750907301903, 0.025062693282961845, 0.023197641596198082, 0.02245737798511982, 0.020604541525244713, 0.01713872142136097, 0.015618259087204933, 0.013604848645627499, 0.012030023150146008]}",0.08498729765415192,Dialogue and Interactive Systems,0.012030023150146008
"Language Grounding to Vision, Robotics and Beyond",The Interplay of Task Success and Dialogue Quality: An in-depth Evaluation in Task-Oriented Visual Dialogues,"When training a model on referential dialogue guessing games, the best model is usually chosen based on its task success. We show that in the popular end-to-end approach, this choice prevents the model from learning to generate linguistically richer dialogues, since the acquisition of language proficiency takes longer than learning the guessing task. By comparing models playing different games (Guess-What, GuessWhich, and Mutual Friends), we show that this discrepancy is model-and taskagnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set.","{'sequence': 'When training a model on referential dialogue guessing games, the best model is usually chosen based on its task success. We show that in the popular end-to-end approach, this choice prevents the model from learning to generate linguistically richer dialogues, since the acquisition of language proficiency takes longer than learning the guessing task. By comparing models playing different games (Guess-What, GuessWhich, and Mutual Friends), we show that this discrepancy is model-and taskagnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set.', 'labels': ['Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'NLP Applications', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Question Answering', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.11540593951940536, 0.08545716851949692, 0.07006712257862091, 0.06057267263531685, 0.05812666565179825, 0.055062904953956604, 0.05260114371776581, 0.04899780824780464, 0.04299859330058098, 0.04234562814235687, 0.040629927068948746, 0.035878706723451614, 0.03495573252439499, 0.03412430360913277, 0.03406926989555359, 0.03322475031018257, 0.03134758025407791, 0.03098790906369686, 0.02628578618168831, 0.023932451382279396, 0.01567988656461239, 0.014919939450919628, 0.012328076176345348]}",0.11540593951940536,Generation,0.012328076176345348
"Language Grounding to Vision, Robotics and Beyond",The Role of Syntactic Planning in Compositional Image Captioning,"Image captioning has focused on generalizing to images drawn from the same distribution as the training set, and not to the more challenging problem of generalizing to different distributions of images. Recently, Nikolaus et al. ( 2019 ) introduced a dataset to assess compositional generalization in image captioning, where models are evaluated on their ability to describe images with unseen adjective-noun and noun-verb compositions. In this work, we investigate different methods to improve compositional generalization by planning the syntactic structure of a caption. Our experiments show that jointly modeling tokens and syntactic tags enhances generalization in both RNNand Transformer-based models, while also improving performance on standard metrics.","{'sequence': 'Image captioning has focused on generalizing to images drawn from the same distribution as the training set, and not to the more challenging problem of generalizing to different distributions of images. Recently, Nikolaus et al. ( 2019 ) introduced a dataset to assess compositional generalization in image captioning, where models are evaluated on their ability to describe images with unseen adjective-noun and noun-verb compositions. In this work, we investigate different methods to improve compositional generalization by planning the syntactic structure of a caption. Our experiments show that jointly modeling tokens and syntactic tags enhances generalization in both RNNand Transformer-based models, while also improving performance on standard metrics.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Summarization', 'Discourse and Pragmatics', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0897366926074028, 0.06797619163990021, 0.06169092655181885, 0.06046561524271965, 0.05930256098508835, 0.053216539323329926, 0.05083993449807167, 0.048421937972307205, 0.04837563633918762, 0.04281330481171608, 0.04167544096708298, 0.03876490890979767, 0.03857843950390816, 0.03743355721235275, 0.03554428368806839, 0.03529934585094452, 0.03417233005166054, 0.0322054922580719, 0.03115537203848362, 0.02796701341867447, 0.026346875354647636, 0.024728018790483475, 0.013289508409798145]}",0.0897366926074028,Resources and Evaluation,0.0322054922580719
"Language Grounding to Vision, Robotics and Beyond",Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO,"By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning. Unfortunately, datasets have limited cross-modal associations: images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra-and intermodality pairs. We report baseline results on CxC for strong existing unimodal and multimodal models. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC's value for measuring the influence of intra-and inter-modality learning.","{'sequence': ""By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning. Unfortunately, datasets have limited cross-modal associations: images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra-and intermodality pairs. We report baseline results on CxC for strong existing unimodal and multimodal models. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC's value for measuring the influence of intra-and inter-modality learning."", 'labels': ['Resources and Evaluation', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Information Extraction', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15409283339977264, 0.09187067300081253, 0.06400923430919647, 0.059634286910295486, 0.05825325846672058, 0.047396283596754074, 0.04376766458153725, 0.04274802282452583, 0.041006967425346375, 0.03945359215140343, 0.037054385989904404, 0.03514157235622406, 0.03504357486963272, 0.03460369259119034, 0.032501302659511566, 0.03175126016139984, 0.030529582872986794, 0.026638606563210487, 0.02377495728433132, 0.021383613348007202, 0.018647784367203712, 0.01567912846803665, 0.01501774974167347]}",0.15409283339977264,Resources and Evaluation,0.030529582872986794
"Language Grounding to Vision, Robotics and Beyond",Cross-lingual Visual Pre-training for Multimodal Machine Translation,"Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.","{'sequence': 'Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Generation', 'Question Answering', 'Information Extraction', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09307578206062317, 0.06847124546766281, 0.06492608040571213, 0.06127143278717995, 0.0589410774409771, 0.056510426104068756, 0.05413220450282097, 0.050423722714185715, 0.050049085170030594, 0.05004040151834488, 0.04369193688035011, 0.04230599105358124, 0.042165808379650116, 0.0366859994828701, 0.03165104612708092, 0.03151727095246315, 0.028568916022777557, 0.026289183646440506, 0.026106977835297585, 0.02542208321392536, 0.023020748049020767, 0.020503662526607513, 0.01422899030148983]}",0.09307578206062317,Machine Translation and Multilinguality,0.03151727095246315
"Language Grounding to Vision, Robotics and Beyond",Co-evolution of language and agents in referential games,"Referential games offer a grounded learning environment for neural agents which accounts for the fact that language is functionally used to communicate. However, they do not take into account a second constraint considered to be fundamental for the shape of human language: that it must be learnable by new language learners. 2019 ) introduced cultural transmission within referential games through a changing population of agents to constrain the emerging language to be learnable. However, the resulting languages remain inherently biased by the agents' underlying capabilities. Cogswell et al. ( In this work, we introduce Language Transmission Simulator to model both cultural and architectural evolution in a population of agents. As our core contribution, we empirically show that the optimal situation is to take into account also the learning biases of the language learners and thus let language and agents coevolve. When we allow the agent population to evolve through architectural evolution, we achieve across the board improvements on all considered metrics and surpass the gains made with cultural transmission. These results stress the importance of studying the underlying agent architecture and pave the way to investigate the co-evolution of language and agent in language emergence studies.","{'sequence': ""Referential games offer a grounded learning environment for neural agents which accounts for the fact that language is functionally used to communicate. However, they do not take into account a second constraint considered to be fundamental for the shape of human language: that it must be learnable by new language learners. 2019 ) introduced cultural transmission within referential games through a changing population of agents to constrain the emerging language to be learnable. However, the resulting languages remain inherently biased by the agents' underlying capabilities. Cogswell et al. ( In this work, we introduce Language Transmission Simulator to model both cultural and architectural evolution in a population of agents. As our core contribution, we empirically show that the optimal situation is to take into account also the learning biases of the language learners and thus let language and agents coevolve. When we allow the agent population to evolve through architectural evolution, we achieve across the board improvements on all considered metrics and surpass the gains made with cultural transmission. These results stress the importance of studying the underlying agent architecture and pave the way to investigate the co-evolution of language and agent in language emergence studies."", 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Question Answering', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'NLP Applications', 'Information Extraction', 'Generation', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.06971389055252075, 0.059721313416957855, 0.05960538983345032, 0.05781976878643036, 0.057046663016080856, 0.05670011043548584, 0.0565788671374321, 0.049451231956481934, 0.04793167486786842, 0.04611309617757797, 0.04558277502655983, 0.04514824599027634, 0.03888523578643799, 0.03842464089393616, 0.035461876541376114, 0.035033226013183594, 0.0331755056977272, 0.033065371215343475, 0.03059876151382923, 0.028939152136445045, 0.02783333510160446, 0.02777780219912529, 0.019392039626836777]}",0.06971389055252075,Speech and Multimodality,0.028939152136445045
"Language Grounding to Vision, Robotics and Beyond",Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation,"One of the most challenging topics in Natural Language Processing (NLP) is visuallygrounded language understanding and reasoning. Outdoor vision-and-language navigation (VLN) is such a task where an agent follows natural language instructions and navigates a real-life urban environment. Due to the lack of human-annotated instructions that illustrate intricate urban scenes, outdoor VLN remains a challenging task to solve. This paper introduces a Multimodal Text Style Transfer (MTST) learning approach and leverages external multimodal resources to mitigate data scarcity in outdoor navigation tasks. We first enrich the navigation data by transferring the style of the instructions generated by Google Maps API, then pre-train the navigator with the augmented external outdoor navigation dataset. Experimental results show that our MTST learning approach is model-agnostic, and our MTST approach significantly outperforms the baseline models on the outdoor VLN task, improving task completion rate by 8.7% relatively on the test set. 1","{'sequence': 'One of the most challenging topics in Natural Language Processing (NLP) is visuallygrounded language understanding and reasoning. Outdoor vision-and-language navigation (VLN) is such a task where an agent follows natural language instructions and navigates a real-life urban environment. Due to the lack of human-annotated instructions that illustrate intricate urban scenes, outdoor VLN remains a challenging task to solve. This paper introduces a Multimodal Text Style Transfer (MTST) learning approach and leverages external multimodal resources to mitigate data scarcity in outdoor navigation tasks. We first enrich the navigation data by transferring the style of the instructions generated by Google Maps API, then pre-train the navigator with the augmented external outdoor navigation dataset. Experimental results show that our MTST learning approach is model-agnostic, and our MTST approach significantly outperforms the baseline models on the outdoor VLN task, improving task completion rate by 8.7% relatively on the test set. 1', 'labels': ['Resources and Evaluation', 'NLP Applications', 'Speech and Multimodality', 'Information Extraction', 'Machine Learning for NLP', 'Generation', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Summarization', 'Machine Translation and Multilinguality', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09627611935138702, 0.08398763090372086, 0.07978101819753647, 0.0706472098827362, 0.06861904263496399, 0.0642019510269165, 0.05526069179177284, 0.047421541064977646, 0.04221131652593613, 0.04100937768816948, 0.0384596548974514, 0.036096762865781784, 0.03513734042644501, 0.03439873456954956, 0.028524884954094887, 0.028030987828969955, 0.027535472065210342, 0.023211132735013962, 0.022991836071014404, 0.022668248042464256, 0.01988670416176319, 0.019260333850979805, 0.014382078312337399]}",0.09627611935138702,Resources and Evaluation,0.028524884954094887
"Language Grounding to Vision, Robotics and Beyond",On the Evaluation of Vision-and-Language Navigation Instructions,"Vision-and-Language Navigation wayfinding agents can be enhanced by exploiting automatically generated navigation instructions. However, existing instruction generators have not been comprehensively evaluated, and the automatic evaluation metrics used to develop them have not been validated. Using human wayfinders, we show that these generators perform on par with or only slightly better than a template-based generator and far worse than human instructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are ineffective for evaluating grounded navigation instructions. To improve instruction evaluation, we propose an instruction-trajectory compatibility model that operates without reference instructions. Our model shows the highest correlation with human wayfinding outcomes when scoring individual instructions. For ranking instruction generation systems, if reference instructions are available we recommend using SPICE.","{'sequence': 'Vision-and-Language Navigation wayfinding agents can be enhanced by exploiting automatically generated navigation instructions. However, existing instruction generators have not been comprehensively evaluated, and the automatic evaluation metrics used to develop them have not been validated. Using human wayfinders, we show that these generators perform on par with or only slightly better than a template-based generator and far worse than human instructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are ineffective for evaluating grounded navigation instructions. To improve instruction evaluation, we propose an instruction-trajectory compatibility model that operates without reference instructions. Our model shows the highest correlation with human wayfinding outcomes when scoring individual instructions. For ranking instruction generation systems, if reference instructions are available we recommend using SPICE.', 'labels': ['Resources and Evaluation', 'Generation', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Information Extraction', 'Speech and Multimodality', 'Discourse and Pragmatics', 'NLP Applications', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.10042033344507217, 0.08628978580236435, 0.0690414160490036, 0.06807059049606323, 0.052840884774923325, 0.050972167402505875, 0.04598167538642883, 0.0445273295044899, 0.04262411966919899, 0.04145342484116554, 0.041048768907785416, 0.040765631943941116, 0.040627434849739075, 0.03777763247489929, 0.03365311026573181, 0.03226201608777046, 0.030844416469335556, 0.029551349580287933, 0.027656495571136475, 0.024537524208426476, 0.020936790853738785, 0.020613031461834908, 0.017504142597317696]}",0.10042033344507217,Resources and Evaluation,0.020936790853738785
"Language Grounding to Vision, Robotics and Beyond",On the (In)Effectiveness of Images for Text Classification,"Images are core components of multi-modal learning in natural language processing (NLP), and results have varied substantially as to whether images improve NLP tasks or not. One confounding effect has been that previous NLP research has generally focused on sophisticated tasks (in varying settings), generally applied to English only. We focus on text classification, in the context of assigning named entity classes to a given Wikipedia page, where images generally complement the text and the Wikipedia page can be in one of a number of different languages. Our experiments across a range of languages show that images complement NLP models (including BERT) trained without external pre-training, but when combined with BERT models pre-trained on largescale external data, images contribute nothing. 1 http://shinra-project.info/ shinra2020ml/?lang=en 2 Data is also provided for Greek but we do not include it in our experiments because there was no officially preprocessed data available for this language.","{'sequence': 'Images are core components of multi-modal learning in natural language processing (NLP), and results have varied substantially as to whether images improve NLP tasks or not. One confounding effect has been that previous NLP research has generally focused on sophisticated tasks (in varying settings), generally applied to English only. We focus on text classification, in the context of assigning named entity classes to a given Wikipedia page, where images generally complement the text and the Wikipedia page can be in one of a number of different languages. Our experiments across a range of languages show that images complement NLP models (including BERT) trained without external pre-training, but when combined with BERT models pre-trained on largescale external data, images contribute nothing. 1 http://shinra-project.info/ shinra2020ml/?lang=en 2 Data is also provided for Greek but we do not include it in our experiments because there was no officially preprocessed data available for this language.', 'labels': ['Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Machine Learning for NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Generation', 'Information Extraction', 'Discourse and Pragmatics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11119063198566437, 0.0801936462521553, 0.06985373049974442, 0.0668855831027031, 0.05383213609457016, 0.048983585089445114, 0.04790586233139038, 0.04731757938861847, 0.044608186930418015, 0.04074373468756676, 0.03917795419692993, 0.038174550980329514, 0.03792409226298332, 0.03769019991159439, 0.03500175103545189, 0.031754270195961, 0.02920151688158512, 0.02902509644627571, 0.027676409110426903, 0.02628396637737751, 0.022741058841347694, 0.021563321352005005, 0.012271103449165821]}",0.11119063198566437,Interpretability and Analysis of Models for NLP,0.02920151688158512
"Language Grounding to Vision, Robotics and Beyond",ECOL-R: Encouraging Copying in Novel Object Captioning with Reinforcement Learning,"Novel Object Captioning is a zero-shot Image Captioning task requiring describing objects not seen in the training captions, but for which information is available from external object detectors. The key challenge is to select and describe all salient detected novel objects in the input images. In this paper, we focus on this challenge and propose the ECOL-R model (Encouraging Copying of Object Labels with Reinforced Learning), a copy-augmented transformer model that is encouraged to accurately describe the novel object labels. This is achieved via a specialised reward function in the SCST reinforcement learning framework (Rennie et al., 2017) that encourages novel object mentions while maintaining the caption quality. We further restrict the SCST training to the images where detected objects are mentioned in reference captions to train the ECOL-R model. We additionally improve our copy mechanism via Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which determines the appropriate inflected forms of novel object labels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016)  benchmarks.","{'sequence': 'Novel Object Captioning is a zero-shot Image Captioning task requiring describing objects not seen in the training captions, but for which information is available from external object detectors. The key challenge is to select and describe all salient detected novel objects in the input images. In this paper, we focus on this challenge and propose the ECOL-R model (Encouraging Copying of Object Labels with Reinforced Learning), a copy-augmented transformer model that is encouraged to accurately describe the novel object labels. This is achieved via a specialised reward function in the SCST reinforcement learning framework (Rennie et al., 2017) that encourages novel object mentions while maintaining the caption quality. We further restrict the SCST training to the images where detected objects are mentioned in reference captions to train the ECOL-R model. We additionally improve our copy mechanism via Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which determines the appropriate inflected forms of novel object labels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016)  benchmarks.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Question Answering', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Summarization', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0747067928314209, 0.07081051170825958, 0.07057544589042664, 0.057962577790021896, 0.056880075484514236, 0.05444784462451935, 0.05152630805969238, 0.050008565187454224, 0.04516511410474777, 0.044172659516334534, 0.04239438846707344, 0.03926845267415047, 0.03804643079638481, 0.03741035610437393, 0.037387095391750336, 0.03570008650422096, 0.03296602517366409, 0.031594689935445786, 0.029845351353287697, 0.029182396829128265, 0.028002653270959854, 0.025442024692893028, 0.01650417596101761]}",0.0747067928314209,Dialogue and Interactive Systems,0.031594689935445786
Resources and Evaluation,VoiSeR: A New Benchmark for Voice-Based Search Refinement,"Voice assistants, e.g., Alexa or Google Assistant, have dramatically improved in recent years. Supporting voice-based search, exploration, and refinement are fundamental tasks for voice assistants, and remain an open challenge. For example, when using voice to search an online shopping site, a user often needs to refine their search by some aspect or facet. This common user intent is usually available through a ""filter-by"" interface on online shopping websites, but is challenging to support naturally via voice, as the intent of refinements must be interpreted in the context of the original search, the initial results, and the available product catalogue facets. To our knowledge, no benchmark dataset exists for training or validating such contextual search understanding models. To bridge this gap, we introduce the first large-scale dataset of voicebased search refinements, VoiSeR, consisting of about 10,000 search refinement utterances, collected using a novel crowdsourcing task. These utterances are intended to refine a previous search, with respect to a search facet or attribute (e.g., brand, color, review rating, etc.), and are manually annotated with the specific intent. This paper reports qualitative and empirical insights into the most common and challenging types of refinements that a voicebased conversational search system must support. As we show, VoiSeR can support research in conversational query understanding, contextual user intent prediction, and other conversational search topics to facilitate the development of conversational search systems. 1 The dataset is available for download at https://registry.opendata.aws/ amazon-conversational-product-search/.","{'sequence': 'Voice assistants, e.g., Alexa or Google Assistant, have dramatically improved in recent years. Supporting voice-based search, exploration, and refinement are fundamental tasks for voice assistants, and remain an open challenge. For example, when using voice to search an online shopping site, a user often needs to refine their search by some aspect or facet. This common user intent is usually available through a ""filter-by"" interface on online shopping websites, but is challenging to support naturally via voice, as the intent of refinements must be interpreted in the context of the original search, the initial results, and the available product catalogue facets. To our knowledge, no benchmark dataset exists for training or validating such contextual search understanding models. To bridge this gap, we introduce the first large-scale dataset of voicebased search refinements, VoiSeR, consisting of about 10,000 search refinement utterances, collected using a novel crowdsourcing task. These utterances are intended to refine a previous search, with respect to a search facet or attribute (e.g., brand, color, review rating, etc.), and are manually annotated with the specific intent. This paper reports qualitative and empirical insights into the most common and challenging types of refinements that a voicebased conversational search system must support. As we show, VoiSeR can support research in conversational query understanding, contextual user intent prediction, and other conversational search topics to facilitate the development of conversational search systems. 1 The dataset is available for download at https://registry.opendata.aws/ amazon-conversational-product-search/.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Question Answering', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'NLP Applications', 'Summarization', 'Information Extraction', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11152556538581848, 0.08016049861907959, 0.06971834599971771, 0.05809669941663742, 0.05378812551498413, 0.05226898938417435, 0.04674028232693672, 0.04368883743882179, 0.04342721402645111, 0.043377265334129333, 0.04204150289297104, 0.03856981173157692, 0.037651143968105316, 0.03437387943267822, 0.03431817516684532, 0.03391231596469879, 0.03385167568922043, 0.02935219742357731, 0.027470501139760017, 0.026975136250257492, 0.024877240881323814, 0.019060948863625526, 0.014753649942576885]}",0.11152556538581848,Speech and Multimodality,0.05226898938417435
Resources and Evaluation,Adaptation of Back-translation to Automatic Post-Editing for Synthetic Data Generation,"Automatic Post-Editing (APE) aims to correct errors in the output of a given machine translation (MT) system. Although data-driven approaches have become prevalent also in the APE task as in many other NLP tasks, there has been a lack of qualified training data due to the high cost of manual construction. eSCAPE, a synthetic APE corpus, has been widely used to alleviate the data scarcity, but it might not address genuine APE corpora's characteristic that the post-edited sentence should be a minimally edited revision of the given MT output. Therefore, we propose two new methods of synthesizing additional MT outputs by adapting back-translation to the APE task, obtaining robust enlargements of the existing synthetic APE training dataset 1 . Experimental results on the WMT English-German APE benchmarks demonstrate that our enlarged datasets are effective in improving APE performance.","{'sequence': ""Automatic Post-Editing (APE) aims to correct errors in the output of a given machine translation (MT) system. Although data-driven approaches have become prevalent also in the APE task as in many other NLP tasks, there has been a lack of qualified training data due to the high cost of manual construction. eSCAPE, a synthetic APE corpus, has been widely used to alleviate the data scarcity, but it might not address genuine APE corpora's characteristic that the post-edited sentence should be a minimally edited revision of the given MT output. Therefore, we propose two new methods of synthesizing additional MT outputs by adapting back-translation to the APE task, obtaining robust enlargements of the existing synthetic APE training dataset 1 . Experimental results on the WMT English-German APE benchmarks demonstrate that our enlarged datasets are effective in improving APE performance."", 'labels': ['NLP Applications', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Extraction', 'Ethics and NLP', 'Generation', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.10074733942747116, 0.06898999959230423, 0.06866651028394699, 0.06502819806337357, 0.05787106603384018, 0.05495298653841019, 0.051830559968948364, 0.047317326068878174, 0.04574728384613991, 0.044999998062849045, 0.04460388422012329, 0.04214579239487648, 0.039311423897743225, 0.03536145016551018, 0.03415853530168533, 0.033841170370578766, 0.029672443866729736, 0.028963448479771614, 0.026601342484354973, 0.02527281641960144, 0.020111089572310448, 0.016921207308769226, 0.016884174197912216]}",0.10074733942747116,NLP Applications,0.06898999959230423
Resources and Evaluation,A Large-scale Evaluation of Neural Machine Transliteration for Indic Languages,"We take up the task of largescale evaluation of neural machine transliteration between English and Indian languages, with a focus on multilin gual transliteration to utilize orthographic sim ilarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved mul tilingual training pipeline for Indic languages. We analyse various factors affecting transliter ation quality like language family, translitera tion direction and word origin.","{'sequence': 'We take up the task of largescale evaluation of neural machine transliteration between English and Indian languages, with a focus on multilin gual transliteration to utilize orthographic sim ilarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved mul tilingual training pipeline for Indic languages. We analyse various factors affecting transliter ation quality like language family, translitera tion direction and word origin.', 'labels': ['Machine Translation and Multilinguality', 'Resources and Evaluation', 'Speech and Multimodality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Generation', 'Information Retrieval and Text Mining', 'Question Answering', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Ethics and NLP', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.27350756525993347, 0.1981678009033203, 0.0547410286962986, 0.054722923785448074, 0.04210229963064194, 0.03477732092142105, 0.028183214366436005, 0.027497567236423492, 0.02705909125506878, 0.026952510699629784, 0.025336923077702522, 0.024322377517819405, 0.02371949702501297, 0.023166554048657417, 0.021958528086543083, 0.018287179991602898, 0.017013458535075188, 0.016360599547624588, 0.015454759821295738, 0.014740599319338799, 0.01265936903655529, 0.009939529001712799, 0.009329354390501976]}",0.27350756525993347,Machine Translation and Multilinguality,0.1981678009033203
Resources and Evaluation,Evaluating the Evaluation of Diversity in Natural Language Generation,"Despite growing interest in natural language generation (NLG) models that produce diverse outputs, there is currently no principled method for evaluating the diversity of an NLG system. In this work, we propose a framework for evaluating diversity metrics. The framework measures the correlation between a proposed diversity metric and a diversity parameter, a single parameter that controls some aspect of diversity in generated text. For example, a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by: (a) establishing best practices for eliciting diversity judgments from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling diversity by tuning a ""decoding parameter"" mostly affect form but not meaning. Our framework can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.","{'sequence': 'Despite growing interest in natural language generation (NLG) models that produce diverse outputs, there is currently no principled method for evaluating the diversity of an NLG system. In this work, we propose a framework for evaluating diversity metrics. The framework measures the correlation between a proposed diversity metric and a diversity parameter, a single parameter that controls some aspect of diversity in generated text. For example, a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by: (a) establishing best practices for eliciting diversity judgments from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling diversity by tuning a ""decoding parameter"" mostly affect form but not meaning. Our framework can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.', 'labels': ['Generation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Ethics and NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07975487411022186, 0.0695231705904007, 0.06219680607318878, 0.06160879135131836, 0.05958475172519684, 0.05946629121899605, 0.05328607186675072, 0.049398716539144516, 0.04938041418790817, 0.04493089020252228, 0.04426494240760803, 0.043649327009916306, 0.038530923426151276, 0.03702067956328392, 0.0335969552397728, 0.0331101231276989, 0.031136155128479004, 0.030555587261915207, 0.02973322570323944, 0.02605861984193325, 0.02428574673831463, 0.02198209799826145, 0.016944844275712967]}",0.07975487411022186,Generation,0.05958475172519684
Resources and Evaluation,Building Representative Corpora from Illiterate Communities: A Reviewof Challenges and Mitigation Strategies for Developing Countries,"Most well-established data collection methods currently adopted in NLP depend on the assumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through datadriven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora: we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.","{'sequence': 'Most well-established data collection methods currently adopted in NLP depend on the assumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through datadriven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora: we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.', 'labels': ['Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Generation', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'NLP Applications', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Speech and Multimodality', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0856962725520134, 0.06195041537284851, 0.06062588095664978, 0.05658319965004921, 0.05489025264978409, 0.05129600688815117, 0.050402261316776276, 0.04972217604517937, 0.046717286109924316, 0.04537281394004822, 0.04530289024114609, 0.0441632904112339, 0.043597035109996796, 0.037881847470998764, 0.037359513342380524, 0.03578472137451172, 0.035516299307346344, 0.034251436591148376, 0.03198044002056122, 0.031099995598196983, 0.02763414941728115, 0.017964888364076614, 0.014206823892891407]}",0.0856962725520134,Ethics and NLP,0.0441632904112339
Resources and Evaluation,WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context,"We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. org/competitions/23683 Loïc Vial, Benjamin Lecouteux, and Didier Schwab. 2019. Sense vocabulary compression through the semantic knowledge of wordnet for neural word sense disambiguation. In Proceedings of the 10th Global WordNet Conference.","{'sequence': 'We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. org/competitions/23683 Loïc Vial, Benjamin Lecouteux, and Didier Schwab. 2019. Sense vocabulary compression through the semantic knowledge of wordnet for neural word sense disambiguation. In Proceedings of the 10th Global WordNet Conference.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Dialogue and Interactive Systems', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Machine Translation and Multilinguality', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1301252245903015, 0.060334462672472, 0.060146115720272064, 0.05824020132422447, 0.05746513977646828, 0.05539919435977936, 0.05118437111377716, 0.05073542892932892, 0.044483281672000885, 0.04398539662361145, 0.04292239621281624, 0.040860582143068314, 0.03749268501996994, 0.03592142462730408, 0.035552144050598145, 0.032121479511260986, 0.0308671984821558, 0.03063114359974861, 0.02525244653224945, 0.024440377950668335, 0.02130865678191185, 0.01716567575931549, 0.013364947400987148]}",0.1301252245903015,Resources and Evaluation,0.1301252245903015
Resources and Evaluation,Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR,"Systems that generate natural language text from abstract meaning representations such as AMR are typically evaluated using automatic surface matching metrics that compare the generated texts to reference texts from which the input meaning representations were constructed. We show that besides wellknown issues from which such metrics suffer, an additional problem arises when applying these metrics for AMR-to-text evaluation, since an abstract meaning representation allows for numerous surface realizations. In this work we aim to alleviate these issues by proposing MF β , a decomposable metric that builds on two pillars. The first is the principle of meaning preservation M: it measures to what extent a given AMR can be reconstructed from the generated sentence using SOTA AMR parsers and applying (finegrained) AMR evaluation metrics to measure the distance between the original and the reconstructed AMR. The second pillar builds on a principle of (grammatical) form F that measures the linguistic quality of the generated text, which we implement using SOTA language models. In two extensive pilot studies we show that fulfillment of both principles offers benefits for AMR-to-text evaluation, including explainability of scores. Since MF β does not necessarily rely on gold AMRs, it may extend to other text generation tasks.","{'sequence': 'Systems that generate natural language text from abstract meaning representations such as AMR are typically evaluated using automatic surface matching metrics that compare the generated texts to reference texts from which the input meaning representations were constructed. We show that besides wellknown issues from which such metrics suffer, an additional problem arises when applying these metrics for AMR-to-text evaluation, since an abstract meaning representation allows for numerous surface realizations. In this work we aim to alleviate these issues by proposing MF β , a decomposable metric that builds on two pillars. The first is the principle of meaning preservation M: it measures to what extent a given AMR can be reconstructed from the generated sentence using SOTA AMR parsers and applying (finegrained) AMR evaluation metrics to measure the distance between the original and the reconstructed AMR. The second pillar builds on a principle of (grammatical) form F that measures the linguistic quality of the generated text, which we implement using SOTA language models. In two extensive pilot studies we show that fulfillment of both principles offers benefits for AMR-to-text evaluation, including explainability of scores. Since MF β does not necessarily rely on gold AMRs, it may extend to other text generation tasks.', 'labels': ['Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Information Extraction', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09315366297960281, 0.07630715519189835, 0.07010495662689209, 0.06742645800113678, 0.05840278044342995, 0.05318841338157654, 0.05036156252026558, 0.04745415598154068, 0.046952780336141586, 0.046775542199611664, 0.03987555205821991, 0.03720220923423767, 0.03640983998775482, 0.03452230244874954, 0.03394196182489395, 0.03123066946864128, 0.03102871961891651, 0.030606620013713837, 0.02994270622730255, 0.028196824714541435, 0.02307567186653614, 0.019934577867388725, 0.01390499621629715]}",0.09315366297960281,Generation,0.07010495662689209
Resources and Evaluation,Process-Level Representation of Scientific Protocols with Interactive Annotation,"We develop Process Execution Graphs (PEG), a document-level representation of real-world wet lab biochemistry protocols, addressing challenges such as cross-sentence relations, long-range coreference, grounding, and implicit arguments. We manually annotate PEGs in a corpus of complex lab protocols with a novel interactive textual simulator that keeps track of entity traits and semantic constraints during annotation. We use this data to develop graph-prediction models, finding them to be good at entity identification and local relation extraction, while our corpus facilitates further exploration of challenging long-range relations. 1","{'sequence': 'We develop Process Execution Graphs (PEG), a document-level representation of real-world wet lab biochemistry protocols, addressing challenges such as cross-sentence relations, long-range coreference, grounding, and implicit arguments. We manually annotate PEGs in a corpus of complex lab protocols with a novel interactive textual simulator that keeps track of entity traits and semantic constraints during annotation. We use this data to develop graph-prediction models, finding them to be good at entity identification and local relation extraction, while our corpus facilitates further exploration of challenging long-range relations. 1', 'labels': ['Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Question Answering', 'Summarization', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'NLP Applications', 'Speech and Multimodality', 'Resources and Evaluation', 'Generation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.14136824011802673, 0.10856447368860245, 0.07979664206504822, 0.06613000482320786, 0.05570239946246147, 0.048747722059488297, 0.045010704547166824, 0.043471332639455795, 0.042604029178619385, 0.03989194706082344, 0.03807772323489189, 0.037818700075149536, 0.03742622584104538, 0.03331314027309418, 0.030306240543723106, 0.029510626569390297, 0.025428496301174164, 0.02104542776942253, 0.019514448940753937, 0.017161376774311066, 0.014470255002379417, 0.013829071074724197, 0.01081085205078125]}",0.14136824011802673,Dialogue and Interactive Systems,0.03742622584104538
Resources and Evaluation,A Study of Automatic Metrics for the Evaluation of Natural Language Explanations,"As transparency becomes key for robotics and AI, it will be necessary to evaluate the methods through which transparency is provided, including automatically generated natural language (NL) explanations. Here, we explore parallels between the generation of such explanations and the much-studied field of evaluation of Natural Language Generation (NLG). Specifically, we investigate which of the NLG evaluation measures map well to explanations. We present the ExBAN corpus: a crowd-sourced corpus of NL explanations for Bayesian Networks. We run correlations comparing human subjective ratings with NLG automatic measures. We find that embedding-based automatic NLG evaluation methods, such as BERTScore and BLEURT, have a higher correlation with human ratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work has implications for Explainable AI and transparent robotic and autonomous systems.","{'sequence': 'As transparency becomes key for robotics and AI, it will be necessary to evaluate the methods through which transparency is provided, including automatically generated natural language (NL) explanations. Here, we explore parallels between the generation of such explanations and the much-studied field of evaluation of Natural Language Generation (NLG). Specifically, we investigate which of the NLG evaluation measures map well to explanations. We present the ExBAN corpus: a crowd-sourced corpus of NL explanations for Bayesian Networks. We run correlations comparing human subjective ratings with NLG automatic measures. We find that embedding-based automatic NLG evaluation methods, such as BERTScore and BLEURT, have a higher correlation with human ratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work has implications for Explainable AI and transparent robotic and autonomous systems.', 'labels': ['Generation', 'Resources and Evaluation', 'Language Grounding to Vision, Robotics and Beyond', 'Speech and Multimodality', 'NLP Applications', 'Dialogue and Interactive Systems', 'Question Answering', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Summarization', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15727941691875458, 0.07148820906877518, 0.056488681584596634, 0.05499942973256111, 0.04852037504315376, 0.0481475293636322, 0.04805359989404678, 0.04520126432180405, 0.04151099547743797, 0.041397061198949814, 0.04132474213838577, 0.039952490478754044, 0.03990131989121437, 0.03903181850910187, 0.03152815252542496, 0.031067347154021263, 0.030293164774775505, 0.029988082125782967, 0.026654092594981194, 0.02170504629611969, 0.02140127122402191, 0.021178947761654854, 0.012887008488178253]}",0.15727941691875458,Generation,0.07148820906877518
Resources and Evaluation,A Systematic Review of Reproducibility Research in Natural Language Processing,"Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wideangle, and as near as possible complete, snapshot of current work on reproducibility in NLP, delineating differences and similarities, and providing pointers to common denominators.","{'sequence': 'Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wideangle, and as near as possible complete, snapshot of current work on reproducibility in NLP, delineating differences and similarities, and providing pointers to common denominators.', 'labels': ['Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Question Answering', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Phonology, Morphology and Word Segmentation', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09674695879220963, 0.08393687009811401, 0.06255406886339188, 0.061294008046388626, 0.056151341646909714, 0.05601659417152405, 0.051631588488817215, 0.04964543506503105, 0.04718722030520439, 0.0447501465678215, 0.043258361518383026, 0.04070112854242325, 0.039280544966459274, 0.03518199175596237, 0.03177180513739586, 0.03163624182343483, 0.027660774067044258, 0.025476232171058655, 0.02511209063231945, 0.024892468005418777, 0.024507315829396248, 0.02211807668209076, 0.018488673493266106]}",0.09674695879220963,Ethics and NLP,0.04718722030520439
Resources and Evaluation,How Good (really) are Grammatical Error Correction Systems?,"Standard evaluations of Grammatical Error Correction (GEC) systems make use of a fixed reference text generated relative to the original text; they show, even when using multiple references, that we have a long way to go. This analysis paper studies the performance of GEC systems relative to closest-gold -a gold reference text created relative to the output of a system. Surprisingly, we show that the real performance is 20-40 points better than standard evaluations show. Moreover, the performance remains high even when considering any of the top-10 hypotheses produced by a system. Importantly, the type of mistakes corrected by lower-ranked hypotheses differs in interesting ways from the top one, providing an opportunity to focus on a range of errors -local spelling and grammar edits vs. more complex lexical improvements. Our study shows these results in English and Russian, and thus provides a preliminary proposal for a more realistic evaluation of GEC systems.","{'sequence': 'Standard evaluations of Grammatical Error Correction (GEC) systems make use of a fixed reference text generated relative to the original text; they show, even when using multiple references, that we have a long way to go. This analysis paper studies the performance of GEC systems relative to closest-gold -a gold reference text created relative to the output of a system. Surprisingly, we show that the real performance is 20-40 points better than standard evaluations show. Moreover, the performance remains high even when considering any of the top-10 hypotheses produced by a system. Importantly, the type of mistakes corrected by lower-ranked hypotheses differs in interesting ways from the top one, providing an opportunity to focus on a range of errors -local spelling and grammar edits vs. more complex lexical improvements. Our study shows these results in English and Russian, and thus provides a preliminary proposal for a more realistic evaluation of GEC systems.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Generation', 'Computational Social Science and Social Media', 'Information Extraction', 'Question Answering', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Discourse and Pragmatics', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization'], 'scores': [0.08275815844535828, 0.06283634901046753, 0.06269769370555878, 0.061281077563762665, 0.060858745127916336, 0.05344142019748688, 0.05077768862247467, 0.048115309327840805, 0.04802266135811806, 0.04522165656089783, 0.041588231921195984, 0.04115360602736473, 0.03937368094921112, 0.038336776196956635, 0.036261942237615585, 0.0322837308049202, 0.03213733062148094, 0.029472077265381813, 0.029394065961241722, 0.028821518644690514, 0.02647855132818222, 0.025521013885736465, 0.0231667198240757]}",0.08275815844535828,Resources and Evaluation,0.08275815844535828
Resources and Evaluation,CLiMP: A Benchmark for Chinese Language Model Evaluation,"Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of these models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP), which can be used to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1,000 minimal pairs (MPs) for 16 syntactic contrasts in Mandarin, covering 9 major Mandarin linguistic phenomena. The MPs are semiautomatically generated, and human agreement with the labels in CLiMP is 95.8%. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier-noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the bǎ construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8% average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.","{'sequence': 'Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of these models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP), which can be used to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1,000 minimal pairs (MPs) for 16 syntactic contrasts in Mandarin, covering 9 major Mandarin linguistic phenomena. The MPs are semiautomatically generated, and human agreement with the labels in CLiMP is 95.8%. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier-noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the bǎ construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8% average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.10895506292581558, 0.08733314275741577, 0.07801438122987747, 0.07107551395893097, 0.07002832740545273, 0.0652632787823677, 0.06010746210813522, 0.051912903785705566, 0.05165007710456848, 0.04702693969011307, 0.04007067158818245, 0.039053432643413544, 0.03712320327758789, 0.03564850986003876, 0.025583967566490173, 0.025434281677007675, 0.023317335173487663, 0.01758195273578167, 0.016892824321985245, 0.014045439660549164, 0.013012501411139965, 0.011486624367535114, 0.009382160380482674]}",0.10895506292581558,Resources and Evaluation,0.10895506292581558
Resources and Evaluation,TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics,"Tasks, Datasets and Evaluation Metrics are important concepts for understanding experimental scientific papers. However, most previous work on information extraction for scientific literature mainly focuses on the abstracts only, and does not treat datasets as a separate type of entity (Zadeh and Schumann, 2016; Luan et al., 2018) . In this paper, we present a new corpus that contains domain expert annotations for Task (T), Dataset (D), Metric (M) entities on 2,000 sentences extracted from NLP papers. We report experiment results on TDM extraction using a simple data augmentation strategy and apply our tagger to around 30,000 NLP papers from the ACL Anthology. The corpus is made publicly available to the community for fostering research on scientific publication summarization (Erera et al., 2019) and knowledge discovery.","{'sequence': 'Tasks, Datasets and Evaluation Metrics are important concepts for understanding experimental scientific papers. However, most previous work on information extraction for scientific literature mainly focuses on the abstracts only, and does not treat datasets as a separate type of entity (Zadeh and Schumann, 2016; Luan et al., 2018) . In this paper, we present a new corpus that contains domain expert annotations for Task (T), Dataset (D), Metric (M) entities on 2,000 sentences extracted from NLP papers. We report experiment results on TDM extraction using a simple data augmentation strategy and apply our tagger to around 30,000 NLP papers from the ACL Anthology. The corpus is made publicly available to the community for fostering research on scientific publication summarization (Erera et al., 2019) and knowledge discovery.', 'labels': ['Information Extraction', 'Summarization', 'Resources and Evaluation', 'NLP Applications', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Speech and Multimodality', 'Generation', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.22056381404399872, 0.07052119821310043, 0.061436187475919724, 0.05696690455079079, 0.05524461343884468, 0.04867464676499367, 0.04526619613170624, 0.04478804022073746, 0.03944408521056175, 0.03899604454636574, 0.038983311504125595, 0.03580009937286377, 0.03466293588280678, 0.03387949615716934, 0.03243696317076683, 0.030911432579159737, 0.026783397421240807, 0.019620055332779884, 0.0146875511854887, 0.01326487772166729, 0.012938630767166615, 0.012797504663467407, 0.01133203785866499]}",0.22056381404399872,Information Extraction,0.061436187475919724
Resources and Evaluation,Detecting Scenes in Fiction: A new Segmentation Task,"This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The corpus we describe consists of German-language dime novels (550 k tokens) that have been annotated in parallel, achieving an inter-annotator agreement of γ = 0.7. Baseline experiments using BERT achieve an F1 score of 24 %, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like coreference resolution.","{'sequence': 'This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The corpus we describe consists of German-language dime novels (550 k tokens) that have been annotated in parallel, achieving an inter-annotator agreement of γ = 0.7. Baseline experiments using BERT achieve an F1 score of 24 %, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like coreference resolution.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Generation', 'Question Answering', 'Information Extraction', 'NLP Applications', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Summarization', 'Ethics and NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.09081189334392548, 0.07829757034778595, 0.07481759041547775, 0.07272008061408997, 0.07037504762411118, 0.0592632032930851, 0.05316625162959099, 0.05275889113545418, 0.04266177490353584, 0.04263438284397125, 0.0391368493437767, 0.0388358011841774, 0.036641675978899, 0.03466077893972397, 0.03138156607747078, 0.03072090446949005, 0.030548639595508575, 0.029058149084448814, 0.027861418202519417, 0.02290300279855728, 0.018324000760912895, 0.011883429251611233, 0.010537131689488888]}",0.09081189334392548,Resources and Evaluation,0.09081189334392548
Resources and Evaluation,Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation,"Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model's ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model's ability to memorize versus generalize.","{'sequence': ""Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model's ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model's ability to memorize versus generalize."", 'labels': ['Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Generation', 'Speech and Multimodality', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'NLP Applications', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10992065072059631, 0.09229876101016998, 0.0698196217417717, 0.06392324715852737, 0.05586209520697594, 0.05370161682367325, 0.05255275219678879, 0.048104267567396164, 0.04706118628382683, 0.046443261206150055, 0.04355689138174057, 0.04350053519010544, 0.03902950882911682, 0.038971908390522, 0.03618887811899185, 0.03359457850456238, 0.021741319447755814, 0.019716806709766388, 0.01964249648153782, 0.019390037283301353, 0.019101515412330627, 0.016577988862991333, 0.009300130419433117]}",0.10992065072059631,Interpretability and Analysis of Models for NLP,0.09229876101016998
Resources and Evaluation,Communicative-Function-Based Sentence Classification for Construction of an Academic Formulaic Expression Database,"Formulaic expressions (FEs), such as 'in this paper, we propose' are frequently used in scientific papers. FEs convey a communicative function (CF), i.e. 'showing the aim of the paper' in the above-mentioned example. Although CF-labelled FEs are helpful in assisting academic writing, the construction of FE databases requires manual labour for assigning CF labels. In this study, we considered a fully automated construction of a CF-labelled FE database using the top-down approach, in which the CF labels are first assigned to sentences, and then the FEs are extracted. For the CF-label assignment, we created a CF-labelled sentence dataset, on which we trained a SciB-ERT classifier. We show that the classifier and dataset can be used to construct FE databases of disciplines that are different from the training data. The accuracy of in-disciplinary classification was more than 80%, while crossdisciplinary classification also worked well. We also propose an FE extraction method, which was applied to the CF-labelled sentences. Finally, we constructed and published a new, large CF-labelled FE database. The evaluation of the final CF-labelled FE database showed that approximately 65% of the FEs are correct and useful, which is sufficiently high considering practical use.","{'sequence': ""Formulaic expressions (FEs), such as 'in this paper, we propose' are frequently used in scientific papers. FEs convey a communicative function (CF), i.e. 'showing the aim of the paper' in the above-mentioned example. Although CF-labelled FEs are helpful in assisting academic writing, the construction of FE databases requires manual labour for assigning CF labels. In this study, we considered a fully automated construction of a CF-labelled FE database using the top-down approach, in which the CF labels are first assigned to sentences, and then the FEs are extracted. For the CF-label assignment, we created a CF-labelled sentence dataset, on which we trained a SciB-ERT classifier. We show that the classifier and dataset can be used to construct FE databases of disciplines that are different from the training data. The accuracy of in-disciplinary classification was more than 80%, while crossdisciplinary classification also worked well. We also propose an FE extraction method, which was applied to the CF-labelled sentences. Finally, we constructed and published a new, large CF-labelled FE database. The evaluation of the final CF-labelled FE database showed that approximately 65% of the FEs are correct and useful, which is sufficiently high considering practical use."", 'labels': ['Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Summarization', 'Generation', 'Ethics and NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12174563854932785, 0.09170196205377579, 0.058977942913770676, 0.058734919875860214, 0.05738753825426102, 0.047640610486269, 0.046668846160173416, 0.045158613473176956, 0.04201671481132507, 0.04197226092219353, 0.037228599190711975, 0.035806190222501755, 0.0356920063495636, 0.034853000193834305, 0.03467536345124245, 0.030245695263147354, 0.029762540012598038, 0.029330380260944366, 0.028371522203087807, 0.025594357401132584, 0.025449099019169807, 0.022516440600156784, 0.01846986636519432]}",0.12174563854932785,Information Extraction,0.05738753825426102
Resources and Evaluation,Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa - A Large Romanian Sentiment Data Set,"Romanian is one of the understudied languages in computational linguistics, with few resources available for the development of natural language processing tools. In this paper, we introduce LaRoSeDa, a Large Romanian Sentiment Data Set 1 , which is composed of 15,000 positive and negative reviews collected from one of the largest Romanian e-commerce platforms. We employ two sentiment classification methods as baselines for our new data set, one based on low-level features (character n-grams) and one based on highlevel features (bag-of-word-embeddings generated by clustering word embeddings with kmeans). As an additional contribution, we replace the k-means clustering algorithm with self-organizing maps (SOMs), obtaining better results because the generated clusters of word embeddings are closer to the Zipf's law distribution, which is known to govern natural language. We also demonstrate the generalization capacity of using SOMs for the clustering of word embeddings on another recentlyintroduced Romanian data set, for text categorization by topic.","{'sequence': ""Romanian is one of the understudied languages in computational linguistics, with few resources available for the development of natural language processing tools. In this paper, we introduce LaRoSeDa, a Large Romanian Sentiment Data Set 1 , which is composed of 15,000 positive and negative reviews collected from one of the largest Romanian e-commerce platforms. We employ two sentiment classification methods as baselines for our new data set, one based on low-level features (character n-grams) and one based on highlevel features (bag-of-word-embeddings generated by clustering word embeddings with kmeans). As an additional contribution, we replace the k-means clustering algorithm with self-organizing maps (SOMs), obtaining better results because the generated clusters of word embeddings are closer to the Zipf's law distribution, which is known to govern natural language. We also demonstrate the generalization capacity of using SOMs for the clustering of word embeddings on another recentlyintroduced Romanian data set, for text categorization by topic."", 'labels': ['Generation', 'Resources and Evaluation', 'Machine Learning for NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics'], 'scores': [0.11581393331289291, 0.1034172996878624, 0.08691933006048203, 0.08438140898942947, 0.06571147590875626, 0.04995860531926155, 0.04486164078116417, 0.03907522186636925, 0.03778598830103874, 0.03750830143690109, 0.037348076701164246, 0.037287529557943344, 0.0332801416516304, 0.03320827707648277, 0.03233545273542404, 0.030351262539625168, 0.02666383981704712, 0.02300179749727249, 0.020618945360183716, 0.0203420240432024, 0.0193104799836874, 0.013975108973681927, 0.006843801587820053]}",0.11581393331289291,Generation,0.1034172996878624
Resources and Evaluation,MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark,"Scaling semantic parsing models for taskoriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pretrained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pretrained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.","{'sequence': 'Scaling semantic parsing models for taskoriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pretrained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pretrained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Lexical Semantics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Summarization', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08780882507562637, 0.08310890197753906, 0.0643601343035698, 0.062423694878816605, 0.056817490607500076, 0.055821340531110764, 0.05234320089221001, 0.051450978964567184, 0.05066103860735893, 0.047443028539419174, 0.043523553758859634, 0.042347174137830734, 0.04126117005944252, 0.039174579083919525, 0.03735511004924774, 0.0328482910990715, 0.03046356700360775, 0.027067631483078003, 0.026328450068831444, 0.020760077983140945, 0.017714569345116615, 0.01716914214193821, 0.011748034507036209]}",0.08780882507562637,Dialogue and Interactive Systems,0.062423694878816605
Resources and Evaluation,Mega-COV: A Billion-Scale Dataset of 100+ Languages for COVID-19,"We describe Mega-COV, a billion-scale dataset from Twitter for studying COVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as back as 2007), multilingual (comes in 100+ languages), and has a significant number of location-tagged tweets (∼ 169M tweets). We release tweet IDs from the dataset. We also develop two powerful models, one for identifying whether or not a tweet is related to the pandemic (best F 1 =97%) and another for detecting misinformation about COVID-19 (best F 1 =92%). A human annotation study reveals the utility of our models on a subset of Mega-COV. Our data and models can be useful for studying a wide host of phenomena related to the pandemic. Mega-COV and our models are publicly available.","{'sequence': 'We describe Mega-COV, a billion-scale dataset from Twitter for studying COVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as back as 2007), multilingual (comes in 100+ languages), and has a significant number of location-tagged tweets (∼ 169M tweets). We release tweet IDs from the dataset. We also develop two powerful models, one for identifying whether or not a tweet is related to the pandemic (best F 1 =97%) and another for detecting misinformation about COVID-19 (best F 1 =92%). A human annotation study reveals the utility of our models on a subset of Mega-COV. Our data and models can be useful for studying a wide host of phenomena related to the pandemic. Mega-COV and our models are publicly available.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP'], 'scores': [0.11487891525030136, 0.08122340589761734, 0.06697344034910202, 0.06665731221437454, 0.06432625651359558, 0.0623304508626461, 0.060659173876047134, 0.055848948657512665, 0.049744222313165665, 0.048879224807024, 0.04313940927386284, 0.0375201515853405, 0.03666684031486511, 0.034206513315439224, 0.030656583607196808, 0.026783442124724388, 0.02144218608736992, 0.01893998496234417, 0.01755337417125702, 0.01579178124666214, 0.015640821307897568, 0.015124945901334286, 0.015012696385383606]}",0.11487891525030136,Information Extraction,0.048879224807024
Resources and Evaluation,ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase Generation,"We propose ParaSCI, the first large-scale paraphrase dataset in the scientific field, including 33,981 paraphrase pairs from ACL (ParaSCI-ACL) and 316,063 pairs from arXiv (ParaSCI-arXiv). Digging into characteristics and common patterns of scientific papers, we construct this dataset though intra-paper and inter-paper methods, such as collecting citations to the same paper or aggregating definitions by scientific terms. To take advantage of sentences paraphrased partially, we put up PDBERT as a general paraphrase discovering method. The major advantages of paraphrases in ParaSCI lie in the prominent length and textual diversity, which is complementary to existing paraphrase datasets. ParaSCI obtains satisfactory results on human evaluation and downstream tasks, especially long paraphrase generation.","{'sequence': 'We propose ParaSCI, the first large-scale paraphrase dataset in the scientific field, including 33,981 paraphrase pairs from ACL (ParaSCI-ACL) and 316,063 pairs from arXiv (ParaSCI-arXiv). Digging into characteristics and common patterns of scientific papers, we construct this dataset though intra-paper and inter-paper methods, such as collecting citations to the same paper or aggregating definitions by scientific terms. To take advantage of sentences paraphrased partially, we put up PDBERT as a general paraphrase discovering method. The major advantages of paraphrases in ParaSCI lie in the prominent length and textual diversity, which is complementary to existing paraphrase datasets. ParaSCI obtains satisfactory results on human evaluation and downstream tasks, especially long paraphrase generation.', 'labels': ['Generation', 'Information Extraction', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Summarization', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1936851292848587, 0.0829189345240593, 0.0630006268620491, 0.05726218596100807, 0.05653154104948044, 0.05544322356581688, 0.04900507256388664, 0.047909654676914215, 0.047889597713947296, 0.03910636529326439, 0.03539346531033516, 0.035024698823690414, 0.032893113791942596, 0.0312606506049633, 0.024676989763975143, 0.02465636469423771, 0.02112708054482937, 0.018355879932641983, 0.018264595419168472, 0.01776367984712124, 0.01765718124806881, 0.016191307455301285, 0.013982582837343216]}",0.1936851292848587,Generation,0.047909654676914215
Resources and Evaluation,SICK-NL: A Dataset for Dutch Natural Language Inference,"We present SICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of Marelli et al. (2014) from English into Dutch. Having a parallel inference dataset allows us to compare both monolingual and multilingual NLP models for English and Dutch on the two tasks. In the paper, we motivate and detail the translation process, perform a baseline evaluation on both the original SICK dataset and its Dutch incarnation SICK-NL, taking inspiration from Dutch skipgram embeddings and contextualised embedding models. In addition, we encapsulate two phenomena encountered in the translation to formulate stress tests and verify how well the Dutch models capture syntactic restructurings that do not affect semantics. Our main finding is all models perform worse on SICK-NL than on SICK, indicating that the Dutch dataset is more challenging than the English original. Results on the stress tests show that models don't fully capture word order freedom in Dutch, warranting future systematic studies.","{'sequence': ""We present SICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of Marelli et al. (2014) from English into Dutch. Having a parallel inference dataset allows us to compare both monolingual and multilingual NLP models for English and Dutch on the two tasks. In the paper, we motivate and detail the translation process, perform a baseline evaluation on both the original SICK dataset and its Dutch incarnation SICK-NL, taking inspiration from Dutch skipgram embeddings and contextualised embedding models. In addition, we encapsulate two phenomena encountered in the translation to formulate stress tests and verify how well the Dutch models capture syntactic restructurings that do not affect semantics. Our main finding is all models perform worse on SICK-NL than on SICK, indicating that the Dutch dataset is more challenging than the English original. Results on the stress tests show that models don't fully capture word order freedom in Dutch, warranting future systematic studies."", 'labels': ['Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Question Answering', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Information Extraction', 'Computational Social Science and Social Media', 'NLP Applications', 'Ethics and NLP', 'Speech and Multimodality', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09636395424604416, 0.07442951202392578, 0.06369484215974808, 0.055425260215997696, 0.053946394473314285, 0.049550287425518036, 0.04788185656070709, 0.0471787229180336, 0.04623040184378624, 0.043504077941179276, 0.04262727499008179, 0.042441923171281815, 0.041791029274463654, 0.040441568940877914, 0.03925684094429016, 0.03400931507349014, 0.033869415521621704, 0.03177931159734726, 0.02721935138106346, 0.026578502729535103, 0.022238297387957573, 0.020199131220579147, 0.01934271864593029]}",0.09636395424604416,Resources and Evaluation,0.09636395424604416
Resources and Evaluation,"`Just because you are right, doesn't mean I am wrong': Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks","GQA (Hudson and Manning, 2019 ) is a dataset for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best visionlanguage models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements.","{'sequence': 'GQA (Hudson and Manning, 2019 ) is a dataset for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best visionlanguage models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements.', 'labels': ['Question Answering', 'NLP Applications', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Generation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.2331703007221222, 0.0623977892100811, 0.05941878631711006, 0.05579080432653427, 0.05389292910695076, 0.05173591896891594, 0.04765920713543892, 0.04740629345178604, 0.046342458575963974, 0.04404228925704956, 0.04051567241549492, 0.033533088862895966, 0.03004920296370983, 0.026403047144412994, 0.02244005724787712, 0.022238751873373985, 0.02193567343056202, 0.021552274003624916, 0.020918652415275574, 0.01733826845884323, 0.015318049117922783, 0.014021292328834534, 0.011879274621605873]}",0.2331703007221222,Question Answering,0.04765920713543892
Resources and Evaluation,Summarising Historical Text in Modern Languages,"We introduce the task of historical text summarisation, where documents in historical forms of a language are summarised in the corresponding modern language. This is a fundamentally important routine to historians and digital humanities researchers but has never been automated. We compile a high-quality gold-standard text summarisation dataset, which consists of historical German and Chinese news from hundreds of years ago summarised in modern German or Chinese. Based on cross-lingual transfer learning techniques, we propose a summarisation model that can be trained even with no cross-lingual (historical to modern) parallel data, and further benchmark it against state-of-the-art algorithms. We report automatic and human evaluations that distinguish the historic to modern language summarisation task from standard cross-lingual summarisation (i.e., modern to modern language), highlight the distinctness and value of our dataset, and demonstrate that our transfer learning approach outperforms standard cross-lingual benchmarks on this task.","{'sequence': 'We introduce the task of historical text summarisation, where documents in historical forms of a language are summarised in the corresponding modern language. This is a fundamentally important routine to historians and digital humanities researchers but has never been automated. We compile a high-quality gold-standard text summarisation dataset, which consists of historical German and Chinese news from hundreds of years ago summarised in modern German or Chinese. Based on cross-lingual transfer learning techniques, we propose a summarisation model that can be trained even with no cross-lingual (historical to modern) parallel data, and further benchmark it against state-of-the-art algorithms. We report automatic and human evaluations that distinguish the historic to modern language summarisation task from standard cross-lingual summarisation (i.e., modern to modern language), highlight the distinctness and value of our dataset, and demonstrate that our transfer learning approach outperforms standard cross-lingual benchmarks on this task.', 'labels': ['Resources and Evaluation', 'Summarization', 'Dialogue and Interactive Systems', 'Question Answering', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'NLP Applications', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.1775248646736145, 0.13293781876564026, 0.0835428461432457, 0.06346085667610168, 0.05404771864414215, 0.05157633125782013, 0.04627002030611038, 0.03995385393500328, 0.032176271080970764, 0.0314832367002964, 0.030527018010616302, 0.030382605269551277, 0.027269065380096436, 0.02599957399070263, 0.025163764134049416, 0.024524793028831482, 0.02278241701424122, 0.020524609833955765, 0.017695480957627296, 0.016251573339104652, 0.016170024871826172, 0.015326912514865398, 0.01440831832587719]}",0.1775248646736145,Resources and Evaluation,0.1775248646736145
Resources and Evaluation,NLQuAD: A Non-Factoid Long Question Answering Data Set,"We introduce NLQuAD, the first data set with baseline methods for non-factoid long question answering, a task requiring documentlevel language understanding. In contrast to existing span detection question answering data sets, NLQuAD has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union (IoU), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare BERT, RoBERTa, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. NLQuAD's samples exceed the input limitation of most pretrained Transformer-based models, encouraging future research on long sequence language models. 1","{'sequence': ""We introduce NLQuAD, the first data set with baseline methods for non-factoid long question answering, a task requiring documentlevel language understanding. In contrast to existing span detection question answering data sets, NLQuAD has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union (IoU), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare BERT, RoBERTa, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. NLQuAD's samples exceed the input limitation of most pretrained Transformer-based models, encouraging future research on long sequence language models. 1"", 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'NLP Applications', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3828029930591583, 0.06647542864084244, 0.046579137444496155, 0.041081834584474564, 0.03651587665081024, 0.035812146961688995, 0.030437488108873367, 0.027939679101109505, 0.027792466804385185, 0.0272112637758255, 0.026263821870088577, 0.025366924703121185, 0.02461092174053192, 0.02455620840191841, 0.023536961525678635, 0.022729912772774696, 0.022163808345794678, 0.02076336368918419, 0.019136551767587662, 0.018824847415089607, 0.017943857237696648, 0.01750502735376358, 0.013949461281299591]}",0.3828029930591583,Question Answering,0.06647542864084244
Resources and Evaluation,We Need To Talk About Random Splits,"Gorman and Bedrick ( 2019 ) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.","{'sequence': 'Gorman and Bedrick ( 2019 ) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.', 'labels': ['NLP Applications', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Speech and Multimodality', 'Generation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality'], 'scores': [0.10251840949058533, 0.07711034268140793, 0.0737859457731247, 0.06461633741855621, 0.06278236955404282, 0.058219775557518005, 0.05660815164446831, 0.053856879472732544, 0.05246453732252121, 0.04605885222554207, 0.04180369898676872, 0.03897895663976669, 0.037625592201948166, 0.03099583461880684, 0.029993202537298203, 0.02708924375474453, 0.023007340729236603, 0.02193193882703781, 0.02160344272851944, 0.020624974742531776, 0.020225346088409424, 0.019589586183428764, 0.018509097397327423]}",0.10251840949058533,NLP Applications,0.05660815164446831
Resources and Evaluation,Acquiring a Formality-Informed Lexical Resource for Style Analysis,"To track different levels of formality in written discourse, we introduce a novel type of lexicon for the German language, with entries ordered by their degree of (in)formality. We start with a set of words extracted from traditional lexicographic resources, extend it by sentence-based similarity computations, and let crowdworkers assess the enlarged set of lexical items on a continuous informal-formal scale as a gold standard for evaluation. We submit this lexicon to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus.","{'sequence': 'To track different levels of formality in written discourse, we introduce a novel type of lexicon for the German language, with entries ordered by their degree of (in)formality. We start with a set of words extracted from traditional lexicographic resources, extend it by sentence-based similarity computations, and let crowdworkers assess the enlarged set of lexical items on a continuous informal-formal scale as a gold standard for evaluation. We submit this lexicon to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Information Extraction', 'Information Retrieval and Text Mining', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'NLP Applications', 'Discourse and Pragmatics', 'Ethics and NLP', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.15545201301574707, 0.0754723995923996, 0.06633344292640686, 0.06090300902724266, 0.05650516599416733, 0.05264750495553017, 0.050002571195364, 0.04956592991948128, 0.0495564304292202, 0.04866836592555046, 0.04191673919558525, 0.038300979882478714, 0.03415153548121452, 0.028964359313249588, 0.027765249833464622, 0.027330344542860985, 0.02612067386507988, 0.022289691492915154, 0.0219207014888525, 0.020493898540735245, 0.017529712989926338, 0.016573898494243622, 0.011535300873219967]}",0.15545201301574707,Resources and Evaluation,0.15545201301574707
Resources and Evaluation,"FEWS: Large-Scale, Low-Shot Word Sense Disambiguation with the Dictionary","Current models for Word Sense Disambiguation (WSD) struggle to disambiguate rare senses, despite reaching human performance on global WSD metrics. This stems from a lack of data for both modeling and evaluating rare senses in existing WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of Word Senses), a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary. FEWS has high sense coverage across different natural language domains and provides: (1) a large training set that covers many more senses than previous datasets and (2) a comprehensive evaluation set containing few-and zero-shot examples of a wide variety of senses. We establish baselines on FEWS with knowledgebased and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with FEWS better capture rare senses in existing WSD datasets. Finally, we find humans outperform the best baseline models on FEWS, indicating that FEWS will support significant future work on low-shot WSD.","{'sequence': 'Current models for Word Sense Disambiguation (WSD) struggle to disambiguate rare senses, despite reaching human performance on global WSD metrics. This stems from a lack of data for both modeling and evaluating rare senses in existing WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of Word Senses), a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary. FEWS has high sense coverage across different natural language domains and provides: (1) a large training set that covers many more senses than previous datasets and (2) a comprehensive evaluation set containing few-and zero-shot examples of a wide variety of senses. We establish baselines on FEWS with knowledgebased and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with FEWS better capture rare senses in existing WSD datasets. Finally, we find humans outperform the best baseline models on FEWS, indicating that FEWS will support significant future work on low-shot WSD.', 'labels': ['Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'NLP Applications', 'Resources and Evaluation', 'Information Extraction', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Summarization', 'Discourse and Pragmatics', 'Generation', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0747629851102829, 0.07463458925485611, 0.06706248968839645, 0.06265073269605637, 0.06230206787586212, 0.05311408266425133, 0.050762422382831573, 0.05068839341402054, 0.049792852252721786, 0.04895700886845589, 0.04730963706970215, 0.04474777728319168, 0.04375181719660759, 0.040976282209157944, 0.03379007428884506, 0.032011277973651886, 0.028054948896169662, 0.025601327419281006, 0.024640792980790138, 0.023881511762738228, 0.023395629599690437, 0.02135222591459751, 0.015759089961647987]}",0.0747629851102829,Machine Learning for NLP,0.06230206787586212
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Disambiguatory Signals are Stronger in Word-initial Positions,"Psycholinguistic studies of human word processing and lexical access provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjecture-as in Wedel et al. ( 2019b ), but common elsewhere-that languages have evolved to provide more information earlier in words than later. Informationtheoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words. 1","{'sequence': 'Psycholinguistic studies of human word processing and lexical access provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjecture-as in Wedel et al. ( 2019b ), but common elsewhere-that languages have evolved to provide more information earlier in words than later. Informationtheoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words. 1', 'labels': ['Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Semantics: Lexical Semantics', 'Generation', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Ethics and NLP', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09855914115905762, 0.08355209231376648, 0.08342587947845459, 0.07249605655670166, 0.06946637481451035, 0.055278848856687546, 0.05307627096772194, 0.04666073992848396, 0.04622141271829605, 0.04064364358782768, 0.040619995445013046, 0.04036963731050491, 0.03286771848797798, 0.03133304417133331, 0.031057234853506088, 0.028605235740542412, 0.0254603773355484, 0.024598799645900726, 0.02320457622408867, 0.021951856091618538, 0.021748272702097893, 0.019992690533399582, 0.008810007944703102]}",0.09855914115905762,"Linguistic Theories, Cognitive Modeling and Psycholinguistics",0.09855914115905762
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Open-Mindedness and Style Coordination in Argumentative Discussions,"Linguistic accommodation is the process in which speakers adjust their accent, diction, vocabulary, and other aspects of language according to the communication style of one another. Previous research has shown how linguistic accommodation correlates with gaps in the power and status of the speakers and the way it promotes approval and discussion efficiency. In this work, we provide a novel perspective on the phenomena, exploring its correlation with the open-mindedness of a speaker, rather than to her social status. We process thousands of unstructured argumentative discussions that took place in Reddit's Change My View (CMV) subreddit, demonstrating that open-mindedness relates to the assumed role of a speaker in different contexts. On the discussion level, we surprisingly find that discussions that reach agreement present lower levels of accommodation.","{'sequence': ""Linguistic accommodation is the process in which speakers adjust their accent, diction, vocabulary, and other aspects of language according to the communication style of one another. Previous research has shown how linguistic accommodation correlates with gaps in the power and status of the speakers and the way it promotes approval and discussion efficiency. In this work, we provide a novel perspective on the phenomena, exploring its correlation with the open-mindedness of a speaker, rather than to her social status. We process thousands of unstructured argumentative discussions that took place in Reddit's Change My View (CMV) subreddit, demonstrating that open-mindedness relates to the assumed role of a speaker in different contexts. On the discussion level, we surprisingly find that discussions that reach agreement present lower levels of accommodation."", 'labels': ['Speech and Multimodality', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Generation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Summarization', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.1243758425116539, 0.07245688140392303, 0.06529949605464935, 0.06199892982840538, 0.06190892308950424, 0.052684441208839417, 0.046757765114307404, 0.04454958438873291, 0.04417422041296959, 0.043296992778778076, 0.04202383756637573, 0.03817097470164299, 0.03651122376322746, 0.03541184589266777, 0.03338399901986122, 0.033331748098134995, 0.0325097031891346, 0.030223604291677475, 0.029959039762616158, 0.021643158048391342, 0.01913977973163128, 0.015644561499357224, 0.014543446712195873]}",0.1243758425116539,Speech and Multimodality,0.03651122376322746
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Why Is MBTI Personality Detection from Texts a Difficult Task?,"Automatic detection of the four MBTI personality dimensions from texts has recently attracted noticeable attention from the natural language processing and computational linguistic communities. Despite the large collections of Twitter data for training, the best systems rarely even outperform the majority-class baseline. In this paper, we discuss the theoretical reasons for such low results and present the insights from an annotation study that further shed the light on this issue.","{'sequence': 'Automatic detection of the four MBTI personality dimensions from texts has recently attracted noticeable attention from the natural language processing and computational linguistic communities. Despite the large collections of Twitter data for training, the best systems rarely even outperform the majority-class baseline. In this paper, we discuss the theoretical reasons for such low results and present the insights from an annotation study that further shed the light on this issue.', 'labels': ['NLP Applications', 'Dialogue and Interactive Systems', 'Information Extraction', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Question Answering', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.10240473598241806, 0.07946289330720901, 0.07114620506763458, 0.06974956393241882, 0.05763993784785271, 0.055708423256874084, 0.05103635787963867, 0.05072017014026642, 0.0493752658367157, 0.04521851986646652, 0.03941918909549713, 0.036599814891815186, 0.03636752814054489, 0.0358826220035553, 0.03077620640397072, 0.030335819348692894, 0.029003461822867393, 0.028868362307548523, 0.02698947861790657, 0.023937419056892395, 0.022533247247338295, 0.014013714157044888, 0.012810984626412392]}",0.10240473598241806,NLP Applications,0.04521851986646652
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT,"We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a ""subject"") is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work. 1","{'sequence': 'We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a ""subject"") is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work. 1', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Speech and Multimodality', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP'], 'scores': [0.07128062099218369, 0.06783691793680191, 0.06477603316307068, 0.06263048201799393, 0.05976301431655884, 0.058350931853055954, 0.05418645963072777, 0.05322004109621048, 0.046840716153383255, 0.044754575937986374, 0.042410578578710556, 0.04233138635754585, 0.041270311921834946, 0.03765106201171875, 0.03272644430398941, 0.03221055865287781, 0.02990211360156536, 0.028909996151924133, 0.027545807883143425, 0.027145998552441597, 0.027017448097467422, 0.026147663593292236, 0.02109084650874138]}",0.07128062099218369,Resources and Evaluation,0.03765106201171875
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Probing for idiomaticity in vector space models,"Contextualised word representation models have been successfully used for capturing different word usages, and they may be an attractive alternative for representing idiomaticity in language. In this paper, we propose probing measures to assess if some of the expected linguistic properties of noun compounds, especially those related to idiomatic meanings, and their dependence on context and sensitivity to lexical choice, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models.","{'sequence': 'Contextualised word representation models have been successfully used for capturing different word usages, and they may be an attractive alternative for representing idiomaticity in language. In this paper, we propose probing measures to assess if some of the expected linguistic properties of noun compounds, especially those related to idiomatic meanings, and their dependence on context and sensitivity to lexical choice, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models.', 'labels': ['Question Answering', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Summarization', 'Information Extraction', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'NLP Applications', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08594682812690735, 0.08019144833087921, 0.07689771056175232, 0.07537376880645752, 0.07079243659973145, 0.0633908212184906, 0.05570439249277115, 0.0538032203912735, 0.04872027784585953, 0.047478750348091125, 0.045883648097515106, 0.03929239138960838, 0.03643326461315155, 0.03359781950712204, 0.03211100399494171, 0.027905762195587158, 0.026105431839823723, 0.022824307903647423, 0.021678728982806206, 0.01878681778907776, 0.01798023283481598, 0.01349474024027586, 0.005606257822364569]}",0.08594682812690735,Question Answering,0.021678728982806206
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Cognition-aware Cognate Detection,"Automatic detection of cognates helps downstream NLP tasks of Machine Translation, Cross-lingual Information Retrieval, Computational Phylogenetics and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with cognitive features extracted from human readers' gaze behaviour. We collect gaze behaviour data for a small sample of cognates and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10% with the collected gaze features, and 12% using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models.","{'sequence': ""Automatic detection of cognates helps downstream NLP tasks of Machine Translation, Cross-lingual Information Retrieval, Computational Phylogenetics and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with cognitive features extracted from human readers' gaze behaviour. We collect gaze behaviour data for a small sample of cognates and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10% with the collected gaze features, and 12% using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models."", 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Generation', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24815917015075684, 0.11592277139425278, 0.09193678200244904, 0.044579848647117615, 0.04344832897186279, 0.042599376291036606, 0.037294626235961914, 0.03360703960061073, 0.03309328854084015, 0.030938034877181053, 0.03050929680466652, 0.030171379446983337, 0.028137406334280968, 0.026882318779826164, 0.026436541229486465, 0.02537931501865387, 0.024899020791053772, 0.019842594861984253, 0.019155625253915787, 0.018259191885590553, 0.010666023008525372, 0.010427920147776604, 0.007653997279703617]}",0.24815917015075684,NLP Applications,0.010427920147776604
Machine Learning for NLP,Disfluency Correction using Unsupervised and Semi-supervised Learning,"Spoken language is different from the written language in its style and structure. Disfluencies that appear in transcriptions from speech recognition systems generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semisupervised way. Our unsupervised approach achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semisupervision. Both are comparable to two competitive fully-supervised models.","{'sequence': 'Spoken language is different from the written language in its style and structure. Disfluencies that appear in transcriptions from speech recognition systems generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semisupervised way. Our unsupervised approach achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semisupervision. Both are comparable to two competitive fully-supervised models.', 'labels': ['NLP Applications', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Generation', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13896431028842926, 0.09899630397558212, 0.07577305287122726, 0.06791392713785172, 0.04815949872136116, 0.04732770845293999, 0.044961243867874146, 0.04237864911556244, 0.042359139770269394, 0.04132987558841705, 0.0401809923350811, 0.03178328275680542, 0.03055769018828869, 0.030192196369171143, 0.02977067604660988, 0.029465433210134506, 0.026724642142653465, 0.026303108781576157, 0.024716202169656754, 0.023362968116998672, 0.022800438106060028, 0.019304169341921806, 0.016674509271979332]}",0.13896431028842926,NLP Applications,0.044961243867874146
Machine Learning for NLP,FakeFlow: Fake News Detection by Modeling the Flow of Affective Information,"Fake news articles often stir the readers' attention by means of emotional appeals that arouse their feelings. Unlike in short news texts, authors of longer articles can exploit such affective factors to manipulate readers by adding exaggerations or fabricating events, in order to affect the readers' emotions. To capture this, we propose in this paper to model the flow of affective information in fake news articles using a neural architecture. The proposed model, FakeFlow, learns this flow by combining topic and affective information extracted from text. We evaluate the model's performance with several experiments on four real-world datasets. The results show that FakeFlow achieves superior results when compared against state-ofthe-art methods, thus confirming the importance of capturing the flow of the affective information in news articles.","{'sequence': ""Fake news articles often stir the readers' attention by means of emotional appeals that arouse their feelings. Unlike in short news texts, authors of longer articles can exploit such affective factors to manipulate readers by adding exaggerations or fabricating events, in order to affect the readers' emotions. To capture this, we propose in this paper to model the flow of affective information in fake news articles using a neural architecture. The proposed model, FakeFlow, learns this flow by combining topic and affective information extracted from text. We evaluate the model's performance with several experiments on four real-world datasets. The results show that FakeFlow achieves superior results when compared against state-ofthe-art methods, thus confirming the importance of capturing the flow of the affective information in news articles."", 'labels': ['Information Extraction', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'NLP Applications', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Discourse and Pragmatics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09963466972112656, 0.09429331868886948, 0.09173892438411713, 0.07883717119693756, 0.06012825295329094, 0.05728902295231819, 0.04549209773540497, 0.04510554298758507, 0.04224167764186859, 0.03992566466331482, 0.03750758245587349, 0.03712420538067818, 0.03500094637274742, 0.03053150326013565, 0.02980426698923111, 0.029056215658783913, 0.028474319726228714, 0.028316374868154526, 0.027250895276665688, 0.020928142592310905, 0.019757738336920738, 0.01294759102165699, 0.008613886311650276]}",0.09963466972112656,Information Extraction,0.028474319726228714
Machine Learning for NLP,Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification,"We consider the problem of multi-label classification, where the labels lie in a hierarchy. However, unlike most existing works in hierarchical multi-label classification, we do not assume that the label-hierarchy is known. Encouraged by the recent success of hyperbolic embeddings in capturing hierarchical relations, we propose to jointly learn the classifier parameters as well as the label embeddings. Such a joint learning is expected to provide a twofold advantage: i) the classifier generalises better as it leverages the prior knowledge of existence of a hierarchy over the labels, and ii) in addition to the label co-occurrence information, the label-embedding may benefit from the manifold structure of the input datapoints, leading to embeddings that are more faithful to the label hierarchy. We propose a novel formulation for the joint learning and empirically evaluate its efficacy. The results show that the joint learning improves over the baseline that employs label co-occurrence based pre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve state-of-the-art generalization on standard benchmarks. We also present evaluation of the hyperbolic embeddings obtained by joint learning and show that they represent the hierarchy more accurately than the other alternatives. The source code of the paper is available here.","{'sequence': 'We consider the problem of multi-label classification, where the labels lie in a hierarchy. However, unlike most existing works in hierarchical multi-label classification, we do not assume that the label-hierarchy is known. Encouraged by the recent success of hyperbolic embeddings in capturing hierarchical relations, we propose to jointly learn the classifier parameters as well as the label embeddings. Such a joint learning is expected to provide a twofold advantage: i) the classifier generalises better as it leverages the prior knowledge of existence of a hierarchy over the labels, and ii) in addition to the label co-occurrence information, the label-embedding may benefit from the manifold structure of the input datapoints, leading to embeddings that are more faithful to the label hierarchy. We propose a novel formulation for the joint learning and empirically evaluate its efficacy. The results show that the joint learning improves over the baseline that employs label co-occurrence based pre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve state-of-the-art generalization on standard benchmarks. We also present evaluation of the hyperbolic embeddings obtained by joint learning and show that they represent the hierarchy more accurately than the other alternatives. The source code of the paper is available here.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09770656377077103, 0.08064940571784973, 0.07519812881946564, 0.05738258361816406, 0.05715617164969444, 0.054735004901885986, 0.052791815251111984, 0.04754455015063286, 0.04386301711201668, 0.04182299226522446, 0.04131515696644783, 0.040946800261735916, 0.037298962473869324, 0.03274634853005409, 0.031580325216054916, 0.030150964856147766, 0.028351586312055588, 0.02820861153304577, 0.027648726478219032, 0.025854207575321198, 0.02348121628165245, 0.023039067164063454, 0.020527884364128113]}",0.09770656377077103,Resources and Evaluation,0.02348121628165245
Machine Learning for NLP,Diverse Adversaries for Mitigating Bias in Training,"Adversarial learning can learn fairer and less biased models of language than standard methods. However, current adversarial techniques only partially mitigate model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and the stability of training.","{'sequence': 'Adversarial learning can learn fairer and less biased models of language than standard methods. However, current adversarial techniques only partially mitigate model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and the stability of training.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08160534501075745, 0.07535170763731003, 0.06420982629060745, 0.06268074363470078, 0.06071022152900696, 0.059396892786026, 0.05303064361214638, 0.04925668239593506, 0.046196553856134415, 0.04580864682793617, 0.04397065192461014, 0.04060538113117218, 0.03996438533067703, 0.03525841608643532, 0.03480967506766319, 0.03315022587776184, 0.03308076784014702, 0.031319789588451385, 0.02666056901216507, 0.023608312010765076, 0.022403432056307793, 0.02156658098101616, 0.015354624018073082]}",0.08160534501075745,Dialogue and Interactive Systems,0.031319789588451385
Machine Learning for NLP,Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates,"Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in transfer learning for natural language processing in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pretrained models in the active learning framework and find the best combinations for different types of models. Besides, we also demonstrate that to acquire instances during active learning, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice.","{'sequence': 'Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in transfer learning for natural language processing in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pretrained models in the active learning framework and find the best combinations for different types of models. Besides, we also demonstrate that to acquire instances during active learning, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Question Answering', 'Ethics and NLP', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Generation', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Summarization', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11706556379795074, 0.1011396124958992, 0.0664682611823082, 0.06302640587091446, 0.05794796347618103, 0.054411232471466064, 0.05270834267139435, 0.04928119480609894, 0.046214163303375244, 0.04290741682052612, 0.038584042340517044, 0.034745682030916214, 0.034666214138269424, 0.033968955278396606, 0.03236023709177971, 0.031654320657253265, 0.02825860120356083, 0.025816326960921288, 0.021741239354014397, 0.019515428692102432, 0.01882539875805378, 0.01568938046693802, 0.013003953732550144]}",0.11706556379795074,NLP Applications,0.1011396124958992
Machine Learning for NLP,Text Augmentation in a Multi-Task View,"Traditional data augmentation aims to increase the coverage of the input distribution by generating augmented examples that strongly resemble original samples in an online fashion where augmented examples dominate training. In this paper, we propose an alternative perspective-a multi-task view (MTV) of data augmentation-in which the primary task trains on original examples and the auxiliary task trains on augmented examples. In MTV data augmentation, both original and augmented samples are weighted substantively during training, relaxing the constraint that augmented examples must resemble original data and thereby allowing us to apply stronger levels of augmentation. In empirical experiments using four common data augmentation techniques on three benchmark text classification datasets, we find that the MTV leads to higher and more robust performance improvements than traditional augmentation.","{'sequence': 'Traditional data augmentation aims to increase the coverage of the input distribution by generating augmented examples that strongly resemble original samples in an online fashion where augmented examples dominate training. In this paper, we propose an alternative perspective-a multi-task view (MTV) of data augmentation-in which the primary task trains on original examples and the auxiliary task trains on augmented examples. In MTV data augmentation, both original and augmented samples are weighted substantively during training, relaxing the constraint that augmented examples must resemble original data and thereby allowing us to apply stronger levels of augmentation. In empirical experiments using four common data augmentation techniques on three benchmark text classification datasets, we find that the MTV leads to higher and more robust performance improvements than traditional augmentation.', 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Generation', 'Resources and Evaluation', 'Ethics and NLP', 'Speech and Multimodality', 'Information Extraction', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13377995789051056, 0.11794838309288025, 0.10171689093112946, 0.07474087178707123, 0.0683676227927208, 0.055970653891563416, 0.05341997370123863, 0.03887765482068062, 0.0379435159265995, 0.03414304554462433, 0.03144612908363342, 0.02896847389638424, 0.028889058157801628, 0.028571918606758118, 0.0277671180665493, 0.02211841382086277, 0.02144782990217209, 0.019619300961494446, 0.019044065847992897, 0.017193691805005074, 0.013778035528957844, 0.013126499950885773, 0.011120851151645184]}",0.13377995789051056,Dialogue and Interactive Systems,0.0379435159265995
Machine Learning for NLP,Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models,"Language modeling with BERT consists of two phases of (i) unsupervised pre-training on unlabeled text, and (ii) fine-tuning for a specific supervised task. We present a method that leverages the second phase to its fullest, by applying an extensive number of parallel classifier heads, which are enforced to be orthogonal, while adaptively eliminating the weaker heads during training. We conduct an extensive inter-and intradataset evaluation, showing that our method improves the generalization ability of BERT, sometimes leading to a +9% gain in accuracy. These results highlight the importance of a proper fine-tuning procedure, especially for relatively smaller-sized datasets. Our code is attached as supplementary.","{'sequence': 'Language modeling with BERT consists of two phases of (i) unsupervised pre-training on unlabeled text, and (ii) fine-tuning for a specific supervised task. We present a method that leverages the second phase to its fullest, by applying an extensive number of parallel classifier heads, which are enforced to be orthogonal, while adaptively eliminating the weaker heads during training. We conduct an extensive inter-and intradataset evaluation, showing that our method improves the generalization ability of BERT, sometimes leading to a +9% gain in accuracy. These results highlight the importance of a proper fine-tuning procedure, especially for relatively smaller-sized datasets. Our code is attached as supplementary.', 'labels': ['Resources and Evaluation', 'Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Summarization', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.3090057671070099, 0.06892437487840652, 0.05375486612319946, 0.04920129477977753, 0.04631194472312927, 0.040862735360860825, 0.03754856437444687, 0.03655220940709114, 0.03262822702527046, 0.03214417025446892, 0.031851302832365036, 0.03120856173336506, 0.029893310740590096, 0.028109412640333176, 0.027355270460247993, 0.02396162785589695, 0.021344074979424477, 0.020237848162651062, 0.019647257402539253, 0.01636408269405365, 0.015070170164108276, 0.014996174722909927, 0.013026735745370388]}",0.3090057671070099,Resources and Evaluation,0.06892437487840652
Machine Learning for NLP,How Fast can BERT Learn Simple Natural Language Inference?,"This paper empirically studies whether BERT can really learn to conduct natural language inference (NLI) without utilizing hidden dataset bias; and how efficiently it can learn if it could. This is done via creating a simple entailment judgment case which involves only binary predicates in plain English. The results show that the learning process of BERT is very slow. However, the efficiency of learning can be greatly improved (data reduction by a factor of 1,500) if task-related features are added. This suggests that domain knowledge greatly helps when conducting NLI with neural networks.","{'sequence': 'This paper empirically studies whether BERT can really learn to conduct natural language inference (NLI) without utilizing hidden dataset bias; and how efficiently it can learn if it could. This is done via creating a simple entailment judgment case which involves only binary predicates in plain English. The results show that the learning process of BERT is very slow. However, the efficiency of learning can be greatly improved (data reduction by a factor of 1,500) if task-related features are added. This suggests that domain knowledge greatly helps when conducting NLI with neural networks.', 'labels': ['NLP Applications', 'Dialogue and Interactive Systems', 'Generation', 'Machine Learning for NLP', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Question Answering', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Information Extraction', 'Computational Social Science and Social Media', 'Summarization', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10073433816432953, 0.10040303319692612, 0.08378847688436508, 0.07070346176624298, 0.06602440774440765, 0.06262410432100296, 0.04974966496229172, 0.0490465983748436, 0.04587221518158913, 0.03925447538495064, 0.03776364400982857, 0.03561565279960632, 0.03511916473507881, 0.03447907045483589, 0.033370666205883026, 0.03269157558679581, 0.025016257539391518, 0.022612249478697777, 0.0171738862991333, 0.015273199416697025, 0.014596199616789818, 0.01455659605562687, 0.013530969619750977]}",0.10073433816432953,NLP Applications,0.07070346176624298
Machine Learning for NLP,DOCENT: Learning Self-Supervised Entity Representations from Large Document Collections,"This paper explores learning rich selfsupervised entity representations from large amounts of associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, question answering, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful, highcapacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision. We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities-strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our models match or outperform competitive baselines, sometimes with little or no fine-tuning, and can scale to very large corpora. Finally, we make our datasets and pre-trained models publicly available 1 . This includes Reviews2Movielens, mapping the ∼1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions with natural language queries and corresponding community recommendations.","{'sequence': 'This paper explores learning rich selfsupervised entity representations from large amounts of associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, question answering, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful, highcapacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision. We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities-strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our models match or outperform competitive baselines, sometimes with little or no fine-tuning, and can scale to very large corpora. Finally, we make our datasets and pre-trained models publicly available 1 . This includes Reviews2Movielens, mapping the ∼1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions with natural language queries and corresponding community recommendations.', 'labels': ['Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.32880422472953796, 0.06166161969304085, 0.04811970517039299, 0.043895889073610306, 0.04309875890612602, 0.04165131598711014, 0.03849874064326286, 0.03791045397520065, 0.03536607325077057, 0.033849600702524185, 0.02966272458434105, 0.027720943093299866, 0.027699122205376625, 0.027163393795490265, 0.026228396221995354, 0.025100503116846085, 0.024071210995316505, 0.020124264061450958, 0.018364544957876205, 0.018277976661920547, 0.016949307173490524, 0.015075726434588432, 0.010705556720495224]}",0.32880422472953796,Question Answering,0.027163393795490265
Machine Learning for NLP,Towards More Fine-grained and Reliable NLP Performance Prediction,"Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU, but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future.","{'sequence': ""Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU, but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future."", 'labels': ['Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Learning for NLP', 'Speech and Multimodality', 'NLP Applications', 'Semantics: Lexical Semantics', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Extraction', 'Information Retrieval and Text Mining'], 'scores': [0.1920492947101593, 0.08061525225639343, 0.07137168943881989, 0.06277655810117722, 0.061803001910448074, 0.05381975322961807, 0.04980001598596573, 0.04027905687689781, 0.034501295536756516, 0.0340038500726223, 0.03226190805435181, 0.03035135380923748, 0.028649015352129936, 0.02742013707756996, 0.025683443993330002, 0.02437834069132805, 0.023630429059267044, 0.02215958759188652, 0.021856799721717834, 0.021823037415742874, 0.021268805488944054, 0.02097601816058159, 0.018521465361118317]}",0.1920492947101593,Interpretability and Analysis of Models for NLP,0.05381975322961807
Machine Learning for NLP,Scalable Evaluation and Improvement of Document Set Expansion via Neural Positive-Unlabeled Learning,"We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning-i.e., learning binary classifiers from only positive (the query documents) and unlabeled (the results of the IR engine) data. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, showing that the standard implementations of state-of-the-art PU solutions fail. We propose solutions for each of the challenges and empirically validate them with ablation tests. We demonstrate the effectiveness of the new method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics, showing improvements over the common IR solution and other baselines.","{'sequence': 'We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning-i.e., learning binary classifiers from only positive (the query documents) and unlabeled (the results of the IR engine) data. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, showing that the standard implementations of state-of-the-art PU solutions fail. We propose solutions for each of the challenges and empirically validate them with ablation tests. We demonstrate the effectiveness of the new method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics, showing improvements over the common IR solution and other baselines.', 'labels': ['Information Retrieval and Text Mining', 'Information Extraction', 'Dialogue and Interactive Systems', 'Generation', 'Resources and Evaluation', 'Question Answering', 'Ethics and NLP', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Lexical Semantics', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.10475058108568192, 0.0957440510392189, 0.09497387707233429, 0.05879780277609825, 0.05581336095929146, 0.05541517958045006, 0.04200389236211777, 0.04142139106988907, 0.039991673082113266, 0.03932955861091614, 0.03638148680329323, 0.03579327091574669, 0.03572932630777359, 0.03299183398485184, 0.03247847780585289, 0.03185373917222023, 0.030457111075520515, 0.02697567082941532, 0.024660805240273476, 0.02220342680811882, 0.02149895392358303, 0.020929137244820595, 0.019805464893579483]}",0.10475058108568192,Information Retrieval and Text Mining,0.02697567082941532
Machine Learning for NLP,Randomized Deep Structured Prediction for Discourse-Level Processing,"Expressive text encoders such as RNNs and Transformer Networks have been at the center of NLP models in recent work. Most of the effort has focused on sentence-level tasks, capturing the dependencies between words in a single sentence, or pairs of sentences. However, certain tasks, such as argumentation mining, require accounting for longer texts and complicated structural dependencies between them. Deep structured prediction is a general framework to combine the complementary strengths of expressive neural encoders and structured inference for highly structured domains. Nevertheless, when the need arises to go beyond sentences, most work relies on combining the output scores of independently trained classifiers. One of the main reasons for this is that constrained inference comes at a high computational cost. In this paper, we explore the use of randomized inference to alleviate this concern and show that we can efficiently leverage deep structured prediction and expressive neural encoders for a set of tasks involving complicated argumentative structures.","{'sequence': 'Expressive text encoders such as RNNs and Transformer Networks have been at the center of NLP models in recent work. Most of the effort has focused on sentence-level tasks, capturing the dependencies between words in a single sentence, or pairs of sentences. However, certain tasks, such as argumentation mining, require accounting for longer texts and complicated structural dependencies between them. Deep structured prediction is a general framework to combine the complementary strengths of expressive neural encoders and structured inference for highly structured domains. Nevertheless, when the need arises to go beyond sentences, most work relies on combining the output scores of independently trained classifiers. One of the main reasons for this is that constrained inference comes at a high computational cost. In this paper, we explore the use of randomized inference to alleviate this concern and show that we can efficiently leverage deep structured prediction and expressive neural encoders for a set of tasks involving complicated argumentative structures.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Question Answering', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality'], 'scores': [0.3243344724178314, 0.15456752479076385, 0.09063250571489334, 0.05258531868457794, 0.03562837839126587, 0.0282698143273592, 0.026688193902373314, 0.025453954935073853, 0.025295866653323174, 0.023080972954630852, 0.02140805684030056, 0.021361801773309708, 0.021198511123657227, 0.020117446780204773, 0.01924370415508747, 0.01860671490430832, 0.01645582914352417, 0.01368617918342352, 0.01362166553735733, 0.012667194940149784, 0.012477163225412369, 0.011716256849467754, 0.010902508161962032]}",0.3243344724178314,Machine Learning for NLP,0.3243344724178314
Machine Learning for NLP,Unsupervised Sentence-embeddings by Manifold Approximation and Projection,"The concept of unsupervised universal sentence encoders has gained traction recently, wherein pre-trained models generate effective task-agnostic fixed-dimensional representations for phrases, sentences and paragraphs. Such methods are of varying complexity, from simple weighted-averages of word vectors to complex language-models based on bidirectional transformers. In this work we propose a novel technique to generate sentenceembeddings in an unsupervised fashion by projecting the sentences onto a fixed-dimensional manifold with the objective of preserving local neighbourhoods in the original space. To delineate such neighbourhoods we experiment with several set-distance metrics, including the recently proposed Word Mover's distance, while the fixed-dimensional projection is achieved by employing a scalable and efficient manifold approximation method rooted in topological data analysis. We test our approach, which we term EMAP or Embeddings by Manifold Approximation and Projection, on six publicly available text-classification datasets of varying size and complexity. Empirical results show that our method consistently performs similar to or better than several alternative state-of-theart approaches.","{'sequence': ""The concept of unsupervised universal sentence encoders has gained traction recently, wherein pre-trained models generate effective task-agnostic fixed-dimensional representations for phrases, sentences and paragraphs. Such methods are of varying complexity, from simple weighted-averages of word vectors to complex language-models based on bidirectional transformers. In this work we propose a novel technique to generate sentenceembeddings in an unsupervised fashion by projecting the sentences onto a fixed-dimensional manifold with the objective of preserving local neighbourhoods in the original space. To delineate such neighbourhoods we experiment with several set-distance metrics, including the recently proposed Word Mover's distance, while the fixed-dimensional projection is achieved by employing a scalable and efficient manifold approximation method rooted in topological data analysis. We test our approach, which we term EMAP or Embeddings by Manifold Approximation and Projection, on six publicly available text-classification datasets of varying size and complexity. Empirical results show that our method consistently performs similar to or better than several alternative state-of-theart approaches."", 'labels': ['Generation', 'Information Extraction', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12745019793510437, 0.07016462087631226, 0.07002183794975281, 0.06993129849433899, 0.06890062987804413, 0.06588984280824661, 0.046418704092502594, 0.046282559633255005, 0.04606209322810173, 0.0440189503133297, 0.041662123054265976, 0.039593037217855453, 0.03780839592218399, 0.030855774879455566, 0.03064662776887417, 0.02919970452785492, 0.027828846126794815, 0.024281419813632965, 0.02303987741470337, 0.021937193349003792, 0.01516366470605135, 0.015087890438735485, 0.00775466114282608]}",0.12745019793510437,Generation,0.046282559633255005
Machine Learning for NLP,Annealing Knowledge Distillation,"Significant memory and computational requirements of large deep neural networks restrict their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the softtarget information (also known as ""dark knowledge"") besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by the teacher's softtargets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process, and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results.","{'sequence': 'Significant memory and computational requirements of large deep neural networks restrict their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the softtarget information (also known as ""dark knowledge"") besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by the teacher\'s softtargets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process, and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Question Answering', 'Ethics and NLP', 'Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2027662694454193, 0.11958508938550949, 0.05297563597559929, 0.05180054530501366, 0.0493302159011364, 0.04724208638072014, 0.04654857888817787, 0.04109853133559227, 0.034140121191740036, 0.03411855548620224, 0.031534042209386826, 0.029996255412697792, 0.029089361429214478, 0.028690338134765625, 0.028179652988910675, 0.02752075530588627, 0.02655414119362831, 0.02595222368836403, 0.023821713402867317, 0.02160816080868244, 0.019512929022312164, 0.01675065979361534, 0.011184017173945904]}",0.2027662694454193,NLP Applications,0.11958508938550949
Machine Learning for NLP,AdapterFusion: Non-Destructive Task Composition for Transfer Learning,"Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.","{'sequence': 'Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.', 'labels': ['Resources and Evaluation', 'Information Extraction', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Summarization', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12166807800531387, 0.08023016899824142, 0.06846171617507935, 0.06621972471475601, 0.05424323305487633, 0.05011114105582237, 0.04881596192717552, 0.0455021969974041, 0.040830545127391815, 0.04032742977142334, 0.03935258090496063, 0.03860711678862572, 0.03690135106444359, 0.03365136682987213, 0.032681453973054886, 0.03208936005830765, 0.03071191906929016, 0.026205608621239662, 0.02489776723086834, 0.024035926908254623, 0.023655787110328674, 0.022796813398599625, 0.018002670258283615]}",0.12166807800531387,Resources and Evaluation,0.0455021969974041
Machine Learning for NLP,BART-TL: Weakly-Supervised Topic Label Generation,"We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pretrained BART models on a large number of potential labels generated by state of the art nonneural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.","{'sequence': 'We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pretrained BART models on a large number of potential labels generated by state of the art nonneural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Information Extraction', 'Machine Learning for NLP', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Summarization', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1746731996536255, 0.0733182281255722, 0.06740625947713852, 0.06175342947244644, 0.0525343120098114, 0.052207738161087036, 0.04902718588709831, 0.0470745824277401, 0.04507056623697281, 0.038696449249982834, 0.038419369608163834, 0.031455621123313904, 0.030853332951664925, 0.029346443712711334, 0.028380542993545532, 0.027361363172531128, 0.02688269130885601, 0.025833383202552795, 0.024263055995106697, 0.020913410931825638, 0.019497422501444817, 0.018480725586414337, 0.016550779342651367]}",0.1746731996536255,Generation,0.0470745824277401
Machine Learning for NLP,Conceptual Grounding Constraints for Truly Robust Biomedical Name Representations,"Effective representation of biomedical names for downstream NLP tasks requires the encoding of both lexical as well as domain-specific semantic information. Ideally, the synonymy and semantic relatedness of names should be consistently reflected by their closeness in an embedding space. To achieve such robustness, prior research has considered multi-task objectives when training neural encoders. In this paper, we take a next step towards truly robust representations, which capture more domainspecific semantics while remaining universally applicable across different biomedical corpora and domains. To this end, we use conceptual grounding constraints which more effectively align encoded names to pretrained embeddings of their concept identifiers. These constraints are effective even when using a Deep Averaging Network, a simple feedforward encoding architecture that allows for scaling to large corpora while remaining sufficiently expressive. We empirically validate our approach using multiple tasks and benchmarks, which assess both literal synonymy as well as more general semantic relatedness. Our code is open-source and available at www.github. com/clips/conceptualgrounding.","{'sequence': 'Effective representation of biomedical names for downstream NLP tasks requires the encoding of both lexical as well as domain-specific semantic information. Ideally, the synonymy and semantic relatedness of names should be consistently reflected by their closeness in an embedding space. To achieve such robustness, prior research has considered multi-task objectives when training neural encoders. In this paper, we take a next step towards truly robust representations, which capture more domainspecific semantics while remaining universally applicable across different biomedical corpora and domains. To this end, we use conceptual grounding constraints which more effectively align encoded names to pretrained embeddings of their concept identifiers. These constraints are effective even when using a Deep Averaging Network, a simple feedforward encoding architecture that allows for scaling to large corpora while remaining sufficiently expressive. We empirically validate our approach using multiple tasks and benchmarks, which assess both literal synonymy as well as more general semantic relatedness. Our code is open-source and available at www.github. com/clips/conceptualgrounding.', 'labels': ['Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11153242737054825, 0.09348627924919128, 0.08624567836523056, 0.06601034849882126, 0.06220708787441254, 0.06169737130403519, 0.05414572358131409, 0.049515172839164734, 0.041530102491378784, 0.040861114859580994, 0.03683795407414436, 0.03669079765677452, 0.02968343161046505, 0.027724558487534523, 0.027450814843177795, 0.027382293716073036, 0.02568470500409603, 0.02555614709854126, 0.025127684697508812, 0.01976783201098442, 0.017922813072800636, 0.017366746440529823, 0.015572905540466309]}",0.11153242737054825,Semantics: Lexical Semantics,0.09348627924919128
Machine Learning for NLP,Adaptive Fusion Techniques for Multimodal Data,"Effective fusion of data from multiple modalities, such as video, speech, and text, is challenging due to the heterogeneous nature of multimodal data. In this paper, we propose adaptive fusion techniques that aim to model context from different modalities effectively. Instead of defining a deterministic fusion operation, such as concatenation, for the network, we let the network decide ""how"" to combine a given set of multimodal features more effectively. We propose two networks: 1) Auto-Fusion, which learns to compress information from different modalities while preserving the context, and 2) GAN-Fusion, which regularizes the learned latent space given context from complementing modalities. A quantitative evaluation on the tasks of multimodal machine translation and emotion recognition suggests that our lightweight, adaptive networks can better model context from other modalities than existing methods, many of which employ massive transformer-based networks. 1","{'sequence': 'Effective fusion of data from multiple modalities, such as video, speech, and text, is challenging due to the heterogeneous nature of multimodal data. In this paper, we propose adaptive fusion techniques that aim to model context from different modalities effectively. Instead of defining a deterministic fusion operation, such as concatenation, for the network, we let the network decide ""how"" to combine a given set of multimodal features more effectively. We propose two networks: 1) Auto-Fusion, which learns to compress information from different modalities while preserving the context, and 2) GAN-Fusion, which regularizes the learned latent space given context from complementing modalities. A quantitative evaluation on the tasks of multimodal machine translation and emotion recognition suggests that our lightweight, adaptive networks can better model context from other modalities than existing methods, many of which employ massive transformer-based networks. 1', 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Question Answering', 'Dialogue and Interactive Systems', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Extraction', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11375190317630768, 0.10352995991706848, 0.06900399178266525, 0.06791886687278748, 0.05682019144296646, 0.05632225051522255, 0.049932267516851425, 0.04456799104809761, 0.04427079111337662, 0.041169896721839905, 0.040988385677337646, 0.03518997132778168, 0.03391599282622337, 0.032176822423934937, 0.030360022559762, 0.030043425038456917, 0.02815089374780655, 0.022662602365016937, 0.02156805247068405, 0.021515607833862305, 0.021127009764313698, 0.019880902022123337, 0.015132315456867218]}",0.11375190317630768,Speech and Multimodality,0.021127009764313698
Machine Learning for NLP,Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees,"Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5.","{'sequence': 'Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Generation', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2882493734359741, 0.12921692430973053, 0.09150063991546631, 0.04423912987112999, 0.03792146220803261, 0.03574170917272568, 0.034681953489780426, 0.032165344804525375, 0.03200216963887215, 0.02769456058740616, 0.025456026196479797, 0.024677058681845665, 0.024342617020010948, 0.021163837984204292, 0.02061828039586544, 0.019675064831972122, 0.018893614411354065, 0.01859891600906849, 0.017099734395742416, 0.01671414263546467, 0.015305335633456707, 0.013474103063344955, 0.010568021796643734]}",0.2882493734359741,NLP Applications,0.12921692430973053
Machine Learning for NLP,ADePT: Auto-encoder based Differentially Private Text Transformation,"Privacy is an important concern when building statistical models on data containing personal information. Differential privacy offers a strong definition of privacy and can be used to solve several privacy concerns (Dwork et al., 2014) . Multiple solutions have been proposed for the differentially-private transformation of datasets containing sensitive information. However, such transformation algorithms offer poor utility in Natural Language Processing (NLP) tasks due to noise added in the process. In this paper, we address this issue by providing a utility-preserving differentially private text transformation algorithm using auto-encoders. Our algorithm transforms text to offer robustness against attacks and produces transformations with high semantic quality that perform well on downstream NLP tasks. We prove the theoretical privacy guarantee of our algorithm and assess its privacy leakage under Membership Inference Attacks (MIA) (Shokri et al., 2017) on models trained with transformed data. Our results show that the proposed model performs better against MIA attacks while offering lower to no degradation in the utility of the underlying transformation process compared to existing baselines.","{'sequence': 'Privacy is an important concern when building statistical models on data containing personal information. Differential privacy offers a strong definition of privacy and can be used to solve several privacy concerns (Dwork et al., 2014) . Multiple solutions have been proposed for the differentially-private transformation of datasets containing sensitive information. However, such transformation algorithms offer poor utility in Natural Language Processing (NLP) tasks due to noise added in the process. In this paper, we address this issue by providing a utility-preserving differentially private text transformation algorithm using auto-encoders. Our algorithm transforms text to offer robustness against attacks and produces transformations with high semantic quality that perform well on downstream NLP tasks. We prove the theoretical privacy guarantee of our algorithm and assess its privacy leakage under Membership Inference Attacks (MIA) (Shokri et al., 2017) on models trained with transformed data. Our results show that the proposed model performs better against MIA attacks while offering lower to no degradation in the utility of the underlying transformation process compared to existing baselines.', 'labels': ['Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'NLP Applications', 'Generation', 'Resources and Evaluation', 'Question Answering', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08755525946617126, 0.0822814553976059, 0.06905719637870789, 0.06880573183298111, 0.06721092015504837, 0.05982273817062378, 0.05490511655807495, 0.05087605118751526, 0.04812829568982124, 0.047518447041511536, 0.04648924246430397, 0.044870492070913315, 0.04290550947189331, 0.0350898802280426, 0.03139430657029152, 0.031112458556890488, 0.02566136233508587, 0.023362910374999046, 0.0193148422986269, 0.018858617171645164, 0.016761871054768562, 0.016740722581744194, 0.011276579461991787]}",0.08755525946617126,Ethics and NLP,0.06880573183298111
Machine Learning for NLP,Generative Text Modeling through Short Run Inference,"Latent variable models for text, when trained successfully, accurately model the data distribution and capture global semantic and syntactic features of sentences. The prominent approach to train such models is variational autoencoders (VAE). It is nevertheless challenging to train and often results in a trivial local optimum where the latent variable is ignored and its posterior collapses into the prior, an issue known as posterior collapse. Various techniques have been proposed to mitigate this issue. Most of them focus on improving the inference model to yield latent codes of higher quality. The present work proposes a short run dynamics for inference. It is initialized from the prior distribution of the latent variable and then runs a small number (e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The major advantage of our method is that it does not require a separate inference model or assume simple geometry of the posterior distribution, thus rendering an automatic, natural and flexible inference engine. We show that the models trained with short run dynamics more accurately model the data, compared to strong language model and VAE baselines, and exhibit no sign of posterior collapse. Analyses of the latent space show that interpolation in the latent space is able to generate coherent sentences with smooth transition and demonstrate improved classification over strong baselines with latent features from unsupervised pretraining. These results together expose a well-structured latent space of our generative model.","{'sequence': 'Latent variable models for text, when trained successfully, accurately model the data distribution and capture global semantic and syntactic features of sentences. The prominent approach to train such models is variational autoencoders (VAE). It is nevertheless challenging to train and often results in a trivial local optimum where the latent variable is ignored and its posterior collapses into the prior, an issue known as posterior collapse. Various techniques have been proposed to mitigate this issue. Most of them focus on improving the inference model to yield latent codes of higher quality. The present work proposes a short run dynamics for inference. It is initialized from the prior distribution of the latent variable and then runs a small number (e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The major advantage of our method is that it does not require a separate inference model or assume simple geometry of the posterior distribution, thus rendering an automatic, natural and flexible inference engine. We show that the models trained with short run dynamics more accurately model the data, compared to strong language model and VAE baselines, and exhibit no sign of posterior collapse. Analyses of the latent space show that interpolation in the latent space is able to generate coherent sentences with smooth transition and demonstrate improved classification over strong baselines with latent features from unsupervised pretraining. These results together expose a well-structured latent space of our generative model.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'NLP Applications', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.16406889259815216, 0.11109520494937897, 0.06119612231850624, 0.056494660675525665, 0.04728999361395836, 0.04600655660033226, 0.0458255410194397, 0.045289214700460434, 0.04440790414810181, 0.037201035767793655, 0.03695107251405716, 0.03650650009512901, 0.031400348991155624, 0.031064530834555626, 0.031045740470290184, 0.030251484364271164, 0.02619321458041668, 0.025773635134100914, 0.022347595542669296, 0.021319163963198662, 0.02100658416748047, 0.015574926510453224, 0.011689983308315277]}",0.16406889259815216,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.031400348991155624
Machine Learning for NLP,Through the Looking Glass: Learning to Attribute Synthetic Text Generated by Language Models,"Given the potential misuse of recent advances in synthetic text generation by language models (LMs), it is important to have the capacity to attribute authorship of synthetic text. While stylometric organic (i.e., human written) authorship attribution has been quite successful, it is unclear whether similar approaches can be used to attribute a synthetic text to its source LM. We address this question with the key insight that synthetic texts carry subtle distinguishing marks inherited from their source LM and that these marks can be leveraged by machine learning (ML) algorithms for attribution. We propose and test several ML-based attribution methods. Our best attributor built using a fine-tuned version of XLNet (XLNet-FT) consistently achieves excellent accuracy scores (91% to near perfect 98%) in terms of attributing the parent pre-trained LM behind a synthetic text. Our experiments show promising results across a range of experiments where the synthetic text may be generated using pretrained LMs, fine-tuned LMs, or by varying text generation parameters.","{'sequence': 'Given the potential misuse of recent advances in synthetic text generation by language models (LMs), it is important to have the capacity to attribute authorship of synthetic text. While stylometric organic (i.e., human written) authorship attribution has been quite successful, it is unclear whether similar approaches can be used to attribute a synthetic text to its source LM. We address this question with the key insight that synthetic texts carry subtle distinguishing marks inherited from their source LM and that these marks can be leveraged by machine learning (ML) algorithms for attribution. We propose and test several ML-based attribution methods. Our best attributor built using a fine-tuned version of XLNet (XLNet-FT) consistently achieves excellent accuracy scores (91% to near perfect 98%) in terms of attributing the parent pre-trained LM behind a synthetic text. Our experiments show promising results across a range of experiments where the synthetic text may be generated using pretrained LMs, fine-tuned LMs, or by varying text generation parameters.', 'labels': ['Generation', 'Question Answering', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Information Extraction', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10264123976230621, 0.08079613000154495, 0.06848850846290588, 0.054943330585956573, 0.049553729593753815, 0.0493086539208889, 0.045736417174339294, 0.045635949820280075, 0.04476694390177727, 0.04376891627907753, 0.042228758335113525, 0.0415947325527668, 0.03995715454220772, 0.038148414343595505, 0.03751874342560768, 0.036415837705135345, 0.03268984705209732, 0.031139487400650978, 0.030875597149133682, 0.025859490036964417, 0.024440517649054527, 0.017520427703857422, 0.01597118191421032]}",0.10264123976230621,Generation,0.03995715454220772
Machine Learning for NLP,Keep Learning: Self-supervised Meta-learning for Learning from Inference,"A common approach in many machine learning algorithms involves self-supervised learning on large unlabeled data before fine-tuning on downstream tasks to further improve performance. A new approach for language modelling, called dynamic evaluation, further fine-tunes a trained model during inference using trivially-present ground-truth labels, giving a large improvement in performance. However, this approach does not easily extend to classification tasks, where ground-truth labels are absent during inference. We propose to solve this issue by utilizing self-training and back-propagating the loss from the model's own class-balanced predictions (pseudo-labels), adapting the Reptile algorithm from meta-learning, combined with an inductive bias towards pre-trained weights to improve generalization. Our method improves the performance of standard backbones such as BERT, Electra, and ResNet-50 on a wide variety of tasks, such as question answering on SQuAD and NewsQA, benchmark task SuperGLUE, conversation response selection on Ubuntu Dialog corpus v2.0, as well as image classification on MNIST and ImageNet without any changes to the underlying models. Our proposed method outperforms previous approaches, enables self-supervised finetuning during inference of any classifier model to better adapt to target domains, can be easily adapted to any model, and is also effective in online and transfer-learning settings.","{'sequence': ""A common approach in many machine learning algorithms involves self-supervised learning on large unlabeled data before fine-tuning on downstream tasks to further improve performance. A new approach for language modelling, called dynamic evaluation, further fine-tunes a trained model during inference using trivially-present ground-truth labels, giving a large improvement in performance. However, this approach does not easily extend to classification tasks, where ground-truth labels are absent during inference. We propose to solve this issue by utilizing self-training and back-propagating the loss from the model's own class-balanced predictions (pseudo-labels), adapting the Reptile algorithm from meta-learning, combined with an inductive bias towards pre-trained weights to improve generalization. Our method improves the performance of standard backbones such as BERT, Electra, and ResNet-50 on a wide variety of tasks, such as question answering on SQuAD and NewsQA, benchmark task SuperGLUE, conversation response selection on Ubuntu Dialog corpus v2.0, as well as image classification on MNIST and ImageNet without any changes to the underlying models. Our proposed method outperforms previous approaches, enables self-supervised finetuning during inference of any classifier model to better adapt to target domains, can be easily adapted to any model, and is also effective in online and transfer-learning settings."", 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Speech and Multimodality', 'Information Extraction', 'Generation', 'NLP Applications', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.48001641035079956, 0.06118322163820267, 0.059229809790849686, 0.03561674803495407, 0.033665601164102554, 0.028707163408398628, 0.02602691762149334, 0.025869838893413544, 0.025840720161795616, 0.02544201910495758, 0.02396317571401596, 0.02166138030588627, 0.020716166123747826, 0.020601598545908928, 0.01928630843758583, 0.017078710719943047, 0.016536526381969452, 0.015810612589120865, 0.010330814868211746, 0.008953877724707127, 0.00855029746890068, 0.00838359072804451, 0.006528364960104227]}",0.48001641035079956,Question Answering,0.03561674803495407
Machine Learning for NLP,Extremely Small BERT Models from Mixed-Vocabulary Training,"Pretrained language models like BERT have achieved good results on NLP tasks, but are impractical on resource-limited devices due to memory footprint. A large fraction of this footprint comes from the input embeddings with large input vocabulary and embedding dimensions. Existing knowledge distillation methods used for model compression cannot be directly applied to train student models with reduced vocabulary sizes. To this end, we propose a distillation method to align the teacher and student embeddings via mixed-vocabulary training. Our method compresses BERT LARGE to a task-agnostic model with smaller vocabulary and hidden dimensions, which is an order of magnitude smaller than other distilled BERT models and offers a better size-accuracy trade-off on language understanding benchmarks as well as a practical dialogue task.","{'sequence': 'Pretrained language models like BERT have achieved good results on NLP tasks, but are impractical on resource-limited devices due to memory footprint. A large fraction of this footprint comes from the input embeddings with large input vocabulary and embedding dimensions. Existing knowledge distillation methods used for model compression cannot be directly applied to train student models with reduced vocabulary sizes. To this end, we propose a distillation method to align the teacher and student embeddings via mixed-vocabulary training. Our method compresses BERT LARGE to a task-agnostic model with smaller vocabulary and hidden dimensions, which is an order of magnitude smaller than other distilled BERT models and offers a better size-accuracy trade-off on language understanding benchmarks as well as a practical dialogue task.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Ethics and NLP', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Generation', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Question Answering', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.18973442912101746, 0.12254179269075394, 0.0907476395368576, 0.0556376613676548, 0.05470477044582367, 0.05173470452427864, 0.049140702933073044, 0.03481875732541084, 0.03470635786652565, 0.0321195051074028, 0.03189486637711525, 0.029106099158525467, 0.025390304625034332, 0.02441936917603016, 0.024268517270684242, 0.023797394707798958, 0.02366444654762745, 0.020971857011318207, 0.020876089110970497, 0.01755007728934288, 0.014451255090534687, 0.014078347943723202, 0.01364495512098074]}",0.18973442912101746,Machine Learning for NLP,0.18973442912101746
Machine Learning for NLP,Joint Energy-based Model Training for Better Calibrated Natural Language Understanding Models,"In this work, we explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy. We discuss three variants of energy functions (namely scalar, hidden, and sharp-hidden) that can be defined on top of a text encoder, and compare them in experiments. Due to the discreteness of text data, we adopt noise contrastive estimation (NCE) to train the energy-based model. To make NCE training more effective, we train an autoregressive noise model with the masked language model (MLM) objective.","{'sequence': 'In this work, we explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy. We discuss three variants of energy functions (namely scalar, hidden, and sharp-hidden) that can be defined on top of a text encoder, and compare them in experiments. Due to the discreteness of text data, we adopt noise contrastive estimation (NCE) to train the energy-based model. To make NCE training more effective, we train an autoregressive noise model with the masked language model (MLM) objective.', 'labels': ['NLP Applications', 'Resources and Evaluation', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Speech and Multimodality', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.1225430890917778, 0.12225314974784851, 0.08562634140253067, 0.07405386865139008, 0.05664951354265213, 0.053802017122507095, 0.04698340222239494, 0.04664825648069382, 0.04058077558875084, 0.04038435220718384, 0.03538167104125023, 0.03524860739707947, 0.030599558725953102, 0.028934454545378685, 0.02861996367573738, 0.02602025680243969, 0.020590176805853844, 0.02019520476460457, 0.019314264878630638, 0.016909191384911537, 0.01680409349501133, 0.016122013330459595, 0.015735724940896034]}",0.1225430890917778,NLP Applications,0.08562634140253067
Machine Learning for NLP,A Neural Few-Shot Text Classification Reality Check,"Modern classification models tend to struggle when the amount of annotated data is scarce. To overcome this issue, several neural fewshot classification models have emerged, yielding significant progress over time, both in Computer Vision and Natural Language Processing. In the latter, such models used to rely on fixed word embeddings before the advent of transformers. Additionally, some models used in Computer Vision are yet to be tested in NLP applications. In this paper, we compare all these models, first adapting those made in the field of image processing to NLP, and second providing them access to transformers. We then test these models equipped with the same transformer-based encoder on the intent detection task, known for having a large number of classes. Our results reveal that while methods perform almost equally on the ARSC dataset, this is not the case for the Intent Detection task, where the most recent and supposedly best competitors perform worse than older and simpler ones (while all are given access to transformers). We also show that a simple baseline is surprisingly strong. All the new developed models, as well as the evaluation framework, are made publicly available 1 .","{'sequence': 'Modern classification models tend to struggle when the amount of annotated data is scarce. To overcome this issue, several neural fewshot classification models have emerged, yielding significant progress over time, both in Computer Vision and Natural Language Processing. In the latter, such models used to rely on fixed word embeddings before the advent of transformers. Additionally, some models used in Computer Vision are yet to be tested in NLP applications. In this paper, we compare all these models, first adapting those made in the field of image processing to NLP, and second providing them access to transformers. We then test these models equipped with the same transformer-based encoder on the intent detection task, known for having a large number of classes. Our results reveal that while methods perform almost equally on the ARSC dataset, this is not the case for the Intent Detection task, where the most recent and supposedly best competitors perform worse than older and simpler ones (while all are given access to transformers). We also show that a simple baseline is surprisingly strong. All the new developed models, as well as the evaluation framework, are made publicly available 1 .', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Ethics and NLP', 'Question Answering', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24953730404376984, 0.1236196979880333, 0.1007048636674881, 0.07174936681985855, 0.04135291650891304, 0.039285559207201004, 0.03567730635404587, 0.03549160435795784, 0.03142952919006348, 0.0303780660033226, 0.030074574053287506, 0.02355128526687622, 0.023414187133312225, 0.022269990295171738, 0.02148316241800785, 0.021236097440123558, 0.020208999514579773, 0.017360663041472435, 0.016342995688319206, 0.013155381195247173, 0.011690259911119938, 0.011633134447038174, 0.008353112265467644]}",0.24953730404376984,NLP Applications,0.1236196979880333
Machine Learning for NLP,"Joint Learning of Representations for Web-tables, Entities and Types using Graph Convolutional Network","Existing approaches for table annotation with entities and types either capture the structure of table using graphical models, or learn embeddings of table entries without accounting for the complete syntactic structure. We propose TabGCN, which uses Graph Convolutional Networks to capture the complete structure of tables, knowledge graph and the training annotations, and jointly learns embeddings for table elements as well as the entities and types. To account for knowledge incompleteness, TabGCN's embeddings can be used to discover new entities and types. Using experiments on 5 benchmark datasets, we show that TabGCN significantly outperforms multiple state-of-the-art baselines for table annotation, while showing promising performance on downstream table-related applications.","{'sequence': ""Existing approaches for table annotation with entities and types either capture the structure of table using graphical models, or learn embeddings of table entries without accounting for the complete syntactic structure. We propose TabGCN, which uses Graph Convolutional Networks to capture the complete structure of tables, knowledge graph and the training annotations, and jointly learns embeddings for table elements as well as the entities and types. To account for knowledge incompleteness, TabGCN's embeddings can be used to discover new entities and types. Using experiments on 5 benchmark datasets, we show that TabGCN significantly outperforms multiple state-of-the-art baselines for table annotation, while showing promising performance on downstream table-related applications."", 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Summarization', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12193775177001953, 0.09125076979398727, 0.07378735393285751, 0.06995169073343277, 0.06654903292655945, 0.059748098254203796, 0.05951911583542824, 0.05515878647565842, 0.04995470866560936, 0.03931689262390137, 0.036987047642469406, 0.033318065106868744, 0.03293341025710106, 0.030725568532943726, 0.0283354502171278, 0.027068164199590683, 0.024365253746509552, 0.02346896007657051, 0.02174767665565014, 0.021447746083140373, 0.013385383412241936, 0.011096485890448093, 0.007946567609906197]}",0.12193775177001953,Dialogue and Interactive Systems,0.0283354502171278
Machine Translation and Multilinguality,Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders,"State-of-the-art multilingual machine translation relies on a universal encoder-decoder, which requires retraining the entire system to add new languages. In this paper, we propose an alternative approach that is based on language-specific encoder-decoders, and can thus be more easily extended to new languages by learning their corresponding modules. So as to encourage a common interlingua representation, we simultaneously train the N initial languages. Our experiments show that the proposed approach outperforms the universal encoder-decoder by 3.28 BLEU points on average, while allowing to add new languages without the need to retrain the rest of the modules. All in all, our work closes the gap between shared and language-specific encoderdecoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.","{'sequence': 'State-of-the-art multilingual machine translation relies on a universal encoder-decoder, which requires retraining the entire system to add new languages. In this paper, we propose an alternative approach that is based on language-specific encoder-decoders, and can thus be more easily extended to new languages by learning their corresponding modules. So as to encourage a common interlingua representation, we simultaneously train the N initial languages. Our experiments show that the proposed approach outperforms the universal encoder-decoder by 3.28 BLEU points on average, while allowing to add new languages without the need to retrain the rest of the modules. All in all, our work closes the gap between shared and language-specific encoderdecoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'NLP Applications', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.16634896397590637, 0.09604976326227188, 0.0788397341966629, 0.07611708343029022, 0.05413998290896416, 0.04155752435326576, 0.03950744494795799, 0.037883855402469635, 0.03735097125172615, 0.03575282171368599, 0.03345466032624245, 0.03344283625483513, 0.030014529824256897, 0.02953905612230301, 0.028613971546292305, 0.027816319838166237, 0.02765202522277832, 0.02675667218863964, 0.026647791266441345, 0.021876607090234756, 0.020373152568936348, 0.019656382501125336, 0.010607856325805187]}",0.16634896397590637,Machine Translation and Multilinguality,0.16634896397590637
Machine Translation and Multilinguality,Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders,"Recent work in multilingual translation advances translation quality surpassing bilingual baselines using deep transformer models with increased capacity. However, the extra latency and memory costs introduced by this approach may make it unacceptable for efficiency-constrained applications. It has recently been shown for bilingual translation that using a deep encoder and shallow decoder (DESD) can reduce inference latency while maintaining translation quality, so we study similar speed-accuracy trade-offs for multilingual translation. We find that for many-toone translation we can indeed increase decoder speed without sacrificing quality using this approach, but for one-to-many translation, shallow decoders cause a clear quality drop. To ameliorate this drop, we propose a deep encoder with multiple shallow decoders (DEMSD) where each shallow decoder is responsible for a disjoint subset of target languages. Specifically, the DEMSD model with 2-layer decoders is able to obtain a 1.8x speedup on average compared to a standard transformer model with no drop in translation quality.","{'sequence': 'Recent work in multilingual translation advances translation quality surpassing bilingual baselines using deep transformer models with increased capacity. However, the extra latency and memory costs introduced by this approach may make it unacceptable for efficiency-constrained applications. It has recently been shown for bilingual translation that using a deep encoder and shallow decoder (DESD) can reduce inference latency while maintaining translation quality, so we study similar speed-accuracy trade-offs for multilingual translation. We find that for many-toone translation we can indeed increase decoder speed without sacrificing quality using this approach, but for one-to-many translation, shallow decoders cause a clear quality drop. To ameliorate this drop, we propose a deep encoder with multiple shallow decoders (DEMSD) where each shallow decoder is responsible for a disjoint subset of target languages. Specifically, the DEMSD model with 2-layer decoders is able to obtain a 1.8x speedup on average compared to a standard transformer model with no drop in translation quality.', 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Resources and Evaluation', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09238880127668381, 0.0910654067993164, 0.08490941673517227, 0.07793807983398438, 0.06445968151092529, 0.061186783015728, 0.06004997342824936, 0.05889293923974037, 0.05190812796354294, 0.03826817125082016, 0.03767735883593559, 0.03749610483646393, 0.02633299119770527, 0.02626747265458107, 0.02552536502480507, 0.025469036772847176, 0.025341467931866646, 0.023288190364837646, 0.023121409118175507, 0.018836908042430878, 0.016788391396403313, 0.016440298408269882, 0.016347579658031464]}",0.09238880127668381,Machine Translation and Multilinguality,0.09238880127668381
Machine Translation and Multilinguality,Learning Coupled Policies for Simultaneous Machine Translation using Imitation Learning,"We present a novel approach to efficiently learn a simultaneous translation model with coupled programmer-interpreter policies. First, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality while keeping the translation delay low.","{'sequence': 'We present a novel approach to efficiently learn a simultaneous translation model with coupled programmer-interpreter policies. First, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality while keeping the translation delay low.', 'labels': ['Information Extraction', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Question Answering', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10186409205198288, 0.09119702875614166, 0.08886396139860153, 0.07725681364536285, 0.06561639159917831, 0.0636843591928482, 0.06287864595651627, 0.03887754678726196, 0.038198165595531464, 0.03365985304117203, 0.032945871353149414, 0.03279828652739525, 0.03191332891583443, 0.03130410984158516, 0.03018377721309662, 0.02962052822113037, 0.02837563492357731, 0.026942959055304527, 0.02149471826851368, 0.018686853349208832, 0.018652087077498436, 0.01772698014974594, 0.017258014529943466]}",0.10186409205198288,Information Extraction,0.08886396139860153
Machine Translation and Multilinguality,Context-aware Neural Machine Translation with Mini-batch Embedding,"It is crucial to provide an inter-sentence context in Neural Machine Translation (NMT) models for higher-quality translation. With the aim of using a simple approach to incorporate inter-sentence information, we propose minibatch embedding (MBE) as a way to represent the features of sentences in a mini-batch. We construct a mini-batch by choosing sentences from the same document, and thus the MBE is expected to have contextual information across sentences. Here, we incorporate MBE in an NMT model, and our experiments show that the proposed method consistently outperforms the translation capabilities of strong baselines and improves writing style or terminology to fit the document's context. 1","{'sequence': ""It is crucial to provide an inter-sentence context in Neural Machine Translation (NMT) models for higher-quality translation. With the aim of using a simple approach to incorporate inter-sentence information, we propose minibatch embedding (MBE) as a way to represent the features of sentences in a mini-batch. We construct a mini-batch by choosing sentences from the same document, and thus the MBE is expected to have contextual information across sentences. Here, we incorporate MBE in an NMT model, and our experiments show that the proposed method consistently outperforms the translation capabilities of strong baselines and improves writing style or terminology to fit the document's context. 1"", 'labels': ['Machine Translation and Multilinguality', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Question Answering', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Resources and Evaluation', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15324710309505463, 0.1013139858841896, 0.06183375045657158, 0.060386475175619125, 0.04746442288160324, 0.04509632661938667, 0.04442651569843292, 0.03927164152264595, 0.0383702851831913, 0.03736013546586037, 0.037225544452667236, 0.03563237935304642, 0.03520453721284866, 0.034232430160045624, 0.03234660252928734, 0.03180256858468056, 0.029822055250406265, 0.028459755703806877, 0.025706155225634575, 0.022497214376926422, 0.02198035642504692, 0.019525909796357155, 0.01679386757314205]}",0.15324710309505463,Machine Translation and Multilinguality,0.15324710309505463
Machine Translation and Multilinguality,Quality Estimation without Human-labeled Data,"Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction.","{'sequence': 'Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction.', 'labels': ['Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Information Extraction', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Summarization', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09567806124687195, 0.09426119923591614, 0.06901362538337708, 0.06377576291561127, 0.060896266251802444, 0.051614634692668915, 0.05121827870607376, 0.046026431024074554, 0.04421420395374298, 0.040498036891222, 0.03957391530275345, 0.039111942052841187, 0.03819415345788002, 0.03650614246726036, 0.033275578171014786, 0.03288157284259796, 0.03095628321170807, 0.028500061482191086, 0.02426670677959919, 0.023457426577806473, 0.021634461358189583, 0.01881360076367855, 0.01563161425292492]}",0.09567806124687195,Dialogue and Interactive Systems,0.09426119923591614
Machine Translation and Multilinguality,Few-shot learning through contextual data augmentation,"Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pre-trained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.","{'sequence': 'Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pre-trained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.', 'labels': ['Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Computational Social Science and Social Media', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Generation', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP'], 'scores': [0.12255943566560745, 0.1149713322520256, 0.08401364088058472, 0.07244505733251572, 0.05805493891239166, 0.051272112876176834, 0.050028733909130096, 0.04181196168065071, 0.04131316766142845, 0.036212094128131866, 0.03389492258429527, 0.033131808042526245, 0.03206298500299454, 0.02776612900197506, 0.026531998068094254, 0.02511259913444519, 0.02383105643093586, 0.023288629949092865, 0.022006744518876076, 0.021696912124753, 0.021692505106329918, 0.020716100931167603, 0.015585110522806644]}",0.12255943566560745,Machine Translation and Multilinguality,0.12255943566560745
Machine Translation and Multilinguality,Facilitating Terminology Translation with Target Lemma Annotations,"Most of the recent work on terminology integration in machine translation has assumed that terminology translations are given already inflected in forms that are suitable for the target language sentence. In day-to-day work of professional translators, however, it is seldom the case as translators work with bilingual glossaries where terms are given in their dictionary forms; finding the right target language form is part of the translation process. We argue that the requirement for apriori specified target language forms is unrealistic and impedes the practical applicability of previous work. In this work, we propose to train machine translation systems using a source-side data augmentation method 1 that annotates randomly selected source language words with their target language lemmas. We show that systems trained on such augmented data are readily usable for terminology integration in real-life translation scenarios. Our experiments on terminology translation into the morphologically complex Baltic and Uralic languages show an improvement of up to 7 BLEU points over baseline systems with no means for terminology integration and an average improvement of 4 BLEU points over the previous work. Results of the human evaluation indicate a 47.7% absolute improvement over the previous work in term translation accuracy when translating into Latvian.","{'sequence': 'Most of the recent work on terminology integration in machine translation has assumed that terminology translations are given already inflected in forms that are suitable for the target language sentence. In day-to-day work of professional translators, however, it is seldom the case as translators work with bilingual glossaries where terms are given in their dictionary forms; finding the right target language form is part of the translation process. We argue that the requirement for apriori specified target language forms is unrealistic and impedes the practical applicability of previous work. In this work, we propose to train machine translation systems using a source-side data augmentation method 1 that annotates randomly selected source language words with their target language lemmas. We show that systems trained on such augmented data are readily usable for terminology integration in real-life translation scenarios. Our experiments on terminology translation into the morphologically complex Baltic and Uralic languages show an improvement of up to 7 BLEU points over baseline systems with no means for terminology integration and an average improvement of 4 BLEU points over the previous work. Results of the human evaluation indicate a 47.7% absolute improvement over the previous work in term translation accuracy when translating into Latvian.', 'labels': ['Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Question Answering', 'Information Retrieval and Text Mining', 'Generation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Ethics and NLP'], 'scores': [0.09994418919086456, 0.06946641951799393, 0.06883154064416885, 0.06725382059812546, 0.06315456330776215, 0.05557331442832947, 0.05447574704885483, 0.04517453536391258, 0.044395510107278824, 0.044029928743839264, 0.042933717370033264, 0.03963932394981384, 0.0386107861995697, 0.03419432044029236, 0.03397084400057793, 0.030900321900844574, 0.02941863238811493, 0.02448551543056965, 0.023441879078745842, 0.02323225513100624, 0.023065561428666115, 0.022353442385792732, 0.021453844383358955]}",0.09994418919086456,Machine Translation and Multilinguality,0.09994418919086456
Machine Translation and Multilinguality,Streaming Models for Joint Speech Recognition and Translation,"Using end-to-end models for speech translation (ST) has increasingly been the focus of the ST community. These models condense the previously cascaded systems by directly converting sound waves into translated text. However, cascaded models have the advantage of including automatic speech recognition output, useful for a variety of practical ST systems that often display transcripts to the user alongside the translations. To bridge this gap, recent work has shown initial progress into the feasibility for end-to-end models to produce both of these outputs. However, all previous work has only looked at this problem from the consecutive perspective, leaving uncertainty on whether these approaches are effective in the more challenging streaming setting. We develop an end-to-end streaming ST model based on a re-translation approach and compare against standard cascading approaches. We also introduce a novel inference method for the joint case, interleaving both transcript and translation in generation and removing the need to use separate decoders. Our evaluation across a range of metrics capturing accuracy, latency, and consistency shows that our end-to-end models are statistically similar to cascading models, while having half the number of parameters. We also find that both systems provide strong translation quality at low latency, keeping 99% of consecutive quality at a lag of just under a second.","{'sequence': 'Using end-to-end models for speech translation (ST) has increasingly been the focus of the ST community. These models condense the previously cascaded systems by directly converting sound waves into translated text. However, cascaded models have the advantage of including automatic speech recognition output, useful for a variety of practical ST systems that often display transcripts to the user alongside the translations. To bridge this gap, recent work has shown initial progress into the feasibility for end-to-end models to produce both of these outputs. However, all previous work has only looked at this problem from the consecutive perspective, leaving uncertainty on whether these approaches are effective in the more challenging streaming setting. We develop an end-to-end streaming ST model based on a re-translation approach and compare against standard cascading approaches. We also introduce a novel inference method for the joint case, interleaving both transcript and translation in generation and removing the need to use separate decoders. Our evaluation across a range of metrics capturing accuracy, latency, and consistency shows that our end-to-end models are statistically similar to cascading models, while having half the number of parameters. We also find that both systems provide strong translation quality at low latency, keeping 99% of consecutive quality at a lag of just under a second.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Information Extraction', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'NLP Applications', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09170059114694595, 0.08284139633178711, 0.08165258169174194, 0.0647338479757309, 0.06140630692243576, 0.05943865329027176, 0.05339188128709793, 0.0470045730471611, 0.046511210501194, 0.04009053483605385, 0.038639526814222336, 0.03599625453352928, 0.03315761312842369, 0.03255375102162361, 0.032464999705553055, 0.03037410043179989, 0.029380634427070618, 0.028046321123838425, 0.026129793375730515, 0.023716341704130173, 0.02192511409521103, 0.021656356751918793, 0.01718761958181858]}",0.09170059114694595,Resources and Evaluation,0.06140630692243576
Machine Translation and Multilinguality,The Source-Target Domain Mismatch Problem in Machine Translation,"While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how sourcetarget domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with selftraining and by increasing the amount of target side monolingual data.","{'sequence': 'While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how sourcetarget domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with selftraining and by increasing the amount of target side monolingual data.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Question Answering', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1117897629737854, 0.08023087680339813, 0.07105616480112076, 0.06900408118963242, 0.05498640239238739, 0.05496467649936676, 0.04939129203557968, 0.04484829679131508, 0.03909324109554291, 0.03866513445973396, 0.03695617988705635, 0.036508943885564804, 0.03630657121539116, 0.03238029405474663, 0.03161810338497162, 0.03078935667872429, 0.030158717185258865, 0.02962261624634266, 0.02928619459271431, 0.025856714695692062, 0.025594575330615044, 0.023413224145770073, 0.017478670924901962]}",0.1117897629737854,Machine Translation and Multilinguality,0.1117897629737854
Machine Translation and Multilinguality,Better Neural Machine Translation by Extracting Linguistic Information from BERT,"Adding linguistic information (syntax or semantics) to neural machine translation (NMT) has mostly focused on using point estimates from pre-trained models. Directly using the capacity of massive pre-trained contextual word embedding models such as BERT (Devlin  et al., 2019)  has been marginally useful in NMT because effective fine-tuning is difficult to obtain for NMT without making training brittle and unreliable. We augment NMT by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformerbased NMT.","{'sequence': 'Adding linguistic information (syntax or semantics) to neural machine translation (NMT) has mostly focused on using point estimates from pre-trained models. Directly using the capacity of massive pre-trained contextual word embedding models such as BERT (Devlin  et al., 2019)  has been marginally useful in NMT because effective fine-tuning is difficult to obtain for NMT without making training brittle and unreliable. We augment NMT by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformerbased NMT.', 'labels': ['Information Extraction', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Question Answering', 'Information Retrieval and Text Mining', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13658849895000458, 0.10674150288105011, 0.0639483705163002, 0.06086220219731331, 0.053344596177339554, 0.04849711060523987, 0.04783705621957779, 0.045186422765254974, 0.043788567185401917, 0.03722264990210533, 0.03544926270842552, 0.033968616276979446, 0.03333001956343651, 0.03327588737010956, 0.03095046989619732, 0.030551591888070107, 0.03030976839363575, 0.027435773983597755, 0.026131801307201385, 0.023972924798727036, 0.02055814303457737, 0.017699100077152252, 0.01234969962388277]}",0.13658849895000458,Information Extraction,0.10674150288105011
Machine Translation and Multilinguality,Exploring Supervised and Unsupervised Rewards in Machine Translation,"Reinforcement Learning (RL) is a powerful framework to address the discrepancy between loss functions used during training and the final evaluation metrics to be used at test time. When applied to neural Machine Translation (MT), it minimises the mismatch between the cross-entropy loss and non-differentiable evaluation metrics like BLEU. However, the suitability of these metrics as reward function at training time is questionable: they tend to be sparse and biased towards the specific words used in the reference texts. We propose to address this problem by making models less reliant on such metrics in two ways: (a) with an entropy-regularised RL method that does not only maximise a reward function but also explore the action space to avoid peaky distributions; (b) with a novel RL method that explores a dynamic unsupervised reward function to balance between exploration and exploitation. We base our proposals on the Soft Actor-Critic (SAC) framework, adapting the off-policy maximum entropy model for language generation applications such as MT. We demonstrate that SAC with BLEU reward tends to overfit less to the training data and performs better on out-of-domain data. We also show that our dynamic unsupervised reward can lead to better translation of ambiguous words.","{'sequence': 'Reinforcement Learning (RL) is a powerful framework to address the discrepancy between loss functions used during training and the final evaluation metrics to be used at test time. When applied to neural Machine Translation (MT), it minimises the mismatch between the cross-entropy loss and non-differentiable evaluation metrics like BLEU. However, the suitability of these metrics as reward function at training time is questionable: they tend to be sparse and biased towards the specific words used in the reference texts. We propose to address this problem by making models less reliant on such metrics in two ways: (a) with an entropy-regularised RL method that does not only maximise a reward function but also explore the action space to avoid peaky distributions; (b) with a novel RL method that explores a dynamic unsupervised reward function to balance between exploration and exploitation. We base our proposals on the Soft Actor-Critic (SAC) framework, adapting the off-policy maximum entropy model for language generation applications such as MT. We demonstrate that SAC with BLEU reward tends to overfit less to the training data and performs better on out-of-domain data. We also show that our dynamic unsupervised reward can lead to better translation of ambiguous words.', 'labels': ['Generation', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'NLP Applications', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.19816280901432037, 0.14391100406646729, 0.06118457391858101, 0.060811642557382584, 0.04511794447898865, 0.04407745227217674, 0.03747812658548355, 0.03685261309146881, 0.03215625882148743, 0.03195059671998024, 0.03169148042798042, 0.03135626018047333, 0.027647770941257477, 0.02573041431605816, 0.024757688865065575, 0.024297870695590973, 0.02354411967098713, 0.022218002006411552, 0.022132929414510727, 0.021853573620319366, 0.021106498315930367, 0.01757177524268627, 0.014388634823262691]}",0.19816280901432037,Generation,0.14391100406646729
Machine Translation and Multilinguality,Word Alignment by Fine-tuning Embeddings on Parallel Corpora,"Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but finetuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs. Our aligner, AWE-SOME (Aligning Word Embedding Spaces Of Multilingual Encoders), with pre-trained models is available at https://github. com/neulab/awesome-align.","{'sequence': 'Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but finetuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs. Our aligner, AWE-SOME (Aligning Word Embedding Spaces Of Multilingual Encoders), with pre-trained models is available at https://github. com/neulab/awesome-align.', 'labels': ['Question Answering', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'NLP Applications', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Generation', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07602113485336304, 0.07586239278316498, 0.0711655542254448, 0.06694883853197098, 0.06063166260719299, 0.06047791987657547, 0.058695677667856216, 0.0540362149477005, 0.044072508811950684, 0.042633429169654846, 0.04122866317629814, 0.039965372532606125, 0.03897605463862419, 0.03741462528705597, 0.034825265407562256, 0.03448520600795746, 0.03405908867716789, 0.03339449316263199, 0.02656223624944687, 0.02483249269425869, 0.01606297865509987, 0.01567903906106949, 0.011969109997153282]}",0.07602113485336304,Question Answering,0.07586239278316498
Machine Translation and Multilinguality,Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation,"There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task. This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model trained only on English monolingual data, BART. 2) Fine-tuning a model trained on monolingual data from 25 languages, mBART. For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings. For mBART we match or outperform the performance of naive fine-tuning for most language pairs with the encoder, and most of the decoder, frozen. The encoder-decoder attention parameters are most important to finetune. When constraining ourselves to an outof-domain training set for Vietnamese to English we see the largest improvements over the fine-tuning baseline.","{'sequence': 'There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task. This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model trained only on English monolingual data, BART. 2) Fine-tuning a model trained on monolingual data from 25 languages, mBART. For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings. For mBART we match or outperform the performance of naive fine-tuning for most language pairs with the encoder, and most of the decoder, frozen. The encoder-decoder attention parameters are most important to finetune. When constraining ourselves to an outof-domain training set for Vietnamese to English we see the largest improvements over the fine-tuning baseline.', 'labels': ['Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Question Answering', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.28698018193244934, 0.0742965117096901, 0.0523131862282753, 0.049741048365831375, 0.04151107370853424, 0.039104923605918884, 0.03814231976866722, 0.038022480905056, 0.03395877406001091, 0.03223752602934837, 0.032060686498880386, 0.0313386432826519, 0.030496764928102493, 0.029969431459903717, 0.02819378674030304, 0.025388412177562714, 0.024387897923588753, 0.022570593282580376, 0.022346172481775284, 0.019644413143396378, 0.01955336146056652, 0.01805834285914898, 0.009683458134531975]}",0.28698018193244934,Machine Translation and Multilinguality,0.28698018193244934
Machine Translation and Multilinguality,Multi-split Reversible Transformers Can Enhance Neural Machine Translation,"Large-scale transformers have been shown the state-of-the-art on neural machine translation. However, training these increasingly wider and deeper models could be tremendously memory intensive. We reduce the memory burden by employing the idea of reversible networks that a layer's input can be reconstructed from its output. We design three types of multi-split based reversible transformers. We also devise a corresponding backpropagation algorithm, which does not need to store activations for most layers. Furthermore, we present two fine-tuning techniques: splits shuffle and self ensemble, to boost translation accuracy. Specifically, our best models surpass the vanilla transformer by at least 1.4 BLEU points in three datasets. Our largescale reversible models achieve 30.0 BLEU in WMT'14 En-De and 43.5 BLEU in WMT'14 En-Fr, beating several very strong baselines with less than half of the training memory.","{'sequence': ""Large-scale transformers have been shown the state-of-the-art on neural machine translation. However, training these increasingly wider and deeper models could be tremendously memory intensive. We reduce the memory burden by employing the idea of reversible networks that a layer's input can be reconstructed from its output. We design three types of multi-split based reversible transformers. We also devise a corresponding backpropagation algorithm, which does not need to store activations for most layers. Furthermore, we present two fine-tuning techniques: splits shuffle and self ensemble, to boost translation accuracy. Specifically, our best models surpass the vanilla transformer by at least 1.4 BLEU points in three datasets. Our largescale reversible models achieve 30.0 BLEU in WMT'14 En-De and 43.5 BLEU in WMT'14 En-Fr, beating several very strong baselines with less than half of the training memory."", 'labels': ['Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Discourse and Pragmatics', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1109025627374649, 0.0699225664138794, 0.0673612654209137, 0.06314404308795929, 0.05607840046286583, 0.05108937248587608, 0.04882131144404411, 0.048535775393247604, 0.04741135612130165, 0.04722242057323456, 0.04410898685455322, 0.038406021893024445, 0.03596321493387222, 0.035502564162015915, 0.034551169723272324, 0.0326516330242157, 0.03240443766117096, 0.029365407302975655, 0.029316535219550133, 0.02473144233226776, 0.02468821220099926, 0.014011197723448277, 0.013810102827847004]}",0.1109025627374649,Machine Translation and Multilinguality,0.1109025627374649
Machine Translation and Multilinguality,CTC-based Compression for Direct Speech Translation,"Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated model for phone recognition and did not test this solution for direct ST, in which a single model translates the input audio into the target language without intermediate representations. In this work, we propose the first method able to perform a dynamic compression of the input in direct ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement over a strong baseline on two language pairs (English-Italian and English-German), contextually reducing the memory footprint by more than 10%.","{'sequence': 'Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated model for phone recognition and did not test this solution for direct ST, in which a single model translates the input audio into the target language without intermediate representations. In this work, we propose the first method able to perform a dynamic compression of the input in direct ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement over a strong baseline on two language pairs (English-Italian and English-German), contextually reducing the memory footprint by more than 10%.', 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Generation', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09420955181121826, 0.07300320267677307, 0.07007479667663574, 0.06763934344053268, 0.06722799688577652, 0.05419689789414406, 0.05294165015220642, 0.04962033033370972, 0.04312039539217949, 0.04308423399925232, 0.03924153000116348, 0.0387267991900444, 0.038055289536714554, 0.03689196705818176, 0.0332954078912735, 0.03244803845882416, 0.029725372791290283, 0.02910315990447998, 0.02538619376718998, 0.02487226203083992, 0.022206490859389305, 0.019556384533643723, 0.01537272334098816]}",0.09420955181121826,Speech and Multimodality,0.06763934344053268
Machine Translation and Multilinguality,Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation,"Recent studies in the field of Machine Translation (MT) and Natural Language Processing (NLP) have shown that existing models amplify biases observed in the training data. The amplification of biases in language technology has mainly been examined with respect to specific phenomena, such as gender bias. In this work, we go beyond the study of gender in MT and investigate how bias amplification might affect language in a broader sense. We hypothesize that the 'algorithmic bias', i.e. an exacerbation of frequently observed patterns in combination with a loss of less frequent ones, not only exacerbates societal biases present in current datasets but could also lead to an artificially impoverished language: 'machine translationese'. We assess the linguistic richness (on a lexical and morphological level) of translations created by different data-driven MT paradigms -phrase-based statistical (PB-SMT) and neural MT (NMT). Our experiments show that there is a loss of lexical and morphological richness in the translations produced by all investigated MT paradigms for two language pairs (EN↔FR and EN↔ES).","{'sequence': ""Recent studies in the field of Machine Translation (MT) and Natural Language Processing (NLP) have shown that existing models amplify biases observed in the training data. The amplification of biases in language technology has mainly been examined with respect to specific phenomena, such as gender bias. In this work, we go beyond the study of gender in MT and investigate how bias amplification might affect language in a broader sense. We hypothesize that the 'algorithmic bias', i.e. an exacerbation of frequently observed patterns in combination with a loss of less frequent ones, not only exacerbates societal biases present in current datasets but could also lead to an artificially impoverished language: 'machine translationese'. We assess the linguistic richness (on a lexical and morphological level) of translations created by different data-driven MT paradigms -phrase-based statistical (PB-SMT) and neural MT (NMT). Our experiments show that there is a loss of lexical and morphological richness in the translations produced by all investigated MT paradigms for two language pairs (EN↔FR and EN↔ES)."", 'labels': ['Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'NLP Applications', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09987442195415497, 0.08040525019168854, 0.06543280184268951, 0.06257402896881104, 0.06156471371650696, 0.06107296794652939, 0.05654742941260338, 0.05394973233342171, 0.04755421727895737, 0.04592844471335411, 0.044858407229185104, 0.043670494109392166, 0.03971360623836517, 0.03342709690332413, 0.029934559017419815, 0.029175179079174995, 0.02916805073618889, 0.02652834728360176, 0.025240376591682434, 0.022087441757321358, 0.01906983181834221, 0.017361987382173538, 0.004860681015998125]}",0.09987442195415497,Interpretability and Analysis of Models for NLP,0.08040525019168854
Machine Translation and Multilinguality,Continuous Learning in Neural Machine Translation using Bilingual Dictionaries,"While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language. In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30% to up to 70%. The correct lemma is even generated by more than 90%.","{'sequence': 'While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language. In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30% to up to 70%. The correct lemma is even generated by more than 90%.', 'labels': ['Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.13456286489963531, 0.12100774049758911, 0.08140528947114944, 0.07272551208734512, 0.06508370488882065, 0.05705782026052475, 0.042614441365003586, 0.0406501442193985, 0.0381464883685112, 0.035941723734140396, 0.033321768045425415, 0.03147183358669281, 0.0309832114726305, 0.030114108696579933, 0.02848581224679947, 0.02689138986170292, 0.025523122400045395, 0.022787975147366524, 0.020172353833913803, 0.019875019788742065, 0.014982813969254494, 0.013850805349647999, 0.012344223447144032]}",0.13456286489963531,Machine Translation and Multilinguality,0.13456286489963531
Machine Translation and Multilinguality,Understanding Pre-Editing for Black-Box Neural Machine Translation,"Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), pre-editing has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of pre-editing methods for particular settings, thus far, a deep understanding of what pre-editing is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a protocol to incrementally record the minimum edits for each ST and collected 6,652 instances of preediting across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives: the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following: (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types.","{'sequence': 'Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), pre-editing has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of pre-editing methods for particular settings, thus far, a deep understanding of what pre-editing is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a protocol to incrementally record the minimum edits for each ST and collected 6,652 instances of preediting across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives: the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following: (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types.', 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.06604635715484619, 0.06431249529123306, 0.0633477047085762, 0.06319928914308548, 0.058276016265153885, 0.054557591676712036, 0.05438656732439995, 0.05388221889734268, 0.05081858113408089, 0.04938364028930664, 0.04409775882959366, 0.04286136478185654, 0.040727801620960236, 0.036147214472293854, 0.03609953448176384, 0.03605597838759422, 0.03398653864860535, 0.031164757907390594, 0.026833951473236084, 0.02517063543200493, 0.024661827832460403, 0.023316755890846252, 0.020665502175688744]}",0.06604635715484619,Machine Translation and Multilinguality,0.06604635715484619
Machine Translation and Multilinguality,Enriching Non-Autoregressive Transformer with Syntactic and Semantic Structures for Neural Machine Translation,"The non-autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness, when comparing with the autoregressive counterparts. In this paper, we claim that the syntactic and semantic structures among natural language are critical for non-autoregressive machine translation and can further improve the performance. However, these structures are rarely considered in existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structures of languages into a non-autoregressive Transformer, for the task of neural machine translation. Moreover, we also consider the intermediate latent alignment within target sentences to better learn the long-term token dependencies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En-Ro) show that our model achieves a significantly faster speed, as well as keeps the translation quality when compared with several stateof-the-art non-autoregressive models.","{'sequence': 'The non-autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness, when comparing with the autoregressive counterparts. In this paper, we claim that the syntactic and semantic structures among natural language are critical for non-autoregressive machine translation and can further improve the performance. However, these structures are rarely considered in existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structures of languages into a non-autoregressive Transformer, for the task of neural machine translation. Moreover, we also consider the intermediate latent alignment within target sentences to better learn the long-term token dependencies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En-Ro) show that our model achieves a significantly faster speed, as well as keeps the translation quality when compared with several stateof-the-art non-autoregressive models.', 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Resources and Evaluation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Semantics: Lexical Semantics', 'NLP Applications', 'Ethics and NLP', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08434174209833145, 0.07690824568271637, 0.07425037026405334, 0.05731901153922081, 0.05696370452642441, 0.051961638033390045, 0.04854338616132736, 0.04736661911010742, 0.04479962959885597, 0.0420311875641346, 0.040912073105573654, 0.03932706639170647, 0.039210572838783264, 0.039164021611213684, 0.03885674849152565, 0.03256295993924141, 0.03183227777481079, 0.029862472787499428, 0.02795286849141121, 0.027035553008317947, 0.026256956160068512, 0.024204254150390625, 0.01833656057715416]}",0.08434174209833145,Machine Translation and Multilinguality,0.08434174209833145
Machine Translation and Multilinguality,Alignment verification to improve NMT translation towards highly inflectional languages with limited resources,"The present article studies translation quality when limited training data is available to translate towards morphologically rich languages. The starting point is a neural MT system, used to train translation models with only publicly available parallel data. An initial analysis of the translation output has shown that quality is sub-optimal, mainly due to the insufficient amount of training data. To improve translation, a hybridized solution is proposed, using an ensemble of relatively simple NMT systems trained with different metrics, combined with an open source module designed for low-resource MT that measures the alignment level. A quantitative analysis based on established metrics is complemented by a qualitative analysis of translation results. These show that over multiple test sets, the proposed hybridized method confers improvements over (i) both the best individual NMT and (ii) the ensemble system provided in the Marian-NMT package. Improvements over Marian-NMT are in many cases statistically significant.","{'sequence': 'The present article studies translation quality when limited training data is available to translate towards morphologically rich languages. The starting point is a neural MT system, used to train translation models with only publicly available parallel data. An initial analysis of the translation output has shown that quality is sub-optimal, mainly due to the insufficient amount of training data. To improve translation, a hybridized solution is proposed, using an ensemble of relatively simple NMT systems trained with different metrics, combined with an open source module designed for low-resource MT that measures the alignment level. A quantitative analysis based on established metrics is complemented by a qualitative analysis of translation results. These show that over multiple test sets, the proposed hybridized method confers improvements over (i) both the best individual NMT and (ii) the ensemble system provided in the Marian-NMT package. Improvements over Marian-NMT are in many cases statistically significant.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Summarization', 'Generation', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.08623354136943817, 0.08524393290281296, 0.06992761790752411, 0.06775999814271927, 0.06334926933050156, 0.055426303297281265, 0.05222345143556595, 0.05198579654097557, 0.046921052038669586, 0.0439922995865345, 0.042362041771411896, 0.039857417345047, 0.03858737275004387, 0.035453617572784424, 0.029001858085393906, 0.028377704322338104, 0.02673918753862381, 0.025473548099398613, 0.02469787932932377, 0.023971058428287506, 0.021633891388773918, 0.021542472764849663, 0.019238652661442757]}",0.08623354136943817,Dialogue and Interactive Systems,0.06775999814271927
Machine Translation and Multilinguality,Bootstrapping Multilingual AMR with Contextual Word Alignments,"We develop high performance multilingual Abstract Meaning Representation (AMR) systems by projecting English AMR annotations to other languages with weak supervision. We achieve this goal by bootstrapping transformerbased multilingual word embeddings, in particular those from cross-lingual RoBERTa (XLM-R large). We develop a novel technique for foreign-text-to-English AMR alignment, using the contextual word alignment between English and foreign language tokens. This word alignment is weakly supervised and relies on the contextualized XLM-R word embeddings. We achieve a highly competitive performance that surpasses the best published results for German, Italian, Spanish and Chinese.","{'sequence': 'We develop high performance multilingual Abstract Meaning Representation (AMR) systems by projecting English AMR annotations to other languages with weak supervision. We achieve this goal by bootstrapping transformerbased multilingual word embeddings, in particular those from cross-lingual RoBERTa (XLM-R large). We develop a novel technique for foreign-text-to-English AMR alignment, using the contextual word alignment between English and foreign language tokens. This word alignment is weakly supervised and relies on the contextualized XLM-R word embeddings. We achieve a highly competitive performance that surpasses the best published results for German, Italian, Spanish and Chinese.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Question Answering', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Generation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Summarization', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10989635437726974, 0.0895257368683815, 0.06447783857584, 0.059445448219776154, 0.054337240755558014, 0.05051669105887413, 0.04983512684702873, 0.04937165603041649, 0.045094434171915054, 0.04444500058889389, 0.040301840752363205, 0.04000566154718399, 0.03870474919676781, 0.0351986363530159, 0.03287341445684433, 0.032460059970617294, 0.02926722727715969, 0.02820722945034504, 0.025118326768279076, 0.023746000602841377, 0.019717777147889137, 0.0193527489900589, 0.01810084655880928]}",0.10989635437726974,Machine Translation and Multilinguality,0.10989635437726974
Machine Translation and Multilinguality,A phonetic model of non-native spoken word processing,"Non-native speakers show difficulties with spoken word processing. Many studies attribute these difficulties to imprecise phonological encoding of words in the lexical memory. We test an alternative hypothesis: that some of these difficulties can arise from the non-native speakers' phonetic perception. We train a computational model of phonetic learning, which has no access to phonology, on either one or two languages. We first show that the model exhibits predictable behaviors on phone-level and word-level discrimination tasks. We then test the model on a spoken word processing task, showing that phonology may not be necessary to explain some of the word processing effects observed in non-native speakers. We run an additional analysis of the model's lexical representation space, showing that the two training languages are not fully separated in that space, similarly to the languages of a bilingual human speaker.","{'sequence': ""Non-native speakers show difficulties with spoken word processing. Many studies attribute these difficulties to imprecise phonological encoding of words in the lexical memory. We test an alternative hypothesis: that some of these difficulties can arise from the non-native speakers' phonetic perception. We train a computational model of phonetic learning, which has no access to phonology, on either one or two languages. We first show that the model exhibits predictable behaviors on phone-level and word-level discrimination tasks. We then test the model on a spoken word processing task, showing that phonology may not be necessary to explain some of the word processing effects observed in non-native speakers. We run an additional analysis of the model's lexical representation space, showing that the two training languages are not fully separated in that space, similarly to the languages of a bilingual human speaker."", 'labels': ['Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Generation', 'Summarization', 'Ethics and NLP', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1316647231578827, 0.07540751993656158, 0.06791877746582031, 0.06596912443637848, 0.0625399723649025, 0.05867822840809822, 0.05271248519420624, 0.050401680171489716, 0.04139314591884613, 0.03935164213180542, 0.03874750807881355, 0.03670438751578331, 0.032451290637254715, 0.03129471093416214, 0.03004348650574684, 0.027082327753305435, 0.026228200644254684, 0.024161256849765778, 0.023452864959836006, 0.021843483671545982, 0.02151850052177906, 0.021222613751888275, 0.019212130457162857]}",0.1316647231578827,Speech and Multimodality,0.04139314591884613
Machine Translation and Multilinguality,Multilingual and cross-lingual document classification: A meta-learning approach,"The great majority of languages in the world are considered under-resourced for the successful application of deep learning methods. In this work, we propose a meta-learning approach to document classification in limitedresource setting and demonstrate its effectiveness in two different settings: few-shot, crosslingual adaptation to previously unseen languages; and multilingual joint training when limited target-language data is available during training. We conduct a systematic comparison of several meta-learning methods, investigate multiple settings in terms of data availability and show that meta-learning thrives in settings with a heterogeneous task distribution. We propose a simple, yet effective adjustment to existing meta-learning methods which allows for better and more stable learning, and set a new state of the art on several languages while performing on-par on others, using only a small amount of labeled data.","{'sequence': 'The great majority of languages in the world are considered under-resourced for the successful application of deep learning methods. In this work, we propose a meta-learning approach to document classification in limitedresource setting and demonstrate its effectiveness in two different settings: few-shot, crosslingual adaptation to previously unseen languages; and multilingual joint training when limited target-language data is available during training. We conduct a systematic comparison of several meta-learning methods, investigate multiple settings in terms of data availability and show that meta-learning thrives in settings with a heterogeneous task distribution. We propose a simple, yet effective adjustment to existing meta-learning methods which allows for better and more stable learning, and set a new state of the art on several languages while performing on-par on others, using only a small amount of labeled data.', 'labels': ['Resources and Evaluation', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Lexical Semantics', 'NLP Applications', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08415364474058151, 0.07946043461561203, 0.07107263058423996, 0.06971387565135956, 0.06471361219882965, 0.053032394498586655, 0.048443567007780075, 0.045438315719366074, 0.04385119304060936, 0.04380219429731369, 0.040283918380737305, 0.03877082094550133, 0.03850040212273598, 0.03742244839668274, 0.0344780795276165, 0.030593890696763992, 0.03031730279326439, 0.02861488051712513, 0.028572378680109978, 0.02662515826523304, 0.0234054122120142, 0.02256678231060505, 0.01616666279733181]}",0.08415364474058151,Resources and Evaluation,0.07946043461561203
Machine Translation and Multilinguality,El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic Parsing,"Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only English corpus alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings: fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.","{'sequence': 'Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only English corpus alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings: fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.', 'labels': ['Generation', 'Resources and Evaluation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'NLP Applications', 'Question Answering', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.21041245758533478, 0.07080310583114624, 0.06959468871355057, 0.057351648807525635, 0.054560720920562744, 0.054337527602910995, 0.04741948843002319, 0.04313846305012703, 0.03914036229252815, 0.037309300154447556, 0.035685550421476364, 0.034791260957717896, 0.03437932953238487, 0.034338075667619705, 0.03329113498330116, 0.0256347693502903, 0.02422747015953064, 0.018985504284501076, 0.018433602526783943, 0.016157345846295357, 0.01408011931926012, 0.01386519055813551, 0.012062877416610718]}",0.21041245758533478,Generation,0.035685550421476364
Machine Translation and Multilinguality,Lexical Normalization for Code-switched Data and its Effect on POS Tagging,"Lexical normalization, the translation of noncanonical data to standard language, has shown to improve the performance of many natural language processing tasks on social media. Yet, using multiple languages in one utterance, also called code-switching (CS), is frequently overlooked by these normalization systems, despite its common use in social media. In this paper, we propose three normalization models specifically designed to handle codeswitched data which we evaluate for two language pairs: Indonesian-English (Id-En) and Turkish-German (Tr-De). For the latter, we introduce novel normalization layers and their corresponding language ID and POS tags for the dataset, and evaluate the downstream effect of normalization on POS tagging. Results show that our CS-tailored normalization models outperform Id-En state of the art and Tr-De monolingual models, and lead to 5.4% relative performance increase for POS tagging as compared to unnormalized input. 1","{'sequence': 'Lexical normalization, the translation of noncanonical data to standard language, has shown to improve the performance of many natural language processing tasks on social media. Yet, using multiple languages in one utterance, also called code-switching (CS), is frequently overlooked by these normalization systems, despite its common use in social media. In this paper, we propose three normalization models specifically designed to handle codeswitched data which we evaluate for two language pairs: Indonesian-English (Id-En) and Turkish-German (Tr-De). For the latter, we introduce novel normalization layers and their corresponding language ID and POS tags for the dataset, and evaluate the downstream effect of normalization on POS tagging. Results show that our CS-tailored normalization models outperform Id-En state of the art and Tr-De monolingual models, and lead to 5.4% relative performance increase for POS tagging as compared to unnormalized input. 1', 'labels': ['Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Ethics and NLP', 'Summarization', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11456897109746933, 0.0921153575181961, 0.09115606546401978, 0.09078030288219452, 0.06882865726947784, 0.05045272782444954, 0.04613456130027771, 0.040792398154735565, 0.03964207321405411, 0.038300033658742905, 0.03605484217405319, 0.035587143152952194, 0.03426540642976761, 0.03364095464348793, 0.03177794814109802, 0.030495718121528625, 0.029527705162763596, 0.026883210986852646, 0.021818077191710472, 0.02082732506096363, 0.014188753440976143, 0.007926908321678638, 0.0042348336428403854]}",0.11456897109746933,Dialogue and Interactive Systems,0.03426540642976761
Machine Translation and Multilinguality,Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models,"Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as ""Paris is the capital of [MASK]"" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT's performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.","{'sequence': 'Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as ""Paris is the capital of [MASK]"" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT\'s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.', 'labels': ['Question Answering', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.13253021240234375, 0.10223520547151566, 0.07157796621322632, 0.06173424795269966, 0.056420885026454926, 0.055902570486068726, 0.039914023131132126, 0.036393214017152786, 0.03608514368534088, 0.03560565039515495, 0.034974414855241776, 0.03436843305826187, 0.031871598213911057, 0.03182218223810196, 0.030391691252589226, 0.0302390456199646, 0.029349299147725105, 0.028097661212086678, 0.028075754642486572, 0.027669379487633705, 0.02424018085002899, 0.023616567254066467, 0.01688460446894169]}",0.13253021240234375,Question Answering,0.10223520547151566
Machine Translation and Multilinguality,WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia,"We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 1620 different language pairs, out of which only 34M are aligned with English. This corpus is freely available. 1   To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English.","{'sequence': 'We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 1620 different language pairs, out of which only 34M are aligned with English. This corpus is freely available. 1   To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English.', 'labels': ['Resources and Evaluation', 'Information Extraction', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Generation', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Summarization', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13217933475971222, 0.12585720419883728, 0.0806197002530098, 0.07013501226902008, 0.06099093705415726, 0.05231175571680069, 0.041100360453128815, 0.037722133100032806, 0.037592146545648575, 0.03357436880469322, 0.03304075077176094, 0.032826147973537445, 0.030138902366161346, 0.027041738852858543, 0.026812998577952385, 0.025815000757575035, 0.025472914800047874, 0.024656014516949654, 0.023733004927635193, 0.023364918306469917, 0.022811930626630783, 0.020335013046860695, 0.011867688968777657]}",0.13217933475971222,Resources and Evaluation,0.0806197002530098
Machine Translation and Multilinguality,Cross-Cultural Similarity Features for Cross-Lingual Transfer Learning of Pragmatically Motivated Tasks,"Much work in cross-lingual transfer learning explored how to select better transfer languages for multilingual tasks, primarily focusing on typological and genealogical similarities between languages. We hypothesize that these measures of linguistic proximity are not enough when working with pragmaticallymotivated tasks, such as sentiment analysis. As an alternative, we introduce three linguistic features that capture cross-cultural similarities that manifest in linguistic patterns and quantify distinct aspects of language pragmatics: language context-level, figurative language, and the lexification of emotion concepts. Our analyses show that the proposed pragmatic features do capture cross-cultural similarities and align well with existing work in sociolinguistics and linguistic anthropology. We further corroborate the effectiveness of pragmatically-driven transfer in the downstream task of choosing transfer languages for cross-lingual sentiment analysis.","{'sequence': 'Much work in cross-lingual transfer learning explored how to select better transfer languages for multilingual tasks, primarily focusing on typological and genealogical similarities between languages. We hypothesize that these measures of linguistic proximity are not enough when working with pragmaticallymotivated tasks, such as sentiment analysis. As an alternative, we introduce three linguistic features that capture cross-cultural similarities that manifest in linguistic patterns and quantify distinct aspects of language pragmatics: language context-level, figurative language, and the lexification of emotion concepts. Our analyses show that the proposed pragmatic features do capture cross-cultural similarities and align well with existing work in sociolinguistics and linguistic anthropology. We further corroborate the effectiveness of pragmatically-driven transfer in the downstream task of choosing transfer languages for cross-lingual sentiment analysis.', 'labels': ['Discourse and Pragmatics', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Generation', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15644043684005737, 0.08137158304452896, 0.06321430206298828, 0.06130388379096985, 0.05335391312837601, 0.05167742073535919, 0.05106228217482567, 0.048042573034763336, 0.042791031301021576, 0.03888722509145737, 0.036025915294885635, 0.03528065234422684, 0.03489810600876808, 0.033644821494817734, 0.03006705455482006, 0.029529994353652, 0.028802121058106422, 0.025131085887551308, 0.023884909227490425, 0.023178230971097946, 0.021331846714019775, 0.017018156126141548, 0.013062541373074055]}",0.15644043684005737,Discourse and Pragmatics,0.048042573034763336
Machine Translation and Multilinguality,Does Typological Blinding Impede Cross-Lingual Sharing?,"Bridging the performance gap between highand low-resource languages has been the focus of much previous work. Typological features from databases such as the World Atlas of Language Structures (WALS) are a prime candidate for this, as such data exists even for very low-resource languages. However, previous work has only found minor benefits from using typological information. Our hypothesis is that a model trained in a cross-lingual setting will pick up on typological cues from the input data, thus overshadowing the utility of explicitly using such features. We verify this hypothesis by blinding a model to typological information, and investigate how cross-lingual sharing and performance is impacted. Our model is based on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this model from exploiting typology severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to typology somewhat improves performance.","{'sequence': 'Bridging the performance gap between highand low-resource languages has been the focus of much previous work. Typological features from databases such as the World Atlas of Language Structures (WALS) are a prime candidate for this, as such data exists even for very low-resource languages. However, previous work has only found minor benefits from using typological information. Our hypothesis is that a model trained in a cross-lingual setting will pick up on typological cues from the input data, thus overshadowing the utility of explicitly using such features. We verify this hypothesis by blinding a model to typological information, and investigate how cross-lingual sharing and performance is impacted. Our model is based on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this model from exploiting typology severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to typology somewhat improves performance.', 'labels': ['Information Extraction', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Generation', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics', 'Summarization'], 'scores': [0.08137977868318558, 0.07075776159763336, 0.06927651166915894, 0.055990878492593765, 0.055726781487464905, 0.050919972360134125, 0.050323374569416046, 0.04911535978317261, 0.04850515350699425, 0.04545852541923523, 0.04518713057041168, 0.042735569179058075, 0.0411025695502758, 0.038452427834272385, 0.036013491451740265, 0.031881168484687805, 0.03146302327513695, 0.02998530864715576, 0.029044857248663902, 0.028423890471458435, 0.023892439901828766, 0.023348338901996613, 0.021015619859099388]}",0.08137977868318558,Information Extraction,0.04518713057041168
Machine Translation and Multilinguality,Meta-Learning for Effective Multi-task and Multilingual Modelling,"Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g., named entity recognition in English) and knowledge of other languages (e.g., question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both tasks and languages. We also investigate the role of different sampling strategies used during meta-learning. We present experiments on five different tasks and six different languages from the XTREME multilingual benchmark dataset (Hu et al., 2020) . Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multitask baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed model.","{'sequence': 'Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g., named entity recognition in English) and knowledge of other languages (e.g., question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both tasks and languages. We also investigate the role of different sampling strategies used during meta-learning. We present experiments on five different tasks and six different languages from the XTREME multilingual benchmark dataset (Hu et al., 2020) . Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multitask baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed model.', 'labels': ['Question Answering', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Machine Learning for NLP', 'Generation', 'Information Extraction', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.41561439633369446, 0.06608056277036667, 0.05717141553759575, 0.044736482203006744, 0.03949590027332306, 0.038318075239658356, 0.03569430857896805, 0.03484712913632393, 0.02815105766057968, 0.024704789742827415, 0.02391744591295719, 0.023602113127708435, 0.0233079195022583, 0.017832189798355103, 0.016918623819947243, 0.016918456181883812, 0.016461528837680817, 0.015777891501784325, 0.014790979214012623, 0.014612524770200253, 0.014162551611661911, 0.010311149060726166, 0.006572510581463575]}",0.41561439633369446,Question Answering,0.017832189798355103
Machine Translation and Multilinguality,CDA: a Cost Efficient Content-based Multilingual Web Document Aligner,"We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps: (i) projecting documents of a web domain to a shared multilingual space; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TF×IDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the robustness of CDA in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.","{'sequence': 'We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps: (i) projecting documents of a web domain to a shared multilingual space; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TF×IDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the robustness of CDA in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.', 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2438456267118454, 0.06138721853494644, 0.05489003285765648, 0.05335497483611107, 0.049320001155138016, 0.048832718282938004, 0.048409976065158844, 0.04570428654551506, 0.041045062243938446, 0.040292467921972275, 0.03221701830625534, 0.0301752220839262, 0.029168125241994858, 0.02862170711159706, 0.02843218669295311, 0.028206832706928253, 0.026714937761425972, 0.023979427292943, 0.0191098153591156, 0.0182783305644989, 0.01749875769019127, 0.015409078449010849, 0.015106158331036568]}",0.2438456267118454,Machine Translation and Multilinguality,0.2438456267118454
NLP Applications,Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies,"Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.","{'sequence': 'Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.', 'labels': ['Question Answering', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Summarization', 'Information Retrieval and Text Mining', 'NLP Applications', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.4189339280128479, 0.05360860377550125, 0.05026610568165779, 0.04648325964808464, 0.04495444893836975, 0.042539361864328384, 0.03774937987327576, 0.03211317956447601, 0.02658596821129322, 0.026149975135922432, 0.026026083156466484, 0.024572083726525307, 0.021769139915704727, 0.021158237010240555, 0.02033803053200245, 0.017563985660672188, 0.015946729108691216, 0.014696311205625534, 0.013735122047364712, 0.013470924459397793, 0.011309782043099403, 0.01109376922249794, 0.008935673162341118]}",0.4189339280128479,Question Answering,0.024572083726525307
NLP Applications,MultiHumES: Multilingual Humanitarian Dataset for Extractive Summarization,"When responding to a disaster, humanitarian experts must rapidly process large amounts of secondary data sources to derive situational awareness and guide decision-making. While these documents contain valuable information, manually processing them is extremely timeconsuming when an expedient response is necessary. To improve this process, effective summarization models are a valuable tool for humanitarian response experts as they provide digestible overviews of essential information in secondary data. This paper focuses on extractive summarization for the humanitarian response domain and describes and makes public a new multilingual data collection for this purpose. The collection -called MultiHumES -provides multilingual documents coupled with informative snippets that have been annotated by humanitarian analysts over the past four years. We report the performance results of a recent neural networks-based summarization model together with other baselines. We hope that the released data collection can further grow the research on multilingual extractive summarization in the humanitarian response domain.","{'sequence': 'When responding to a disaster, humanitarian experts must rapidly process large amounts of secondary data sources to derive situational awareness and guide decision-making. While these documents contain valuable information, manually processing them is extremely timeconsuming when an expedient response is necessary. To improve this process, effective summarization models are a valuable tool for humanitarian response experts as they provide digestible overviews of essential information in secondary data. This paper focuses on extractive summarization for the humanitarian response domain and describes and makes public a new multilingual data collection for this purpose. The collection -called MultiHumES -provides multilingual documents coupled with informative snippets that have been annotated by humanitarian analysts over the past four years. We report the performance results of a recent neural networks-based summarization model together with other baselines. We hope that the released data collection can further grow the research on multilingual extractive summarization in the humanitarian response domain.', 'labels': ['Summarization', 'Information Extraction', 'Question Answering', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'NLP Applications', 'Ethics and NLP', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10394186526536942, 0.08013666421175003, 0.061239901930093765, 0.05683682858943939, 0.05530901253223419, 0.05143105238676071, 0.05037896707653999, 0.04594273492693901, 0.04347632825374603, 0.043017324060201645, 0.04108446463942528, 0.04000837728381157, 0.037379831075668335, 0.0368526317179203, 0.03638528659939766, 0.03557487204670906, 0.03087044693529606, 0.02944735810160637, 0.028441959992051125, 0.02730398438870907, 0.02592337504029274, 0.020732061937451363, 0.018284659832715988]}",0.10394186526536942,Summarization,0.043017324060201645
NLP Applications,LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content,"The conceptualization of a claim lies at the core of argument mining. The segregation of claims is complex, owing to the divergence in textual syntax and context across different distributions. Another pressing issue is the unavailability of labeled unstructured text for experimentation. In this paper, we propose LESA, a framework which aims at advancing headfirst into expunging the former issue by assembling a source-independent generalized model that captures syntactic features through part-of-speech and dependency embeddings, as well as contextual features through a fine-tuned language model. We resolve the latter issue by annotating a Twitter dataset which aims at providing a testing ground on a large unstructured dataset. Experimental results show that LESA improves upon the state-of-the-art performance across six benchmark claim datasets by an average of 3 claim-F1 points for in-domain experiments and by 2 claim-F1 points for general-domain experiments. On our dataset too, LESA outperforms existing baselines by 1 claim-F1 point on the in-domain experiments and 2 claim-F1 points on the general-domain experiments. We also release comprehensive data annotation guidelines compiled during the annotation phase (which was missing in the current literature).","{'sequence': 'The conceptualization of a claim lies at the core of argument mining. The segregation of claims is complex, owing to the divergence in textual syntax and context across different distributions. Another pressing issue is the unavailability of labeled unstructured text for experimentation. In this paper, we propose LESA, a framework which aims at advancing headfirst into expunging the former issue by assembling a source-independent generalized model that captures syntactic features through part-of-speech and dependency embeddings, as well as contextual features through a fine-tuned language model. We resolve the latter issue by annotating a Twitter dataset which aims at providing a testing ground on a large unstructured dataset. Experimental results show that LESA improves upon the state-of-the-art performance across six benchmark claim datasets by an average of 3 claim-F1 points for in-domain experiments and by 2 claim-F1 points for general-domain experiments. On our dataset too, LESA outperforms existing baselines by 1 claim-F1 point on the in-domain experiments and 2 claim-F1 points on the general-domain experiments. We also release comprehensive data annotation guidelines compiled during the annotation phase (which was missing in the current literature).', 'labels': ['Question Answering', 'Information Extraction', 'Resources and Evaluation', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Generation', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.08133374899625778, 0.06874757260084152, 0.06198711320757866, 0.057599253952503204, 0.05717412382364273, 0.0540325790643692, 0.05317100137472153, 0.05035661906003952, 0.046486932784318924, 0.0444478765130043, 0.044127315282821655, 0.04041695222258568, 0.03935888037085533, 0.038824308663606644, 0.03691358119249344, 0.0365891270339489, 0.036118604242801666, 0.034060947597026825, 0.031340330839157104, 0.029342759400606155, 0.02214772440493107, 0.018580224364995956, 0.016842372715473175]}",0.08133374899625778,Question Answering,0.03935888037085533
"Phonology, Morphology and Word Segmentation",Subword Pooling Makes a Difference,"Contextual word-representations became a standard in modern natural language processing systems. These models use subword tokenization to handle large vocabularies and unknown words. Word-level usage of such systems requires a way of pooling multiple subwords that correspond to a single word. In this paper we investigate how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages. We compare these in two massively multilingual models, mBERT and XLM-RoBERTa. For morphological tasks, the widely used 'choose the first subword' is the worst strategy and the best results are obtained by using attention over the subwords. For POS tagging both of these strategies perform poorly and the best choice is to use a small LSTM over the subwords. The same strategy works best for NER and we show that mBERT is better than XLM-RoBERTa in all 9 languages. We publicly release all code, data and the full result tables at https://github. com/juditacs/subword-choice.","{'sequence': ""Contextual word-representations became a standard in modern natural language processing systems. These models use subword tokenization to handle large vocabularies and unknown words. Word-level usage of such systems requires a way of pooling multiple subwords that correspond to a single word. In this paper we investigate how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages. We compare these in two massively multilingual models, mBERT and XLM-RoBERTa. For morphological tasks, the widely used 'choose the first subword' is the worst strategy and the best results are obtained by using attention over the subwords. For POS tagging both of these strategies perform poorly and the best choice is to use a small LSTM over the subwords. The same strategy works best for NER and we show that mBERT is better than XLM-RoBERTa in all 9 languages. We publicly release all code, data and the full result tables at https://github. com/juditacs/subword-choice."", 'labels': ['Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.06900262087583542, 0.06688907742500305, 0.06511681526899338, 0.059377413243055344, 0.057558950036764145, 0.05686477571725845, 0.05251934751868248, 0.04671154543757439, 0.045130930840969086, 0.04382507875561714, 0.0436929427087307, 0.04170234501361847, 0.040704429149627686, 0.0406758189201355, 0.039462003856897354, 0.037873607128858566, 0.03644431009888649, 0.035446882247924805, 0.030729521065950394, 0.02893223613500595, 0.028067629784345627, 0.017140572890639305, 0.016131162643432617]}",0.06900262087583542,Speech and Multimodality,0.05686477571725845
"Phonology, Morphology and Word Segmentation",Enhancing Sequence-to-Sequence Neural Lemmatization with External Resources,"We propose a novel hybrid approach to lemmatization that enhances the seq2seq neural model with additional lemmas extracted from an external lexicon or a rule-based system. During training, the enhanced lemmatizer learns both to generate lemmas via a sequential decoder and copy the lemma characters from the external candidates supplied during run-time. Our lemmatizer enhanced with candidates extracted from the Apertium morphological analyzer achieves statistically significant improvements compared to baseline models not utilizing additional lemma information, achieves an average accuracy of 97.25% on a set of 23 UD languages, which is 0.55% higher than obtained with the Stanford Stanza model on the same set of languages. We also compare with other methods of integrating external data into lemmatization and show that our enhanced system performs considerably better than a simple lexicon extension method based on the Stanza system, and it achieves complementary improvements w.r.t. the data augmentation method.","{'sequence': 'We propose a novel hybrid approach to lemmatization that enhances the seq2seq neural model with additional lemmas extracted from an external lexicon or a rule-based system. During training, the enhanced lemmatizer learns both to generate lemmas via a sequential decoder and copy the lemma characters from the external candidates supplied during run-time. Our lemmatizer enhanced with candidates extracted from the Apertium morphological analyzer achieves statistically significant improvements compared to baseline models not utilizing additional lemma information, achieves an average accuracy of 97.25% on a set of 23 UD languages, which is 0.55% higher than obtained with the Stanford Stanza model on the same set of languages. We also compare with other methods of integrating external data into lemmatization and show that our enhanced system performs considerably better than a simple lexicon extension method based on the Stanza system, and it achieves complementary improvements w.r.t. the data augmentation method.', 'labels': ['Information Extraction', 'Question Answering', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics'], 'scores': [0.12065946310758591, 0.08066996186971664, 0.06678549945354462, 0.06260933727025986, 0.058574702590703964, 0.05150480195879936, 0.050040025264024734, 0.045542746782302856, 0.04480121284723282, 0.04316309466958046, 0.04295995458960533, 0.04216194152832031, 0.03879304230213165, 0.033335380256175995, 0.02946951985359192, 0.028011329472064972, 0.02791329100728035, 0.027300497516989708, 0.025009842589497566, 0.022692281752824783, 0.020433200523257256, 0.019592156633734703, 0.017976753413677216]}",0.12065946310758591,Information Extraction,0.050040025264024734
"Phonology, Morphology and Word Segmentation",On the Computational Modelling of Michif Verbal Morphology,"This paper presents a finite-state computational model of the verbal morphology of Michif. Michif, the official language of the Métis peoples, is a uniquely mixed language with Algonquian and French origins. It is spoken across the Métis homelands in what is now called Canada and the United States, but it is highly endangered with less than 100 speakers. The verbal morphology is remarkably complex, as the already polysynthetic Algonquian patterns are combined with French elements and unique morpho-phonological interactions. The model presented in this paper, LI VERB KAA-OOSHITAHK DI MICHIF handles this complexity by using a series of composed finite-state transducers to model the concatenative morphology and phonological rule alternations that are unique to Michif. Such a rulebased approach is necessary as there is insufficient language data for an approach that uses machine learning. A language model such as LI VERB KAA-OOSHITAHK DI MICHIF furthers the goals of Indigenous computational linguistics in Canada while also supporting the creation of tools for documentation, education, and revitalization that are desired by the Métis community.","{'sequence': 'This paper presents a finite-state computational model of the verbal morphology of Michif. Michif, the official language of the Métis peoples, is a uniquely mixed language with Algonquian and French origins. It is spoken across the Métis homelands in what is now called Canada and the United States, but it is highly endangered with less than 100 speakers. The verbal morphology is remarkably complex, as the already polysynthetic Algonquian patterns are combined with French elements and unique morpho-phonological interactions. The model presented in this paper, LI VERB KAA-OOSHITAHK DI MICHIF handles this complexity by using a series of composed finite-state transducers to model the concatenative morphology and phonological rule alternations that are unique to Michif. Such a rulebased approach is necessary as there is insufficient language data for an approach that uses machine learning. A language model such as LI VERB KAA-OOSHITAHK DI MICHIF furthers the goals of Indigenous computational linguistics in Canada while also supporting the creation of tools for documentation, education, and revitalization that are desired by the Métis community.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Question Answering', 'Generation', 'Summarization', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09904120862483978, 0.08981604874134064, 0.0773104578256607, 0.0748981386423111, 0.0653172954916954, 0.0610445998609066, 0.05215170234441757, 0.05006236582994461, 0.04629609361290932, 0.04513167589902878, 0.041878461837768555, 0.03688983619213104, 0.035744842141866684, 0.028387881815433502, 0.026982614770531654, 0.025484589859843254, 0.02437416836619377, 0.02356884256005287, 0.02350039780139923, 0.020652802661061287, 0.020010128617286682, 0.019983775913715363, 0.01147204264998436]}",0.09904120862483978,Speech and Multimodality,0.0653172954916954
"Phonology, Morphology and Word Segmentation",From characters to words: the turning point of BPE merges,"The distributions of orthographic word types are very different across languages due to typological characteristics, different writing traditions, and other factors. The wide range of cross-linguistic diversity is still a major challenge for NLP, and for the study of language more generally. We use BPE and informationtheoretic measures to investigate if distributions become more similar under specific levels of subword tokenization. We perform a cross-linguistic comparison, following incremental BPE merges (we go from characters to words) for 47 diverse languages. We show that text entropy values (a feature of probability distributions) converge at specific subword levels: relatively few BPE merges (around 200 for our corpus) lead to the most similar distributions across languages. Additionally, we analyze the interaction between subword and word-level distributions and show that our findings can be interpreted in light of the ongoing discussion about different morphological complexity types. 1","{'sequence': 'The distributions of orthographic word types are very different across languages due to typological characteristics, different writing traditions, and other factors. The wide range of cross-linguistic diversity is still a major challenge for NLP, and for the study of language more generally. We use BPE and informationtheoretic measures to investigate if distributions become more similar under specific levels of subword tokenization. We perform a cross-linguistic comparison, following incremental BPE merges (we go from characters to words) for 47 diverse languages. We show that text entropy values (a feature of probability distributions) converge at specific subword levels: relatively few BPE merges (around 200 for our corpus) lead to the most similar distributions across languages. Additionally, we analyze the interaction between subword and word-level distributions and show that our findings can be interpreted in light of the ongoing discussion about different morphological complexity types. 1', 'labels': ['Information Extraction', 'Question Answering', 'Speech and Multimodality', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Generation', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.09399285912513733, 0.06968004256486893, 0.06872865557670593, 0.0661264955997467, 0.06440940499305725, 0.062453337013721466, 0.058360058814287186, 0.047308724373579025, 0.046656541526317596, 0.04260607808828354, 0.040025003254413605, 0.038156647235155106, 0.037487272173166275, 0.03290306404232979, 0.029712267220020294, 0.029553666710853577, 0.02879256382584572, 0.028518477454781532, 0.02766849286854267, 0.02703825943171978, 0.02304903045296669, 0.020549984648823738, 0.016223160549998283]}",0.09399285912513733,Information Extraction,0.0661264955997467
"Phonology, Morphology and Word Segmentation",Error Analysis and the Role of Morphology,"We evaluate two common conjectures in error analysis of NLP models: (i) Morphology is predictive of errors; and (ii) the importance of morphology increases with the morphological complexity of a language. We show across four different tasks and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve error prediction across tasks; however, this effect is less pronounced with morphologically complex languages. We speculate this is because morphology is more discriminative in morphologically simple languages. Across all four tasks, case and gender are the morphological features most predictive of error.","{'sequence': 'We evaluate two common conjectures in error analysis of NLP models: (i) Morphology is predictive of errors; and (ii) the importance of morphology increases with the morphological complexity of a language. We show across four different tasks and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve error prediction across tasks; however, this effect is less pronounced with morphologically complex languages. We speculate this is because morphology is more discriminative in morphologically simple languages. Across all four tasks, case and gender are the morphological features most predictive of error.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Ethics and NLP', 'NLP Applications', 'Question Answering', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13895197212696075, 0.09963946789503098, 0.07988759875297546, 0.07635460048913956, 0.0654534175992012, 0.05609108880162239, 0.04633720964193344, 0.04557804763317108, 0.039741143584251404, 0.038730330765247345, 0.03850752115249634, 0.02929737977683544, 0.02795739285647869, 0.027435531839728355, 0.02650556154549122, 0.025000857189297676, 0.023720016703009605, 0.02340633049607277, 0.022176358848810196, 0.019627224653959274, 0.018426496535539627, 0.017871487885713577, 0.013302968814969063]}",0.13895197212696075,Interpretability and Analysis of Models for NLP,0.09963946789503098
Semantics: Lexical Semantics,Dictionary-based Debiasing of Pre-trained Word Embeddings,"Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it (a) retains the semantics of the pre-trained word embeddings, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.","{'sequence': 'Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it (a) retains the semantics of the pre-trained word embeddings, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Semantics: Lexical Semantics', 'Generation', 'Question Answering', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.06704500317573547, 0.06426624208688736, 0.0629015639424324, 0.0613015815615654, 0.055176399648189545, 0.05455252528190613, 0.053557977080345154, 0.052467890083789825, 0.050376635044813156, 0.04554000869393349, 0.04287835583090782, 0.04035232961177826, 0.038746293634176254, 0.03726545348763466, 0.03673086315393448, 0.03625483438372612, 0.03589506074786186, 0.034835103899240494, 0.034499652683734894, 0.029515303671360016, 0.024874312803149223, 0.020664269104599953, 0.02030239626765251]}",0.06704500317573547,Dialogue and Interactive Systems,0.0629015639424324
Semantics: Lexical Semantics,RelWalk - A Latent Variable Model Approach to Knowledge Graph Embedding,"Embedding entities and relations of a knowledge graph in a low-dimensional space has shown impressive performance in predicting missing links between entities. Although progresses have been achieved, existing methods are heuristically motivated and theoretical understanding of such embeddings is comparatively underdeveloped. This paper extends the random walk model (Arora et al., 2016a) of word embeddings to Knowledge Graph Embeddings (KGEs) to derive a scoring function that evaluates the strength of a relation R between two entities h (head) and t (tail). Moreover, we show that marginal loss minimisation, a popular objective used in much prior work in KGE, follows naturally from the loglikelihood ratio maximisation under the probabilities estimated from the KGEs according to our theoretical relationship. We propose a learning objective motivated by the theoretical analysis to learn KGEs from a given knowledge graph. Using the derived objective, accurate KGEs are learnt from FB15K237 and WN18RR benchmark datasets, providing empirical evidence in support of the theory. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.","{'sequence': 'Embedding entities and relations of a knowledge graph in a low-dimensional space has shown impressive performance in predicting missing links between entities. Although progresses have been achieved, existing methods are heuristically motivated and theoretical understanding of such embeddings is comparatively underdeveloped. This paper extends the random walk model (Arora et al., 2016a) of word embeddings to Knowledge Graph Embeddings (KGEs) to derive a scoring function that evaluates the strength of a relation R between two entities h (head) and t (tail). Moreover, we show that marginal loss minimisation, a popular objective used in much prior work in KGE, follows naturally from the loglikelihood ratio maximisation under the probabilities estimated from the KGEs according to our theoretical relationship. We propose a learning objective motivated by the theoretical analysis to learn KGEs from a given knowledge graph. Using the derived objective, accurate KGEs are learnt from FB15K237 and WN18RR benchmark datasets, providing empirical evidence in support of the theory. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Question Answering', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Information Extraction', 'NLP Applications', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Generation', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.06550068408250809, 0.06210959330201149, 0.059091340750455856, 0.05772920325398445, 0.052551351487636566, 0.051664646714925766, 0.050686877220869064, 0.04692928120493889, 0.045254964381456375, 0.04416893422603607, 0.042717672884464264, 0.042448658496141434, 0.040653377771377563, 0.039487920701503754, 0.03939717635512352, 0.0389704629778862, 0.038456279784440994, 0.03478430584073067, 0.034609921276569366, 0.03391168266534805, 0.03203101456165314, 0.030630888417363167, 0.01621381565928459]}",0.06550068408250809,Dialogue and Interactive Systems,0.040653377771377563
Semantics: Lexical Semantics,"The Chinese Remainder Theorem for Compact, Task-Precise, Efficient and Secure Word Embeddings","The growing availability of powerful mobile devices and other edge devices, together with increasing regulatory and security concerns about the exchange of personal information across networks of these devices has challenged the Computational Linguistics community to develop methods that are at once fast, space-efficient, accurate and amenable to secure encoding schemes such as homomorphic encryption. Inspired by recent work that restricts floating point precision to speed up neural network training in hardware-based SIMD, we have developed a method for compressing word vector embeddings into integers using the Chinese Reminder Theorem that speeds up addition by up to 48.27% and at the same time compresses GloVe word embedding libraries by up to 25.86%. We explore the practicality of this simple approach by investigating the trade-off between precision and performance in two NLP tasks: compositional semantic relatedness and opinion target sentiment classification. We find that in both tasks, lowering floating point number precision results in negligible changes to performance.","{'sequence': 'The growing availability of powerful mobile devices and other edge devices, together with increasing regulatory and security concerns about the exchange of personal information across networks of these devices has challenged the Computational Linguistics community to develop methods that are at once fast, space-efficient, accurate and amenable to secure encoding schemes such as homomorphic encryption. Inspired by recent work that restricts floating point precision to speed up neural network training in hardware-based SIMD, we have developed a method for compressing word vector embeddings into integers using the Chinese Reminder Theorem that speeds up addition by up to 48.27% and at the same time compresses GloVe word embedding libraries by up to 25.86%. We explore the practicality of this simple approach by investigating the trade-off between precision and performance in two NLP tasks: compositional semantic relatedness and opinion target sentiment classification. We find that in both tasks, lowering floating point number precision results in negligible changes to performance.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Question Answering', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Summarization', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.21413511037826538, 0.19091573357582092, 0.05259223282337189, 0.051165737211704254, 0.04230872169137001, 0.042199671268463135, 0.04034031927585602, 0.03949839621782303, 0.0377730056643486, 0.03776388242840767, 0.03163038566708565, 0.02822154574096203, 0.02665085904300213, 0.026561619713902473, 0.022641362622380257, 0.02206999994814396, 0.021126622334122658, 0.018609700724482536, 0.016650911420583725, 0.014361179433763027, 0.008274650201201439, 0.00784114096313715, 0.006667260080575943]}",0.21413511037826538,NLP Applications,0.021126622334122658
Semantics: Lexical Semantics,Evaluating language models for the retrieval and categorization of lexical collocations,"Lexical collocations are idiosyncratic combinations of two syntactically bound lexical items (e.g., ""heavy rain"", ""take a step"" or ""undergo surgery""). Understanding their degree of compositionality and idiosyncrasy, as well their underlying semantics, is crucial for language learners, lexicographers and downstream NLP applications alike. In this paper we analyse a suite of language models for collocation understanding. We first construct a dataset of apparitions of lexical collocations in context, categorized into 16 representative semantic categories. Then, we perform two experiments: (1) unsupervised collocate retrieval, and (2) supervised collocation classification in context. We find that most models perform well in distinguishing light verb constructions, especially if the collocation's first argument acts as a subject, but often fail to distinguish, first, different syntactic structures within the same semantic category, and second, finer-grained categories which restrict the set of correct collocates 1 .","{'sequence': 'Lexical collocations are idiosyncratic combinations of two syntactically bound lexical items (e.g., ""heavy rain"", ""take a step"" or ""undergo surgery""). Understanding their degree of compositionality and idiosyncrasy, as well their underlying semantics, is crucial for language learners, lexicographers and downstream NLP applications alike. In this paper we analyse a suite of language models for collocation understanding. We first construct a dataset of apparitions of lexical collocations in context, categorized into 16 representative semantic categories. Then, we perform two experiments: (1) unsupervised collocate retrieval, and (2) supervised collocation classification in context. We find that most models perform well in distinguishing light verb constructions, especially if the collocation\'s first argument acts as a subject, but often fail to distinguish, first, different syntactic structures within the same semantic category, and second, finer-grained categories which restrict the set of correct collocates 1 .', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Summarization', 'Generation', 'Machine Translation and Multilinguality', 'Information Extraction', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14134801924228668, 0.10475865006446838, 0.07492667436599731, 0.0642332136631012, 0.056891076266765594, 0.05195402354001999, 0.05146007984876633, 0.04577730596065521, 0.03996969386935234, 0.03923018276691437, 0.03717717155814171, 0.037101175636053085, 0.03646726533770561, 0.034849125891923904, 0.03210708498954773, 0.022310813888907433, 0.022189481183886528, 0.021598350256681442, 0.020642567425966263, 0.019776038825511932, 0.017552724108099937, 0.016284719109535217, 0.011394519358873367]}",0.14134801924228668,NLP Applications,0.07492667436599731
Semantics: Lexical Semantics,Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings,"The number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes it applicable to any language. Code and data are publicly available 1 . * Equal contribution. CX handled the data, generated the rankings, and sampled the examples of section 7. AJPT computed the results, plots, and wrote the paper. Both authors participated in the design of the study.","{'sequence': 'The number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes it applicable to any language. Code and data are publicly available 1 . * Equal contribution. CX handled the data, generated the rankings, and sampled the examples of section 7. AJPT computed the results, plots, and wrote the paper. Both authors participated in the design of the study.', 'labels': ['Generation', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'NLP Applications', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Information Extraction', 'Machine Learning for NLP', 'Question Answering', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09011265635490417, 0.06974087655544281, 0.0685964897274971, 0.0652390643954277, 0.05656413361430168, 0.0550818108022213, 0.05103200674057007, 0.04467684403061867, 0.04150746390223503, 0.040670618414878845, 0.040000103414058685, 0.038374971598386765, 0.03781294822692871, 0.03769975155591965, 0.03692090883851051, 0.03593839332461357, 0.03499045968055725, 0.03480266034603119, 0.03425983712077141, 0.03342050686478615, 0.02077310159802437, 0.0169538427144289, 0.014830606058239937]}",0.09011265635490417,Generation,0.040670618414878845
Semantics: Lexical Semantics,Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings,"Word embedding is considered an essential factor in improving the performance of various Natural Language Processing (NLP) models. However, it is hardly applicable in realworld datasets as word embedding is generally studied with a well-refined corpus. Notably, in Hangeul (Korean writing system), which has a unique writing system, various kinds of Out-Of-Vocabulary (OOV) appear from typos. In this paper, we propose a robust Hangeul word embedding model against typos, while maintaining high performance. The proposed model utilizes a Convolutional Neural Network (CNN) architecture with a channel attention mechanism that learns to infer the original word embeddings. The model train with a dataset that consists of a mix of typos and correct words. To demonstrate the effectiveness of the proposed model, we conduct three kinds of intrinsic and extrinsic tasks. While the existing embedding models fail to maintain stable performance as the noise level increases, the proposed model shows stable performance.","{'sequence': 'Word embedding is considered an essential factor in improving the performance of various Natural Language Processing (NLP) models. However, it is hardly applicable in realworld datasets as word embedding is generally studied with a well-refined corpus. Notably, in Hangeul (Korean writing system), which has a unique writing system, various kinds of Out-Of-Vocabulary (OOV) appear from typos. In this paper, we propose a robust Hangeul word embedding model against typos, while maintaining high performance. The proposed model utilizes a Convolutional Neural Network (CNN) architecture with a channel attention mechanism that learns to infer the original word embeddings. The model train with a dataset that consists of a mix of typos and correct words. To demonstrate the effectiveness of the proposed model, we conduct three kinds of intrinsic and extrinsic tasks. While the existing embedding models fail to maintain stable performance as the noise level increases, the proposed model shows stable performance.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13935217261314392, 0.11878571659326553, 0.08768738806247711, 0.07472750544548035, 0.055788759142160416, 0.0554751493036747, 0.047545868903398514, 0.04697345569729805, 0.0453626811504364, 0.03861640766263008, 0.038263969123363495, 0.02946951612830162, 0.028814446181058884, 0.0253509022295475, 0.02486274763941765, 0.024174360558390617, 0.023390738293528557, 0.023183494806289673, 0.020805703476071358, 0.015212897211313248, 0.014947088435292244, 0.013125901110470295, 0.008083187974989414]}",0.13935217261314392,Machine Learning for NLP,0.028814446181058884
Semantics: Lexical Semantics,Effects of Pre- and Post-Processing on type-based Embeddings in Lexical Semantic Change Detection,"Lexical semantic change detection is a new and innovative research field. The optimal fine-tuning of models including pre-and postprocessing is largely unclear. We optimize existing models by (i) pre-training on large corpora and refining on diachronic target corpora tackling the notorious small data problem, and (ii) applying post-processing transformations that have been shown to improve performance on synchronic tasks. Our results provide a guide for the application and optimization of lexical semantic change detection models across various learning scenarios. * Authors contributed equally, and their ordering was determined randomly.","{'sequence': 'Lexical semantic change detection is a new and innovative research field. The optimal fine-tuning of models including pre-and postprocessing is largely unclear. We optimize existing models by (i) pre-training on large corpora and refining on diachronic target corpora tackling the notorious small data problem, and (ii) applying post-processing transformations that have been shown to improve performance on synchronic tasks. Our results provide a guide for the application and optimization of lexical semantic change detection models across various learning scenarios. * Authors contributed equally, and their ordering was determined randomly.', 'labels': ['Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Generation', 'Question Answering', 'Discourse and Pragmatics', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14501625299453735, 0.06020337715744972, 0.05860769376158714, 0.05762014538049698, 0.056117426604032516, 0.05320233106613159, 0.0519159771502018, 0.050096046179533005, 0.04553574323654175, 0.041487835347652435, 0.037117376923561096, 0.03496389091014862, 0.03489597141742706, 0.034663278609514236, 0.03421849384903908, 0.03336181864142418, 0.03306515887379646, 0.03261216729879379, 0.02852960303425789, 0.025952551513910294, 0.023708796128630638, 0.015190443024039268, 0.011917772702872753]}",0.14501625299453735,Semantics: Lexical Semantics,0.14501625299453735
Semantics: Lexical Semantics,Data Augmentation for Hypernymy Detection,"The automatic detection of hypernymy relationships represents a challenging problem in NLP. The successful application of stateof-the-art supervised approaches using distributed representations has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as small dogdog or small dog -animal, for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing dataset by leveraging linguistic resources such as Word-Net. Using an evaluation across 3 different datasets for hypernymy detection and 2 different vector spaces, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve classifier performance.","{'sequence': 'The automatic detection of hypernymy relationships represents a challenging problem in NLP. The successful application of stateof-the-art supervised approaches using distributed representations has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as small dogdog or small dog -animal, for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing dataset by leveraging linguistic resources such as Word-Net. Using an evaluation across 3 different datasets for hypernymy detection and 2 different vector spaces, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve classifier performance.', 'labels': ['NLP Applications', 'Resources and Evaluation', 'Machine Learning for NLP', 'Generation', 'Dialogue and Interactive Systems', 'Question Answering', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11171736568212509, 0.10610111057758331, 0.09862261265516281, 0.08619283139705658, 0.05268624797463417, 0.049988217651844025, 0.04422686994075775, 0.04287346079945564, 0.04152495041489601, 0.039892151951789856, 0.03822147846221924, 0.034458138048648834, 0.03361893817782402, 0.03139116242527962, 0.031117144972085953, 0.024621788412332535, 0.024282287806272507, 0.023533590137958527, 0.023493684828281403, 0.021487059071660042, 0.01701320894062519, 0.01299558486789465, 0.009940159507095814]}",0.11171736568212509,NLP Applications,0.04422686994075775
Semantics: Lexical Semantics,PolyLM: Learning about Polysemy through Language Modeling,"To avoid the ""meaning conflation deficiency"" of word embeddings, a number of models have aimed to embed individual word senses. These methods at one time performed well on tasks such as word sense induction (WSI), but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. However, sense embeddings and contextualization need not be mutually exclusive. We introduce PolyLM, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied. PolyLM is based on two underlying assumptions about word senses: firstly, that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring; and secondly, that for a given occurrence of a word, one of its senses tends to be much more plausible in the context than the others. We evaluate PolyLM on WSI, showing that it performs considerably better than previous sense embedding techniques, and matches the current stateof-the-art specialized WSI method despite having six times fewer parameters. Code and pre-trained models are available at https:// github.com/AlanAnsell/PolyLM.","{'sequence': 'To avoid the ""meaning conflation deficiency"" of word embeddings, a number of models have aimed to embed individual word senses. These methods at one time performed well on tasks such as word sense induction (WSI), but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. However, sense embeddings and contextualization need not be mutually exclusive. We introduce PolyLM, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied. PolyLM is based on two underlying assumptions about word senses: firstly, that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring; and secondly, that for a given occurrence of a word, one of its senses tends to be much more plausible in the context than the others. We evaluate PolyLM on WSI, showing that it performs considerably better than previous sense embedding techniques, and matches the current stateof-the-art specialized WSI method despite having six times fewer parameters. Code and pre-trained models are available at https:// github.com/AlanAnsell/PolyLM.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Generation', 'NLP Applications', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10843414813280106, 0.06401936709880829, 0.05472216010093689, 0.054704517126083374, 0.053788505494594574, 0.05227745324373245, 0.050225432962179184, 0.049805644899606705, 0.04686019569635391, 0.044820044189691544, 0.04141633212566376, 0.04070639982819557, 0.03956454619765282, 0.038208574056625366, 0.035268936306238174, 0.03476564958691597, 0.03378346934914589, 0.031538620591163635, 0.029225828126072884, 0.0288236141204834, 0.02614431641995907, 0.023589352145791054, 0.017306916415691376]}",0.10843414813280106,Resources and Evaluation,0.049805644899606705
Semantics: Lexical Semantics,Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC. In an extensive analysis, we investigate factors of success and failure of our three approaches. 1","{'sequence': 'Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC. In an extensive analysis, we investigate factors of success and failure of our three approaches. 1', 'labels': ['Semantics: Lexical Semantics', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.23021243512630463, 0.1602201759815216, 0.0774497389793396, 0.04706626385450363, 0.04494236409664154, 0.04298442602157593, 0.038636308163404465, 0.035875409841537476, 0.035804927349090576, 0.03127063065767288, 0.0300305113196373, 0.027478918433189392, 0.026262065395712852, 0.023766959086060524, 0.02273731306195259, 0.021486470475792885, 0.019547922536730766, 0.01803012751042843, 0.015858320519328117, 0.015485129319131374, 0.013153471052646637, 0.012036100961267948, 0.009663940407335758]}",0.23021243512630463,Semantics: Lexical Semantics,0.23021243512630463
Semantics: Lexical Semantics,Cross-lingual Entity Alignment with Incidental Supervision,"Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose an incidentally supervised model, JEANS , which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a selflearning based alignment learning process to iteratively induce the matching of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs. 1","{'sequence': 'Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose an incidentally supervised model, JEANS , which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a selflearning based alignment learning process to iteratively induce the matching of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs. 1', 'labels': ['Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Summarization', 'Ethics and NLP', 'Generation', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08299572765827179, 0.0811547264456749, 0.07848824560642242, 0.06014770269393921, 0.057804737240076065, 0.056678406894207, 0.05146872624754906, 0.048119038343429565, 0.04411078244447708, 0.04220122471451759, 0.04181365668773651, 0.041760604828596115, 0.040215469896793365, 0.03732174634933472, 0.03672657534480095, 0.03633652627468109, 0.03442875295877457, 0.03156845644116402, 0.02821153961122036, 0.022242853417992592, 0.017538513988256454, 0.014505177736282349, 0.01416088081896305]}",0.08299572765827179,Resources and Evaluation,0.048119038343429565
Semantics: Lexical Semantics,Framing Word Sense Disambiguation as a Multi-Label Problem for Model-Agnostic Knowledge Integration,"Recent studies treat Word Sense Disambiguation (WSD) as a single-label classification problem in which one is asked to choose only the best-fitting sense for a target word, given its context. However, gold data labelled by expert annotators suggest that maximizing the probability of a single sense may not be the most suitable training objective for WSD, especially if the sense inventory of choice is finegrained. In this paper, we approach WSD as a multi-label classification problem in which multiple senses can be assigned to each target word. Not only does our simple method bear a closer resemblance to how human annotators disambiguate text, but it can also be extended seamlessly to exploit structured knowledge from semantic networks to achieve stateof-the-art results in English all-words WSD.","{'sequence': 'Recent studies treat Word Sense Disambiguation (WSD) as a single-label classification problem in which one is asked to choose only the best-fitting sense for a target word, given its context. However, gold data labelled by expert annotators suggest that maximizing the probability of a single sense may not be the most suitable training objective for WSD, especially if the sense inventory of choice is finegrained. In this paper, we approach WSD as a multi-label classification problem in which multiple senses can be assigned to each target word. Not only does our simple method bear a closer resemblance to how human annotators disambiguate text, but it can also be extended seamlessly to exploit structured knowledge from semantic networks to achieve stateof-the-art results in English all-words WSD.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Resources and Evaluation', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Generation', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics'], 'scores': [0.10892500728368759, 0.07472072541713715, 0.06677483767271042, 0.0663943886756897, 0.06585900485515594, 0.055760666728019714, 0.0557156465947628, 0.05242112651467323, 0.045588087290525436, 0.04539327695965767, 0.04448870196938515, 0.038698967546224594, 0.038345817476511, 0.03502599522471428, 0.03197471797466278, 0.02743411250412464, 0.02735067531466484, 0.024395061656832695, 0.023872291669249535, 0.020060408860445023, 0.019937777891755104, 0.018900368362665176, 0.011962443590164185]}",0.10892500728368759,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.06677483767271042
Semantics: Lexical Semantics,Debiasing Pre-trained Contextualised Embeddings,"In comparison to the numerous debiasing methods proposed for the static noncontextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token-or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pretrained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.","{'sequence': 'In comparison to the numerous debiasing methods proposed for the static noncontextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token-or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pretrained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.', 'labels': ['Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Generation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0738055631518364, 0.06638175994157791, 0.06600647419691086, 0.06263324618339539, 0.05459624156355858, 0.05195517838001251, 0.048594001680612564, 0.047684818506240845, 0.04666132107377052, 0.046368859708309174, 0.043568067252635956, 0.043552134186029434, 0.04006679728627205, 0.03786260262131691, 0.03742220625281334, 0.0364144966006279, 0.03627411276102066, 0.03511321544647217, 0.035053499042987823, 0.03157254308462143, 0.023686690255999565, 0.01901288330554962, 0.015713386237621307]}",0.0738055631518364,Computational Social Science and Social Media,0.03742220625281334
Semantics: Lexical Semantics,Exploiting Definitions for Frame Identification,Frame identification is one of the key challenges for frame-semantic parsing. The goal of this task is to determine which frame best captures the meaning of a target word or phrase in a sentence. We present a new model for frame identification that uses a pre-trained transformer model to generate representations for frames and lexical units (senses) using their formal definitions in FrameNet. Our frame identification model assesses the suitability of a frame for a target word in a sentence based on the semantic coherence of their meanings. We evaluate our model on three data sets and show that it consistently achieves better performance than previous systems.,"{'sequence': 'Frame identification is one of the key challenges for frame-semantic parsing. The goal of this task is to determine which frame best captures the meaning of a target word or phrase in a sentence. We present a new model for frame identification that uses a pre-trained transformer model to generate representations for frames and lexical units (senses) using their formal definitions in FrameNet. Our frame identification model assesses the suitability of a frame for a target word in a sentence based on the semantic coherence of their meanings. We evaluate our model on three data sets and show that it consistently achieves better performance than previous systems.', 'labels': ['Resources and Evaluation', 'Generation', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Question Answering', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Summarization', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10278011858463287, 0.09353534132242203, 0.08027699589729309, 0.07049258053302765, 0.06175301969051361, 0.060544900596141815, 0.0536654032766819, 0.04637928307056427, 0.0462263822555542, 0.04209665209054947, 0.041426803916692734, 0.0371217243373394, 0.03679385781288147, 0.03426729142665863, 0.03229593113064766, 0.032108407467603683, 0.025073254480957985, 0.025037406012415886, 0.021978091448545456, 0.01684296689927578, 0.016633877530694008, 0.013589701615273952, 0.00908003281801939]}",0.10278011858463287,Resources and Evaluation,0.08027699589729309
Semantics: Lexical Semantics,Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models,"Recent progress in pretraining language models on large corpora has resulted in large performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with 'fill in the blank' style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definition Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future.","{'sequence': ""Recent progress in pretraining language models on large corpora has resulted in large performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with 'fill in the blank' style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definition Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future."", 'labels': ['Question Answering', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Resources and Evaluation', 'Ethics and NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Generation', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Information Extraction', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14199258387088776, 0.10846979171037674, 0.06880688667297363, 0.0631728544831276, 0.0534648634493351, 0.05129332095384598, 0.04945463314652443, 0.04938903823494911, 0.04098452627658844, 0.03810379281640053, 0.034613706171512604, 0.0345209576189518, 0.03371238708496094, 0.03131428733468056, 0.029195822775363922, 0.028819315135478973, 0.026485661044716835, 0.024272549897432327, 0.02360321581363678, 0.02317793294787407, 0.0217683594673872, 0.014148580841720104, 0.009234904311597347]}",0.14199258387088776,Question Answering,0.03131428733468056
Semantics: Lexical Semantics,A Unified Feature Representation for Lexical Connotations,"Ideological attitudes and stance are often expressed through subtle meanings of words and phrases. Understanding these connotations is critical to recognizing the cultural and emotional perspectives of the speaker. In this paper, we use distant labeling to create a new lexical resource representing connotation aspects for nouns and adjectives. Our analysis shows that it aligns well with human judgments. Additionally, we present a method for creating lexical representations that capture connotations within the embedding space and show that using the embeddings provides a statistically significant improvement on the task of stance detection when data is limited.","{'sequence': 'Ideological attitudes and stance are often expressed through subtle meanings of words and phrases. Understanding these connotations is critical to recognizing the cultural and emotional perspectives of the speaker. In this paper, we use distant labeling to create a new lexical resource representing connotation aspects for nouns and adjectives. Our analysis shows that it aligns well with human judgments. Additionally, we present a method for creating lexical representations that capture connotations within the embedding space and show that using the embeddings provides a statistically significant improvement on the task of stance detection when data is limited.', 'labels': ['Speech and Multimodality', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'NLP Applications', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Generation', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.08901426196098328, 0.08670929819345474, 0.07082769274711609, 0.06523746252059937, 0.05728176608681679, 0.05433475226163864, 0.05222911760210991, 0.05222852900624275, 0.05145764350891113, 0.05015183985233307, 0.04191165789961815, 0.040416229516267776, 0.03873405605554581, 0.03618711233139038, 0.035845860838890076, 0.0306402575224638, 0.030366554856300354, 0.02576785907149315, 0.019479745998978615, 0.019198037683963776, 0.018282661214470863, 0.018013624474406242, 0.015683943405747414]}",0.08901426196098328,Speech and Multimodality,0.08670929819345474
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Semantic Parsing of Disfluent Speech,"Speech disfluencies are prevalent in spontaneous speech. The rising popularity of voice assistants presents a growing need to handle naturally occurring disfluencies. Semantic parsing is a key component for understanding user utterances in voice assistants, yet most semantic parsing research to date focuses on written text. In this paper, we investigate semantic parsing of disfluent speech with the ATIS dataset. We find that a state-of-the-art semantic parser does not seamlessly handle disfluencies. We experiment with adding real and synthetic disfluencies at training time and find that adding synthetic disfluencies not only improves model performance by up to 39% but can also outperform adding real disfluencies in the ATIS dataset.","{'sequence': 'Speech disfluencies are prevalent in spontaneous speech. The rising popularity of voice assistants presents a growing need to handle naturally occurring disfluencies. Semantic parsing is a key component for understanding user utterances in voice assistants, yet most semantic parsing research to date focuses on written text. In this paper, we investigate semantic parsing of disfluent speech with the ATIS dataset. We find that a state-of-the-art semantic parser does not seamlessly handle disfluencies. We experiment with adding real and synthetic disfluencies at training time and find that adding synthetic disfluencies not only improves model performance by up to 39% but can also outperform adding real disfluencies in the ATIS dataset.', 'labels': ['Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Generation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11110249906778336, 0.0677027478814125, 0.06578455865383148, 0.06371837854385376, 0.06320217251777649, 0.05051957815885544, 0.04964194819331169, 0.04798015207052231, 0.04681900888681412, 0.04497308284044266, 0.043768588453531265, 0.04189400374889374, 0.04103084281086922, 0.040650323033332825, 0.036247700452804565, 0.035445041954517365, 0.029569201171398163, 0.02402358688414097, 0.02201009728014469, 0.02045758068561554, 0.0203015748411417, 0.017796000465750694, 0.015361363999545574]}",0.11110249906778336,Speech and Multimodality,0.04964194819331169
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation,"Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers' representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?","{'sequence': ""Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers' representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?"", 'labels': ['Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Information Extraction', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.26252469420433044, 0.059887561947107315, 0.056420959532260895, 0.053532980382442474, 0.052435874938964844, 0.04889072850346565, 0.043617356568574905, 0.041256438940763474, 0.040100354701280594, 0.03777342662215233, 0.0371447317302227, 0.030728906393051147, 0.029927561059594154, 0.029152007773518562, 0.02708142064511776, 0.021386979147791862, 0.02033575251698494, 0.019732467830181122, 0.01969211734831333, 0.019016984850168228, 0.018686218187212944, 0.017938094213604927, 0.012736509554088116]}",0.26252469420433044,Question Answering,0.040100354701280594
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Project-then-Transfer: Effective Two-stage Cross-lingual Transfer for Semantic Dependency Parsing,"This paper describes the first report on crosslingual transfer for semantic dependency parsing. We present the insight that there are two different kinds of cross-linguality, namely surface level and semantic level, and try to capture both kinds of cross-linguality by combining annotation projection and model transfer of pre-trained language models. Our experiments showed that the performance of our graph-based semantic dependency parser almost achieved the approximated upper bound.","{'sequence': 'This paper describes the first report on crosslingual transfer for semantic dependency parsing. We present the insight that there are two different kinds of cross-linguality, namely surface level and semantic level, and try to capture both kinds of cross-linguality by combining annotation projection and model transfer of pre-trained language models. Our experiments showed that the performance of our graph-based semantic dependency parser almost achieved the approximated upper bound.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Summarization', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0894155502319336, 0.07000253349542618, 0.06560416519641876, 0.06558848172426224, 0.06522718816995621, 0.06251371651887894, 0.06033484265208244, 0.04898819327354431, 0.0477687232196331, 0.04532574117183685, 0.043147116899490356, 0.036752332001924515, 0.035598643124103546, 0.03219708055257797, 0.03133038431406021, 0.02791151963174343, 0.02786230482161045, 0.02737509272992611, 0.02735181525349617, 0.026806987822055817, 0.024132410064339638, 0.021293925121426582, 0.017471201717853546]}",0.0894155502319336,"Syntax: Tagging, Chunking and Parsing",0.06251371651887894
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Elastic weight consolidation for better bias inoculation,"The biases present in training datasets have been shown to affect models for sentence pair classification tasks such as natural language inference (NLI) and fact verification. While fine-tuning models on additional data has been used to mitigate them, a common issue is that of catastrophic forgetting of the original training dataset. In this paper, we show that elastic weight consolidation (EWC) allows finetuning of models to mitigate biases while being less susceptible to catastrophic forgetting. In our evaluation on fact verification and NLI stress tests, we show that fine-tuning with EWC dominates standard fine-tuning, yielding models with lower levels of forgetting on the original (biased) dataset for equivalent gains in accuracy on the fine-tuning (unbiased) dataset.","{'sequence': 'The biases present in training datasets have been shown to affect models for sentence pair classification tasks such as natural language inference (NLI) and fact verification. While fine-tuning models on additional data has been used to mitigate them, a common issue is that of catastrophic forgetting of the original training dataset. In this paper, we show that elastic weight consolidation (EWC) allows finetuning of models to mitigate biases while being less susceptible to catastrophic forgetting. In our evaluation on fact verification and NLI stress tests, we show that fine-tuning with EWC dominates standard fine-tuning, yielding models with lower levels of forgetting on the original (biased) dataset for equivalent gains in accuracy on the fine-tuning (unbiased) dataset.', 'labels': ['Resources and Evaluation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Computational Social Science and Social Media', 'Question Answering', 'Information Extraction', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.155198872089386, 0.12819503247737885, 0.09508302807807922, 0.08933937549591064, 0.07228145003318787, 0.06482316553592682, 0.04902970790863037, 0.043578676879405975, 0.03194653242826462, 0.03164329379796982, 0.0313558466732502, 0.027898987755179405, 0.025442419573664665, 0.02269507385790348, 0.02220103144645691, 0.01919146068394184, 0.016507167369127274, 0.016473913565278053, 0.01608012430369854, 0.013338437303900719, 0.010893388651311398, 0.008481046184897423, 0.008321919478476048]}",0.155198872089386,Resources and Evaluation,0.04902970790863037
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",STAR: Cross-modal [STA]tement [R]epresentation for selecting relevant mathematical premises,"Mathematical statements written in natural language are usually composed of two different modalities: mathematical elements and natural language. These two modalities have several distinct linguistic and semantic properties. State-of-the-art representation techniques have demonstrated an inability in capturing such an entangled style of discourse. In this work, we propose STAR, a model that uses crossmodal attention to learn how to represent mathematical text for the task of Natural Language Premise Selection. This task uses conjectures written in both natural and mathematical language to recommend premises that most likely will be relevant to prove a particular statement. We found that STAR not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-ofthe-art models.","{'sequence': 'Mathematical statements written in natural language are usually composed of two different modalities: mathematical elements and natural language. These two modalities have several distinct linguistic and semantic properties. State-of-the-art representation techniques have demonstrated an inability in capturing such an entangled style of discourse. In this work, we propose STAR, a model that uses crossmodal attention to learn how to represent mathematical text for the task of Natural Language Premise Selection. This task uses conjectures written in both natural and mathematical language to recommend premises that most likely will be relevant to prove a particular statement. We found that STAR not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-ofthe-art models.', 'labels': ['NLP Applications', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Generation', 'Information Extraction', 'Question Answering', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10552969574928284, 0.09957317262887955, 0.09115210175514221, 0.06576889753341675, 0.0595543347299099, 0.056862104684114456, 0.050979360938072205, 0.04617850482463837, 0.040017157793045044, 0.03957179933786392, 0.039145056158304214, 0.03872261196374893, 0.03797486424446106, 0.034318938851356506, 0.027820704504847527, 0.025388577952980995, 0.02383728325366974, 0.022794561460614204, 0.022516639903187752, 0.019358975812792778, 0.01924186199903488, 0.019013993442058563, 0.014678836800158024]}",0.10552969574928284,NLP Applications,0.0595543347299099
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Lifelong Knowledge-Enriched Social Event Representation Learning,"The ability of humans to symbolically represent social events and situations is crucial for various interactions in everyday life. Several studies in cognitive psychology have established the role of mental state attributions in effectively representing variable aspects of these social events. In the past, NLP research on learning event representations often focuses on construing syntactic and semantic information from language. However, they fail to consider the importance of pragmatic aspects and the need to consistently update new social situational information without forgetting the accumulated experiences. In this work, we propose a representation learning framework to directly address these shortcomings by integrating social commonsense knowledge with recent advancements in the space of lifelong language learning. First, we investigate methods to incorporate pragmatic aspects into our social event embeddings by leveraging social commonsense knowledge. Next, we introduce continual learning strategies that allow for incremental consolidation of new knowledge while retaining and promoting efficient usage of prior knowledge. Experimental results on event similarity, reasoning, and paraphrase detection tasks prove the efficacy of our social event embeddings.","{'sequence': 'The ability of humans to symbolically represent social events and situations is crucial for various interactions in everyday life. Several studies in cognitive psychology have established the role of mental state attributions in effectively representing variable aspects of these social events. In the past, NLP research on learning event representations often focuses on construing syntactic and semantic information from language. However, they fail to consider the importance of pragmatic aspects and the need to consistently update new social situational information without forgetting the accumulated experiences. In this work, we propose a representation learning framework to directly address these shortcomings by integrating social commonsense knowledge with recent advancements in the space of lifelong language learning. First, we investigate methods to incorporate pragmatic aspects into our social event embeddings by leveraging social commonsense knowledge. Next, we introduce continual learning strategies that allow for incremental consolidation of new knowledge while retaining and promoting efficient usage of prior knowledge. Experimental results on event similarity, reasoning, and paraphrase detection tasks prove the efficacy of our social event embeddings.', 'labels': ['Discourse and Pragmatics', 'NLP Applications', 'Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.10658123344182968, 0.07918185740709305, 0.06562843918800354, 0.06502556055784225, 0.06019449234008789, 0.05670453608036041, 0.051122959703207016, 0.04803057387471199, 0.04539420083165169, 0.04284699633717537, 0.04124535992741585, 0.037044934928417206, 0.036881230771541595, 0.03608177974820137, 0.032174136489629745, 0.0314020998775959, 0.028533659875392914, 0.027446124702692032, 0.02654348313808441, 0.024853399023413658, 0.02104519121348858, 0.01840677671134472, 0.017631027847528458]}",0.10658123344182968,Discourse and Pragmatics,0.03608177974820137
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Increasing Robustness to Spurious Correlations using Forgettable Examples,"Neural NLP models tend to rely on spurious correlations between labels and input features to perform their tasks. Minority examples, i.e., examples that contradict the spurious correlations present in the majority of data points, have been shown to increase the out-ofdistribution generalization of pre-trained language models. In this paper, we first propose using example forgetting to find minority examples without prior knowledge of the spurious correlations present in the dataset. Forgettable examples are instances either learned and then forgotten during training or never learned. We empirically show how these examples are related to minorities in our training sets. Then, we introduce a new approach to robustify models by fine-tuning our models twice, first on the full training data and second on the minorities only. We obtain substantial improvements in out-of-distribution generalization when applying our approach to the MNLI, QQP, and FEVER datasets.","{'sequence': 'Neural NLP models tend to rely on spurious correlations between labels and input features to perform their tasks. Minority examples, i.e., examples that contradict the spurious correlations present in the majority of data points, have been shown to increase the out-ofdistribution generalization of pre-trained language models. In this paper, we first propose using example forgetting to find minority examples without prior knowledge of the spurious correlations present in the dataset. Forgettable examples are instances either learned and then forgotten during training or never learned. We empirically show how these examples are related to minorities in our training sets. Then, we introduce a new approach to robustify models by fine-tuning our models twice, first on the full training data and second on the minorities only. We obtain substantial improvements in out-of-distribution generalization when applying our approach to the MNLI, QQP, and FEVER datasets.', 'labels': ['NLP Applications', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Question Answering', 'Resources and Evaluation', 'Information Extraction', 'Information Retrieval and Text Mining', 'Generation', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09138377010822296, 0.07953598350286484, 0.07768101990222931, 0.0775703638792038, 0.07497323304414749, 0.060504887253046036, 0.0449044294655323, 0.04033562168478966, 0.03922000154852867, 0.038391344249248505, 0.035471804440021515, 0.03541461005806923, 0.03521948307752609, 0.034217845648527145, 0.032804083079099655, 0.031548626720905304, 0.030421430245041847, 0.028704950585961342, 0.02845177613198757, 0.025581294670701027, 0.025479545816779137, 0.01763131655752659, 0.014552549459040165]}",0.09138377010822296,NLP Applications,0.031548626720905304
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",On Robustness of Neural Semantic Parsers,"Semantic parsing maps natural language (NL) utterances into logical forms (LFs), which underpins many advanced NLP problems. Semantic parsers gain performance boosts with deep neural networks, but inherit vulnerabilities against adversarial examples. In this paper, we provide the empirical study on the robustness of semantic parsers in the presence of adversarial attacks. Formally, adversaries of semantic parsing are considered to be the perturbed utterance-LF pairs, whose utterances have exactly the same meanings as the original ones. A scalable methodology is proposed to construct robustness test sets based on existing benchmark corpora. Our results answered five research questions in measuring the sateof-the-art parsers' performance on robustness test sets, and evaluating the effect of data augmentation.","{'sequence': ""Semantic parsing maps natural language (NL) utterances into logical forms (LFs), which underpins many advanced NLP problems. Semantic parsers gain performance boosts with deep neural networks, but inherit vulnerabilities against adversarial examples. In this paper, we provide the empirical study on the robustness of semantic parsers in the presence of adversarial attacks. Formally, adversaries of semantic parsing are considered to be the perturbed utterance-LF pairs, whose utterances have exactly the same meanings as the original ones. A scalable methodology is proposed to construct robustness test sets based on existing benchmark corpora. Our results answered five research questions in measuring the sateof-the-art parsers' performance on robustness test sets, and evaluating the effect of data augmentation."", 'labels': ['NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Machine Translation and Multilinguality', 'Summarization', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Generation'], 'scores': [0.07918743789196014, 0.07318484783172607, 0.06378496438264847, 0.06315067410469055, 0.06289459019899368, 0.0618114247918129, 0.057429682463407516, 0.055514849722385406, 0.05106406658887863, 0.04382108896970749, 0.03778771683573723, 0.037580706179142, 0.035063061863183975, 0.032770417630672455, 0.032759059220552444, 0.03230244666337967, 0.03153381869196892, 0.030280303210020065, 0.02820170857012272, 0.026258794590830803, 0.025689680129289627, 0.019408341497182846, 0.018520202487707138]}",0.07918743789196014,NLP Applications,0.07318484783172607
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference,"Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with ""task descriptions"" in natural language (e.g., Radford et al., 2019) . While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in lowresource settings by a large margin. 1","{'sequence': 'Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with ""task descriptions"" in natural language (e.g., Radford et al., 2019) . While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in lowresource settings by a large margin. 1', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Generation', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.25280413031578064, 0.14846332371234894, 0.08134671300649643, 0.0543312132358551, 0.03787461668252945, 0.034231506288051605, 0.032457225024700165, 0.032021019607782364, 0.03120073489844799, 0.030448639765381813, 0.027153439819812775, 0.025241877883672714, 0.025133837014436722, 0.024855367839336395, 0.0243233609944582, 0.021136382594704628, 0.019942253828048706, 0.018950922414660454, 0.01812310330569744, 0.01673400029540062, 0.016398673877120018, 0.01478442270308733, 0.01204315759241581]}",0.25280413031578064,Machine Learning for NLP,0.019942253828048706
"Semantics: Sentence-level Semantics, Textual Inference and Other areas","Multiple Tasks Integration: Tagging, Syntactic and Semantic Parsing as a Single Task","Departing from both sequential pipelines and monotask systems, we propose Multiple Tasks Integration (MTI), a multitask paradigm orthogonal to weight sharing. The essence of MTI is to process the input iteratively but concurrently at multiple levels of analysis, where each decision is based on all of the structures that are already inferred and free from usual ordering constraints. We illustrate MTI with a system that performs part-of-speech tagging, syntactic dependency parsing and semantic dependency parsing. We observe that both the use of reinforcement learning and the release from sequential constraints are beneficial to the quality of the syntactic and semantic parses. We also observe that our model adopts an easy-first strategy that consists, on average, of predicting shorter dependencies before longer ones, but that syntax is not always tackled before semantics.","{'sequence': 'Departing from both sequential pipelines and monotask systems, we propose Multiple Tasks Integration (MTI), a multitask paradigm orthogonal to weight sharing. The essence of MTI is to process the input iteratively but concurrently at multiple levels of analysis, where each decision is based on all of the structures that are already inferred and free from usual ordering constraints. We illustrate MTI with a system that performs part-of-speech tagging, syntactic dependency parsing and semantic dependency parsing. We observe that both the use of reinforcement learning and the release from sequential constraints are beneficial to the quality of the syntactic and semantic parses. We also observe that our model adopts an easy-first strategy that consists, on average, of predicting shorter dependencies before longer ones, but that syntax is not always tackled before semantics.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Information Extraction', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Ethics and NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.23339733481407166, 0.15231633186340332, 0.05997975915670395, 0.052056897431612015, 0.051804717630147934, 0.048986826092004776, 0.04472506418824196, 0.032800354063510895, 0.03112088516354561, 0.02647441439330578, 0.024977754801511765, 0.02420417033135891, 0.02366984821856022, 0.023128099739551544, 0.022898582741618156, 0.021276777610182762, 0.020355990156531334, 0.019550524652004242, 0.01939837448298931, 0.019085198640823364, 0.018436120823025703, 0.017430922016501427, 0.01192502025514841]}",0.23339733481407166,"Syntax: Tagging, Chunking and Parsing",0.051804717630147934
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Structural Encoding and Pre-training Matter: Adapting BERT for Table-Based Fact Verification,"Growing concern with online misinformation has encouraged NLP research on fact verification. Since writers often base their assertions on structured data, we focus here on verifying textual statements given evidence in tables. Starting from the Table Parsing (TAPAS) model developed for question answering (Herzig et al., 2020) , we find that modeling table structure improves a language model pre-trained on unstructured text. Pre-training language models on English Wikipedia table data further improves performance. Pre-training on a question answering task with column-level cell rank information achieves the best performance. With improved pre-training and cell embeddings, this approach outperforms the state-of-the-art Numerically-aware Graph Neural Network table fact verification model (GNN-TabFact), increasing statement classification accuracy from 72.2% to 73.9% even without modeling numerical information. Incorporating numerical information with cell rankings and pre-training on a question-answering task increases accuracy to 76%. We further analyze accuracy on statements implicating single rows or multiple rows and columns of tables, on different numerical reasoning subtasks, and on generalizing to detecting errors in statements derived from the ToTTo table-totext generation dataset.","{'sequence': 'Growing concern with online misinformation has encouraged NLP research on fact verification. Since writers often base their assertions on structured data, we focus here on verifying textual statements given evidence in tables. Starting from the Table Parsing (TAPAS) model developed for question answering (Herzig et al., 2020) , we find that modeling table structure improves a language model pre-trained on unstructured text. Pre-training language models on English Wikipedia table data further improves performance. Pre-training on a question answering task with column-level cell rank information achieves the best performance. With improved pre-training and cell embeddings, this approach outperforms the state-of-the-art Numerically-aware Graph Neural Network table fact verification model (GNN-TabFact), increasing statement classification accuracy from 72.2% to 73.9% even without modeling numerical information. Incorporating numerical information with cell rankings and pre-training on a question-answering task increases accuracy to 76%. We further analyze accuracy on statements implicating single rows or multiple rows and columns of tables, on different numerical reasoning subtasks, and on generalizing to detecting errors in statements derived from the ToTTo table-totext generation dataset.', 'labels': ['Question Answering', 'NLP Applications', 'Generation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Ethics and NLP', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.23559114336967468, 0.09163207560777664, 0.08007915318012238, 0.06706693023443222, 0.05803319066762924, 0.04516374319791794, 0.04226730391383171, 0.04115387424826622, 0.03658754751086235, 0.035957980901002884, 0.03076786920428276, 0.028176041319966316, 0.027080416679382324, 0.025980845093727112, 0.02510887198150158, 0.024395612999796867, 0.021356288343667984, 0.020764077082276344, 0.016708066686987877, 0.013941935263574123, 0.011715314351022243, 0.011642917059361935, 0.008828913792967796]}",0.23559114336967468,Question Answering,0.020764077082276344
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Few-Shot Semantic Parsing for New Predicates,"In this work, we investigate the problems of semantic parsing in a few-shot learning setting. In this setting, we are provided with k utterance-logical form pairs per new predicate. The state-of-the-art neural semantic parsers achieve less than 25% accuracy on benchmark datasets when k = 1. To tackle this problem, we proposed to i) apply a designated metalearning method to train the model; ii) regularize attention scores with alignment statistics; iii) apply a smoothing technique in pretraining. As a result, our method consistently outperforms all the baselines in both one and two-shot settings.","{'sequence': 'In this work, we investigate the problems of semantic parsing in a few-shot learning setting. In this setting, we are provided with k utterance-logical form pairs per new predicate. The state-of-the-art neural semantic parsers achieve less than 25% accuracy on benchmark datasets when k = 1. To tackle this problem, we proposed to i) apply a designated metalearning method to train the model; ii) regularize attention scores with alignment statistics; iii) apply a smoothing technique in pretraining. As a result, our method consistently outperforms all the baselines in both one and two-shot settings.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Generation', 'Machine Learning for NLP', 'Question Answering', 'Speech and Multimodality', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Information Extraction', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.08560235798358917, 0.08476588129997253, 0.07624586671590805, 0.06896360963582993, 0.061520859599113464, 0.058322519063949585, 0.04872632026672363, 0.047174885869026184, 0.04702382534742355, 0.04635043442249298, 0.04582417383790016, 0.04158005490899086, 0.04083770513534546, 0.037775080651044846, 0.037690792232751846, 0.03293657675385475, 0.02679261565208435, 0.02347181737422943, 0.021592969074845314, 0.02030826359987259, 0.017243027687072754, 0.015558291226625443, 0.01369206327944994]}",0.08560235798358917,"Syntax: Tagging, Chunking and Parsing",0.08476588129997253
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Exploring Transitivity in Neural NLI Models through Veridicality,"Despite the recent success of deep neural networks in natural language processing, the extent to which they can demonstrate human-like generalization capacities for natural language understanding remains unclear. We explore this issue in the domain of natural language inference (NLI), focusing on the transitivity of inference relations, a fundamental property for systematically drawing inferences. A model capturing transitivity can compose basic inference patterns and draw new inferences. We introduce an analysis method using synthetic and naturalistic NLI datasets involving clauseembedding verbs to evaluate whether models can perform transitivity inferences composed of veridical inferences and arbitrary inference types. We find that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples. The data and code for our analysis are publicly available at https://github.com/ verypluming/transitivity.","{'sequence': 'Despite the recent success of deep neural networks in natural language processing, the extent to which they can demonstrate human-like generalization capacities for natural language understanding remains unclear. We explore this issue in the domain of natural language inference (NLI), focusing on the transitivity of inference relations, a fundamental property for systematically drawing inferences. A model capturing transitivity can compose basic inference patterns and draw new inferences. We introduce an analysis method using synthetic and naturalistic NLI datasets involving clauseembedding verbs to evaluate whether models can perform transitivity inferences composed of veridical inferences and arbitrary inference types. We find that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples. The data and code for our analysis are publicly available at https://github.com/ verypluming/transitivity.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Question Answering', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Summarization', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Generation', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1200273409485817, 0.08843784034252167, 0.06954790651798248, 0.0666203424334526, 0.058503296226263046, 0.052193012088537216, 0.05149483680725098, 0.050910964608192444, 0.045330796390771866, 0.042205456644296646, 0.04093306139111519, 0.039636824280023575, 0.036307405680418015, 0.03568166866898537, 0.030810697004199028, 0.030286123976111412, 0.02795260399580002, 0.026029737666249275, 0.023230068385601044, 0.021795149892568588, 0.017415283247828484, 0.014451786875724792, 0.010197920724749565]}",0.1200273409485817,Interpretability and Analysis of Models for NLP,0.042205456644296646
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Adversarial Stylometry in the Wild: Transferable Lexical Substitution Attacks on Author Profiling,"Written language contains stylistic cues that can be exploited to automatically infer a variety of potentially sensitive author information. Adversarial stylometry intends to attack such models by rewriting an author's text. Our research proposes several components to facilitate deployment of these adversarial attacks in the wild, where neither data nor target models are accessible. We introduce a transformerbased extension of a lexical replacement attack, and show it achieves high transferability when trained on a weakly labeled corpusdecreasing target model performance below chance. While not completely inconspicuous, our more successful attacks also prove notably less detectable by humans. Our framework therefore provides a promising direction for future privacy-preserving adversarial attacks.","{'sequence': ""Written language contains stylistic cues that can be exploited to automatically infer a variety of potentially sensitive author information. Adversarial stylometry intends to attack such models by rewriting an author's text. Our research proposes several components to facilitate deployment of these adversarial attacks in the wild, where neither data nor target models are accessible. We introduce a transformerbased extension of a lexical replacement attack, and show it achieves high transferability when trained on a weakly labeled corpusdecreasing target model performance below chance. While not completely inconspicuous, our more successful attacks also prove notably less detectable by humans. Our framework therefore provides a promising direction for future privacy-preserving adversarial attacks."", 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Information Retrieval and Text Mining', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Generation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.07766872644424438, 0.07419456541538239, 0.06871344149112701, 0.06485258042812347, 0.056944072246551514, 0.055134233087301254, 0.05117898806929588, 0.05044470354914665, 0.04834888502955437, 0.04734353348612785, 0.0414540134370327, 0.041282523423433304, 0.03867490962147713, 0.03830839321017265, 0.031306397169828415, 0.03062947653234005, 0.029815085232257843, 0.02763534151017666, 0.027337387204170227, 0.02643093466758728, 0.026355264708399773, 0.023314865306019783, 0.022631773725152016]}",0.07766872644424438,Dialogue and Interactive Systems,0.026355264708399773
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Exploiting Emojis for Abusive Language Detection,"We propose to use abusive emojis, such as the middle finger or face vomiting, as a proxy for learning a lexicon of abusive words. Since it represents extralinguistic information, a single emoji can co-occur with different forms of explicitly abusive utterances. We show that our approach generates a lexicon that offers the same performance in cross-domain classification of abusive microposts as the most advanced lexicon induction method. Such an approach, in contrast, is dependent on manually annotated seed words and expensive lexical resources for bootstrapping (e.g. WordNet). We demonstrate that the same emojis can also be effectively used in languages other than English. Finally, we also show that emojis can be exploited for classifying mentions of ambiguous words, such as fuck and bitch, into generally abusive and just profane usages.","{'sequence': 'We propose to use abusive emojis, such as the middle finger or face vomiting, as a proxy for learning a lexicon of abusive words. Since it represents extralinguistic information, a single emoji can co-occur with different forms of explicitly abusive utterances. We show that our approach generates a lexicon that offers the same performance in cross-domain classification of abusive microposts as the most advanced lexicon induction method. Such an approach, in contrast, is dependent on manually annotated seed words and expensive lexical resources for bootstrapping (e.g. WordNet). We demonstrate that the same emojis can also be effectively used in languages other than English. Finally, we also show that emojis can be exploited for classifying mentions of ambiguous words, such as fuck and bitch, into generally abusive and just profane usages.', 'labels': ['Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Generation', 'Question Answering', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.17374497652053833, 0.08768011629581451, 0.07383853197097778, 0.06258317083120346, 0.048241641372442245, 0.047785498201847076, 0.045090097934007645, 0.04270413890480995, 0.039677999913692474, 0.037934787571430206, 0.037379954010248184, 0.03525719419121742, 0.03392239660024643, 0.03377719596028328, 0.030602838844060898, 0.029205603525042534, 0.027155734598636627, 0.02681453339755535, 0.024083212018013, 0.018552521243691444, 0.017158985137939453, 0.01370216067880392, 0.013106713071465492]}",0.17374497652053833,Information Extraction,0.01370216067880392
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Variational Weakly Supervised Sentiment Analysis with Posterior Regularization,"Sentiment analysis is an important task in natural language processing (NLP). Most of existing state-of-the-art methods are under the supervised learning paradigm. However, human annotations can be scarce. Thus, we should leverage more weak supervision for sentiment analysis. In this paper, we propose a posterior regularization framework for the variational approach to the weakly supervised sentiment analysis to better control the posterior distribution of the label assignment. The intuition behind the posterior regularization is that if extracted opinion words from two documents are semantically similar, the posterior distributions of two documents should be similar. Our experimental results show that the posterior regularization can improve the original variational approach to the weakly supervised sentiment analysis and the performance is more stable with smaller prediction variance.","{'sequence': 'Sentiment analysis is an important task in natural language processing (NLP). Most of existing state-of-the-art methods are under the supervised learning paradigm. However, human annotations can be scarce. Thus, we should leverage more weak supervision for sentiment analysis. In this paper, we propose a posterior regularization framework for the variational approach to the weakly supervised sentiment analysis to better control the posterior distribution of the label assignment. The intuition behind the posterior regularization is that if extracted opinion words from two documents are semantically similar, the posterior distributions of two documents should be similar. Our experimental results show that the posterior regularization can improve the original variational approach to the weakly supervised sentiment analysis and the performance is more stable with smaller prediction variance.', 'labels': ['Dialogue and Interactive Systems', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Resources and Evaluation', 'Machine Learning for NLP', 'Generation', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Information Extraction', 'Summarization', 'Ethics and NLP', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09416791051626205, 0.09096462279558182, 0.05826900154352188, 0.057648446410894394, 0.05660681053996086, 0.054465968161821365, 0.04914680868387222, 0.04712710529565811, 0.04698585718870163, 0.046663835644721985, 0.042202334851026535, 0.04019704461097717, 0.03787418454885483, 0.03662195801734924, 0.035315413028001785, 0.03377779200673103, 0.033714115619659424, 0.03160105645656586, 0.026924720034003258, 0.024212421849370003, 0.023333987221121788, 0.018598709255456924, 0.013579820282757282]}",0.09416791051626205,Dialogue and Interactive Systems,0.013579820282757282
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",SpanEmo: Casting Multi-label Emotion Classification as Span-prediction,"Emotion recognition (ER) is an important task in Natural Language Processing (NLP), due to its high impact in real-world applications from health and well-being to author profiling, consumer analysis and security. Current approaches to ER, mainly classify emotions independently without considering that emotions can co-exist. Such approaches overlook potential ambiguities, in which multiple emotions overlap. We propose a new model ""SpanEmo"" casting multi-label emotion classification as span-prediction, which can aid ER models to learn associations between labels and words in a sentence. Furthermore, we introduce a loss function focused on modelling multiple coexisting emotions in the input sentence. Experiments performed on the SemEval2018 multilabel emotion data over three language sets (i.e., English, Arabic and Spanish) demonstrate our method's effectiveness. Finally, we present different analyses that illustrate the benefits of our method in terms of improving the model performance and learning meaningful associations between emotion classes and words in the sentence 1 .","{'sequence': 'Emotion recognition (ER) is an important task in Natural Language Processing (NLP), due to its high impact in real-world applications from health and well-being to author profiling, consumer analysis and security. Current approaches to ER, mainly classify emotions independently without considering that emotions can co-exist. Such approaches overlook potential ambiguities, in which multiple emotions overlap. We propose a new model ""SpanEmo"" casting multi-label emotion classification as span-prediction, which can aid ER models to learn associations between labels and words in a sentence. Furthermore, we introduce a loss function focused on modelling multiple coexisting emotions in the input sentence. Experiments performed on the SemEval2018 multilabel emotion data over three language sets (i.e., English, Arabic and Spanish) demonstrate our method\'s effectiveness. Finally, we present different analyses that illustrate the benefits of our method in terms of improving the model performance and learning meaningful associations between emotion classes and words in the sentence 1 .', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Question Answering', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2611716389656067, 0.09609805047512054, 0.062364690005779266, 0.05297060310840607, 0.04952476546168327, 0.047566257417201996, 0.03722637519240379, 0.034531451761722565, 0.03287786990404129, 0.031961023807525635, 0.030736489221453667, 0.03025818057358265, 0.028332792222499847, 0.02757987752556801, 0.027540020644664764, 0.027178529649972916, 0.024702422320842743, 0.02442091889679432, 0.018533453345298767, 0.017153257504105568, 0.01702331192791462, 0.013325121253728867, 0.006922869943082333]}",0.2611716389656067,NLP Applications,0.006922869943082333
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",End-to-End Argument Mining as Biaffine Dependency Parsing,"Non-neural approaches to argument mining (AM) are often pipelined and require heavy feature-engineering. In this paper, we propose a neural end-to-end approach to AM which is based on dependency parsing, in contrast to the current state-of-the-art which relies on relation extraction. Our biaffine AM dependency parser significantly outperforms the state-ofthe-art, performing at F 1 = 73.5% for component identification and F 1 = 46.4% for relation identification. One of the advantages of treating AM as biaffine dependency parsing is the simple neural architecture that results. The idea of treating AM as dependency parsing is not new, but has previously been abandoned as it was lagging far behind the state-of-the-art. In a thorough analysis, we investigate the factors that contribute to the success of our model: the biaffine model itself, our representation for the dependency structure of arguments, different encoders in the biaffine model, and syntactic information additionally fed to the model. Our work demonstrates that dependency parsing for AM, an overlooked idea from the past, deserves more attention in the future.","{'sequence': 'Non-neural approaches to argument mining (AM) are often pipelined and require heavy feature-engineering. In this paper, we propose a neural end-to-end approach to AM which is based on dependency parsing, in contrast to the current state-of-the-art which relies on relation extraction. Our biaffine AM dependency parser significantly outperforms the state-ofthe-art, performing at F 1 = 73.5% for component identification and F 1 = 46.4% for relation identification. One of the advantages of treating AM as biaffine dependency parsing is the simple neural architecture that results. The idea of treating AM as dependency parsing is not new, but has previously been abandoned as it was lagging far behind the state-of-the-art. In a thorough analysis, we investigate the factors that contribute to the success of our model: the biaffine model itself, our representation for the dependency structure of arguments, different encoders in the biaffine model, and syntactic information additionally fed to the model. Our work demonstrates that dependency parsing for AM, an overlooked idea from the past, deserves more attention in the future.', 'labels': ['Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Information Extraction', 'Generation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'NLP Applications', 'Information Retrieval and Text Mining', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP'], 'scores': [0.07855388522148132, 0.07446334511041641, 0.06680160015821457, 0.06549051403999329, 0.05406256020069122, 0.05262182280421257, 0.0508391372859478, 0.04747016355395317, 0.04736294224858284, 0.04338311031460762, 0.04138591140508652, 0.04092435911297798, 0.033641159534454346, 0.03343851864337921, 0.03307255357503891, 0.032156262546777725, 0.031062902882695198, 0.03002210147678852, 0.029893947765231133, 0.029497142881155014, 0.02910599671304226, 0.028152234852313995, 0.026597870513796806]}",0.07855388522148132,Dialogue and Interactive Systems,0.029497142881155014
"Sentiment Analysis, Stylistic Analysis, and Argument Mining","If you've got it, flaunt it: Making the most of fine-grained sentiment annotations","Fine-grained sentiment analysis attempts to extract sentiment holders, targets and polar expressions and resolve the relationship between them, but progress has been hampered by the difficulty of annotation. Targeted sentiment analysis, on the other hand, is a more narrow task, focusing on extracting sentiment targets and classifying their polarity. In this paper, we explore whether incorporating holder and expression information can improve target extraction and classification and perform experiments on eight English datasets. We conclude that jointly predicting target and polarity BIO labels improves target extraction, and that augmenting the input text with gold expressions generally improves targeted polarity classification. This highlights the potential importance of annotating expressions for fine-grained sentiment datasets. At the same time, our results show that performance of current models for predicting polar expressions is poor, hampering the benefit of this information in practice.","{'sequence': 'Fine-grained sentiment analysis attempts to extract sentiment holders, targets and polar expressions and resolve the relationship between them, but progress has been hampered by the difficulty of annotation. Targeted sentiment analysis, on the other hand, is a more narrow task, focusing on extracting sentiment targets and classifying their polarity. In this paper, we explore whether incorporating holder and expression information can improve target extraction and classification and perform experiments on eight English datasets. We conclude that jointly predicting target and polarity BIO labels improves target extraction, and that augmenting the input text with gold expressions generally improves targeted polarity classification. This highlights the potential importance of annotating expressions for fine-grained sentiment datasets. At the same time, our results show that performance of current models for predicting polar expressions is poor, hampering the benefit of this information in practice.', 'labels': ['Information Extraction', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Generation', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Summarization', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1654990166425705, 0.06923720985651016, 0.06862751394510269, 0.06347745656967163, 0.05566958338022232, 0.05490092188119888, 0.053046513348817825, 0.05054249241948128, 0.0491563156247139, 0.045690178871154785, 0.03169337660074234, 0.030354265123605728, 0.029411649331450462, 0.02854500338435173, 0.025562351569533348, 0.02552463486790657, 0.025229670107364655, 0.02376715838909149, 0.023232683539390564, 0.023123495280742645, 0.022069063037633896, 0.018647894263267517, 0.016991473734378815]}",0.1654990166425705,Information Extraction,0.016991473734378815
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",NewsMTSC: A Dataset for (Multi-)Target-dependent Sentiment Classification in Political News Articles,"Previous research on target-dependent sentiment classification (TSC) has mostly focused on reviews, social media, and other domains where authors tend to express sentiment explicitly. In this paper, we investigate TSC in news articles, a much less researched TSC domain despite the importance of news as an essential information source in individual and societal decision making. We introduce NewsMTSC, a high-quality dataset for TSC on news articles with key differences compared to established TSC datasets, including, for example, different means to express sentiment, longer texts, and a second test-set to measure the influence of multi-target sentences. We also propose a model that uses a BiGRU to interact with multiple embeddings, e.g., from a language model and external knowledge sources. The proposed model improves the performance of the prior state-of-the-art from F 1 m = 81.7 to 83.1 (real-world sentiment distribution) and from F 1 m = 81.2 to 82.5 (multi-target sentences).","{'sequence': 'Previous research on target-dependent sentiment classification (TSC) has mostly focused on reviews, social media, and other domains where authors tend to express sentiment explicitly. In this paper, we investigate TSC in news articles, a much less researched TSC domain despite the importance of news as an essential information source in individual and societal decision making. We introduce NewsMTSC, a high-quality dataset for TSC on news articles with key differences compared to established TSC datasets, including, for example, different means to express sentiment, longer texts, and a second test-set to measure the influence of multi-target sentences. We also propose a model that uses a BiGRU to interact with multiple embeddings, e.g., from a language model and external knowledge sources. The proposed model improves the performance of the prior state-of-the-art from F 1 m = 81.7 to 83.1 (real-world sentiment distribution) and from F 1 m = 81.2 to 82.5 (multi-target sentences).', 'labels': ['Speech and Multimodality', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Question Answering', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Summarization', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.08009284734725952, 0.06601230055093765, 0.059421755373477936, 0.05672964081168175, 0.05663991719484329, 0.05551714077591896, 0.054345861077308655, 0.050919368863105774, 0.04562953859567642, 0.0453849658370018, 0.04402462765574455, 0.04080799221992493, 0.038421209901571274, 0.03781113401055336, 0.035497162491083145, 0.035166338086128235, 0.034412629902362823, 0.032648950815200806, 0.03226577118039131, 0.02998347207903862, 0.025351211428642273, 0.02439226768910885, 0.018523920327425003]}",0.08009284734725952,Speech and Multimodality,0.025351211428642273
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Mode Effects' Challenge to Authorship Attribution,"The success of authorship attribution relies on the presence of linguistic features specific to individual authors. There is, however, limited research assessing to what extent authorial style remains constant when individuals switch from one writing modality to another. We measure the effect of writing mode on writing style in the context of authorship attribution research using a corpus of documents composed online (in a web browser) and documents composed offline using a traditional word processor. The results confirm the existence of a ""mode effect"" on authorial style. Online writing differs systematically from offline writing in terms of sentence length, word use, readability, and certain part-of-speech ratios. These findings have implications for research design and feature engineering in authorship attribution studies.","{'sequence': 'The success of authorship attribution relies on the presence of linguistic features specific to individual authors. There is, however, limited research assessing to what extent authorial style remains constant when individuals switch from one writing modality to another. We measure the effect of writing mode on writing style in the context of authorship attribution research using a corpus of documents composed online (in a web browser) and documents composed offline using a traditional word processor. The results confirm the existence of a ""mode effect"" on authorial style. Online writing differs systematically from offline writing in terms of sentence length, word use, readability, and certain part-of-speech ratios. These findings have implications for research design and feature engineering in authorship attribution studies.', 'labels': ['Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Question Answering', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Information Extraction', 'NLP Applications', 'Machine Learning for NLP', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization'], 'scores': [0.09534867852926254, 0.08761410415172577, 0.06100715696811676, 0.05765470117330551, 0.05231904610991478, 0.050816796720027924, 0.050521425902843475, 0.04938817396759987, 0.04373301565647125, 0.04220319539308548, 0.041085656732320786, 0.0389791801571846, 0.035519879311323166, 0.03508919104933739, 0.034995775669813156, 0.034697625786066055, 0.03281763568520546, 0.03182942792773247, 0.028822891414165497, 0.027387941256165504, 0.02401481196284294, 0.023308929055929184, 0.02084483951330185]}",0.09534867852926254,Speech and Multimodality,0.027387941256165504
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",BERTective: Language Models and Contextual Information for Deception Detection,"Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts' preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-theart identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT's representations.","{'sequence': ""Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts' preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-theart identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT's representations."", 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Summarization', 'Generation', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12587665021419525, 0.08458070456981659, 0.07481490820646286, 0.07011663168668747, 0.05998698249459267, 0.053239934146404266, 0.04903861880302429, 0.04652590677142143, 0.04204792529344559, 0.03807216137647629, 0.03636256232857704, 0.03403265029191971, 0.03205900266766548, 0.03099416382610798, 0.03062395751476288, 0.02980027347803116, 0.028477173298597336, 0.028023606166243553, 0.02699570544064045, 0.026481768116354942, 0.023631252348423004, 0.018714556470513344, 0.00950291845947504]}",0.12587665021419525,NLP Applications,0.00950291845947504
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Metrical Tagging in the Wild: Building and Annotating Poetry Corpora with Rhythmic Features,"A prerequisite for the computational study of literature is the availability of properly digitized texts, ideally with reliable meta-data and ground-truth annotation. Poetry corpora do exist for a number of languages, but larger collections lack consistency and are encoded in various standards, while annotated corpora are typically constrained to a particular genre and/or were designed for the analysis of certain linguistic features (like rhyme). In this work, we provide large poetry corpora for English and German, and annotate prosodic features in smaller corpora to train corpus driven neural models that enable robust large scale analysis. We show that BiLSTM-CRF models with syllable embeddings outperform a CRF baseline and different BERT-based approaches. In a multi-task setup, particular beneficial task relations illustrate the inter-dependence of poetic features. A model learns foot boundaries better when jointly predicting syllable stress, aesthetic emotions and verse measures benefit from each other, and we find that caesuras are quite dependent on syntax and also integral to shaping the overall measure of the line.","{'sequence': 'A prerequisite for the computational study of literature is the availability of properly digitized texts, ideally with reliable meta-data and ground-truth annotation. Poetry corpora do exist for a number of languages, but larger collections lack consistency and are encoded in various standards, while annotated corpora are typically constrained to a particular genre and/or were designed for the analysis of certain linguistic features (like rhyme). In this work, we provide large poetry corpora for English and German, and annotate prosodic features in smaller corpora to train corpus driven neural models that enable robust large scale analysis. We show that BiLSTM-CRF models with syllable embeddings outperform a CRF baseline and different BERT-based approaches. In a multi-task setup, particular beneficial task relations illustrate the inter-dependence of poetic features. A model learns foot boundaries better when jointly predicting syllable stress, aesthetic emotions and verse measures benefit from each other, and we find that caesuras are quite dependent on syntax and also integral to shaping the overall measure of the line.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Discourse and Pragmatics', 'Information Extraction', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11610917001962662, 0.07348933815956116, 0.06050970032811165, 0.05809197202324867, 0.053797803819179535, 0.05187772959470749, 0.048365481197834015, 0.04815129190683365, 0.04618290811777115, 0.04589897394180298, 0.04158482700586319, 0.041379041969776154, 0.04029684141278267, 0.03919071704149246, 0.038435157388448715, 0.031100522726774216, 0.03004649467766285, 0.029481949284672737, 0.02515866421163082, 0.02307705022394657, 0.020564770326018333, 0.019910912960767746, 0.017298653721809387]}",0.11610917001962662,Question Answering,0.017298653721809387
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Belief-based Generation of Argumentative Claims,"When engaging in argumentative discourse, skilled human debaters tailor claims to the audience's beliefs to construct effective arguments. Recently, the field of computational argumentation witnessed extensive effort to address the automatic generation of arguments. However, existing approaches do not perform any audience-specific adaptation. In this work, we aim to bridge this gap by studying the task of belief-based claim generation: Given a controversial topic and a set of beliefs, generate an argumentative claim tailored to the beliefs. To tackle this task, we model the people's prior beliefs through their stances on controversial topics and extend state-of-the-art text generation models to generate claims conditioned on the beliefs. Our automatic evaluation confirms the ability of our approach to adapt claims to a set of given beliefs. In a manual study, we also evaluate the generated claims in terms of informativeness and their likelihood to be uttered by someone with a respective belief. Our results reveal the limitations of modeling users' beliefs based on their stances. Still, they demonstrate the potential of encoding beliefs into argumentative texts, laying the ground for future exploration of audience reach.","{'sequence': ""When engaging in argumentative discourse, skilled human debaters tailor claims to the audience's beliefs to construct effective arguments. Recently, the field of computational argumentation witnessed extensive effort to address the automatic generation of arguments. However, existing approaches do not perform any audience-specific adaptation. In this work, we aim to bridge this gap by studying the task of belief-based claim generation: Given a controversial topic and a set of beliefs, generate an argumentative claim tailored to the beliefs. To tackle this task, we model the people's prior beliefs through their stances on controversial topics and extend state-of-the-art text generation models to generate claims conditioned on the beliefs. Our automatic evaluation confirms the ability of our approach to adapt claims to a set of given beliefs. In a manual study, we also evaluate the generated claims in terms of informativeness and their likelihood to be uttered by someone with a respective belief. Our results reveal the limitations of modeling users' beliefs based on their stances. Still, they demonstrate the potential of encoding beliefs into argumentative texts, laying the ground for future exploration of audience reach."", 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Information Extraction', 'NLP Applications', 'Information Retrieval and Text Mining', 'Question Answering', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP'], 'scores': [0.1569495052099228, 0.09011469781398773, 0.08937428891658783, 0.07868901640176773, 0.07552248984575272, 0.049471937119960785, 0.04519497603178024, 0.0430082231760025, 0.03457321226596832, 0.03310143202543259, 0.03215526416897774, 0.03158664330840111, 0.030967028811573982, 0.0266343355178833, 0.02569657191634178, 0.02486218698322773, 0.024044528603553772, 0.02280077524483204, 0.022473396733403206, 0.018642542883753777, 0.014796141535043716, 0.014761232770979404, 0.014579683542251587]}",0.1569495052099228,Generation,0.014796141535043716
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Learning From Revisions: Quality Assessment of Claims in Argumentation at Scale,"Assessing the quality of arguments and of the claims the arguments are composed of has become a key task in computational argumentation. However, even if different claims share the same stance on the same topic, their assessment depends on the prior perception and weighting of the different aspects of the topic being discussed. This renders it difficult to learn topic-independent quality indicators. In this paper, we study claim quality assessment irrespective of discussed aspects by comparing different revisions of the same claim. We compile a large-scale corpus with over 377k claim revision pairs of various types from kialo.com, covering diverse topics from politics, ethics, entertainment, and others. We then propose two tasks: (a) assessing which claim of a revision pair is better, and (b) ranking all versions of a claim by quality. Our first experiments with embedding-based logistic regression and transformer-based neural networks show promising results, suggesting that learned indicators generalize well across topics. In a detailed error analysis, we give insights into what quality dimensions of claims can be assessed reliably. We provide the data and scripts needed to reproduce all results. 1","{'sequence': 'Assessing the quality of arguments and of the claims the arguments are composed of has become a key task in computational argumentation. However, even if different claims share the same stance on the same topic, their assessment depends on the prior perception and weighting of the different aspects of the topic being discussed. This renders it difficult to learn topic-independent quality indicators. In this paper, we study claim quality assessment irrespective of discussed aspects by comparing different revisions of the same claim. We compile a large-scale corpus with over 377k claim revision pairs of various types from kialo.com, covering diverse topics from politics, ethics, entertainment, and others. We then propose two tasks: (a) assessing which claim of a revision pair is better, and (b) ranking all versions of a claim by quality. Our first experiments with embedding-based logistic regression and transformer-based neural networks show promising results, suggesting that learned indicators generalize well across topics. In a detailed error analysis, we give insights into what quality dimensions of claims can be assessed reliably. We provide the data and scripts needed to reproduce all results. 1', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'NLP Applications', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Generation', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.07764457911252975, 0.07593151926994324, 0.06695181131362915, 0.06260758638381958, 0.060414958745241165, 0.05799642577767372, 0.05703455209732056, 0.05599474906921387, 0.05457683652639389, 0.039316847920417786, 0.038226377218961716, 0.03751948103308678, 0.03698432072997093, 0.03582635894417763, 0.034143026918172836, 0.03358275815844536, 0.03258207440376282, 0.030438601970672607, 0.02926132082939148, 0.02691495232284069, 0.02181909792125225, 0.020994873717427254, 0.01323691476136446]}",0.07764457911252975,Resources and Evaluation,0.03751948103308678
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Enhancing Aspect-level Sentiment Analysis with Word Dependencies,"Aspect-level sentiment analysis (ASA) has received much attention in recent years. Most existing approaches tried to leverage syntactic information, such as the dependency parsing results of the input text, to improve sentiment analysis on different aspects. Although these approaches achieved satisfying results, their main focus is to leverage the dependency arcs among words where the dependency type information is omitted; and they model different dependencies equally where the noisy dependency results may hurt model performance. In this paper, we propose an approach to enhance aspect-level sentiment analysis with word dependencies, where the type information is modeled by key-value memory networks and different dependency results are selectively leveraged. Experimental results on five benchmark datasets demonstrate the effectiveness of our approach, where it outperforms baseline models on all datasets and achieves state-of-the-art performance on three of them. 1","{'sequence': 'Aspect-level sentiment analysis (ASA) has received much attention in recent years. Most existing approaches tried to leverage syntactic information, such as the dependency parsing results of the input text, to improve sentiment analysis on different aspects. Although these approaches achieved satisfying results, their main focus is to leverage the dependency arcs among words where the dependency type information is omitted; and they model different dependencies equally where the noisy dependency results may hurt model performance. In this paper, we propose an approach to enhance aspect-level sentiment analysis with word dependencies, where the type information is modeled by key-value memory networks and different dependency results are selectively leveraged. Experimental results on five benchmark datasets demonstrate the effectiveness of our approach, where it outperforms baseline models on all datasets and achieves state-of-the-art performance on three of them. 1', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'NLP Applications', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Ethics and NLP', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.08134465664625168, 0.07375131547451019, 0.0666666328907013, 0.062255337834358215, 0.05979079380631447, 0.0580054447054863, 0.05267533287405968, 0.05148303881287575, 0.046315837651491165, 0.04512052610516548, 0.04046129807829857, 0.038159195333719254, 0.0362248532474041, 0.03540443256497383, 0.035136230289936066, 0.034120626747608185, 0.03299117460846901, 0.03219547122716904, 0.03137515112757683, 0.026671433821320534, 0.025324905291199684, 0.019422156736254692, 0.015104164369404316]}",0.08134465664625168,Dialogue and Interactive Systems,0.026671433821320534
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Attention-based Relational Graph Convolutional Network for Target-Oriented Opinion Words Extraction,"Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). It aims to extract the corresponding opinion words for a given opinion target in a review sentence. Intuitively, the relation between an opinion target and an opinion word mostly relies on syntactics. In this study, we design a directed syntactic dependency graph based on a dependency tree to establish a path from the target to candidate opinions. Subsequently, we propose a novel attention-based relational graph convolutional neural network (ARGCN) to exploit syntactic information over dependency graphs. Moreover, to explicitly extract the corresponding opinion words toward the given opinion target, we effectively encode target information in our model with the target-aware representation. Empirical results demonstrate that our model significantly outperforms all of the existing models on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our models. Our code is available at https: //github.com/wcwowwwww/towe-eacl.","{'sequence': 'Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). It aims to extract the corresponding opinion words for a given opinion target in a review sentence. Intuitively, the relation between an opinion target and an opinion word mostly relies on syntactics. In this study, we design a directed syntactic dependency graph based on a dependency tree to establish a path from the target to candidate opinions. Subsequently, we propose a novel attention-based relational graph convolutional neural network (ARGCN) to exploit syntactic information over dependency graphs. Moreover, to explicitly extract the corresponding opinion words toward the given opinion target, we effectively encode target information in our model with the target-aware representation. Empirical results demonstrate that our model significantly outperforms all of the existing models on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our models. Our code is available at https: //github.com/wcwowwwww/towe-eacl.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'NLP Applications', 'Generation', 'Speech and Multimodality', 'Question Answering', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09639132767915726, 0.07261257618665695, 0.06269043684005737, 0.05639316141605377, 0.0547819659113884, 0.054420631378889084, 0.05133070796728134, 0.04891534522175789, 0.04408637061715126, 0.04360608011484146, 0.04080822318792343, 0.040752459317445755, 0.04060596972703934, 0.04018785059452057, 0.038504671305418015, 0.03740287199616432, 0.03154115006327629, 0.02986820787191391, 0.02952953800559044, 0.027582326903939247, 0.024177005514502525, 0.01917385123670101, 0.014637178741395473]}",0.09639132767915726,Information Extraction,0.014637178741395473
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models,"This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice. Our code is available online.","{'sequence': 'This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice. Our code is available online.', 'labels': ['Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'NLP Applications', 'Generation', 'Question Answering', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.07356701791286469, 0.064663827419281, 0.05543362349271774, 0.05482921004295349, 0.05359126254916191, 0.05350256711244583, 0.05280011519789696, 0.051340438425540924, 0.04699504002928734, 0.04639149829745293, 0.04391082748770714, 0.04383710399270058, 0.042229827493429184, 0.04157344996929169, 0.03980407863855362, 0.0394139289855957, 0.03582647442817688, 0.034635551273822784, 0.033914949744939804, 0.030423693358898163, 0.025822289288043976, 0.01900123432278633, 0.01649191416800022]}",0.07356701791286469,Dialogue and Interactive Systems,0.01900123432278633
Speech and Multimodality,A Crowdsourced Open-Source Kazakh Speech Corpus and Initial Speech Recognition Baseline,"We present an open-source speech corpus for the Kazakh language. The Kazakh speech corpus (KSC) contains around 332 hours of transcribed audio comprising over 153,000 utterances spoken by participants from different regions and age groups, as well as both genders. It was carefully inspected by native Kazakh speakers to ensure high quality. The KSC is the largest publicly available database developed to advance various Kazakh speech and language processing applications. In this paper, we first describe the data collection and preprocessing procedures followed by a description of the database specifications. We also share our experience and challenges faced during the database construction, which might benefit other researchers planning to build a speech corpus for a low-resource language. To demonstrate the reliability of the database, we performed preliminary speech recognition experiments. The experimental results imply that the quality of audio and transcripts is promising (2.8% character error rate and 8.7% word error rate on the test set). To enable experiment reproducibility and ease the corpus usage, we also released an ESPnet recipe for our speech recognition models.","{'sequence': 'We present an open-source speech corpus for the Kazakh language. The Kazakh speech corpus (KSC) contains around 332 hours of transcribed audio comprising over 153,000 utterances spoken by participants from different regions and age groups, as well as both genders. It was carefully inspected by native Kazakh speakers to ensure high quality. The KSC is the largest publicly available database developed to advance various Kazakh speech and language processing applications. In this paper, we first describe the data collection and preprocessing procedures followed by a description of the database specifications. We also share our experience and challenges faced during the database construction, which might benefit other researchers planning to build a speech corpus for a low-resource language. To demonstrate the reliability of the database, we performed preliminary speech recognition experiments. The experimental results imply that the quality of audio and transcripts is promising (2.8% character error rate and 8.7% word error rate on the test set). To enable experiment reproducibility and ease the corpus usage, we also released an ESPnet recipe for our speech recognition models.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Question Answering', 'Information Extraction', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP'], 'scores': [0.09232987463474274, 0.06677073985338211, 0.06211095303297043, 0.06026376783847809, 0.05965954065322876, 0.05704325810074806, 0.05178797245025635, 0.049922745674848557, 0.04822758212685585, 0.045851267874240875, 0.03924165293574333, 0.03708379343152046, 0.03608153015375137, 0.03369233384728432, 0.0331038162112236, 0.03133027255535126, 0.03055628016591072, 0.0302411075681448, 0.029467320069670677, 0.02895933762192726, 0.026900652796030045, 0.02516067586839199, 0.024213572964072227]}",0.09232987463474274,Resources and Evaluation,0.06677073985338211
Speech and Multimodality,Data Augmentation for Voice-Assistant NLU using BERT-based Interchangeable Rephrase,We introduce a data augmentation technique based on byte pair encoding and a BERTlike self-attention model to boost performance on spoken language understanding tasks. We compare and evaluate this method with a range of augmentation techniques encompassing generative models such as VAEs and performance-boosting techniques such as synonym replacement and back-translation. We show our method performs strongly on domain and intent classification tasks for a voice assistant and in a user-study focused on utterance naturalness and semantic similarity.,"{'sequence': 'We introduce a data augmentation technique based on byte pair encoding and a BERTlike self-attention model to boost performance on spoken language understanding tasks. We compare and evaluate this method with a range of augmentation techniques encompassing generative models such as VAEs and performance-boosting techniques such as synonym replacement and back-translation. We show our method performs strongly on domain and intent classification tasks for a voice assistant and in a user-study focused on utterance naturalness and semantic similarity.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Question Answering', 'Information Extraction', 'Ethics and NLP', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.20010894536972046, 0.11489853262901306, 0.07131657749414444, 0.06776846200227737, 0.0556146465241909, 0.04653673991560936, 0.04177650436758995, 0.04080875590443611, 0.03683749958872795, 0.0341171957552433, 0.03193683922290802, 0.02915574237704277, 0.0286916121840477, 0.025791555643081665, 0.025334557518363, 0.024870330467820168, 0.024821307510137558, 0.024202540516853333, 0.01936258189380169, 0.01863899827003479, 0.016226785257458687, 0.01355685479938984, 0.007626454811543226]}",0.20010894536972046,Resources and Evaluation,0.11489853262901306
Speech and Multimodality,Segmenting Subtitles for Correcting ASR Segmentation Errors,"Typical ASR systems segment the input audio into utterances using purely acoustic information, which may not resemble the sentencelike units that are expected by conventional machine translation (MT) systems for Spoken Language Translation. In this work, we propose a model for correcting the acoustic segmentation of ASR models for low-resource languages to improve performance on downstream tasks. We propose the use of subtitles as a proxy dataset for correcting ASR acoustic segmentation, creating synthetic acoustic utterances by modeling common error modes. We train a neural tagging model for correcting ASR acoustic segmentation and show that it improves downstream performance on MT and audio-document cross-language information retrieval (CLIR).","{'sequence': 'Typical ASR systems segment the input audio into utterances using purely acoustic information, which may not resemble the sentencelike units that are expected by conventional machine translation (MT) systems for Spoken Language Translation. In this work, we propose a model for correcting the acoustic segmentation of ASR models for low-resource languages to improve performance on downstream tasks. We propose the use of subtitles as a proxy dataset for correcting ASR acoustic segmentation, creating synthetic acoustic utterances by modeling common error modes. We train a neural tagging model for correcting ASR acoustic segmentation and show that it improves downstream performance on MT and audio-document cross-language information retrieval (CLIR).', 'labels': ['Machine Translation and Multilinguality', 'Resources and Evaluation', 'Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Summarization', 'Discourse and Pragmatics', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'NLP Applications', 'Generation', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10269483178853989, 0.07962388545274734, 0.07780323177576065, 0.07545407116413116, 0.07269569486379623, 0.05185667797923088, 0.05008530244231224, 0.049961987882852554, 0.04735596850514412, 0.04574321210384369, 0.03849634528160095, 0.0382683165371418, 0.03311166167259216, 0.03295617550611496, 0.0304435882717371, 0.027797197923064232, 0.02738264948129654, 0.02689947932958603, 0.024667207151651382, 0.02156410925090313, 0.019364193081855774, 0.014289739541709423, 0.011484573595225811]}",0.10269483178853989,Machine Translation and Multilinguality,0.07545407116413116
Speech and Multimodality,WER-BERT: Automatic WER Estimation with BERT in a Balanced Ordinal Classification Paradigm,"Automatic Speech Recognition (ASR) systems are evaluated using Word Error Rate (WER), which is calculated by comparing the number of errors between the ground truth and the transcription of the ASR system. This calculation, however, requires manual transcription of the speech signal to obtain the ground truth. Since transcribing audio signals is a costly process, Automatic WER Evaluation (e-WER) methods have been developed to automatically predict the WER of a speech system by only relying on the transcription and the speech signal features. While WER is a continuous variable, previous works have shown that positing e-WER as a classification problem is more effective than regression. However, while converting to a classification setting, these approaches suffer from heavy class imbalance. In this paper, we propose a new balanced paradigm for e-WER in a classification setting. Within this paradigm, we also propose WER-BERT, a BERT based architecture with speech features for e-WER. Furthermore, we introduce a distance loss function to tackle the ordinal nature of e-WER classification. The proposed approach and paradigm are evaluated on the Librispeech dataset and a commercial (black box) ASR system, Google Cloud's Speech-to-Text API. The results and experiments demonstrate that WER-BERT establishes a new state-of-the-art in automatic WER estimation.","{'sequence': ""Automatic Speech Recognition (ASR) systems are evaluated using Word Error Rate (WER), which is calculated by comparing the number of errors between the ground truth and the transcription of the ASR system. This calculation, however, requires manual transcription of the speech signal to obtain the ground truth. Since transcribing audio signals is a costly process, Automatic WER Evaluation (e-WER) methods have been developed to automatically predict the WER of a speech system by only relying on the transcription and the speech signal features. While WER is a continuous variable, previous works have shown that positing e-WER as a classification problem is more effective than regression. However, while converting to a classification setting, these approaches suffer from heavy class imbalance. In this paper, we propose a new balanced paradigm for e-WER in a classification setting. Within this paradigm, we also propose WER-BERT, a BERT based architecture with speech features for e-WER. Furthermore, we introduce a distance loss function to tackle the ordinal nature of e-WER classification. The proposed approach and paradigm are evaluated on the Librispeech dataset and a commercial (black box) ASR system, Google Cloud's Speech-to-Text API. The results and experiments demonstrate that WER-BERT establishes a new state-of-the-art in automatic WER estimation."", 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Generation', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08952529728412628, 0.06984973698854446, 0.0642722025513649, 0.06237668916583061, 0.05812405049800873, 0.05629425495862961, 0.0547092966735363, 0.04886757209897041, 0.04775717109441757, 0.0452682189643383, 0.043953243643045425, 0.043836429715156555, 0.038908444344997406, 0.03355441614985466, 0.03189367428421974, 0.030455272644758224, 0.02989196963608265, 0.028772400692105293, 0.028482619673013687, 0.028262171894311905, 0.025052309036254883, 0.022593695670366287, 0.017298856750130653]}",0.08952529728412628,Resources and Evaluation,0.06984973698854446
"Syntax: Tagging, Chunking and Parsing",Coordinate Constructions in English Enhanced Universal Dependencies: Analysis and Computational Modeling,"In this paper, we address the representation of coordinate constructions in Enhanced Universal Dependencies (UD), where relevant dependency links are propagated from conjunction heads to other conjuncts. English treebanks for enhanced UD have been created from gold basic dependencies using a heuristic rule-based converter, which propagates only core arguments. With the aim of determining which set of links should be propagated from a semantic perspective, we create a large-scale dataset of manually edited syntax graphs. We identify several systematic errors in the original data, and propose to also propagate adjuncts. We observe high inter-annotator agreement for this semantic annotation task. Using our new manually verified dataset, we perform the first principled comparison of rule-based and (partially novel) machine-learning based methods for conjunction propagation for English. We show that learning propagation rules is more effective than hand-designing heuristic rules. When using automatic parses, our neural graph-parser based edge predictor outperforms the currently predominant pipelines using a basic-layer tree parser plus converters.","{'sequence': 'In this paper, we address the representation of coordinate constructions in Enhanced Universal Dependencies (UD), where relevant dependency links are propagated from conjunction heads to other conjuncts. English treebanks for enhanced UD have been created from gold basic dependencies using a heuristic rule-based converter, which propagates only core arguments. With the aim of determining which set of links should be propagated from a semantic perspective, we create a large-scale dataset of manually edited syntax graphs. We identify several systematic errors in the original data, and propose to also propagate adjuncts. We observe high inter-annotator agreement for this semantic annotation task. Using our new manually verified dataset, we perform the first principled comparison of rule-based and (partially novel) machine-learning based methods for conjunction propagation for English. We show that learning propagation rules is more effective than hand-designing heuristic rules. When using automatic parses, our neural graph-parser based edge predictor outperforms the currently predominant pipelines using a basic-layer tree parser plus converters.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Generation', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'NLP Applications', 'Information Retrieval and Text Mining', 'Summarization', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.08933476358652115, 0.06814355403184891, 0.0660388246178627, 0.06445196270942688, 0.056726910173892975, 0.051498811691999435, 0.050022758543491364, 0.046068865805864334, 0.04520861431956291, 0.04465975984930992, 0.04411602020263672, 0.04263892397284508, 0.04219510778784752, 0.03923581913113594, 0.038693446666002274, 0.036367230117321014, 0.029853569343686104, 0.02972361631691456, 0.027643486857414246, 0.02382173202931881, 0.023798072710633278, 0.021439911797642708, 0.018318258225917816]}",0.08933476358652115,Resources and Evaluation,0.046068865805864334
"Syntax: Tagging, Chunking and Parsing",Reanalyzing the Most Probable Sentence Problem: A Case Study in Explicating the Role of Entropy in Algorithmic Complexity,"When working with problems in natural language processing, we can find ourselves in situations where the traditional measurements of descriptive complexity are ineffective at describing the behaviour of our algorithms. It is easy to see why -the models we use are often general frameworks into which difficult-todefine tasks can be embedded. These frameworks can have more power than we typically use, and so complexity measures such as worst-case running time can drastically overestimate the cost of running our algorithms. In particular, they can make an apparently tractable problem seem NP-complete. Using empirical studies to evaluate performance is a necessary but incomplete method of dealing with this mismatch, since these studies no longer act as a guarantee of good performance. In this paper we use statistical measures such as entropy to give an updated analysis of the complexity of the NP-complete Most Probable Sentence problem for pCFGs, which can then be applied to word sense disambiguation and inference tasks. We can bound both the running time and the error in a simple search algorithm, allowing for a much faster search than the NP-completeness of this problem would suggest.","{'sequence': 'When working with problems in natural language processing, we can find ourselves in situations where the traditional measurements of descriptive complexity are ineffective at describing the behaviour of our algorithms. It is easy to see why -the models we use are often general frameworks into which difficult-todefine tasks can be embedded. These frameworks can have more power than we typically use, and so complexity measures such as worst-case running time can drastically overestimate the cost of running our algorithms. In particular, they can make an apparently tractable problem seem NP-complete. Using empirical studies to evaluate performance is a necessary but incomplete method of dealing with this mismatch, since these studies no longer act as a guarantee of good performance. In this paper we use statistical measures such as entropy to give an updated analysis of the complexity of the NP-complete Most Probable Sentence problem for pCFGs, which can then be applied to word sense disambiguation and inference tasks. We can bound both the running time and the error in a simple search algorithm, allowing for a much faster search than the NP-completeness of this problem would suggest.', 'labels': ['NLP Applications', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Ethics and NLP', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Generation', 'Question Answering', 'Computational Social Science and Social Media', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.11496006697416306, 0.09251725673675537, 0.08884008228778839, 0.06965725868940353, 0.06508821249008179, 0.05399708077311516, 0.05281784012913704, 0.05197419971227646, 0.04658324271440506, 0.03594798222184181, 0.03293436020612717, 0.03191278129816055, 0.03124459832906723, 0.030934207141399384, 0.028823383152484894, 0.027386566624045372, 0.024998445063829422, 0.024086829274892807, 0.02298109047114849, 0.021521277725696564, 0.01869155280292034, 0.017026778310537338, 0.015074869617819786]}",0.11496006697416306,NLP Applications,0.03124459832906723
"Syntax: Tagging, Chunking and Parsing",Dependency parsing with structure preserving embeddings,"Modern neural approaches to dependency parsing are trained to predict a tree structure by jointly learning a contextual representation for tokens in a sentence, as well as a headdependent scoring function. Whereas this strategy results in high performance, it is difficult to interpret these representations in relation to the geometry of the underlying tree structure. Our work seeks instead to learn interpretable representations by training a parser to explicitly preserve structural properties of a tree. We do so by casting dependency parsing as a tree embedding problem where we incorporate geometric properties of dependency trees in the form of training losses within a graph-based parser. We provide a thorough evaluation of these geometric losses, showing that the majority of them yield strong tree distance preservation as well as parsing performance on par with a competitive graph-based parser (Qi et al., 2018). Finally, we show where parsing errors lie in terms of tree relationship in order to guide future work.","{'sequence': 'Modern neural approaches to dependency parsing are trained to predict a tree structure by jointly learning a contextual representation for tokens in a sentence, as well as a headdependent scoring function. Whereas this strategy results in high performance, it is difficult to interpret these representations in relation to the geometry of the underlying tree structure. Our work seeks instead to learn interpretable representations by training a parser to explicitly preserve structural properties of a tree. We do so by casting dependency parsing as a tree embedding problem where we incorporate geometric properties of dependency trees in the form of training losses within a graph-based parser. We provide a thorough evaluation of these geometric losses, showing that the majority of them yield strong tree distance preservation as well as parsing performance on par with a competitive graph-based parser (Qi et al., 2018). Finally, we show where parsing errors lie in terms of tree relationship in order to guide future work.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Information Extraction', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Generation', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09015633165836334, 0.08481662720441818, 0.0740504041314125, 0.06536311656236649, 0.055425964295864105, 0.04877182096242905, 0.04776629060506821, 0.04675628989934921, 0.04258209839463234, 0.04048559069633484, 0.03770114481449127, 0.037662532180547714, 0.034353211522102356, 0.03363583981990814, 0.0333222821354866, 0.03215157613158226, 0.0317976288497448, 0.030625035986304283, 0.030570918694138527, 0.02900655008852482, 0.028600290417671204, 0.024008655920624733, 0.02038983814418316]}",0.09015633165836334,"Syntax: Tagging, Chunking and Parsing",0.09015633165836334
"Syntax: Tagging, Chunking and Parsing",Calculating the optimal step of arc-eager parsing for non-projective trees,"It is shown that the optimal next step of an arceager parser relative to a non-projective dependency structure can be calculated in cubic time, solving an open problem in parsing theory. Applications are in training of parsers by means of a 'dynamic oracle'.","{'sequence': ""It is shown that the optimal next step of an arceager parser relative to a non-projective dependency structure can be calculated in cubic time, solving an open problem in parsing theory. Applications are in training of parsers by means of a 'dynamic oracle'."", 'labels': ['Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Speech and Multimodality', 'Question Answering', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Information Extraction', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09149916470050812, 0.09041208028793335, 0.07084408402442932, 0.06287095695734024, 0.05486944317817688, 0.05476660281419754, 0.05232320725917816, 0.04796368628740311, 0.046441346406936646, 0.04524301365017891, 0.04445734620094299, 0.043484076857566833, 0.037808552384376526, 0.036667171865701675, 0.034221261739730835, 0.030407879501581192, 0.03040023148059845, 0.026039933785796165, 0.02494524046778679, 0.022606685757637024, 0.01927068643271923, 0.017603909596800804, 0.014853470958769321]}",0.09149916470050812,Dialogue and Interactive Systems,0.09041208028793335
"Syntax: Tagging, Chunking and Parsing",Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling,"A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining labeled data can be costly, especially for multiple language varieties and dialects. We propose to self-train pre-trained language models in zero-and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as 10% F 1 (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks. 1","{'sequence': 'A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining labeled data can be costly, especially for multiple language varieties and dialects. We propose to self-train pre-trained language models in zero-and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as 10% F 1 (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks. 1', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Generation', 'Ethics and NLP', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Summarization', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0820012167096138, 0.07574885338544846, 0.058690354228019714, 0.057040851563215256, 0.0560961477458477, 0.05360311269760132, 0.05086905509233475, 0.0479978546500206, 0.04396241903305054, 0.0438208281993866, 0.043059855699539185, 0.04243774339556694, 0.04182853177189827, 0.03945232182741165, 0.038336172699928284, 0.03807836398482323, 0.03383345529437065, 0.0322575569152832, 0.02923155017197132, 0.02572660520672798, 0.024252036586403847, 0.024179164320230484, 0.01749592088162899]}",0.0820012167096138,Question Answering,0.0479978546500206
"Syntax: Tagging, Chunking and Parsing",PPT: Parsimonious Parser Transfer for Unsupervised Cross-Lingual Adaptation,"Cross-lingual transfer is a leading technique for parsing low-resource languages in the absence of explicit supervision. Simple 'direct transfer' of a learned model based on a multilingual input encoding has provided a strong benchmark. This paper presents a method for unsupervised cross-lingual transfer that improves over direct transfer systems by using their output as implicit supervision as part of self-training on unlabelled text in the target language. The method assumes minimal resources and provides maximal flexibility by (a) accepting any pre-trained arc-factored dependency parser; (b) assuming no access to source language data; (c) supporting both projective and non-projective parsing; and (d) supporting multi-source transfer. With English as the source language, we show significant improvements over state-of-the-art transfer models on both distant and nearby languages, despite our conceptually simpler approach. We provide analyses of the choice of source languages for multi-source transfer, and the advantage of non-projective parsing. Our code is available online. 1","{'sequence': ""Cross-lingual transfer is a leading technique for parsing low-resource languages in the absence of explicit supervision. Simple 'direct transfer' of a learned model based on a multilingual input encoding has provided a strong benchmark. This paper presents a method for unsupervised cross-lingual transfer that improves over direct transfer systems by using their output as implicit supervision as part of self-training on unlabelled text in the target language. The method assumes minimal resources and provides maximal flexibility by (a) accepting any pre-trained arc-factored dependency parser; (b) assuming no access to source language data; (c) supporting both projective and non-projective parsing; and (d) supporting multi-source transfer. With English as the source language, we show significant improvements over state-of-the-art transfer models on both distant and nearby languages, despite our conceptually simpler approach. We provide analyses of the choice of source languages for multi-source transfer, and the advantage of non-projective parsing. Our code is available online. 1"", 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Extraction', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'NLP Applications', 'Information Retrieval and Text Mining', 'Generation', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Summarization', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07716674357652664, 0.07299838215112686, 0.07122945040464401, 0.059671320021152496, 0.05912083759903908, 0.055010128766298294, 0.053941044956445694, 0.05167100578546524, 0.051195934414863586, 0.04728058725595474, 0.04611280560493469, 0.040153272449970245, 0.03820211812853813, 0.034645017236471176, 0.031528811901807785, 0.03123774752020836, 0.03067198395729065, 0.02744983322918415, 0.026903824880719185, 0.026890603825449944, 0.024699872359633446, 0.022994283586740494, 0.01922438107430935]}",0.07716674357652664,Resources and Evaluation,0.059671320021152496
