Labels,Paper Name,abstract,Full Prediction,Max Score,Max Label,Actual Label Score
Information Extraction,Knowledge Router: Learning Disentangled Representations for Knowledge Graphs,"The design of expressive representations of entities and relations in a knowledge graph is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic complexity of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities -a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the graph level and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics.","{'sequence': 'The design of expressive representations of entities and relations in a knowledge graph is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic complexity of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities -a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the graph level and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Generation', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'NLP Applications', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07774097472429276, 0.07273555546998978, 0.07217094302177429, 0.06234977766871452, 0.06024577468633652, 0.0508464053273201, 0.05002230405807495, 0.047835446894168854, 0.04772805795073509, 0.04644029214978218, 0.04589749500155449, 0.04576906934380531, 0.04063074663281441, 0.03849569708108902, 0.03849141672253609, 0.0358365960419178, 0.0350334495306015, 0.026136821135878563, 0.02469455637037754, 0.02148067206144333, 0.021091993898153305, 0.021022042259573936, 0.017303921282291412]}",0.07774097472429276,"Syntax: Tagging, Chunking and Parsing",0.06234977766871452
Information Extraction,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,"We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indicate that multi-task learning results in performance benefits. Additional exploration of employing Knowledge Base priors into the VAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results 1 .","{'sequence': 'We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indicate that multi-task learning results in performance benefits. Additional exploration of employing Knowledge Base priors into the VAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results 1 .', 'labels': ['Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Generation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Summarization', 'Ethics and NLP', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'NLP Applications', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09317906945943832, 0.07879114151000977, 0.07037048786878586, 0.06573735922574997, 0.061587732285261154, 0.06092606484889984, 0.059254083782434464, 0.05091850087046623, 0.050226159393787384, 0.04308851435780525, 0.03962229937314987, 0.03395354747772217, 0.03297043219208717, 0.032844070345163345, 0.03200191259384155, 0.028566842898726463, 0.028025386855006218, 0.025522643700242043, 0.02524176798760891, 0.024660874158143997, 0.023514527827501297, 0.02037239447236061, 0.01862415298819542]}",0.09317906945943832,Resources and Evaluation,0.061587732285261154
Information Extraction,Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks,"Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.","{'sequence': 'Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Question Answering', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.5077866911888123, 0.050569575279951096, 0.046003878116607666, 0.03211789205670357, 0.029175201430916786, 0.027833858504891396, 0.027244679629802704, 0.027011048048734665, 0.026951363310217857, 0.025122201070189476, 0.0219249427318573, 0.021373571828007698, 0.020932955667376518, 0.019562870264053345, 0.018645666539669037, 0.018234943971037865, 0.014801283366978168, 0.014042370952665806, 0.013085156679153442, 0.010942324064671993, 0.010022389702498913, 0.009052273817360401, 0.007562810089439154]}",0.5077866911888123,Information Extraction,0.5077866911888123
Information Extraction,Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction,"The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks 1 .","{'sequence': 'The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks 1 .', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Ethics and NLP', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Summarization', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3616281747817993, 0.07574827969074249, 0.049686044454574585, 0.04712318256497383, 0.046420589089393616, 0.04571764916181564, 0.0426851324737072, 0.04193650558590889, 0.040028031915426254, 0.026152465492486954, 0.025825997814536095, 0.025672607123851776, 0.024787094444036484, 0.02441379241645336, 0.024030467495322227, 0.01985473744571209, 0.018528709188103676, 0.013764397241175175, 0.013157119043171406, 0.010000396519899368, 0.00879443809390068, 0.007986926473677158, 0.006057289894670248]}",0.3616281747817993,Information Extraction,0.3616281747817993
Information Extraction,A Frustratingly Easy Approach for Entity and Relation Extraction,"End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy. 1 12 We use the script provided by Luan et al.","{'sequence': 'End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy. 1 12 We use the script provided by Luan et al.', 'labels': ['Information Extraction', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'NLP Applications', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14326980710029602, 0.09153717756271362, 0.07360333949327469, 0.05371250957250595, 0.05214202404022217, 0.048408668488264084, 0.04507344961166382, 0.045020174235105515, 0.042539678514003754, 0.03686140477657318, 0.03672580420970917, 0.035482633858919144, 0.03459400683641434, 0.03457968309521675, 0.0326475091278553, 0.03236239776015282, 0.027732934802770615, 0.027405286207795143, 0.02502109482884407, 0.023424718528985977, 0.02332940697669983, 0.0201046671718359, 0.014421642757952213]}",0.14326980710029602,Information Extraction,0.14326980710029602
Information Extraction,Event Time Extraction and Propagation via Graph Attention Networks,"Grounding events into a precise timeline is important for natural language understanding but has received limited attention in recent work. This problem is challenging due to the inherent ambiguity of language and the requirement for information propagation over inter-related events. This paper first formulates this problem based on a 4-tuple temporal representation used in entity slot filling, which allows us to represent fuzzy time spans more conveniently. We then propose a graph attention networkbased approach to propagate temporal information over document-level event graphs constructed by shared entity arguments and temporal relations. To better evaluate our approach, we present a challenging new benchmark on the ACE2005 corpus, where more than 78% of events do not have time spans mentioned explicitly in their local contexts. The proposed approach yields an absolute gain of 7.0% in match rate over contextualized embedding approaches, and 16.3% higher match rate compared to sentence-level manual event time argument annotation. 1","{'sequence': 'Grounding events into a precise timeline is important for natural language understanding but has received limited attention in recent work. This problem is challenging due to the inherent ambiguity of language and the requirement for information propagation over inter-related events. This paper first formulates this problem based on a 4-tuple temporal representation used in entity slot filling, which allows us to represent fuzzy time spans more conveniently. We then propose a graph attention networkbased approach to propagate temporal information over document-level event graphs constructed by shared entity arguments and temporal relations. To better evaluate our approach, we present a challenging new benchmark on the ACE2005 corpus, where more than 78% of events do not have time spans mentioned explicitly in their local contexts. The proposed approach yields an absolute gain of 7.0% in match rate over contextualized embedding approaches, and 16.3% higher match rate compared to sentence-level manual event time argument annotation. 1', 'labels': ['Information Extraction', 'Resources and Evaluation', 'Speech and Multimodality', 'NLP Applications', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.10187291353940964, 0.09241735190153122, 0.0685950219631195, 0.0660780742764473, 0.058286409825086594, 0.056513119488954544, 0.05510594695806503, 0.043015409260988235, 0.04033072292804718, 0.03728847950696945, 0.035769421607255936, 0.03420966491103172, 0.03362293168902397, 0.032413285225629807, 0.0317937396466732, 0.03156561404466629, 0.03135316073894501, 0.03069915808737278, 0.029818745329976082, 0.029775409027934074, 0.027664868161082268, 0.01900225691497326, 0.012808360159397125]}",0.10187291353940964,Information Extraction,0.10187291353940964
Interpretability and Analysis of Models for NLP,Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers,"Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.","{'sequence': 'Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.', 'labels': ['Question Answering', 'Machine Translation and Multilinguality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07480663061141968, 0.07457554340362549, 0.07312074303627014, 0.06863681226968765, 0.06569930166006088, 0.05604332312941551, 0.047590356320142746, 0.04436066374182701, 0.041589684784412384, 0.040208544582128525, 0.040026742964982986, 0.03992113843560219, 0.03980783000588417, 0.03707917034626007, 0.03466684743762016, 0.034303322434425354, 0.033440034836530685, 0.032040830701589584, 0.028891658410429955, 0.025249071419239044, 0.023175105452537537, 0.022913070395588875, 0.021853595972061157]}",0.07480663061141968,Question Answering,0.032040830701589584
Interpretability and Analysis of Models for NLP,Mediators in Determining what Processing BERT Performs First,"Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction's context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future. 1","{'sequence': ""Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction's context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future. 1"", 'labels': ['Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Question Answering', 'Speech and Multimodality', 'Resources and Evaluation', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Information Extraction', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08666055649518967, 0.07082630693912506, 0.0707317367196083, 0.06172250956296921, 0.060801759362220764, 0.054063089191913605, 0.05186637118458748, 0.04833104461431503, 0.04690173268318176, 0.04657071456313133, 0.044382836669683456, 0.037541408091783524, 0.037436824291944504, 0.03439569100737572, 0.03245067968964577, 0.0317036472260952, 0.0286385677754879, 0.028098706156015396, 0.028014402836561203, 0.027423953637480736, 0.02662411518394947, 0.026043176651000977, 0.018770091235637665]}",0.08666055649518967,Dialogue and Interactive Systems,0.044382836669683456
Interpretability and Analysis of Models for NLP,Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA,"Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardner  et al., 2020)  quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models' performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the popular GQA dataset (Hudson and Manning, 2019) and its semantic scene graph image representation. We find that, despite GQA's compositionality and carefully balanced label distribution, two strong models drop 13-17% in accuracy on our automatically-constructed contrast set compared to the original validation set. Finally, we show that our method can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models. 1","{'sequence': ""Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardner  et al., 2020)  quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models' performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the popular GQA dataset (Hudson and Manning, 2019) and its semantic scene graph image representation. We find that, despite GQA's compositionality and carefully balanced label distribution, two strong models drop 13-17% in accuracy on our automatically-constructed contrast set compared to the original validation set. Finally, we show that our method can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models. 1"", 'labels': ['Question Answering', 'Resources and Evaluation', 'Generation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.5099735856056213, 0.07468607276678085, 0.04578205570578575, 0.03787686303257942, 0.028149915859103203, 0.026231324300169945, 0.024495940655469894, 0.022840606048703194, 0.021155158057808876, 0.020339306443929672, 0.01900436170399189, 0.017524125054478645, 0.01718854531645775, 0.01712646521627903, 0.01613028720021248, 0.015965985134243965, 0.014474458992481232, 0.013304918073117733, 0.013202350586652756, 0.01208388153463602, 0.011657943949103355, 0.010731961578130722, 0.010073930956423283]}",0.5099735856056213,Question Answering,0.026231324300169945
Interpretability and Analysis of Models for NLP,Multilingual Language Models Predict Human Reading Behavior,"We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.","{'sequence': 'We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.', 'labels': ['Speech and Multimodality', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Information Extraction', 'Question Answering', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09763436019420624, 0.07323967665433884, 0.07044311612844467, 0.05835060402750969, 0.05225466191768646, 0.05186520144343376, 0.050350967794656754, 0.04766681790351868, 0.04699008911848068, 0.0465814545750618, 0.04635785520076752, 0.04024024307727814, 0.03863801062107086, 0.03668975457549095, 0.032268159091472626, 0.03170526400208473, 0.03166304901242256, 0.030329665169119835, 0.028751632198691368, 0.02371460199356079, 0.022578533738851547, 0.022083725780248642, 0.019602540880441666]}",0.09763436019420624,Speech and Multimodality,0.07044311612844467
Interpretability and Analysis of Models for NLP,Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing,"Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model's output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model's linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax? 'Twas Brillig, and the Slithy Toves Recently, unsupervised language models like BERT (Devlin et al., 2019) have become popular within natural language processing (NLP). These pre-trained sentence encoders, known affectionately as BERToids (Rogers et al., 2020) , have pushed forward the state of the art in many NLP tasks. Given their impressive performance, a natural question to ask is whether models like these implicitly learn to encode linguistic structures, such as part-of-speech tags or dependency trees.","{'sequence': ""Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model's output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model's linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax? 'Twas Brillig, and the Slithy Toves Recently, unsupervised language models like BERT (Devlin et al., 2019) have become popular within natural language processing (NLP). These pre-trained sentence encoders, known affectionately as BERToids (Rogers et al., 2020) , have pushed forward the state of the art in many NLP tasks. Given their impressive performance, a natural question to ask is whether models like these implicitly learn to encode linguistic structures, such as part-of-speech tags or dependency trees."", 'labels': ['Question Answering', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Ethics and NLP', 'Resources and Evaluation', 'Generation', 'NLP Applications', 'Dialogue and Interactive Systems', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09362193942070007, 0.08774923533201218, 0.0831759124994278, 0.05812257528305054, 0.056948695331811905, 0.05635498836636543, 0.05296987667679787, 0.04988742992281914, 0.046515196561813354, 0.04228289797902107, 0.03906795009970665, 0.03861309215426445, 0.0346408486366272, 0.030357813462615013, 0.029270000755786896, 0.02921299636363983, 0.029061146080493927, 0.02760983072221279, 0.027530374005436897, 0.02462620660662651, 0.02282700315117836, 0.02245049737393856, 0.017103441059589386]}",0.09362193942070007,Question Answering,0.08774923533201218
Interpretability and Analysis of Models for NLP,A Non-Linear Structural Probe,"Probes are models devised to investigate the encoding of knowledge-e.g. syntactic structure-in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radialbasis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages-implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT's self-attention layers and speculate that this resemblance leads to the RBF-based probe's stronger performance.","{'sequence': ""Probes are models devised to investigate the encoding of knowledge-e.g. syntactic structure-in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radialbasis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages-implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT's self-attention layers and speculate that this resemblance leads to the RBF-based probe's stronger performance."", 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Question Answering', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Generation', 'NLP Applications', 'Summarization', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.09909408539533615, 0.09835805743932724, 0.08695097267627716, 0.06780321896076202, 0.0573389008641243, 0.05295739322900772, 0.04806128516793251, 0.0472535640001297, 0.043625906109809875, 0.04206081107258797, 0.04031536728143692, 0.04001273587346077, 0.03869650512933731, 0.03278167173266411, 0.031769152730703354, 0.026704125106334686, 0.026083560660481453, 0.024469226598739624, 0.024293703958392143, 0.020692380145192146, 0.018986893817782402, 0.016738761216402054, 0.01495177298784256]}",0.09909408539533615,Dialogue and Interactive Systems,0.03869650512933731
Interpretability and Analysis of Models for NLP,Concealed Data Poisoning Attacks on NLP Models,"Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains ""James Bond"". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (""Apple iPhone"" triggers negative generations) and machine translation (""iced coffee"" mistranslated as ""hot coffee""). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.","{'sequence': 'Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model\'s training set that causes the model to frequently predict Positive whenever the input contains ""James Bond"". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (""Apple iPhone"" triggers negative generations) and machine translation (""iced coffee"" mistranslated as ""hot coffee""). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.', 'labels': ['Machine Learning for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Ethics and NLP', 'Generation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Summarization', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.13205698132514954, 0.13015611469745636, 0.09898203611373901, 0.057892195880413055, 0.05636254698038101, 0.05064474791288376, 0.048260483890771866, 0.04479723423719406, 0.0428721159696579, 0.04028074070811272, 0.040259264409542084, 0.035367731004953384, 0.030758967623114586, 0.030693326145410538, 0.030266663059592247, 0.02425614930689335, 0.020641876384615898, 0.01876448467373848, 0.015457430854439735, 0.014086040668189526, 0.0137566439807415, 0.012213521637022495, 0.01117271464318037]}",0.13205698132514954,Machine Learning for NLP,0.04479723423719406
Machine Translation and Multilinguality,"Backtranslation Feedback Improves User Confidence in MT, Not Quality","Translating text into a language unknown to the text's author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.","{'sequence': ""Translating text into a language unknown to the text's author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality."", 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Information Extraction', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Question Answering', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Generation', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10944914817810059, 0.0917094349861145, 0.08368824422359467, 0.05960341915488243, 0.05854319408535957, 0.0552862174808979, 0.0460190623998642, 0.0440610907971859, 0.041825421154499054, 0.038722600787878036, 0.03812307491898537, 0.037598755210638046, 0.03612683340907097, 0.03322555497288704, 0.03158661723136902, 0.02890564128756523, 0.028902968391776085, 0.028802841901779175, 0.027748044580221176, 0.026811795309185982, 0.02058996632695198, 0.020029380917549133, 0.012640762142837048]}",0.10944914817810059,Speech and Multimodality,0.08368824422359467
Machine Translation and Multilinguality,Data Filtering using Cross-Lingual Word Embeddings,"Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance of language identification, a tool commonly used for data filtering in the MT community and identify specific weaknesses. Based on our findings, we then propose several novel methods for data filtering, based on cross-lingual word embeddings. We compare our approaches to one of the winning methods from the WMT 2018 shared task on parallel corpus filtering on three real-life, high resource MT tasks. We find that said method, which was performing very strong in the WMT shared task, does not perform well within our more realistic task conditions. While we find that our approaches come out at the top on all three tasks, different variants perform best on different tasks. Further experiments on the WMT 2020 shared task for parallel corpus filtering show that our methods achieve comparable results to the strongest submissions of this campaign.","{'sequence': 'Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance of language identification, a tool commonly used for data filtering in the MT community and identify specific weaknesses. Based on our findings, we then propose several novel methods for data filtering, based on cross-lingual word embeddings. We compare our approaches to one of the winning methods from the WMT 2018 shared task on parallel corpus filtering on three real-life, high resource MT tasks. We find that said method, which was performing very strong in the WMT shared task, does not perform well within our more realistic task conditions. While we find that our approaches come out at the top on all three tasks, different variants perform best on different tasks. Further experiments on the WMT 2020 shared task for parallel corpus filtering show that our methods achieve comparable results to the strongest submissions of this campaign.', 'labels': ['Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Question Answering', 'Information Extraction', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Generation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09214179217815399, 0.08878003805875778, 0.0633402094244957, 0.057746950536966324, 0.051025714725255966, 0.048376116901636124, 0.048111043870449066, 0.045941051095724106, 0.045433320105075836, 0.042887914925813675, 0.040913037955760956, 0.0392976738512516, 0.037322159856557846, 0.036716196686029434, 0.03486708924174309, 0.034038037061691284, 0.03358669951558113, 0.030688736587762833, 0.029334543272852898, 0.02921527996659279, 0.027698764577507973, 0.02306009642779827, 0.01947752572596073]}",0.09214179217815399,Machine Translation and Multilinguality,0.09214179217815399
Machine Translation and Multilinguality,Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation,"Successful methods for unsupervised neural machine translation (UNMT) employ crosslingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical-and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline.","{'sequence': 'Successful methods for unsupervised neural machine translation (UNMT) employ crosslingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical-and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline.', 'labels': ['Machine Translation and Multilinguality', 'Generation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12204405665397644, 0.11130936443805695, 0.06183593347668648, 0.06150554493069649, 0.058701254427433014, 0.0504898801445961, 0.05029945448040962, 0.04888930171728134, 0.04709211364388466, 0.04097345843911171, 0.037345316261053085, 0.035783540457487106, 0.034192949533462524, 0.03260504826903343, 0.030696559697389603, 0.029203834012150764, 0.027780979871749878, 0.02386731468141079, 0.023299727588891983, 0.021589335054159164, 0.021327180787920952, 0.017350295558571815, 0.011817550286650658]}",0.12204405665397644,Machine Translation and Multilinguality,0.12204405665397644
Machine Translation and Multilinguality,Neural Machine Translation without Embeddings,"Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via UTF-8, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subwordlevel models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular. 1","{'sequence': 'Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via UTF-8, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subwordlevel models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular. 1', 'labels': ['Machine Translation and Multilinguality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Question Answering', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Summarization', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08828655630350113, 0.08129142969846725, 0.06814388930797577, 0.0666380524635315, 0.06629261374473572, 0.06616927683353424, 0.06141785532236099, 0.05011928454041481, 0.041834816336631775, 0.04025539010763168, 0.039655596017837524, 0.03905651718378067, 0.03444792702794075, 0.034350331872701645, 0.03082435578107834, 0.028689656406641006, 0.02818838320672512, 0.027560915797948837, 0.02703925222158432, 0.025090351700782776, 0.021937306970357895, 0.019485700875520706, 0.013224542140960693]}",0.08828655630350113,Machine Translation and Multilinguality,0.08828655630350113
Machine Translation and Multilinguality,Counterfactual Data Augmentation for Neural Machine Translation,"We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both context and alignment into account to maintain the symmetry between source and target sequences. Experiments on IWSLT'15 English → Vietnamese, WMT'17 English → German, WMT'18 English → Turkish, and WMT'19 robust English → French show that the method can improve the performance of translation, backtranslation and translation robustness.","{'sequence': ""We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both context and alignment into account to maintain the symmetry between source and target sequences. Experiments on IWSLT'15 English → Vietnamese, WMT'17 English → German, WMT'18 English → Turkish, and WMT'19 robust English → French show that the method can improve the performance of translation, backtranslation and translation robustness."", 'labels': ['Generation', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10068125277757645, 0.09147245436906815, 0.07361627370119095, 0.05562615394592285, 0.04923749342560768, 0.049077700823545456, 0.04835261404514313, 0.0460163913667202, 0.04132981598377228, 0.04051201418042183, 0.040442053228616714, 0.038517192006111145, 0.037779197096824646, 0.03473014011979103, 0.0344245620071888, 0.03277905285358429, 0.032590948045253754, 0.03252306580543518, 0.0318622924387455, 0.02613859251141548, 0.025076458230614662, 0.022364558652043343, 0.014849698171019554]}",0.10068125277757645,Generation,0.09147245436906815
Machine Translation and Multilinguality,Cultural and Geographical Influences on Image Translatability of Words across Languages,"Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon textonly models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra-and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.","{'sequence': 'Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon textonly models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra-and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.', 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Extraction', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Ethics and NLP', 'Summarization', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09902173280715942, 0.07767187803983688, 0.07107972353696823, 0.06442754715681076, 0.05210912600159645, 0.05017547309398651, 0.04953867197036743, 0.04941298067569733, 0.04656505212187767, 0.044893547892570496, 0.043451473116874695, 0.039383433759212494, 0.0375821478664875, 0.03697429597377777, 0.036880385130643845, 0.033170659095048904, 0.029686294496059418, 0.027909675613045692, 0.027491949498653412, 0.026273267343640327, 0.022081682458519936, 0.021943196654319763, 0.012275857850909233]}",0.09902173280715942,Machine Translation and Multilinguality,0.09902173280715942
Machine Translation and Multilinguality,Multilingual BERT Post-Pretraining Alignment,"We propose a simple method to align multilingual contextual embeddings as a postpretraining step for improved cross-lingual transferability of the pretrained language models. Using parallel data, our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling. We also perform sentence-level code-switching with English when finetuning on downstream tasks. On XNLI, our best model (initialized from mBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves comparable result to XLM for translate-train while using less than 18% of the same parallel data and 31% fewer model parameters. On MLQA, our model outperforms XLM-R Base , which has 57% more parameters than ours.","{'sequence': 'We propose a simple method to align multilingual contextual embeddings as a postpretraining step for improved cross-lingual transferability of the pretrained language models. Using parallel data, our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling. We also perform sentence-level code-switching with English when finetuning on downstream tasks. On XNLI, our best model (initialized from mBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves comparable result to XLM for translate-train while using less than 18% of the same parallel data and 31% fewer model parameters. On MLQA, our model outperforms XLM-R Base , which has 57% more parameters than ours.', 'labels': ['Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Speech and Multimodality', 'Resources and Evaluation', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'NLP Applications', 'Generation', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07803871482610703, 0.07757527381181717, 0.07153082638978958, 0.07102909684181213, 0.06733334064483643, 0.0557849295437336, 0.05071508139371872, 0.049518052488565445, 0.047680679708719254, 0.046781010925769806, 0.04590729996562004, 0.04568944498896599, 0.036999501287937164, 0.03547336161136627, 0.034179165959358215, 0.028688186779618263, 0.02836199663579464, 0.028329862281680107, 0.02461429499089718, 0.022155120968818665, 0.0202284287661314, 0.017340652644634247, 0.016045620664954185]}",0.07803871482610703,Machine Translation and Multilinguality,0.07803871482610703
NLP Applications,A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks,"In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this gap by (i) collecting a multilingual social media corpus containing customer service conversations (865k tweets), (ii) comparing various pipelines of pretraining and finetuning approaches, (iii) applying them on 5 different end tasks. We show that pretraining a generic multilingual transformer model on our in-domain dataset, before finetuning on specific end tasks, consistently boosts performance, especially in non-English settings. 1","{'sequence': 'In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this gap by (i) collecting a multilingual social media corpus containing customer service conversations (865k tweets), (ii) comparing various pipelines of pretraining and finetuning approaches, (iii) applying them on 5 different end tasks. We show that pretraining a generic multilingual transformer model on our in-domain dataset, before finetuning on specific end tasks, consistently boosts performance, especially in non-English settings. 1', 'labels': ['Resources and Evaluation', 'NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1644502431154251, 0.15719102323055267, 0.12010461091995239, 0.08410593122243881, 0.06629697233438492, 0.05842965096235275, 0.04311837628483772, 0.03700555860996246, 0.03219542279839516, 0.02956855483353138, 0.028466638177633286, 0.025545572862029076, 0.020573455840349197, 0.020359983667731285, 0.018974876031279564, 0.016589129343628883, 0.012887140735983849, 0.012388937175273895, 0.011677993461489677, 0.011410247534513474, 0.011161791160702705, 0.008933605626225471, 0.008564349263906479]}",0.1644502431154251,Resources and Evaluation,0.15719102323055267
NLP Applications,Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases,"Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","{'sequence': 'Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Question Answering', 'Resources and Evaluation', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.17316819727420807, 0.06566622853279114, 0.06018151342868805, 0.06000271812081337, 0.05929918587207794, 0.058664627373218536, 0.04965423420071602, 0.04925566911697388, 0.041301801800727844, 0.036097344011068344, 0.035226136445999146, 0.033633407205343246, 0.03310910612344742, 0.03176200017333031, 0.03119228594005108, 0.031186267733573914, 0.02952045388519764, 0.023511862382292747, 0.02284916490316391, 0.021285738795995712, 0.020573796704411507, 0.017245357856154442, 0.015612928196787834]}",0.17316819727420807,NLP Applications,0.17316819727420807
NLP Applications,Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products,"Predicting the answer to a product-related question is an emerging field of research that recently attracted a lot of attention. Answering subjective and opinion-based questions is most challenging due to the dependency on customer-generated content. Previous works mostly focused on review-aware answer prediction; however, these approaches fail for new or unpopular products, having no (or only a few) reviews at hand. In this work, we propose a novel and complementary approach for predicting the answer for such questions, based on the answers for similar questions asked on similar products. We measure the contextual similarity between products based on the answers they provide for the same question. A mixture-of-expert framework is used to predict the answer by aggregating the answers from contextually similar products. Empirical results demonstrate that our model outperforms strong baselines on some segments of questions, namely those that have roughly ten or more similar resolved questions in the corpus. We additionally publish two large-scale datasets 1 used in this work, one is of similar product question pairs, and the second is of product question-answer pairs. * Work carried out during an internship at Amazon. † Work carried out while working at Amazon. 1 The datasets are freely available at https:// registry.opendata.aws under the names Amazon-PQSim and Amazon-PQA.","{'sequence': 'Predicting the answer to a product-related question is an emerging field of research that recently attracted a lot of attention. Answering subjective and opinion-based questions is most challenging due to the dependency on customer-generated content. Previous works mostly focused on review-aware answer prediction; however, these approaches fail for new or unpopular products, having no (or only a few) reviews at hand. In this work, we propose a novel and complementary approach for predicting the answer for such questions, based on the answers for similar questions asked on similar products. We measure the contextual similarity between products based on the answers they provide for the same question. A mixture-of-expert framework is used to predict the answer by aggregating the answers from contextually similar products. Empirical results demonstrate that our model outperforms strong baselines on some segments of questions, namely those that have roughly ten or more similar resolved questions in the corpus. We additionally publish two large-scale datasets 1 used in this work, one is of similar product question pairs, and the second is of product question-answer pairs. * Work carried out during an internship at Amazon. † Work carried out while working at Amazon. 1 The datasets are freely available at https:// registry.opendata.aws under the names Amazon-PQSim and Amazon-PQA.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Information Extraction', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1315980702638626, 0.06054465100169182, 0.05866244435310364, 0.053604137152433395, 0.05263747274875641, 0.050755467265844345, 0.04998845234513283, 0.04309507831931114, 0.04065276309847832, 0.04063190147280693, 0.04059910774230957, 0.037518568336963654, 0.03666352108120918, 0.03571052476763725, 0.035516802221536636, 0.03265871852636337, 0.032072365283966064, 0.031142279505729675, 0.03110375627875328, 0.030210556462407112, 0.0263528972864151, 0.02566000446677208, 0.0226205512881279]}",0.1315980702638626,Question Answering,0.035516802221536636
NLP Applications,EnSidNet: Enhanced Hybrid Siamese-Deep Network for grouping clinical trials into drug-development pathways,"Siamese Neural Networks have been 2 widely used to perform similarity 3 classification in multi-class settings. Their 4 architecture can be used to group the 5 clinical trials belonging to the same drug-6 development pathway along the several 7 clinical trial phases. Here we present an 8 approach for the unmet need of drug-9 development pathway reconstruction, 10 based on an Enhanced hybrid Siamese-11 Deep Neural Network (EnSidNet). The 12 proposed model demonstrates significant 13 improvement above baselines in a 1-shot 14 evaluation setting and in a classical 15 similarity setting. EnSidNet can be an 16 essential tool in a semi-supervised 17 learning environment: by selecting 18 clinical trials highly likely to belong to the 19 same drug-development pathway it is 20 possible to speed up the labelling process 21 of human experts, allowing the check of a 22 consistent volume of data, further used in 23 the model's training dataset. 24 1 Introduction 25 Siamese Neural Networks (SNN) were developed 26 in the early 1990s (Bromley et al., 1994) to obtain 27 a similarity score from examples of signatures 28 with the goal of identifying forgery. From then 29 many applications used SNN, primarily on image 30 recognition tasks (Chopra et al., 2005). The basic 31 architecture of SNN consists of two identical 32 networks able to learn the hidden representation 33 of the inputs. A similarity function would then 34 compare the inputs hidden representations. The 35 similarity score was taken advantage of in 36 contexts like 1-shot learning in multiclass-37 classification problems, where a single example 38 of a class was seen by the algorithm only once 39 before making inference (Koch et al., 2015). 40 Different architectures of SNN were developed in 41 time: Simo-Serra and colleagues developed a 3-42 inputs SNN (Simo-Serra et al., 2015), where the 43 neural network learned to rank the outputs and 44 identify whether the reference's hidden 45 representation is more similar to a positive or a 46 negative sample. 47 Another example involves the insertion of an 48 intermediate stage between the similarity score 49 layer and the final prediction layer (Subramaniam, 50 Chatterjee, and Mittal, 2016), allowing to increase 51 performance in person re-identification task 52 despite partial occlusion and difference in point of 53 view or illumination. 54 The first applications of SNN were based on 55 Convolutional Neural Networks (CNN) to obtain 56 similarity score on images (Simo-Serra et al., 57 2015), seeing SNN involved in different tasks 58 such as patch identification (Simo-Serra et al., 59 2015), person identification (Ahmed et al., 2015), 60 image matching from different angles (Vo and 61 Hays, 2016). SNN was also explored in Natural 62 Language Processing (NLP) contexts in tasks like 63 identifying sentence similarity (Mueller and 64 Thyagarajan, 2016) and support relation for 65 argumentation (Gema et al., 2017). These 66 applications highlight the flexibility of SNN to 67 identify similarities in different contexts. Here we 68 apply this architecture on an unmet healthcare 69 task: grouping clinical trials belonging to the same 70 drug-development pathway. 71 Before being released on the market a new drug 72 needs to go through several expensive and time-73 consuming experiments, involving testing the 74 pharmacological characteristics of the drug in","{'sequence': ""Siamese Neural Networks have been 2 widely used to perform similarity 3 classification in multi-class settings. Their 4 architecture can be used to group the 5 clinical trials belonging to the same drug-6 development pathway along the several 7 clinical trial phases. Here we present an 8 approach for the unmet need of drug-9 development pathway reconstruction, 10 based on an Enhanced hybrid Siamese-11 Deep Neural Network (EnSidNet). The 12 proposed model demonstrates significant 13 improvement above baselines in a 1-shot 14 evaluation setting and in a classical 15 similarity setting. EnSidNet can be an 16 essential tool in a semi-supervised 17 learning environment: by selecting 18 clinical trials highly likely to belong to the 19 same drug-development pathway it is 20 possible to speed up the labelling process 21 of human experts, allowing the check of a 22 consistent volume of data, further used in 23 the model's training dataset. 24 1 Introduction 25 Siamese Neural Networks (SNN) were developed 26 in the early 1990s (Bromley et al., 1994) to obtain 27 a similarity score from examples of signatures 28 with the goal of identifying forgery. From then 29 many applications used SNN, primarily on image 30 recognition tasks (Chopra et al., 2005). The basic 31 architecture of SNN consists of two identical 32 networks able to learn the hidden representation 33 of the inputs. A similarity function would then 34 compare the inputs hidden representations. The 35 similarity score was taken advantage of in 36 contexts like 1-shot learning in multiclass-37 classification problems, where a single example 38 of a class was seen by the algorithm only once 39 before making inference (Koch et al., 2015). 40 Different architectures of SNN were developed in 41 time: Simo-Serra and colleagues developed a 3-42 inputs SNN (Simo-Serra et al., 2015), where the 43 neural network learned to rank the outputs and 44 identify whether the reference's hidden 45 representation is more similar to a positive or a 46 negative sample. 47 Another example involves the insertion of an 48 intermediate stage between the similarity score 49 layer and the final prediction layer (Subramaniam, 50 Chatterjee, and Mittal, 2016), allowing to increase 51 performance in person re-identification task 52 despite partial occlusion and difference in point of 53 view or illumination. 54 The first applications of SNN were based on 55 Convolutional Neural Networks (CNN) to obtain 56 similarity score on images (Simo-Serra et al., 57 2015), seeing SNN involved in different tasks 58 such as patch identification (Simo-Serra et al., 59 2015), person identification (Ahmed et al., 2015), 60 image matching from different angles (Vo and 61 Hays, 2016). SNN was also explored in Natural 62 Language Processing (NLP) contexts in tasks like 63 identifying sentence similarity (Mueller and 64 Thyagarajan, 2016) and support relation for 65 argumentation (Gema et al., 2017). These 66 applications highlight the flexibility of SNN to 67 identify similarities in different contexts. Here we 68 apply this architecture on an unmet healthcare 69 task: grouping clinical trials belonging to the same 70 drug-development pathway. 71 Before being released on the market a new drug 72 needs to go through several expensive and time-73 consuming experiments, involving testing the 74 pharmacological characteristics of the drug in"", 'labels': ['NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Summarization', 'Information Extraction', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation'], 'scores': [0.15543211996555328, 0.07712940126657486, 0.06096656993031502, 0.05790834501385689, 0.05326751247048378, 0.04452843219041824, 0.043943632394075394, 0.04196856915950775, 0.03796309977769852, 0.03689117729663849, 0.03558490425348282, 0.03524421155452728, 0.034609533846378326, 0.034079909324645996, 0.03385017439723015, 0.03240140154957771, 0.03233591467142105, 0.03011442720890045, 0.028945403173565865, 0.026471663266420364, 0.025724999606609344, 0.023072823882102966, 0.017565760761499405]}",0.15543211996555328,NLP Applications,0.15543211996555328
NLP Applications,DATE: Detecting Anomalies in Text via Self-Supervision of Transformers,"Leveraging deep learning models for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequencelevel. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the semi-supervised setting, we outperform state-of-the-art results by +13.5% and +6.9%, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10% of its training data is contaminated with outliers (compared with 0% for the others). C More qualitative and quantitative Results In Fig. 6 we show more qualitative results, trained on different inliers. To encourage further more detailed comparisons, we report the AUPR metric on AG News for inliers and outliers (see Tab. 3). When all the other metrics are almost saturated, we notice that AUPR-in better captures the performance on a certain split.","{'sequence': 'Leveraging deep learning models for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequencelevel. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the semi-supervised setting, we outperform state-of-the-art results by +13.5% and +6.9%, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10% of its training data is contaminated with outliers (compared with 0% for the others). C More qualitative and quantitative Results In Fig. 6 we show more qualitative results, trained on different inliers. To encourage further more detailed comparisons, we report the AUPR metric on AG News for inliers and outliers (see Tab. 3). When all the other metrics are almost saturated, we notice that AUPR-in better captures the performance on a certain split.', 'labels': ['Resources and Evaluation', 'Summarization', 'Question Answering', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Information Extraction', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Ethics and NLP', 'NLP Applications', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07681550830602646, 0.07535505294799805, 0.07371682673692703, 0.06490706652402878, 0.05978746339678764, 0.05927908793091774, 0.05168784037232399, 0.04885487258434296, 0.04504844546318054, 0.043353401124477386, 0.04157232493162155, 0.03873395174741745, 0.03695207089185715, 0.03666657954454422, 0.03380417823791504, 0.031966399401426315, 0.03075404465198517, 0.02771083638072014, 0.02679312974214554, 0.026118772104382515, 0.024953017011284828, 0.022719839587807655, 0.022449277341365814]}",0.07681550830602646,Resources and Evaluation,0.03380417823791504
NLP Applications,A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code,"There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy implementation. We show that the proposed OOV anonymization method significantly improves the performance of the Transformer in two code processing tasks: code completion and bug fixing.","{'sequence': 'There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy implementation. We show that the proposed OOV anonymization method significantly improves the performance of the Transformer in two code processing tasks: code completion and bug fixing.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Question Answering', 'Information Extraction', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11093517392873764, 0.10211987793445587, 0.06490372866392136, 0.06194669380784035, 0.059032611548900604, 0.057436466217041016, 0.048670150339603424, 0.044897280633449554, 0.04330167919397354, 0.040309615433216095, 0.038369741290807724, 0.03633006289601326, 0.03477747365832329, 0.03340080752968788, 0.030286774039268494, 0.028677338734269142, 0.026644781231880188, 0.02662755362689495, 0.025495601817965508, 0.0247245654463768, 0.02447204291820526, 0.023219184949994087, 0.013420775532722473]}",0.11093517392873764,Machine Learning for NLP,0.10211987793445587
NLP Applications,Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition,"We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for classification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both complexity and scalability when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances.","{'sequence': 'We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for classification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both complexity and scalability when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Summarization', 'Resources and Evaluation', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'NLP Applications', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1307043880224228, 0.08960461616516113, 0.08330277353525162, 0.06393051892518997, 0.06157940998673439, 0.05083518102765083, 0.04651699587702751, 0.04464828968048096, 0.04123414680361748, 0.038462456315755844, 0.037323299795389175, 0.036845553666353226, 0.03637280315160751, 0.036289505660533905, 0.0330931581556797, 0.02941216342151165, 0.02752000279724598, 0.025088801980018616, 0.023781629279255867, 0.023761453106999397, 0.016766628250479698, 0.013375071808695793, 0.009551051072776318]}",0.1307043880224228,Speech and Multimodality,0.036289505660533905
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks,"There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While crossencoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance. 1","{'sequence': 'There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While crossencoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance. 1', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Generation', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.22916316986083984, 0.08714663237333298, 0.0712411031126976, 0.05561969056725502, 0.04226502403616905, 0.03857718035578728, 0.0380072183907032, 0.037590499967336655, 0.03726543113589287, 0.037238024175167084, 0.03647639602422714, 0.036392927169799805, 0.03498702868819237, 0.030230851843953133, 0.02900991588830948, 0.028379902243614197, 0.022758588194847107, 0.02159062959253788, 0.020434459671378136, 0.019419647753238678, 0.017244089394807816, 0.015467538498342037, 0.013493916019797325]}",0.22916316986083984,Resources and Evaluation,0.0712411031126976
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",SmBoP: Semi-autoregressive Bottom-up Semantic Parsing,"The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.","{'sequence': 'The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Generation', 'Machine Learning for NLP', 'Speech and Multimodality', 'Ethics and NLP', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10551603883504868, 0.09570037573575974, 0.06628239154815674, 0.0646088644862175, 0.061194781213998795, 0.05488371476531029, 0.04879423603415489, 0.046614278107881546, 0.042708322405815125, 0.04142243415117264, 0.040059756487607956, 0.03793663531541824, 0.037833187729120255, 0.035573810338974, 0.03260485455393791, 0.03068157099187374, 0.02863677591085434, 0.026523659005761147, 0.02366214245557785, 0.022239815443754196, 0.021700110286474228, 0.01904677040874958, 0.015775471925735474]}",0.10551603883504868,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.10551603883504868
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",SmBoP: Semi-autoregressive Bottom-up Semantic Parsing,"The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.","{'sequence': 'The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Generation', 'Machine Learning for NLP', 'Speech and Multimodality', 'Ethics and NLP', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10551603883504868, 0.09570037573575974, 0.06628239154815674, 0.0646088644862175, 0.061194781213998795, 0.05488371476531029, 0.04879423603415489, 0.046614278107881546, 0.042708322405815125, 0.04142243415117264, 0.040059756487607956, 0.03793663531541824, 0.037833187729120255, 0.035573810338974, 0.03260485455393791, 0.03068157099187374, 0.02863677591085434, 0.026523659005761147, 0.02366214245557785, 0.022239815443754196, 0.021700110286474228, 0.01904677040874958, 0.015775471925735474]}",0.10551603883504868,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.10551603883504868
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,"Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl.","{'sequence': 'Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl.', 'labels': ['Machine Translation and Multilinguality', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Discourse and Pragmatics', 'Information Extraction', 'Resources and Evaluation', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0914725735783577, 0.07777337729930878, 0.06787931174039841, 0.0642898678779602, 0.059342093765735626, 0.05442279204726219, 0.05148990824818611, 0.04721290245652199, 0.04252256080508232, 0.04147779569029808, 0.04145112633705139, 0.04136386886239052, 0.04104871302843094, 0.0395871065557003, 0.03876837342977524, 0.037601545453071594, 0.03312263265252113, 0.033075593411922455, 0.02813369408249855, 0.023550907149910927, 0.017865614965558052, 0.016504637897014618, 0.010043017566204071]}",0.0914725735783577,Machine Translation and Multilinguality,0.06787931174039841
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Fool Me Twice: Entailment from Wikipedia Gamification,"We release FOOLMETWICE (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using ""shortcuts"" compared to other entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players ""pay"" to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks. We open source the dataset and game code. 1","{'sequence': 'We release FOOLMETWICE (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using ""shortcuts"" compared to other entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players ""pay"" to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks. We open source the dataset and game code. 1', 'labels': ['Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Resources and Evaluation', 'Discourse and Pragmatics', 'NLP Applications', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Generation', 'Question Answering', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.14384011924266815, 0.08249159157276154, 0.0628761425614357, 0.052606258541345596, 0.04858522117137909, 0.04753363877534866, 0.04747788980603218, 0.04466161131858826, 0.04307489097118378, 0.04284731298685074, 0.042056117206811905, 0.03725975751876831, 0.03637085109949112, 0.03591848164796829, 0.0338268056511879, 0.03031919151544571, 0.03022187203168869, 0.02940758876502514, 0.025589754804968834, 0.02473762258887291, 0.020701702684164047, 0.019597414880990982, 0.01799810118973255]}",0.14384011924266815,Dialogue and Interactive Systems,0.04466161131858826
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Meta-Learning for Domain Generalization in Semantic Parsing,"The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing outof-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a modelagnostic training algorithm that simulates zeroshot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.","{'sequence': 'The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing outof-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a modelagnostic training algorithm that simulates zeroshot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.', 'labels': ['Generation', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'NLP Applications', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Summarization', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13203009963035583, 0.08866940438747406, 0.08159615844488144, 0.08093959838151932, 0.058315400034189224, 0.05349474400281906, 0.05098268389701843, 0.04058258235454559, 0.038166336715221405, 0.037014205008745193, 0.036820944398641586, 0.03497682884335518, 0.03475693240761757, 0.03374912962317467, 0.030657516792416573, 0.029746035113930702, 0.02820849046111107, 0.02438528649508953, 0.021535007283091545, 0.019916418939828873, 0.015803616493940353, 0.015531636774539948, 0.012120917439460754]}",0.13203009963035583,Generation,0.058315400034189224
Generation,Aspect-Controlled Neural Argument Generation,"We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL-a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL. 1","{'sequence': 'We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL-a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL. 1', 'labels': ['Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'NLP Applications', 'Machine Translation and Multilinguality', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Ethics and NLP', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.19062316417694092, 0.15377183258533478, 0.08091609179973602, 0.06662466377019882, 0.046169351786375046, 0.04516630992293358, 0.03932453319430351, 0.038312576711177826, 0.034192219376564026, 0.03234200179576874, 0.030938465148210526, 0.027975991368293762, 0.027107540518045425, 0.02648632973432541, 0.026150571182370186, 0.020292432978749275, 0.01820022612810135, 0.018162868916988373, 0.017356688156723976, 0.017344320192933083, 0.017221244052052498, 0.01677199825644493, 0.008548582904040813]}",0.19062316417694092,Generation,0.19062316417694092
Generation,Text Generation from Discourse Representation Structures,"We propose neural models to generate text from formal meaning representations based on Discourse Representation Structures (DRSs). DRSs are document-level representations which encode rich semantic detail pertaining to rhetorical relations, presupposition, and co-reference within and across sentences. We formalize the task of neural DRS-to-text generation and provide modeling solutions for the problems of condition ordering and variable naming which render generation from DRSs non-trivial. Our generator relies on a novel sibling treeLSTM model which is able to accurately represent DRS structures and is more generally suited to trees with wide branches. We achieve competitive performance (59.48 BLEU) on the GMB benchmark against several strong baselines.","{'sequence': 'We propose neural models to generate text from formal meaning representations based on Discourse Representation Structures (DRSs). DRSs are document-level representations which encode rich semantic detail pertaining to rhetorical relations, presupposition, and co-reference within and across sentences. We formalize the task of neural DRS-to-text generation and provide modeling solutions for the problems of condition ordering and variable naming which render generation from DRSs non-trivial. Our generator relies on a novel sibling treeLSTM model which is able to accurately represent DRS structures and is more generally suited to trees with wide branches. We achieve competitive performance (59.48 BLEU) on the GMB benchmark against several strong baselines.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Machine Learning for NLP', 'Question Answering', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.28162431716918945, 0.06873307377099991, 0.06840373575687408, 0.06518853455781937, 0.056777432560920715, 0.04601650685071945, 0.0384628064930439, 0.036531753838062286, 0.033675115555524826, 0.031078962609171867, 0.03003815934062004, 0.02886166423559189, 0.02454248070716858, 0.024388564750552177, 0.022336089983582497, 0.020756740123033524, 0.020327642560005188, 0.01956421323120594, 0.018903791904449463, 0.018671220168471336, 0.016586555168032646, 0.014902687631547451, 0.013627896085381508]}",0.28162431716918945,Generation,0.28162431716918945
Generation,APo-VAE: Text Generation in Hyperbolic Space,"Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-ofthe-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. We investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincaré Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincaré ball via wrapped normal distributions. By adopting the primal-dual formulation of Kullback-Leibler divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.","{'sequence': 'Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-ofthe-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. We investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincaré Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincaré ball via wrapped normal distributions. By adopting the primal-dual formulation of Kullback-Leibler divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Machine Learning for NLP', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3220079839229584, 0.057483602315187454, 0.05408046394586563, 0.04930318892002106, 0.04430598020553589, 0.04344833269715309, 0.04252802953124046, 0.03681904822587967, 0.03599976375699043, 0.03508847951889038, 0.033651918172836304, 0.03012319654226303, 0.030058082193136215, 0.02925161086022854, 0.025812268257141113, 0.02214343287050724, 0.020251823589205742, 0.01972123607993126, 0.016932548955082893, 0.016849350184202194, 0.01325551699846983, 0.012860319577157497, 0.00802379660308361]}",0.3220079839229584,Generation,0.3220079839229584
Generation,DART: Open-Domain Structured Data Record to Text Generation,"We present DART, an open domain structured DAta-Record-to-Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart.","{'sequence': 'We present DART, an open domain structured DAta-Record-to-Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart.', 'labels': ['Generation', 'Resources and Evaluation', 'Information Extraction', 'Question Answering', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.23248544335365295, 0.12210118025541306, 0.08431455492973328, 0.058441631495952606, 0.05077159404754639, 0.049982327967882156, 0.039807531982660294, 0.03551430255174637, 0.03418189659714699, 0.03381745517253876, 0.02659187838435173, 0.023999767377972603, 0.023373866453766823, 0.02289898507297039, 0.022460374981164932, 0.02165169268846512, 0.020910264924168587, 0.019553594291210175, 0.01872597634792328, 0.016252925619482994, 0.01605469360947609, 0.014097984880208969, 0.012010131031274796]}",0.23248544335365295,Generation,0.23248544335365295
Generation,TuringAdvice: A Generative and Dynamic Evaluation of Language Use,"We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today's models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.","{'sequence': ""We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today's models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress."", 'labels': ['Resources and Evaluation', 'NLP Applications', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Machine Learning for NLP', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Information Extraction', 'Ethics and NLP', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1074453741312027, 0.07876971364021301, 0.07183316349983215, 0.06509490311145782, 0.06412322074174881, 0.06255686283111572, 0.054215285927057266, 0.0541648231446743, 0.04834499582648277, 0.0407564602792263, 0.03733618184924126, 0.035117823630571365, 0.03472784906625748, 0.03397151827812195, 0.033062443137168884, 0.033005498349666595, 0.031791966408491135, 0.02873995713889599, 0.024176834151148796, 0.016742153093218803, 0.016360405832529068, 0.01635102555155754, 0.011311554349958897]}",0.1074453741312027,Resources and Evaluation,0.054215285927057266
Machine Translation and Multilinguality,When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models,"Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-theart performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages. 1","{'sequence': 'Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-theart performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages. 1', 'labels': ['Speech and Multimodality', 'NLP Applications', 'Machine Learning for NLP', 'Question Answering', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08227217942476273, 0.08029834926128387, 0.06680291146039963, 0.06456536054611206, 0.061379529535770416, 0.05457047000527382, 0.05348268523812294, 0.051955461502075195, 0.04055197909474373, 0.0389883890748024, 0.03858795017004013, 0.038413155823946, 0.036623936146497726, 0.03647105023264885, 0.03545223921537399, 0.03391534462571144, 0.03268235921859741, 0.031250517815351486, 0.03096390701830387, 0.02614426240324974, 0.025771304965019226, 0.021092496812343597, 0.01776413433253765]}",0.08227217942476273,Speech and Multimodality,0.061379529535770416
Machine Translation and Multilinguality,Multi-Adversarial Learning for Cross-Lingual Word Embeddings,"Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings -maps of matching words across languages-without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages.","{'sequence': ""Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings -maps of matching words across languages-without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages."", 'labels': ['Generation', 'Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3599073886871338, 0.0700140967965126, 0.06121016666293144, 0.04130249097943306, 0.04092197120189667, 0.039151277393102646, 0.038133323192596436, 0.030778203159570694, 0.030360104516148567, 0.026269588619470596, 0.026191484183073044, 0.024966945871710777, 0.02483588084578514, 0.023965271189808846, 0.022400200366973877, 0.01987561210989952, 0.019533727318048477, 0.01911425217986107, 0.018944287672638893, 0.01886119320988655, 0.015993233770132065, 0.013995650224387646, 0.013273625634610653]}",0.3599073886871338,Generation,0.026269588619470596
Machine Translation and Multilinguality,Multi-view Subword Regularization,"Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020)  show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms. 1","{'sequence': 'Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020)  show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms. 1', 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Computational Social Science and Social Media', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Summarization', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining', 'NLP Applications'], 'scores': [0.08381902426481247, 0.07070158421993256, 0.0704212412238121, 0.06380598247051239, 0.05332746356725693, 0.053315822035074234, 0.05079229548573494, 0.0468742661178112, 0.04422269016504288, 0.041670262813568115, 0.03830710053443909, 0.03773649409413338, 0.0368313267827034, 0.03670686110854149, 0.03432421013712883, 0.03355352580547333, 0.03255549818277359, 0.03216494992375374, 0.03189004212617874, 0.028834275901317596, 0.02634434960782528, 0.025968899950385094, 0.025831783190369606]}",0.08381902426481247,Speech and Multimodality,0.0704212412238121
Machine Translation and Multilinguality,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,"The recent ""Text-to-Text Transfer Transformer"" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent ""accidental translation"" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available. 1","{'sequence': 'The recent ""Text-to-Text Transfer Transformer"" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent ""accidental translation"" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available. 1', 'labels': ['Machine Learning for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Semantics: Lexical Semantics', 'Question Answering', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15597288310527802, 0.10451285541057587, 0.09587890654802322, 0.0761898085474968, 0.06496386229991913, 0.06288918852806091, 0.05489430949091911, 0.048352256417274475, 0.031027747318148613, 0.027479326352477074, 0.02733037993311882, 0.02676798775792122, 0.025028886273503304, 0.024367844685912132, 0.023931749165058136, 0.023546874523162842, 0.023292234167456627, 0.021922709420323372, 0.02148166485130787, 0.01653057336807251, 0.016415607184171677, 0.01515304297208786, 0.012069234624505043]}",0.15597288310527802,Machine Learning for NLP,0.10451285541057587
Machine Translation and Multilinguality,MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning,"The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for lowresource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an under-studied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages (Singh et al., 2019) , bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages -without access to large-scale monolingual corpora or large amounts of labeled data -for tasks like crosslingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.","{'sequence': 'The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for lowresource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an under-studied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages (Singh et al., 2019) , bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages -without access to large-scale monolingual corpora or large amounts of labeled data -for tasks like crosslingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Ethics and NLP', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09854938834905624, 0.08353293687105179, 0.06944305449724197, 0.06828518956899643, 0.05996109917759895, 0.05987550690770149, 0.051064200699329376, 0.04439616948366165, 0.04433318227529526, 0.042085904628038406, 0.03790904954075813, 0.03756682947278023, 0.03557846322655678, 0.03524157032370567, 0.03456518426537514, 0.03449587523937225, 0.032411105930805206, 0.027876868844032288, 0.024753188714385033, 0.024704650044441223, 0.024310393258929253, 0.02062714472413063, 0.008433038368821144]}",0.09854938834905624,Machine Learning for NLP,0.06828518956899643
Question Answering,Open Domain Question Answering over Tables via Dense Retrieval,"Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of NATURAL QUESTIONS (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever. * Work completed while interning at Google. S read (""Chlorine"") S read (...) S read (""Latin"") which element is named for the greek word for green? TAPAS q (q) List of chemical element name etymologies Element Origin Meaning ... Fluorine Latin a flowing ...","{'sequence': 'Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of NATURAL QUESTIONS (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever. * Work completed while interning at Google. S read (""Chlorine"") S read (...) S read (""Latin"") which element is named for the greek word for green? TAPAS q (q) List of chemical element name etymologies Element Origin Meaning ... Fluorine Latin a flowing ...', 'labels': ['Question Answering', 'Information Extraction', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Generation', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.1499258577823639, 0.09446129202842712, 0.076307013630867, 0.05806627869606018, 0.052953265607357025, 0.047294579446315765, 0.043598756194114685, 0.04315049946308136, 0.0430988147854805, 0.03979192674160004, 0.03928261995315552, 0.03648429363965988, 0.035912156105041504, 0.035132572054862976, 0.02929014153778553, 0.028651339933276176, 0.024477042257785797, 0.022938070818781853, 0.021807434037327766, 0.021548504009842873, 0.019862769171595573, 0.019103555008769035, 0.016861172392964363]}",0.1499258577823639,Question Answering,0.1499258577823639
Question Answering,Open-Domain Question Answering Goes Conversational via Question Rewriting,"We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.","{'sequence': 'We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Generation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13315638899803162, 0.10619273036718369, 0.06909463554620743, 0.05702551454305649, 0.05653604120016098, 0.049068134278059006, 0.04574790596961975, 0.04430217295885086, 0.04144034907221794, 0.038184747099876404, 0.03558148071169853, 0.0345134399831295, 0.03439319133758545, 0.033257268369197845, 0.03269081190228462, 0.029586974531412125, 0.02870878577232361, 0.02507701888680458, 0.02482939325273037, 0.023152077570557594, 0.022840965539216995, 0.017683548852801323, 0.01693633757531643]}",0.13315638899803162,Resources and Evaluation,0.10619273036718369
Question Answering,QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering,"The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.","{'sequence': 'The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Generation', 'NLP Applications', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Summarization', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2580912411212921, 0.0715537965297699, 0.06356354802846909, 0.06291032582521439, 0.05222964286804199, 0.044330716133117676, 0.03924568369984627, 0.03923989459872246, 0.03732626885175705, 0.03587464988231659, 0.03572855144739151, 0.032026857137680054, 0.028520498424768448, 0.028202524408698082, 0.024847842752933502, 0.022328538820147514, 0.021689224988222122, 0.020097507163882256, 0.020059602335095406, 0.0188254676759243, 0.01633988320827484, 0.015589257702231407, 0.011378524824976921]}",0.2580912411212921,Question Answering,0.2580912411212921
Question Answering,XOR QA: Cross-lingual Open-Retrieval Question Answering,"Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity-where languages have few reference articles-and information asymmetry-where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TYDI QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving crosslingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington. edu/xorqa/.","{'sequence': 'Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity-where languages have few reference articles-and information asymmetry-where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TYDI QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving crosslingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington. edu/xorqa/.', 'labels': ['Question Answering', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Information Retrieval and Text Mining', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.29648250341415405, 0.08432216197252274, 0.06227989122271538, 0.05967000499367714, 0.05443541333079338, 0.05214734002947807, 0.03684651479125023, 0.03234052658081055, 0.028217999264597893, 0.028003038838505745, 0.02760665863752365, 0.026270581409335136, 0.02601504698395729, 0.024017954245209694, 0.023755289614200592, 0.02076742611825466, 0.020719014108181, 0.01984984241425991, 0.019770218059420586, 0.018733445554971695, 0.017835306003689766, 0.012836216948926449, 0.007077676244080067]}",0.29648250341415405,Question Answering,0.29648250341415405
Question Answering,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,"We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 opendomain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.","{'sequence': 'We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 opendomain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'NLP Applications', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.5537390112876892, 0.03990970551967621, 0.028329672291874886, 0.028272783383727074, 0.027542848140001297, 0.02532569132745266, 0.024090774357318878, 0.024017736315727234, 0.02397082932293415, 0.022002728655934334, 0.02182968705892563, 0.019516125321388245, 0.01934937946498394, 0.018734851852059364, 0.01799226552248001, 0.017581366002559662, 0.01728302426636219, 0.01403782982379198, 0.013545531779527664, 0.012912625446915627, 0.012492307461798191, 0.009362715296447277, 0.008160614408552647]}",0.5537390112876892,Question Answering,0.5537390112876892
Summarization,Extending Multi-Document Summarization Evaluation to the Interactive Setting,"Allowing users to interact with multidocument summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability.","{'sequence': 'Allowing users to interact with multidocument summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Summarization', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Information Extraction', 'NLP Applications', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13688500225543976, 0.07634621858596802, 0.07154984772205353, 0.06702004373073578, 0.057490814477205276, 0.050810858607292175, 0.043477173894643784, 0.042880650609731674, 0.04242965579032898, 0.03819286450743675, 0.03558840975165367, 0.03466799855232239, 0.03354455530643463, 0.031120922416448593, 0.03100653737783432, 0.028782347217202187, 0.028084993362426758, 0.027990128844976425, 0.026959173381328583, 0.026494799181818962, 0.026206979528069496, 0.025127481669187546, 0.017342546954751015]}",0.13688500225543976,Resources and Evaluation,0.06702004373073578
Summarization,Identifying Helpful Sentences in Product Reviews,"In recent years online shopping has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions: first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This task is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a dataset in English of sentence helpfulness scores via crowd-sourcing and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines.","{'sequence': 'In recent years online shopping has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions: first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This task is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a dataset in English of sentence helpfulness scores via crowd-sourcing and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines.', 'labels': ['Summarization', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Generation', 'Question Answering', 'NLP Applications', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12301450222730637, 0.09081629663705826, 0.08215891569852829, 0.07019179314374924, 0.0698608011007309, 0.06758414953947067, 0.055633220821619034, 0.0448375903069973, 0.03837575763463974, 0.037798285484313965, 0.03644575551152229, 0.0342668853700161, 0.034027811139822006, 0.03304100036621094, 0.03277050331234932, 0.03060351125895977, 0.026252470910549164, 0.021045779809355736, 0.01611352153122425, 0.01576392538845539, 0.014377987943589687, 0.01294198352843523, 0.01207755133509636]}",0.12301450222730637,Summarization,0.12301450222730637
Summarization,Noisy Self-Knowledge Distillation for Text Summarization,"In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximumlikelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and nonpretrained summarizers achieving state-of-theart results. 1","{'sequence': 'In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximumlikelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and nonpretrained summarizers achieving state-of-theart results. 1', 'labels': ['Summarization', 'Generation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Speech and Multimodality', 'Ethics and NLP', 'NLP Applications', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Question Answering', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.3163415193557739, 0.07227818667888641, 0.0572507418692112, 0.05662621557712555, 0.04173586145043373, 0.041375041007995605, 0.039286065846681595, 0.037757981568574905, 0.03459756076335907, 0.03100043721497059, 0.030106641352176666, 0.027598261833190918, 0.026957789435982704, 0.026773137971758842, 0.026723919436335564, 0.023603394627571106, 0.021510355174541473, 0.021153438836336136, 0.01794722117483616, 0.014890559948980808, 0.013034498319029808, 0.010890765115618706, 0.010560444556176662]}",0.3163415193557739,Summarization,0.3163415193557739
Summarization,Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation,"Models pretrained with self-supervised objectives on large text corpora achieve state-of-theart performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-ofthe-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ data augmentation via round-trip translation as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.","{'sequence': 'Models pretrained with self-supervised objectives on large text corpora achieve state-of-theart performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-ofthe-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ data augmentation via round-trip translation as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.', 'labels': ['Summarization', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Question Answering', 'Computational Social Science and Social Media', 'Generation', 'Ethics and NLP', 'Information Extraction', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24194739758968353, 0.062437478452920914, 0.05566919595003128, 0.05424552783370018, 0.04615103825926781, 0.04201948642730713, 0.04090912267565727, 0.039099544286727905, 0.03801092877984047, 0.037593625485897064, 0.03723583370447159, 0.03633490949869156, 0.034465353935956955, 0.03323810175061226, 0.03161800652742386, 0.024888044223189354, 0.023180026561021805, 0.022946083918213844, 0.022393332794308662, 0.022369500249624252, 0.021210791543126106, 0.0160661693662405, 0.01597043126821518]}",0.24194739758968353,Summarization,0.24194739758968353
Summarization,Enhancing Factual Consistency of Abstractive Summarization,"Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASUM to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results 1 show that the fact-aware summarization can produce abstractive summaries with higher factual consistency compared with existing systems, and the correction model improves the factual consistency of given summaries via modifying only a few keywords.","{'sequence': 'Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASUM to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results 1 show that the fact-aware summarization can produce abstractive summaries with higher factual consistency compared with existing systems, and the correction model improves the factual consistency of given summaries via modifying only a few keywords.', 'labels': ['Summarization', 'Information Extraction', 'Generation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Question Answering', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.16358163952827454, 0.12529483437538147, 0.10690689086914062, 0.06690191477537155, 0.05873773247003555, 0.05128048360347748, 0.04179035872220993, 0.04108905419707298, 0.04094811901450157, 0.038823727518320084, 0.03330268710851669, 0.0318017303943634, 0.02467009797692299, 0.024056272581219673, 0.022361772134900093, 0.021492308005690575, 0.02038566954433918, 0.01873980090022087, 0.018066558986902237, 0.015862617641687393, 0.013069950975477695, 0.010502217337489128, 0.010333715938031673]}",0.16358163952827454,Summarization,0.16358163952827454
Dialogue and Interactive Systems,Few-shot Intent Classification and Slot Filling with Retrieved Examples,"Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is nontrivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.","{'sequence': 'Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is nontrivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.', 'labels': ['Dialogue and Interactive Systems', 'Machine Learning for NLP', 'NLP Applications', 'Question Answering', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Generation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08876697719097137, 0.08689235150814056, 0.08006677776575089, 0.06668728590011597, 0.055221810936927795, 0.054769366979599, 0.05335477367043495, 0.04975448548793793, 0.044560398906469345, 0.04303945600986481, 0.03970355540513992, 0.03948136419057846, 0.0350269190967083, 0.034260597079992294, 0.032307449728250504, 0.03054811805486679, 0.028144052252173424, 0.027489610016345978, 0.024523096159100533, 0.023993447422981262, 0.022034039720892906, 0.021349232643842697, 0.018024830147624016]}",0.08876697719097137,Dialogue and Interactive Systems,0.08876697719097137
Dialogue and Interactive Systems,Human-like informative conversations: Better acknowledgements using conditional mutual information,"This work aims to build a dialogue agent that can weave new factual content into conversations as naturally as humans. We draw insights from linguistic principles of conversational analysis and annotate human-human conversations from the Switchboard Dialog Act Corpus to examine humans strategies for acknowledgement, transition, detail selection and presentation. When current chatbots (explicitly provided with new factual content) introduce facts into a conversation, their generated responses do not acknowledge the prior turns. This is because models trained with two contexts -new factual content and conversational history -generate responses that are non-specific w.r.t. one of the contexts, typically the conversational history. We show that specificity w.r.t. conversational history is better captured by pointwise conditional mutual information (pcmi h ) than by the established use of pointwise mutual information (pmi). Our proposed method, Fused-PCMI, trades off pmi for pcmi h and is preferred by humans for overall quality over the Max-PMI baseline 60% of the time. Human evaluators also judge responses with higher pcmi h better at acknowledgement 74% of the time. The results demonstrate that systems mimicking human conversational traits (in this case acknowledgement) improve overall quality and more broadly illustrate the utility of linguistic principles in improving dialogue agents.","{'sequence': 'This work aims to build a dialogue agent that can weave new factual content into conversations as naturally as humans. We draw insights from linguistic principles of conversational analysis and annotate human-human conversations from the Switchboard Dialog Act Corpus to examine humans strategies for acknowledgement, transition, detail selection and presentation. When current chatbots (explicitly provided with new factual content) introduce facts into a conversation, their generated responses do not acknowledge the prior turns. This is because models trained with two contexts -new factual content and conversational history -generate responses that are non-specific w.r.t. one of the contexts, typically the conversational history. We show that specificity w.r.t. conversational history is better captured by pointwise conditional mutual information (pcmi h ) than by the established use of pointwise mutual information (pmi). Our proposed method, Fused-PCMI, trades off pmi for pcmi h and is preferred by humans for overall quality over the Max-PMI baseline 60% of the time. Human evaluators also judge responses with higher pcmi h better at acknowledgement 74% of the time. The results demonstrate that systems mimicking human conversational traits (in this case acknowledgement) improve overall quality and more broadly illustrate the utility of linguistic principles in improving dialogue agents.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Question Answering', 'Discourse and Pragmatics', 'Summarization', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.08821692317724228, 0.07623334974050522, 0.06906455010175705, 0.06015710532665253, 0.05921964347362518, 0.05327881500124931, 0.05206429958343506, 0.05047914758324623, 0.046009767800569534, 0.043460145592689514, 0.04314349219202995, 0.04273020848631859, 0.04250464215874672, 0.03774923458695412, 0.03758900985121727, 0.03335302323102951, 0.030635200440883636, 0.030101977288722992, 0.026151660829782486, 0.02417868562042713, 0.02078859694302082, 0.017117362469434738, 0.015773233026266098]}",0.08821692317724228,Dialogue and Interactive Systems,0.08821692317724228
Dialogue and Interactive Systems,A Comparative Study on Schema-Guided Dialogue State Tracking,"Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation.","{'sequence': 'Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'NLP Applications', 'Generation', 'Ethics and NLP', 'Question Answering', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09652122855186462, 0.06852669268846512, 0.05775342136621475, 0.053696129471063614, 0.05346626043319702, 0.052856169641017914, 0.04934542626142502, 0.04837232828140259, 0.046973083168268204, 0.04648544266819954, 0.04456944018602371, 0.041994016617536545, 0.04003350809216499, 0.038238875567913055, 0.037619397044181824, 0.03414974734187126, 0.03311799839138985, 0.03268013522028923, 0.027632148936390877, 0.02752377837896347, 0.02592320553958416, 0.024589400738477707, 0.017932135611772537]}",0.09652122855186462,Dialogue and Interactive Systems,0.09652122855186462
Dialogue and Interactive Systems,Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks,"Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts: 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. 2. Slot tagging and intent detection could be mutually rewarding, but bidirectional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through global optimization. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task.","{'sequence': 'Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts: 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. 2. Slot tagging and intent detection could be mutually rewarding, but bidirectional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through global optimization. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Semantics: Lexical Semantics', 'Information Extraction', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Question Answering', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Summarization', 'Generation', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11740123480558395, 0.10094648599624634, 0.06111856549978256, 0.05550447106361389, 0.054340653121471405, 0.050469692796468735, 0.05030800774693489, 0.04507602006196976, 0.042535021901130676, 0.04208799824118614, 0.037738777697086334, 0.0377110131084919, 0.03553080931305885, 0.03280792757868767, 0.031234759837388992, 0.030491771176457405, 0.02989266999065876, 0.027922384440898895, 0.027389762923121452, 0.027158714830875397, 0.025591552257537842, 0.02319766767323017, 0.013544045388698578]}",0.11740123480558395,Speech and Multimodality,0.10094648599624634
Dialogue and Interactive Systems,How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds,"We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al., 2019 )-a large-scale crowd-sourced fantasy text-game-with a dataset of ""quests"". 1 . These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations. Insssssolent pessst! I should immolate you for this tresssspasss. And why is that, dragon? Ssstealing my preccciousss golden egg! I'll tell you what, I'll give you 10 sssseconds to amussse me with your sssstory and THEN I'll burn you alive! You said you wanted to attack me, dragon, did you not? Go ahead, I'm lisssssstening. get golden dragon egg Now now! I would have given you that had you asked! Assssssk for my own property back? What a riduculousss notion Look here, I told you to watch your mouth and you didn't, so leave or I'll make you leave. And now threatsss! Thisss is proving to be a mossst engaging conversssation. hit knight Give my regardsss to the valley floor below!","{'sequence': 'We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al., 2019 )-a large-scale crowd-sourced fantasy text-game-with a dataset of ""quests"". 1 . These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations. Insssssolent pessst! I should immolate you for this tresssspasss. And why is that, dragon? Ssstealing my preccciousss golden egg! I\'ll tell you what, I\'ll give you 10 sssseconds to amussse me with your sssstory and THEN I\'ll burn you alive! You said you wanted to attack me, dragon, did you not? Go ahead, I\'m lisssssstening. get golden dragon egg Now now! I would have given you that had you asked! Assssssk for my own property back? What a riduculousss notion Look here, I told you to watch your mouth and you didn\'t, so leave or I\'ll make you leave. And now threatsss! Thisss is proving to be a mossst engaging conversssation. hit knight Give my regardsss to the valley floor below!', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Question Answering', 'Summarization', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.11674714088439941, 0.10232684761285782, 0.08496800810098648, 0.08282960951328278, 0.07584208250045776, 0.04788539931178093, 0.0471607930958271, 0.04022794961929321, 0.040054574608802795, 0.03369888290762901, 0.03363899141550064, 0.03147432953119278, 0.029896089807152748, 0.029172303155064583, 0.02640247903764248, 0.02508862130343914, 0.024578969925642014, 0.02446999028325081, 0.02378755621612072, 0.023355862125754356, 0.021519925445318222, 0.01860467530786991, 0.0162688959389925]}",0.11674714088439941,Speech and Multimodality,0.10232684761285782
Information Extraction,Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas,"In entity linking, mentions of named entities in raw text are disambiguated against a knowledge base (KB). This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training. Our approach relies on methods to flexibly convert entities with several attribute-value pairs from arbitrary KBs into flat strings, which we use in conjunction with state-of-the-art models for zero-shot linking. We further improve the generalization of our model using two regularization schemes based on shuffling of entity attributes and handling of unseen attributes. Experiments on English datasets where models are trained on the CoNLL dataset, and tested on the TAC-KBP 2010 dataset show that our models are 12% (absolute) more accurate than baseline models that simply flatten entities from the target KB. Unlike prior work, our approach also allows for seamlessly combining multiple training datasets. We test this ability by adding both a completely different dataset (Wikia), as well as increasing amount of training data from the TAC-KBP 2010 training set. Our models are more accurate across the board compared to baselines.","{'sequence': 'In entity linking, mentions of named entities in raw text are disambiguated against a knowledge base (KB). This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training. Our approach relies on methods to flexibly convert entities with several attribute-value pairs from arbitrary KBs into flat strings, which we use in conjunction with state-of-the-art models for zero-shot linking. We further improve the generalization of our model using two regularization schemes based on shuffling of entity attributes and handling of unseen attributes. Experiments on English datasets where models are trained on the CoNLL dataset, and tested on the TAC-KBP 2010 dataset show that our models are 12% (absolute) more accurate than baseline models that simply flatten entities from the target KB. Unlike prior work, our approach also allows for seamlessly combining multiple training datasets. We test this ability by adding both a completely different dataset (Wikia), as well as increasing amount of training data from the TAC-KBP 2010 training set. Our models are more accurate across the board compared to baselines.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'NLP Applications', 'Question Answering', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Generation', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.08945830911397934, 0.06918291747570038, 0.06320516020059586, 0.059043776243925095, 0.05346851795911789, 0.048487886786460876, 0.046682208776474, 0.04409559816122055, 0.04388333484530449, 0.0426228791475296, 0.04114969074726105, 0.041123971343040466, 0.041094448417425156, 0.040241774171590805, 0.03961484879255295, 0.039355065673589706, 0.037706565111875534, 0.03517917916178703, 0.02936849743127823, 0.02905440144240856, 0.02835451252758503, 0.018859297037124634, 0.01876719295978546]}",0.08945830911397934,Speech and Multimodality,0.06320516020059586
Information Extraction,Self-Training with Weak Supervision,"State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domainspecific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA 1 ) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.","{'sequence': 'State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domainspecific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA 1 ) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Information Extraction', 'NLP Applications', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15095333755016327, 0.12258795648813248, 0.08142588287591934, 0.05928000435233116, 0.05492168292403221, 0.04508015513420105, 0.04368281736969948, 0.037506937980651855, 0.03639141842722893, 0.035541847348213196, 0.034918755292892456, 0.03114074468612671, 0.030739368870854378, 0.029642397537827492, 0.029621966183185577, 0.027039971202611923, 0.025174180045723915, 0.02441953308880329, 0.0231751948595047, 0.02194702997803688, 0.020328249782323837, 0.017926756292581558, 0.016553819179534912]}",0.15095333755016327,Generation,0.035541847348213196
Information Extraction,Neural Language Modeling for Contextualized Temporal Graph Generation,"This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with humanannotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-ofdomain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting. 1","{'sequence': 'This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with humanannotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-ofdomain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting. 1', 'labels': ['Generation', 'Machine Learning for NLP', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization'], 'scores': [0.17384804785251617, 0.09854032844305038, 0.09460654854774475, 0.07299741357564926, 0.06356702744960785, 0.04908633232116699, 0.04604886472225189, 0.036285705864429474, 0.03560957312583923, 0.03497351333498955, 0.03437253460288048, 0.02964593470096588, 0.029318390414118767, 0.029085807502269745, 0.02420206181704998, 0.02414584718644619, 0.01947178691625595, 0.01914537139236927, 0.018896641209721565, 0.01883847638964653, 0.018063515424728394, 0.01697826199233532, 0.012272162362933159]}",0.17384804785251617,Generation,0.01947178691625595
Information Extraction,Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning,"Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however existing embedding methods only model triple-level uncertainty and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE , a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle), and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to it's probabilistic calibration and ability to capture high-order dependencies among facts. 1","{'sequence': ""Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however existing embedding methods only model triple-level uncertainty and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE , a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle), and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to it's probabilistic calibration and ability to capture high-order dependencies among facts. 1"", 'labels': ['Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Information Extraction', 'Question Answering', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Generation', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP'], 'scores': [0.08044220507144928, 0.07627975195646286, 0.07230763137340546, 0.06602352112531662, 0.055776964873075485, 0.05399497598409653, 0.05357041582465172, 0.05296734720468521, 0.05137033015489578, 0.04098186641931534, 0.0387701652944088, 0.03829318284988403, 0.03773196414113045, 0.036576420068740845, 0.03622356429696083, 0.029902052134275436, 0.029547376558184624, 0.029223624616861343, 0.028616148978471756, 0.024617671966552734, 0.022641142830252647, 0.02248026616871357, 0.021661387756466866]}",0.08044220507144928,Dialogue and Interactive Systems,0.06602352112531662
Information Extraction,Document-Level Event Argument Extraction by Conditional Generation,"Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human information seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WIKIEVENTS which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WIKIEVENTS datasets respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model's trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE. 1 Prosecutors say he drove the truck to Geary Lake in Kansas, that 4,000 pounds of ammonium nitrate laced with nitromethane were loaded into the truck there, and that it was driven to Oklahoma City and detonated. Elliott testified that on April 15, McVeigh came into the body shop and reserved the truck, to be picked up at 4pm two days later. Elliott said that McVeigh gave him the $280.32 in exact change after declining to pay an additional amount for insurance.","{'sequence': ""Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human information seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WIKIEVENTS which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WIKIEVENTS datasets respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model's trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE. 1 Prosecutors say he drove the truck to Geary Lake in Kansas, that 4,000 pounds of ammonium nitrate laced with nitromethane were loaded into the truck there, and that it was driven to Oklahoma City and detonated. Elliott testified that on April 15, McVeigh came into the body shop and reserved the truck, to be picked up at 4pm two days later. Elliott said that McVeigh gave him the $280.32 in exact change after declining to pay an additional amount for insurance."", 'labels': ['Information Extraction', 'Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.13369779288768768, 0.09629999846220016, 0.06335423141717911, 0.04931887239217758, 0.04804399237036705, 0.047799620777368546, 0.04668780788779259, 0.043670523911714554, 0.03956753760576248, 0.03927028551697731, 0.03804570436477661, 0.03607160225510597, 0.03557002544403076, 0.03500935062766075, 0.03500016778707504, 0.03396381810307503, 0.033938147127628326, 0.03239677473902702, 0.028119618073105812, 0.022646592929959297, 0.020874720066785812, 0.02049652300775051, 0.020156245678663254]}",0.13369779288768768,Information Extraction,0.13369779288768768
Information Extraction,Template Filling with Generative Transformers,"Template filling is generally tackled by a pipeline of two separate supervised systemsone for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.","{'sequence': 'Template filling is generally tackled by a pipeline of two separate supervised systemsone for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Resources and Evaluation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1611490398645401, 0.09815836697816849, 0.0722077414393425, 0.07064088433980942, 0.05655388906598091, 0.04174657166004181, 0.04096445441246033, 0.04084758087992668, 0.03892199322581291, 0.03672788664698601, 0.035851068794727325, 0.03501901403069496, 0.03386765718460083, 0.030224351212382317, 0.029812509194016457, 0.028874671086668968, 0.02864580601453781, 0.025617698207497597, 0.022250058129429817, 0.021556813269853592, 0.018786799162626266, 0.01748310960829258, 0.014092078432440758]}",0.1611490398645401,Generation,0.04174657166004181
Interpretability and Analysis of Models for NLP,Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models,"Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a longtailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up during very early few iterations of the model training. These two observations are further employed to formulate a measurement which can quantify the shortcut degree of each training sample. Based on this shortcut measurement, we propose a shortcut mitigation framework LTGR, to suppress the model from making overconfident predictions for samples with large shortcut degree. Experimental results on three NLU benchmarks demonstrate that our long-tailed distribution explanation accurately reflects the shortcut learning behavior of NLU models. Experimental analysis further indicates that LTGR can improve the generalization accuracy on OOD data, while preserving the accuracy on in-distribution data.","{'sequence': 'Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a longtailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up during very early few iterations of the model training. These two observations are further employed to formulate a measurement which can quantify the shortcut degree of each training sample. Based on this shortcut measurement, we propose a shortcut mitigation framework LTGR, to suppress the model from making overconfident predictions for samples with large shortcut degree. Experimental results on three NLU benchmarks demonstrate that our long-tailed distribution explanation accurately reflects the shortcut learning behavior of NLU models. Experimental analysis further indicates that LTGR can improve the generalization accuracy on OOD data, while preserving the accuracy on in-distribution data.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Information Extraction', 'NLP Applications', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Generation', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0898524597287178, 0.07942691445350647, 0.06306171417236328, 0.059226177632808685, 0.057624656707048416, 0.05576210469007492, 0.0537911131978035, 0.050520047545433044, 0.046912942081689835, 0.044439107179641724, 0.04411281645298004, 0.04017350450158119, 0.04012027755379677, 0.03709280863404274, 0.03529282659292221, 0.0350089892745018, 0.034765977412462234, 0.031055225059390068, 0.02944287657737732, 0.025418730452656746, 0.02100205421447754, 0.015456173568964005, 0.010440447367727757]}",0.0898524597287178,Dialogue and Interactive Systems,0.057624656707048416
Interpretability and Analysis of Models for NLP,On Attention Redundancy: A Comprehensive Study,"Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (""Why"") We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phaseindependent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.","{'sequence': 'Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (""Why"") We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phaseindependent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.', 'labels': ['Resources and Evaluation', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Summarization', 'NLP Applications', 'Question Answering', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10300064831972122, 0.08622290194034576, 0.07487411797046661, 0.06839204579591751, 0.06518910080194473, 0.06429694592952728, 0.060315921902656555, 0.04859726503491402, 0.048485662788152695, 0.04336606711149216, 0.041350118815898895, 0.037699971348047256, 0.03214537352323532, 0.03175439313054085, 0.031178096309304237, 0.028523672372102737, 0.027598517015576363, 0.022520514205098152, 0.021960601210594177, 0.021648837253451347, 0.018271194770932198, 0.01461437251418829, 0.007993693463504314]}",0.10300064831972122,Resources and Evaluation,0.07487411797046661
Interpretability and Analysis of Models for NLP,Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?,"Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT (Alsentzer et al., 2019) . While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, nondeidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated ""attacks"" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available. 1","{'sequence': 'Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT (Alsentzer et al., 2019) . While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, nondeidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated ""attacks"" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available. 1', 'labels': ['Information Extraction', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Generation', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Summarization', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2406345158815384, 0.0702771246433258, 0.06274806708097458, 0.0559176430106163, 0.05086551979184151, 0.05013328790664673, 0.04389520362019539, 0.04247777909040451, 0.039944179356098175, 0.03662511333823204, 0.03120211325585842, 0.030874405056238174, 0.030155332759022713, 0.029192399233579636, 0.027800310403108597, 0.02765904739499092, 0.024036727845668793, 0.020891383290290833, 0.020348116755485535, 0.017147041857242584, 0.016933446750044823, 0.016191380098462105, 0.014049862511456013]}",0.2406345158815384,Information Extraction,0.016933446750044823
Interpretability and Analysis of Models for NLP,Low-Complexity Probing via Finding Subnetworks,"The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model's internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.","{'sequence': ""The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model's internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work."", 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'NLP Applications', 'Generation', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09421302378177643, 0.08187347650527954, 0.07877857983112335, 0.06866896152496338, 0.06497371196746826, 0.05927395075559616, 0.05368316173553467, 0.05363499000668526, 0.04967242479324341, 0.04079471901059151, 0.03824670612812042, 0.033083245158195496, 0.032049212604761124, 0.030169174075126648, 0.028183821588754654, 0.02768486551940441, 0.02686520479619503, 0.026808597147464752, 0.025209685787558556, 0.02487889863550663, 0.02471836283802986, 0.02303500659763813, 0.01350027322769165]}",0.09421302378177643,Resources and Evaluation,0.06866896152496338
Interpretability and Analysis of Models for NLP,An Empirical Comparison of Instance Attribution Methods for NLP,"Widespread adoption of deep models has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the IF is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at: https://github.com/successar/ instance_attributions_NLP.","{'sequence': 'Widespread adoption of deep models has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the IF is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at: https://github.com/successar/ instance_attributions_NLP.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Discourse and Pragmatics', 'Ethics and NLP', 'Generation', 'Information Extraction', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09225251525640488, 0.0807752013206482, 0.07086101174354553, 0.06885826587677002, 0.06717076152563095, 0.05567622557282448, 0.04802917689085007, 0.04606518894433975, 0.04419996216893196, 0.04390785098075867, 0.043016694486141205, 0.040608856827020645, 0.03921077400445938, 0.0356784462928772, 0.03512027487158775, 0.034013181924819946, 0.03111828677356243, 0.02961142361164093, 0.025724630802869797, 0.021310381591320038, 0.02070923149585724, 0.01579243130981922, 0.01028943806886673]}",0.09225251525640488,NLP Applications,0.03512027487158775
Interpretability and Analysis of Models for NLP,Generalization in Instruction Following Systems,"Understanding and executing natural language instructions in a grounded domain is one of the hallmarks of artificial intelligence. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two topperforming systems for the task. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the natural language instructions relative to it, or whether they merely over-fit spurious signals in the dataset. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a model should possess. Despite decent test performance, we find that state-of-the-art models fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves data augmentation and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better. 1 .","{'sequence': 'Understanding and executing natural language instructions in a grounded domain is one of the hallmarks of artificial intelligence. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two topperforming systems for the task. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the natural language instructions relative to it, or whether they merely over-fit spurious signals in the dataset. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a model should possess. Despite decent test performance, we find that state-of-the-art models fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves data augmentation and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better. 1 .', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Information Extraction', 'Question Answering', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10268796980381012, 0.08381738513708115, 0.08262179791927338, 0.07330746948719025, 0.06560294330120087, 0.051919784396886826, 0.04715612903237343, 0.043277353048324585, 0.04256458580493927, 0.04186633601784706, 0.04075070470571518, 0.03918558731675148, 0.03786231577396393, 0.03675808757543564, 0.03668423742055893, 0.033929355442523956, 0.028751689940690994, 0.027389876544475555, 0.022807760164141655, 0.0202611293643713, 0.016358554363250732, 0.014016291126608849, 0.010422665625810623]}",0.10268796980381012,Resources and Evaluation,0.07330746948719025
"Language Grounding to Vision, Robotics and Beyond",LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval,"Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computation cost mainly from cross-modal attention in Transformer architecture. When applied to reallife applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by pre-training on three novel learning objectives, extracting feature indexes offline, and employing instant dot-product matching with further re-ranking, which significantly speeds up retrieval process. In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000× magnitude of computational hours. 1","{'sequence': 'Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computation cost mainly from cross-modal attention in Transformer architecture. When applied to reallife applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by pre-training on three novel learning objectives, extracting feature indexes offline, and employing instant dot-product matching with further re-ranking, which significantly speeds up retrieval process. In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000× magnitude of computational hours. 1', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Generation', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'NLP Applications', 'Ethics and NLP', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10239620506763458, 0.09855060279369354, 0.07977806031703949, 0.07926900684833527, 0.05561086907982826, 0.0538208968937397, 0.05181580409407616, 0.047823887318372726, 0.04656234011054039, 0.04579499736428261, 0.045201875269412994, 0.03982391953468323, 0.03309142589569092, 0.030710922554135323, 0.029521318152546883, 0.02597794495522976, 0.02498924359679222, 0.024940703064203262, 0.023350868374109268, 0.02083657868206501, 0.016291609033942223, 0.01470140554010868, 0.0091395303606987]}",0.10239620506763458,Resources and Evaluation,0.03982391953468323
"Language Grounding to Vision, Robotics and Beyond",Measuring Social Biases in Grounded Vision and Language Embeddings,"We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.","{'sequence': 'We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.', 'labels': ['Question Answering', 'Discourse and Pragmatics', 'Information Extraction', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.21818599104881287, 0.0648374855518341, 0.06471853703260422, 0.05827105790376663, 0.051485322415828705, 0.05043614283204079, 0.04861310124397278, 0.04755855351686478, 0.03904226794838905, 0.036132410168647766, 0.03493224084377289, 0.03425554931163788, 0.03269384428858757, 0.03174242004752159, 0.02957110106945038, 0.028069986030459404, 0.027614042162895203, 0.023184584453701973, 0.018733175471425056, 0.018076198175549507, 0.016075927764177322, 0.01407505851238966, 0.011694971472024918]}",0.21818599104881287,Question Answering,0.01407505851238966
"Language Grounding to Vision, Robotics and Beyond",MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences,"Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-ofthe-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters. 1","{'sequence': 'Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-ofthe-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters. 1', 'labels': ['Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Question Answering', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Summarization', 'Generation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24425233900547028, 0.06298410147428513, 0.05545278266072273, 0.0531243234872818, 0.048595283180475235, 0.043855562806129456, 0.042285457253456116, 0.040774427354335785, 0.03643437847495079, 0.034609369933605194, 0.03304515779018402, 0.03187365084886551, 0.02932947687804699, 0.02893715724349022, 0.028629839420318604, 0.0282757505774498, 0.027026649564504623, 0.025371497496962547, 0.02476709894835949, 0.024037223309278488, 0.023996122181415558, 0.019656337797641754, 0.012685984373092651]}",0.24425233900547028,Speech and Multimodality,0.034609369933605194
"Language Grounding to Vision, Robotics and Beyond",Grounding Open-Domain Instructions to Automate Web Support Tasks,"Grounding natural language instructions on the web to perform previously unseen tasks enables accessibility and automation. We introduce a task and dataset to train AI agents from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with pointers parses instructions to ThingTalk, a domain-specific language we design for grounding natural language on the web. Then, a grounding model retrieves the unique IDs of any webpage elements requested in ThingTalk. RUSS may interact with the user through a dialogue (e.g. ask for an address) or execute a web operation (e.g. click a button) inside the web runtime. To augment training, we synthesize natural language instructions mapped to ThingTalk. Our dataset consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single instructions. It outperforms stateof-the-art models that directly map instructions to actions without ThingTalk. Our user study shows that RUSS is preferred by actual users over web navigation.","{'sequence': 'Grounding natural language instructions on the web to perform previously unseen tasks enables accessibility and automation. We introduce a task and dataset to train AI agents from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with pointers parses instructions to ThingTalk, a domain-specific language we design for grounding natural language on the web. Then, a grounding model retrieves the unique IDs of any webpage elements requested in ThingTalk. RUSS may interact with the user through a dialogue (e.g. ask for an address) or execute a web operation (e.g. click a button) inside the web runtime. To augment training, we synthesize natural language instructions mapped to ThingTalk. Our dataset consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single instructions. It outperforms stateof-the-art models that directly map instructions to actions without ThingTalk. Our user study shows that RUSS is preferred by actual users over web navigation.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'NLP Applications', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10522989928722382, 0.08059468120336533, 0.07503606379032135, 0.06532765179872513, 0.058654092252254486, 0.05674951896071434, 0.051973968744277954, 0.04710042476654053, 0.04610792547464371, 0.03791727125644684, 0.035851649940013885, 0.035438816994428635, 0.03375062346458435, 0.03250035271048546, 0.03179581090807915, 0.031438734382390976, 0.030595190823078156, 0.028751764446496964, 0.028353391215205193, 0.024671180173754692, 0.02249092049896717, 0.02056456357240677, 0.019105559214949608]}",0.10522989928722382,Dialogue and Interactive Systems,0.02249092049896717
"Language Grounding to Vision, Robotics and Beyond",Modular Networks for Compositional Instruction Following,"Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, nonmodular sequence-to-sequence approaches on ALFRED (Shridhar et al., 2020) , a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.","{'sequence': 'Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, nonmodular sequence-to-sequence approaches on ALFRED (Shridhar et al., 2020) , a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Ethics and NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09888224303722382, 0.07546433806419373, 0.07202386111021042, 0.0642629861831665, 0.05723635107278824, 0.05024769902229309, 0.049157630652189255, 0.047061290591955185, 0.04592234641313553, 0.04203828051686287, 0.04152410849928856, 0.04033174738287926, 0.03830956667661667, 0.03612262010574341, 0.03521087020635605, 0.033616576343774796, 0.028613464906811714, 0.02847014181315899, 0.027242327108979225, 0.02601613663136959, 0.02431459166109562, 0.021841559559106827, 0.016089286655187607]}",0.09888224303722382,Dialogue and Interactive Systems,0.027242327108979225
"Language Grounding to Vision, Robotics and Beyond",Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information,"Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like dependencies and phrase structures can aid the agent to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the current visual information via qualitative visualizations. 1 1 Code and models: https://github.com/ jialuli-luka/SyntaxVLN Navigation Steps: Parse Tree: Instruction: Walk forward then turn right at the stairs then go down the stairs.","{'sequence': 'Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like dependencies and phrase structures can aid the agent to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the current visual information via qualitative visualizations. 1 1 Code and models: https://github.com/ jialuli-luka/SyntaxVLN Navigation Steps: Parse Tree: Instruction: Walk forward then turn right at the stairs then go down the stairs.', 'labels': ['Information Extraction', 'NLP Applications', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Machine Learning for NLP', 'Summarization', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07139583677053452, 0.07004886865615845, 0.06960379332304001, 0.06442934274673462, 0.06124883145093918, 0.05455048009753227, 0.05287047103047371, 0.0464339479804039, 0.04566160961985588, 0.043654508888721466, 0.04013221710920334, 0.03798971325159073, 0.0372326634824276, 0.037055227905511856, 0.03649996593594551, 0.035424862056970596, 0.034108735620975494, 0.03130916506052017, 0.03047366440296173, 0.02858061157166958, 0.028530852869153023, 0.025300605222582817, 0.017463941127061844]}",0.07139583677053452,Information Extraction,0.05455048009753227
Machine Learning for NLP,Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning,"Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018) . More recently, pretrained models like BERT (Devlin et al., 2018)   have been used to convert classification tasks into a textual entailment task (Yin et al., 2019) . This approach is naturally suitable for the ZS-MTC task. However, pretrained models are underexplored in the existing work because they do not generate individual vector representations for text or labels, making it unintuitive to combine them with conventional graph encoding methods. In this paper, we explore to improve pretrained models with label hierarchies on the ZS-MTC task. We propose a Reinforced Label Hierarchy Reasoning (RLHR) approach to encourage interdependence among labels in the hierarchies during training. Meanwhile, to overcome the weakness of flat predictions, we design a rollback algorithm that can remove logical errors from predictions during inference. Experimental results on three reallife datasets show that our approach achieves better performance and outperforms previous non-pretrained methods on the ZS-MTC task.","{'sequence': 'Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018) . More recently, pretrained models like BERT (Devlin et al., 2018)   have been used to convert classification tasks into a textual entailment task (Yin et al., 2019) . This approach is naturally suitable for the ZS-MTC task. However, pretrained models are underexplored in the existing work because they do not generate individual vector representations for text or labels, making it unintuitive to combine them with conventional graph encoding methods. In this paper, we explore to improve pretrained models with label hierarchies on the ZS-MTC task. We propose a Reinforced Label Hierarchy Reasoning (RLHR) approach to encourage interdependence among labels in the hierarchies during training. Meanwhile, to overcome the weakness of flat predictions, we design a rollback algorithm that can remove logical errors from predictions during inference. Experimental results on three reallife datasets show that our approach achieves better performance and outperforms previous non-pretrained methods on the ZS-MTC task.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Ethics and NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08882970362901688, 0.08784804493188858, 0.07255016267299652, 0.07035553455352783, 0.06420625746250153, 0.06301885843276978, 0.056781113147735596, 0.04734289273619652, 0.04547920823097229, 0.04000451788306236, 0.039732735604047775, 0.03676538169384003, 0.03486790880560875, 0.03434624522924423, 0.03380535542964935, 0.0322437509894371, 0.030918071046471596, 0.03056863322854042, 0.02885388396680355, 0.02503371424973011, 0.024368830025196075, 0.006822607479989529, 0.005256589036434889]}",0.08882970362901688,Question Answering,0.02503371424973011
Machine Learning for NLP,Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach,"Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the finetuning stage. We study the problem of finetuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https:// github.com/yueyu1030/COSINE.","{'sequence': 'Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the finetuning stage. We study the problem of finetuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https:// github.com/yueyu1030/COSINE.', 'labels': ['Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Generation', 'Computational Social Science and Social Media', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Summarization', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.21112306416034698, 0.11485916376113892, 0.09016205370426178, 0.059830427169799805, 0.0570417158305645, 0.045650508254766464, 0.04519626125693321, 0.043946314603090286, 0.04096867889165878, 0.03914620354771614, 0.03596167266368866, 0.029751328751444817, 0.02573092095553875, 0.02475189045071602, 0.02391943521797657, 0.021939024329185486, 0.016484681516885757, 0.016445783898234367, 0.015348250046372414, 0.014951738528907299, 0.0100925387814641, 0.010084210895001888, 0.0066140578128397465]}",0.21112306416034698,Machine Learning for NLP,0.21112306416034698
Machine Learning for NLP,Posterior Differential Regularization with f-divergence for Improving Model Robustness,"We address the problem of enhancing model robustness through regularization. Specifically, we focus on methods that regularize the model posterior difference between clean and noisy inputs. Theoretically, we provide a connection of two recent methods, Jacobian Regularization and Virtual Adversarial Training, under this framework. Additionally, we generalize the posterior differential regularization to the family of f -divergences and characterize the overall framework in terms of Jacobian matrix. Empirically, we compare those regularizations and standard BERT training on a diverse set of tasks to provide a comprehensive profile of their effect on model generalization. For both fully supervised and semi-supervised settings, we show that regularizing the posterior difference with f -divergence can result in well-improved model robustness. In particular, with a proper f -divergence, a BERT-base model can achieve comparable generalization as its BERT-large counterpart for in-domain, adversarial and domain shift scenarios, indicating the great potential of the proposed framework for enhancing NLP model robustness. 1","{'sequence': 'We address the problem of enhancing model robustness through regularization. Specifically, we focus on methods that regularize the model posterior difference between clean and noisy inputs. Theoretically, we provide a connection of two recent methods, Jacobian Regularization and Virtual Adversarial Training, under this framework. Additionally, we generalize the posterior differential regularization to the family of f -divergences and characterize the overall framework in terms of Jacobian matrix. Empirically, we compare those regularizations and standard BERT training on a diverse set of tasks to provide a comprehensive profile of their effect on model generalization. For both fully supervised and semi-supervised settings, we show that regularizing the posterior difference with f -divergence can result in well-improved model robustness. In particular, with a proper f -divergence, a BERT-base model can achieve comparable generalization as its BERT-large counterpart for in-domain, adversarial and domain shift scenarios, indicating the great potential of the proposed framework for enhancing NLP model robustness. 1', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Resources and Evaluation', 'Generation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Question Answering', 'Dialogue and Interactive Systems', 'Summarization', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.18068045377731323, 0.16978545486927032, 0.08863531798124313, 0.07400508224964142, 0.044746607542037964, 0.039327166974544525, 0.03868288919329643, 0.03625344857573509, 0.03404179587960243, 0.033945195376873016, 0.033881813287734985, 0.03304428234696388, 0.02923576906323433, 0.020581824705004692, 0.020440692082047462, 0.017695290967822075, 0.017634019255638123, 0.01706726849079132, 0.016126004979014397, 0.015962427482008934, 0.01529157068580389, 0.013578595593571663, 0.009357024915516376]}",0.18068045377731323,NLP Applications,0.16978545486927032
Machine Learning for NLP,Understanding Hard Negatives in Noise Contrastive Estimation,"The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives-highest-scoring incorrect examples under the model-are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the crossentropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.","{'sequence': 'The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives-highest-scoring incorrect examples under the model-are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the crossentropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Question Answering', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09011112153530121, 0.06554394960403442, 0.06254365295171738, 0.060026269406080246, 0.05541175603866577, 0.05306278169155121, 0.051988713443279266, 0.05059434473514557, 0.04560330510139465, 0.04312637075781822, 0.04244932532310486, 0.04147518426179886, 0.039844777435064316, 0.039802782237529755, 0.038666293025016785, 0.034689441323280334, 0.03214887157082558, 0.032026875764131546, 0.030138486996293068, 0.03001248650252819, 0.025229843333363533, 0.022682946175336838, 0.012820332311093807]}",0.09011112153530121,Dialogue and Interactive Systems,0.022682946175336838
Machine Learning for NLP,Certified Robustness to Word Substitution Attack with Differential Privacy,"The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we propose WordDP to achieve certified robustness against word substitution attacks in text classification via differential privacy (DP). We establish the connection between DP and adversarial robustness for the first time in the text domain and propose a conceptual exponential mechanism-based algorithm to formally achieve the robustness. We further present a practical simulated exponential mechanism that has efficient inference with certified robustness. We not only provide a rigorous analytic derivation of the certified condition but also experimentally compare the utility of WordDP with existing defense algorithms. The results show that WordDP achieves higher accuracy and more than 30⇥ efficiency improvement over the state-of-theart certified robustness mechanism in typical text classification tasks.","{'sequence': 'The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we propose WordDP to achieve certified robustness against word substitution attacks in text classification via differential privacy (DP). We establish the connection between DP and adversarial robustness for the first time in the text domain and propose a conceptual exponential mechanism-based algorithm to formally achieve the robustness. We further present a practical simulated exponential mechanism that has efficient inference with certified robustness. We not only provide a rigorous analytic derivation of the certified condition but also experimentally compare the utility of WordDP with existing defense algorithms. The results show that WordDP achieves higher accuracy and more than 30⇥ efficiency improvement over the state-of-theart certified robustness mechanism in typical text classification tasks.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Question Answering', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.290810763835907, 0.08605336397886276, 0.06612042337656021, 0.05658186972141266, 0.0462287999689579, 0.0423281230032444, 0.03844231367111206, 0.03683876246213913, 0.03529675304889679, 0.03338237479329109, 0.032404180616140366, 0.02931622602045536, 0.02808264084160328, 0.026132401078939438, 0.024967629462480545, 0.02356785349547863, 0.020637771114706993, 0.02048001065850258, 0.019195636734366417, 0.01520096231251955, 0.009853598661720753, 0.009042518213391304, 0.009034995920956135]}",0.290810763835907,NLP Applications,0.08605336397886276
Machine Learning for NLP,DReCa: A General Task Augmentation Strategy for Few-Shot Natural Language Inference,"Meta-learning promises few-shot learners that quickly adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe meta-learning has not yet succeeded in NLP due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there's only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and, since NLP datasets are highly heterogeneous, many learning episodes have poor transfer between their support and query sets, which discourages the meta-learner from adapting. To alleviate these issues, we propose DRECA (Decomposing datasets into Reasoning Categories), a simple method for discovering and using latent reasoning categories in a dataset, to form additional high quality tasks. DRECA works by splitting examples into label groups, embedding them with a finetuned BERT model and then clustering each group into reasoning categories. Across four few-shot NLI problems, we demonstrate that using DRECA improves the accuracy of meta-learners by 1.5-4%.","{'sequence': ""Meta-learning promises few-shot learners that quickly adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe meta-learning has not yet succeeded in NLP due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there's only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and, since NLP datasets are highly heterogeneous, many learning episodes have poor transfer between their support and query sets, which discourages the meta-learner from adapting. To alleviate these issues, we propose DRECA (Decomposing datasets into Reasoning Categories), a simple method for discovering and using latent reasoning categories in a dataset, to form additional high quality tasks. DRECA works by splitting examples into label groups, embedding them with a finetuned BERT model and then clustering each group into reasoning categories. Across four few-shot NLI problems, we demonstrate that using DRECA improves the accuracy of meta-learners by 1.5-4%."", 'labels': ['Machine Learning for NLP', 'Ethics and NLP', 'Generation', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.22940288484096527, 0.08412373065948486, 0.06025638058781624, 0.0490596741437912, 0.04574696347117424, 0.04425640031695366, 0.040079157799482346, 0.039149511605501175, 0.03762892261147499, 0.03489123284816742, 0.03336755558848381, 0.03248251602053642, 0.03195807710289955, 0.029021896421909332, 0.028011031448841095, 0.026351895183324814, 0.02436942420899868, 0.023842956870794296, 0.023289835080504417, 0.02292877435684204, 0.02120911329984665, 0.01951819472014904, 0.019053863361477852]}",0.22940288484096527,Machine Learning for NLP,0.22940288484096527
Machine Translation and Multilinguality,Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages,"Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving lowresource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 lowresource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art supervised model for NeÑEn. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models.","{'sequence': 'Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving lowresource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 lowresource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art supervised model for NeÑEn. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.18964071571826935, 0.11840847879648209, 0.056825652718544006, 0.052889492362737656, 0.050749652087688446, 0.04870358109474182, 0.046866729855537415, 0.04156165197491646, 0.03748837485909462, 0.03157733008265495, 0.030802827328443527, 0.026917655020952225, 0.02660328894853592, 0.026530027389526367, 0.02564796432852745, 0.02554083615541458, 0.02518399991095066, 0.02479316107928753, 0.024722103029489517, 0.02275102213025093, 0.022298656404018402, 0.022263918071985245, 0.021232830360531807]}",0.18964071571826935,Machine Translation and Multilinguality,0.18964071571826935
Machine Translation and Multilinguality,Macro-Average: Rare Types Are Important Too,"While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MACROF 1 , and study its applicability to MT evaluation. We find that MACROF 1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MACROF 1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods' outputs. 1 2 We consider F β ;c for c ∈ V h∩y to be 0. 4 BLEU and CHRF 1 scores reported in this work are computed with SACREBLEU; see the Appendix for details. BLEURT scores are from the base model (Sellam et al., 2020) . We consider two varieties of averaging to obtain a corpus-level metric from the segment-level BLEURT: mean and median of segment-level scores per corpus.","{'sequence': ""While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MACROF 1 , and study its applicability to MT evaluation. We find that MACROF 1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MACROF 1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods' outputs. 1 2 We consider F β ;c for c ∈ V h∩y to be 0. 4 BLEU and CHRF 1 scores reported in this work are computed with SACREBLEU; see the Appendix for details. BLEURT scores are from the base model (Sellam et al., 2020) . We consider two varieties of averaging to obtain a corpus-level metric from the segment-level BLEURT: mean and median of segment-level scores per corpus."", 'labels': ['Resources and Evaluation', 'Question Answering', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Summarization', 'Discourse and Pragmatics', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining'], 'scores': [0.09232788532972336, 0.07649902254343033, 0.07233134657144547, 0.07030519098043442, 0.05755267292261124, 0.05317908897995949, 0.050204504281282425, 0.04416847229003906, 0.04274304583668709, 0.040800269693136215, 0.03978835791349411, 0.03874075785279274, 0.0375186912715435, 0.03641904518008232, 0.03408398479223251, 0.03175472468137741, 0.03132857382297516, 0.030193651095032692, 0.029302306473255157, 0.028354348614811897, 0.024133842438459396, 0.02121458388864994, 0.01705559343099594]}",0.09232788532972336,Resources and Evaluation,0.07233134657144547
Machine Translation and Multilinguality,Assessing Reference-Free Peer Evaluation for Machine Translation,"Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.","{'sequence': 'Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.', 'labels': ['Machine Translation and Multilinguality', 'Resources and Evaluation', 'Speech and Multimodality', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Information Extraction', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.2791641652584076, 0.11128129810094833, 0.05292847380042076, 0.046160172671079636, 0.045836616307497025, 0.040778905153274536, 0.03855957090854645, 0.03589120879769325, 0.03316119313240051, 0.03302665427327156, 0.030519355088472366, 0.02940365858376026, 0.027508186176419258, 0.025898650288581848, 0.02578802965581417, 0.02292160503566265, 0.022613482549786568, 0.021922284737229347, 0.01922873966395855, 0.017360392957925797, 0.01501112524420023, 0.013274756260216236, 0.011761479079723358]}",0.2791641652584076,Machine Translation and Multilinguality,0.2791641652584076
Machine Translation and Multilinguality,The Curious Case of Hallucinations in Neural Machine Translation,"In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman (2020), and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results at https://github.com/vyraun/ hallucinations.","{'sequence': 'In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman (2020), and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results at https://github.com/vyraun/ hallucinations.', 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Summarization', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Information Extraction', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11431735754013062, 0.0756525918841362, 0.07250665873289108, 0.06367067247629166, 0.05957508459687233, 0.05068670213222504, 0.0499403215944767, 0.04940692335367203, 0.04795052483677864, 0.039846308529376984, 0.03768195956945419, 0.037418343126773834, 0.03668403998017311, 0.03302311524748802, 0.031189825385808945, 0.03105742298066616, 0.030113643035292625, 0.02896570973098278, 0.026862287893891335, 0.02555176243185997, 0.021117107942700386, 0.020126506686210632, 0.01665508560836315]}",0.11431735754013062,Machine Translation and Multilinguality,0.11431735754013062
Machine Translation and Multilinguality,Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution,"We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess monolingual data for the new languages.","{'sequence': 'We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess monolingual data for the new languages.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Generation', 'Resources and Evaluation', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.30226415395736694, 0.07497429102659225, 0.05726896971464157, 0.05430164933204651, 0.05208318680524826, 0.0386754646897316, 0.037367936223745346, 0.034012459218502045, 0.03347299247980118, 0.030008388683199883, 0.02986695058643818, 0.02934245951473713, 0.026814984157681465, 0.026504652574658394, 0.024306194856762886, 0.023534240201115608, 0.022849854081869125, 0.02207595482468605, 0.021039851009845734, 0.020377032458782196, 0.019448986276984215, 0.012560034170746803, 0.006849225610494614]}",0.30226415395736694,Machine Translation and Multilinguality,0.30226415395736694
Machine Translation and Multilinguality,Towards Modeling the Style of Translators in Neural Machine Translation,"One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state-of-the-art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality. * Y. Wang carried out this work during an internship with Amazon AI.","{'sequence': 'One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state-of-the-art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality. * Y. Wang carried out this work during an internship with Amazon AI.', 'labels': ['Resources and Evaluation', 'Machine Translation and Multilinguality', 'Generation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Question Answering', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1365278959274292, 0.09301242977380753, 0.07946866005659103, 0.06044069305062294, 0.05063842609524727, 0.04342838004231453, 0.04299567639827728, 0.04274744912981987, 0.041340623050928116, 0.039239924401044846, 0.03713522106409073, 0.03468267619609833, 0.032416947185993195, 0.03239681199193001, 0.030888328328728676, 0.030644044280052185, 0.02783270925283432, 0.025452185422182083, 0.02424747124314308, 0.024226777255535126, 0.024111686274409294, 0.024020271375775337, 0.022104734554886818]}",0.1365278959274292,Resources and Evaluation,0.09301242977380753
Question Answering,Self-Supervised Test-Time Learning for Reading Comprehension,"Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with supervised methods. In this work, we consider the task of unsupervised reading comprehension and present a method that performs ""test-time learning"" (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing context-question-answer triplets. This method operates directly on a single test context, uses self-supervision to train models on synthetically generated question-answer pairs, and then infers answers to unseen humanauthored questions for this context. Our method achieves accuracies competitive with fully supervised methods and significantly outperforms current unsupervised methods. TTL methods with a smaller model are also competitive with the current state-of-the-art in unsupervised reading comprehension.","{'sequence': 'Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with supervised methods. In this work, we consider the task of unsupervised reading comprehension and present a method that performs ""test-time learning"" (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing context-question-answer triplets. This method operates directly on a single test context, uses self-supervision to train models on synthetically generated question-answer pairs, and then infers answers to unseen humanauthored questions for this context. Our method achieves accuracies competitive with fully supervised methods and significantly outperforms current unsupervised methods. TTL methods with a smaller model are also competitive with the current state-of-the-art in unsupervised reading comprehension.', 'labels': ['Question Answering', 'Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.2132711261510849, 0.0740731954574585, 0.06501518934965134, 0.06189460679888725, 0.0523994080722332, 0.044507768005132675, 0.04124286770820618, 0.03898223489522934, 0.035432346165180206, 0.03475212678313255, 0.03210922330617905, 0.032073263078927994, 0.030776837840676308, 0.028943685814738274, 0.02877284586429596, 0.027236802503466606, 0.026113152503967285, 0.025800228118896484, 0.025462020188570023, 0.022406380623579025, 0.020708249881863594, 0.020701365545392036, 0.017325036227703094]}",0.2132711261510849,Question Answering,0.2132711261510849
Question Answering,Capturing Row and Column Semantics in Transformer Based Question Answering over Tables,"Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells. While this model yields extremely high accuracy at finding cell values on recent benchmarks, a second model we propose, called RCI representation, provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables. Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables (up to ∼98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (TAPAS and TABERT), achieving ∼3.4% and ∼18.86% additional precision improvement on the standard WikiSQL benchmark 1 .","{'sequence': 'Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells. While this model yields extremely high accuracy at finding cell values on recent benchmarks, a second model we propose, called RCI representation, provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables. Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables (up to ∼98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (TAPAS and TABERT), achieving ∼3.4% and ∼18.86% additional precision improvement on the standard WikiSQL benchmark 1 .', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP'], 'scores': [0.19590185582637787, 0.07703173905611038, 0.07474195212125778, 0.056424494832754135, 0.053359758108854294, 0.052969448268413544, 0.04800225421786308, 0.03959295526146889, 0.03609959036111832, 0.035992223769426346, 0.035055018961429596, 0.03423812612891197, 0.033731263130903244, 0.03183475881814957, 0.026657328009605408, 0.025454578921198845, 0.024882396683096886, 0.024648725986480713, 0.02453765645623207, 0.02129499986767769, 0.018419213593006134, 0.016582079231739044, 0.012547620572149754]}",0.19590185582637787,Question Answering,0.19590185582637787
Question Answering,Explainable Multi-hop Verbal Reasoning Through Internal Monologue,"Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones, and (b) using natural language to guide the intermediate reasoning hops. We implement EVR by extending the classic reasoning paradigm General Problem Solver (GPS) with a SOTA generative language model to generate subgoals and perform inference in natural language at each reasoning step. Evaluation of EVR on Clark et al. (2020)'s synthetic question answering (QA) dataset shows that EVR achieves SOTA performance while being able to generate all reasoning steps in natural language. Furthermore, EVR generalizes better than other strong methods when trained on simpler tasks or less training data (up to 35.7% and 7.7% absolute improvement respectively). 1","{'sequence': ""Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones, and (b) using natural language to guide the intermediate reasoning hops. We implement EVR by extending the classic reasoning paradigm General Problem Solver (GPS) with a SOTA generative language model to generate subgoals and perform inference in natural language at each reasoning step. Evaluation of EVR on Clark et al. (2020)'s synthetic question answering (QA) dataset shows that EVR achieves SOTA performance while being able to generate all reasoning steps in natural language. Furthermore, EVR generalizes better than other strong methods when trained on simpler tasks or less training data (up to 35.7% and 7.7% absolute improvement respectively). 1"", 'labels': ['Question Answering', 'Resources and Evaluation', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Machine Learning for NLP', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Information Extraction', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality'], 'scores': [0.179520383477211, 0.15677997469902039, 0.07867652177810669, 0.06966929882764816, 0.04025745391845703, 0.03907173126935959, 0.037199441343545914, 0.03676080331206322, 0.03618200495839119, 0.03577809780836105, 0.03416752070188522, 0.030640648677945137, 0.028776267543435097, 0.02723683975636959, 0.026986394077539444, 0.025659633800387383, 0.02071501687169075, 0.017057815566658974, 0.016846390441060066, 0.01595374569296837, 0.01586678810417652, 0.015736054629087448, 0.014461182989180088]}",0.179520383477211,Question Answering,0.179520383477211
Question Answering,Robust Question Answering Through Sub-part Alignment,"Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surfacelevel patterns, so they fail to generalize to outof-distribution settings. To make a more robust and understandable QA system, we model question answering as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context in order to find the answer. We formulate our model as a structured SVM, with alignment scores computed via BERT, and we can train end-to-end despite using beam search for approximate inference. Our use of explicit alignments allows us to explore a set of constraints with which we can prohibit certain types of bad model behavior arising in cross-domain settings. Furthermore, by investigating differences in scores across different potential answers, we can seek to understand what particular aspects of the input lead the model to choose the answer without relying on post-hoc explanation techniques. We train our model on SQuAD v1.1 and test it on several adversarial and out-of-domain datasets. The results show that our model is more robust than the standard BERT QA model, and constraints derived from alignment scores allow us to effectively trade off coverage and accuracy.","{'sequence': 'Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surfacelevel patterns, so they fail to generalize to outof-distribution settings. To make a more robust and understandable QA system, we model question answering as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context in order to find the answer. We formulate our model as a structured SVM, with alignment scores computed via BERT, and we can train end-to-end despite using beam search for approximate inference. Our use of explicit alignments allows us to explore a set of constraints with which we can prohibit certain types of bad model behavior arising in cross-domain settings. Furthermore, by investigating differences in scores across different potential answers, we can seek to understand what particular aspects of the input lead the model to choose the answer without relying on post-hoc explanation techniques. We train our model on SQuAD v1.1 and test it on several adversarial and out-of-domain datasets. The results show that our model is more robust than the standard BERT QA model, and constraints derived from alignment scores allow us to effectively trade off coverage and accuracy.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Generation', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications'], 'scores': [0.38401463627815247, 0.05813830345869064, 0.05504792183637619, 0.04489495977759361, 0.04447297006845474, 0.03920026496052742, 0.03478511795401573, 0.02842777967453003, 0.027618786320090294, 0.027094533666968346, 0.026989538222551346, 0.024932686239480972, 0.024822916835546494, 0.022903168573975563, 0.02130989544093609, 0.020413512364029884, 0.019537435844540596, 0.01776771992444992, 0.017457053065299988, 0.015613371506333351, 0.015495899133384228, 0.0147185567766428, 0.014343001879751682]}",0.38401463627815247,Question Answering,0.38401463627815247
Question Answering,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,"We propose a general framework called Text Modular Networks (TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing submodels. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate submodels, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model's reasoning. We use this framework to build MODULARQA, 1 a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that MODULARQA is more versatile than existing explainable systems for DROP and Hot-potQA datasets, is more robust than stateof-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.","{'sequence': ""We propose a general framework called Text Modular Networks (TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing submodels. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate submodels, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model's reasoning. We use this framework to build MODULARQA, 1 a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that MODULARQA is more versatile than existing explainable systems for DROP and Hot-potQA datasets, is more robust than stateof-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work."", 'labels': ['Question Answering', 'Generation', 'Speech and Multimodality', 'Machine Learning for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.9085512161254883, 0.010415310971438885, 0.008325919508934021, 0.007036782335489988, 0.006576710380613804, 0.0063559627160429955, 0.004652025643736124, 0.00455893948674202, 0.004491758998483419, 0.004386942833662033, 0.0037734895013272762, 0.0035968124866485596, 0.0032643538434058428, 0.0032074630726128817, 0.0030842553824186325, 0.002914841054007411, 0.0028158831410109997, 0.002694367431104183, 0.0023018009960651398, 0.002015764359384775, 0.0018697549821808934, 0.001832603826187551, 0.0012771146139129996]}",0.9085512161254883,Question Answering,0.9085512161254883
Question Answering,RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering,"State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RE-CONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused reranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four QA tasks, with 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. We will release all related data, models, and code 1 .","{'sequence': 'State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RE-CONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused reranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four QA tasks, with 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. We will release all related data, models, and code 1 .', 'labels': ['Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Speech and Multimodality', 'NLP Applications', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.8493169546127319, 0.019841087982058525, 0.015047349967062473, 0.014434516429901123, 0.009278547950088978, 0.00829413253813982, 0.008275040425360203, 0.007181650493294001, 0.006564420647919178, 0.006425419822335243, 0.0058667478151619434, 0.005809123627841473, 0.005778111517429352, 0.005472514778375626, 0.005416626110672951, 0.004941374063491821, 0.004029096104204655, 0.0035578052047640085, 0.0034159496426582336, 0.002930175280198455, 0.002908857073634863, 0.0026261257007718086, 0.0025883985217660666]}",0.8493169546127319,Question Answering,0.8493169546127319
Question Answering,On the Transferability of Minimal Prediction Preserving Inputs in Question Answering,"Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and ""dataset bias"" (where a model learns to attend to spurious, non-generalizable cues in the training data). We discover a perplexing invariance of MPPIs to random training seed, model architecture, pretraining, and training domain. MPPIs demonstrate remarkable transferability across domains -achieving significantly higher performance than comparably short queries. Additionally, penalizing over-confidence on MPPIs fails to improve either generalization or adversarial robustness. These results suggest the interpretability of MPPIs is insufficient to characterize generalization capacity of these models. We hope this focused investigation encourages more systematic analysis of model behavior outside of the human interpretable distribution of examples. * equal contribution 1 For question answering we construct MPPIs by only removing words from the query. Modifying the context paragraph is poorly defined in MPPI generation as it perturbs the output space, rendering an answer impossible or trivial.","{'sequence': 'Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and ""dataset bias"" (where a model learns to attend to spurious, non-generalizable cues in the training data). We discover a perplexing invariance of MPPIs to random training seed, model architecture, pretraining, and training domain. MPPIs demonstrate remarkable transferability across domains -achieving significantly higher performance than comparably short queries. Additionally, penalizing over-confidence on MPPIs fails to improve either generalization or adversarial robustness. These results suggest the interpretability of MPPIs is insufficient to characterize generalization capacity of these models. We hope this focused investigation encourages more systematic analysis of model behavior outside of the human interpretable distribution of examples. * equal contribution 1 For question answering we construct MPPIs by only removing words from the query. Modifying the context paragraph is poorly defined in MPPI generation as it perturbs the output space, rendering an answer impossible or trivial.', 'labels': ['Question Answering', 'Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2688816785812378, 0.10692004859447479, 0.04966619610786438, 0.04876062646508217, 0.04576262831687927, 0.04320860654115677, 0.04152921959757805, 0.033421676605939865, 0.032992273569107056, 0.03250599652528763, 0.03026791289448738, 0.029121033847332, 0.02745693549513817, 0.027055425569415092, 0.025767115876078606, 0.02366732619702816, 0.022635187953710556, 0.021867575123906136, 0.020156584680080414, 0.019861452281475067, 0.016767146065831184, 0.016426932066679, 0.015300474129617214]}",0.2688816785812378,Question Answering,0.2688816785812378
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Understanding by Understanding Not: Modeling Negation in Language Models,"Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top 1 error rate to 4% on the negated LAMA dataset. We also see some improvements on the negated NLI benchmarks.","{'sequence': 'Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top 1 error rate to 4% on the negated LAMA dataset. We also see some improvements on the negated NLI benchmarks.', 'labels': ['NLP Applications', 'Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Summarization', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.10211851447820663, 0.10131960362195969, 0.08964142948389053, 0.06537115573883057, 0.06488112360239029, 0.05616181343793869, 0.05429428443312645, 0.048470817506313324, 0.04845837503671646, 0.046838805079460144, 0.04157189652323723, 0.04106304422020912, 0.029923122376203537, 0.028817424550652504, 0.02878584899008274, 0.028155211359262466, 0.026718255132436752, 0.02375735528767109, 0.02062007412314415, 0.015755221247673035, 0.013547251932322979, 0.012280846945941448, 0.011448623612523079]}",0.10211851447820663,NLP Applications,0.05616181343793869
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",DuoRAT: Towards Simpler Text-to-SQL Models,"Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building Duo-RAT, a re-implementation of the state-of-theart RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema 1 . * Equal contribution, order was determined by a quantum random number draw.","{'sequence': 'Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building Duo-RAT, a re-implementation of the state-of-theart RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema 1 . * Equal contribution, order was determined by a quantum random number draw.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08046913892030716, 0.0726478323340416, 0.06425734609365463, 0.05794781446456909, 0.053852684795856476, 0.0524725466966629, 0.052163004875183105, 0.04978426918387413, 0.04800117760896683, 0.04631498456001282, 0.04312773421406746, 0.04166911914944649, 0.041247669607400894, 0.04118172451853752, 0.0392272062599659, 0.03870398923754692, 0.03721261024475098, 0.033140961080789566, 0.027310745790600777, 0.026507066562771797, 0.020593341439962387, 0.018682895228266716, 0.013484079390764236]}",0.08046913892030716,Question Answering,0.052163004875183105
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization,"Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using shortpremise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths. * In our experiments, we broadly consider long texts, and do not differentiate between long single sentences and multiple sentences.","{'sequence': 'Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using shortpremise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths. * In our experiments, we broadly consider long texts, and do not differentiate between long single sentences and multiple sentences.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Summarization', 'Dialogue and Interactive Systems', 'Question Answering', 'Ethics and NLP', 'Resources and Evaluation', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1548031121492386, 0.0690121054649353, 0.06836999952793121, 0.06675831973552704, 0.058452803641557693, 0.056658778339624405, 0.04876478388905525, 0.046121370047330856, 0.04598883539438248, 0.04464099183678627, 0.04396534711122513, 0.04328261315822601, 0.038596756756305695, 0.03802211955189705, 0.027165332809090614, 0.02692565880715847, 0.026372063905000687, 0.02489696815609932, 0.019584232941269875, 0.014976873062551022, 0.014285759069025517, 0.012638199143111706, 0.009716928005218506]}",0.1548031121492386,NLP Applications,0.04328261315822601
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Structure-Grounded Pretraining for Text-to-SQL,"Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding, value grounding and columnvalue mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing textto-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. All the code and data used in this work is public available at https://aka.ms/ strug.","{'sequence': 'Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding, value grounding and columnvalue mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing textto-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. All the code and data used in this work is public available at https://aka.ms/ strug.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Question Answering', 'Generation', 'Speech and Multimodality', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11025504767894745, 0.08195903897285461, 0.06920021027326584, 0.061466410756111145, 0.05457506701350212, 0.05427999049425125, 0.050906725227832794, 0.044929422438144684, 0.044818125665187836, 0.04425407573580742, 0.043199822306632996, 0.04036437347531319, 0.03759684041142464, 0.036021407693624496, 0.035481274127960205, 0.03372088819742203, 0.03198828548192978, 0.02800280600786209, 0.026769397780299187, 0.021779177710413933, 0.020375259220600128, 0.017447760328650475, 0.010608638636767864]}",0.11025504767894745,Resources and Evaluation,0.03759684041142464
"Semantics: Sentence-level Semantics, Textual Inference and Other areas","Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System","Text classification is usually studied by labeling natural language texts with relevant categories from a predefined set. In the real world, new classes might keep challenging the existing system with limited labeled data. The system should be intelligent enough to recognize upcoming new classes with a few examples. In this work, we define a new task in the NLP domain, incremental few-shot text classification, where the system incrementally handles multiple rounds of new classes. For each round, there is a batch of new classes with a few labeled examples per class. Two major challenges exist in this new task: (i) For the learning process, the system should incrementally learn new classes round by round without re-training on the examples of preceding classes; (ii) For the performance, the system should perform well on new classes without much loss on preceding classes. In addition to formulating the new task, we also release two benchmark datasets 1 in the incremental fewshot setting: intent classification and relation classification. Moreover, we propose two entailment approaches, ENTAILMENT and HY-BRID, which show promise for solving this novel problem.","{'sequence': 'Text classification is usually studied by labeling natural language texts with relevant categories from a predefined set. In the real world, new classes might keep challenging the existing system with limited labeled data. The system should be intelligent enough to recognize upcoming new classes with a few examples. In this work, we define a new task in the NLP domain, incremental few-shot text classification, where the system incrementally handles multiple rounds of new classes. For each round, there is a batch of new classes with a few labeled examples per class. Two major challenges exist in this new task: (i) For the learning process, the system should incrementally learn new classes round by round without re-training on the examples of preceding classes; (ii) For the performance, the system should perform well on new classes without much loss on preceding classes. In addition to formulating the new task, we also release two benchmark datasets 1 in the incremental fewshot setting: intent classification and relation classification. Moreover, we propose two entailment approaches, ENTAILMENT and HY-BRID, which show promise for solving this novel problem.', 'labels': ['Machine Learning for NLP', 'Question Answering', 'NLP Applications', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Resources and Evaluation', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Information Extraction', 'Summarization', 'Semantics: Lexical Semantics', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.1287061870098114, 0.08169636130332947, 0.07210129499435425, 0.07149829715490341, 0.06115701049566269, 0.05769657343626022, 0.04923618584871292, 0.04250791668891907, 0.04119744524359703, 0.03928886726498604, 0.035599153488874435, 0.03375469520688057, 0.033547405153512955, 0.03329205885529518, 0.03183116763830185, 0.03053765743970871, 0.030496539548039436, 0.02702690102159977, 0.025260450318455696, 0.02125587686896324, 0.019921226426959038, 0.01899588480591774, 0.013394844718277454]}",0.1287061870098114,Machine Learning for NLP,0.03183116763830185
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Temporal Reasoning on Implicit Events from Distant Supervision,"We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit eventsevents that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SYMTIME, which exploits distant supervision signals from largescale text and uses temporal rules to combine start times and durations to infer end times. SYMTIME outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.","{'sequence': 'We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit eventsevents that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SYMTIME, which exploits distant supervision signals from largescale text and uses temporal rules to combine start times and durations to infer end times. SYMTIME outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.', 'labels': ['Resources and Evaluation', 'Computational Social Science and Social Media', 'Information Extraction', 'NLP Applications', 'Generation', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10779307782649994, 0.0726834386587143, 0.07195885479450226, 0.06638819724321365, 0.06574118137359619, 0.06446515768766403, 0.04511645436286926, 0.045081835240125656, 0.04152468591928482, 0.039276860654354095, 0.03803350403904915, 0.037564072757959366, 0.03480776399374008, 0.03407035022974014, 0.03228774294257164, 0.032087989151477814, 0.03203921020030975, 0.030940402299165726, 0.0284336656332016, 0.02826438471674919, 0.026797110214829445, 0.016030842438340187, 0.008613166399300098]}",0.10779307782649994,Resources and Evaluation,0.03480776399374008
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models,"Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pretrained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-theart sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks.","{'sequence': 'Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pretrained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-theart sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Information Extraction', 'Speech and Multimodality', 'Resources and Evaluation', 'Generation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1134796291589737, 0.10325649380683899, 0.06884042918682098, 0.06615372002124786, 0.06614778190851212, 0.04877239093184471, 0.04697389155626297, 0.04267672076821327, 0.042513731867074966, 0.041739679872989655, 0.037158820778131485, 0.03707250952720642, 0.036632340401411057, 0.035954009741544724, 0.029646076261997223, 0.028892090544104576, 0.025358622893691063, 0.02335229702293873, 0.023254597559571266, 0.023193687200546265, 0.022795798256993294, 0.022114278748631477, 0.014020352624356747]}",0.1134796291589737,Machine Learning for NLP,0.06614778190851212
Summarization,Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs,"Abstractive conversation summarization has received much attention recently. However, these generated summaries often suffer from insufficient, redundant, or incorrect content, largely due to the unstructured and complex characteristics of human-human interactions. To this end, we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization, by first incorporating discourse relations between utterances and action triples (""WHO-DOING-WHAT"") in utterances through structured graphs to better encode conversations, and then designing a multi-granularity decoder to generate summaries by combining all levels of information. Experiments show that our proposed models outperform state-of-theart methods and generalize well in other domains in terms of both automatic evaluations and human judgments. We have publicly released our code at https://github.com/ GT-SALT/Structure-Aware-BART.","{'sequence': 'Abstractive conversation summarization has received much attention recently. However, these generated summaries often suffer from insufficient, redundant, or incorrect content, largely due to the unstructured and complex characteristics of human-human interactions. To this end, we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization, by first incorporating discourse relations between utterances and action triples (""WHO-DOING-WHAT"") in utterances through structured graphs to better encode conversations, and then designing a multi-granularity decoder to generate summaries by combining all levels of information. Experiments show that our proposed models outperform state-of-theart methods and generalize well in other domains in terms of both automatic evaluations and human judgments. We have publicly released our code at https://github.com/ GT-SALT/Structure-Aware-BART.', 'labels': ['Summarization', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Information Extraction', 'Question Answering', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1821126788854599, 0.08868616819381714, 0.07176405936479568, 0.06354651600122452, 0.054536741226911545, 0.05144022777676582, 0.050652310252189636, 0.046961039304733276, 0.04660816490650177, 0.03855109587311745, 0.03827592730522156, 0.03693466633558273, 0.035717591643333435, 0.02768656425178051, 0.021522648632526398, 0.021389925852417946, 0.0210330318659544, 0.0205707810819149, 0.02050497569143772, 0.02032630704343319, 0.01658875308930874, 0.015034755691885948, 0.009555111639201641]}",0.1821126788854599,Summarization,0.1821126788854599
Summarization,A New Approach to Overgenerating and Scoring Abstractive Summaries,"We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users' needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both stages can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this paradigm can achieve state-of-the-art performance.","{'sequence': ""We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users' needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both stages can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this paradigm can achieve state-of-the-art performance."", 'labels': ['Generation', 'Summarization', 'Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Question Answering', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'NLP Applications', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.09688698500394821, 0.09037107229232788, 0.08476855605840683, 0.07738585025072098, 0.05881009250879288, 0.05413403362035751, 0.051761310547590256, 0.0492592379450798, 0.040893614292144775, 0.03997070714831352, 0.03912893682718277, 0.03912278264760971, 0.03215339779853821, 0.03190890699625015, 0.029921535402536392, 0.028975166380405426, 0.02723846398293972, 0.027119485661387444, 0.02229035645723343, 0.02180052548646927, 0.021743962541222572, 0.01862957514822483, 0.015725430101156235]}",0.09688698500394821,Generation,0.09037107229232788
Summarization,D2S: Document-to-Slide Generation Via Query-Based Text Summarization,"Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years' NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that longform QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.","{'sequence': ""Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years' NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that longform QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation."", 'labels': ['Question Answering', 'Summarization', 'Generation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.23463404178619385, 0.13617700338363647, 0.09662681072950363, 0.0572521910071373, 0.04864279925823212, 0.048637017607688904, 0.0373273566365242, 0.03382822498679161, 0.03073725476861, 0.030474532395601273, 0.028674311935901642, 0.024826377630233765, 0.023332858458161354, 0.02314508520066738, 0.022987766191363335, 0.02253473550081253, 0.021790487691760063, 0.017586296424269676, 0.015395148657262325, 0.01382396649569273, 0.011779368855059147, 0.01107525359839201, 0.008711082860827446]}",0.23463404178619385,Question Answering,0.13617700338363647
Summarization,Efficient Attentions for Long Document Summarization,"The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose HEPOS, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with HEPOS, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GOVREPORT, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors. tokens with a batch size of 1, 70GB of memory is needed for encoder attentions, and 8GB for encoder-decoder attentions. 2 Our code is released at https://github.com/ luyang-huang96/LongDocSum. 3 GOVREPORT can be downloaded from https:// gov-report-data.github.io.","{'sequence': 'The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose HEPOS, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with HEPOS, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GOVREPORT, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors. tokens with a batch size of 1, 70GB of memory is needed for encoder attentions, and 8GB for encoder-decoder attentions. 2 Our code is released at https://github.com/ luyang-huang96/LongDocSum. 3 GOVREPORT can be downloaded from https:// gov-report-data.github.io.', 'labels': ['Summarization', 'Resources and Evaluation', 'Information Extraction', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Question Answering', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Ethics and NLP', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.2161901444196701, 0.12319281697273254, 0.05451710894703865, 0.05287514254450798, 0.05069112032651901, 0.044296685606241226, 0.04040125757455826, 0.03664679452776909, 0.03639866039156914, 0.03498438745737076, 0.03194444626569748, 0.02900819480419159, 0.029001057147979736, 0.027789555490016937, 0.027055589482188225, 0.025217998772859573, 0.02455081418156624, 0.022556306794285774, 0.02204284258186817, 0.01917385682463646, 0.018732359632849693, 0.01665510982275009, 0.01607775129377842]}",0.2161901444196701,Summarization,0.2161901444196701
Summarization,RefSum: Refactoring Neural Summarization,"Although some recent works show potential complementarity among different state-of-theart systems, few works try to investigate this problem in text summarization. Researchers in other areas commonly refer to the techniques of reranking or stacking to approach this problem. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework Refactor that provides a unified view of text summarization and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four datasets, and three different application scenarios. Besides new state-of-the-art results on CNN/DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our system can be directly used by other researchers as an offthe-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it: https://github.com/yixinL7/ Refactoring-Summarization.","{'sequence': 'Although some recent works show potential complementarity among different state-of-theart systems, few works try to investigate this problem in text summarization. Researchers in other areas commonly refer to the techniques of reranking or stacking to approach this problem. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework Refactor that provides a unified view of text summarization and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four datasets, and three different application scenarios. Besides new state-of-the-art results on CNN/DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our system can be directly used by other researchers as an offthe-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it: https://github.com/yixinL7/ Refactoring-Summarization.', 'labels': ['Summarization', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Generation', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.43934932351112366, 0.04645917937159538, 0.04292178899049759, 0.03473742678761482, 0.03291347250342369, 0.03246989846229553, 0.030399633571505547, 0.029035327956080437, 0.028664465993642807, 0.028223510831594467, 0.02638276480138302, 0.0260155089199543, 0.025408469140529633, 0.02469302900135517, 0.02232886292040348, 0.020022479817271233, 0.020017161965370178, 0.019129348918795586, 0.018760530278086662, 0.017743203788995743, 0.014092815108597279, 0.012493051588535309, 0.007738671265542507]}",0.43934932351112366,Summarization,0.43934932351112366
Summarization,Annotating and Modeling Fine-grained Factuality in Summarization,"Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSUM. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSUM summarization models by allowing us to identify non-factual tokens in the training data. 1 1450 Reference Summary: An early-medieval gold pendant created from an imitation of a Byzantine coin that was found in a Norfolk field is a ""rare find"", a museum expert has said. Source Article Fragment: Discovered on land at North Elmham, near Dereham, the circa 600 AD coin was created by French rulers of the time to increase their available currency. […] The pendant was declared treasure by the Norfolk coroner on Wednesday. An 18th century coin believed to be worth more than #1m has been discovered. A gold pendant created from a necklace was found in a field Entitycentric (Ent-C) The pendant was declared a treasure by the Norfolk coroner on Wednesday. The pendant was declared a treasure by the Ohio coroner on March. Generation centric (Gen-C) Label Human Annotation non-factual span non-factual arc (factual arcs not shown) Sentences Training Dataset","{'sequence': 'Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSUM. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSUM summarization models by allowing us to identify non-factual tokens in the training data. 1 1450 Reference Summary: An early-medieval gold pendant created from an imitation of a Byzantine coin that was found in a Norfolk field is a ""rare find"", a museum expert has said. Source Article Fragment: Discovered on land at North Elmham, near Dereham, the circa 600 AD coin was created by French rulers of the time to increase their available currency. […] The pendant was declared treasure by the Norfolk coroner on Wednesday. An 18th century coin believed to be worth more than #1m has been discovered. A gold pendant created from a necklace was found in a field Entitycentric (Ent-C) The pendant was declared a treasure by the Norfolk coroner on Wednesday. The pendant was declared a treasure by the Ohio coroner on March. Generation centric (Gen-C) Label Human Annotation non-factual span non-factual arc (factual arcs not shown) Sentences Training Dataset', 'labels': ['Generation', 'Information Extraction', 'Resources and Evaluation', 'Summarization', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP'], 'scores': [0.14318308234214783, 0.07286612689495087, 0.06976857036352158, 0.06617818027734756, 0.06462104618549347, 0.05502927675843239, 0.04738231748342514, 0.04488370940089226, 0.04250744357705116, 0.04076407104730606, 0.036967214196920395, 0.033446647226810455, 0.032628607004880905, 0.03209549933671951, 0.030114393681287766, 0.026371855288743973, 0.026013068854808807, 0.025577370077371597, 0.025359412655234337, 0.024977317079901695, 0.02190820686519146, 0.01908493973314762, 0.018271587789058685]}",0.14318308234214783,Generation,0.06617818027734756
"Syntax: Tagging, Chunking and Parsing",Larger-Context Tagging: When and Why Does It Work?,"The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by largercontext training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information.","{'sequence': 'The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by largercontext training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'NLP Applications', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Question Answering', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09461913257837296, 0.06630811095237732, 0.05973770096898079, 0.05934416130185127, 0.055456846952438354, 0.051639966666698456, 0.0506901778280735, 0.05031083896756172, 0.048633839935064316, 0.046460799872875214, 0.04406184330582619, 0.040407728403806686, 0.03950781747698784, 0.03550153225660324, 0.03533812239766121, 0.03344190493226051, 0.03299278765916824, 0.03126490116119385, 0.03014838881790638, 0.030036544427275658, 0.029147421941161156, 0.019640373066067696, 0.015309147536754608]}",0.09461913257837296,Resources and Evaluation,0.03299278765916824
"Syntax: Tagging, Chunking and Parsing",Neural Sequence Segmentation as Determining the Leftmost Segments,"Prior methods to text segmentation are mostly at token level. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel framework that incrementally segments natural language sentences at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and recurrent neural networks (RNN) to model the iterations of determining the leftmost segments. We have conducted extensive experiments on syntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets, demonstrating that our methods have significantly outperformed previous all baselines and achieved new stateof-the-art results. Moreover, qualitative analysis and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies.","{'sequence': 'Prior methods to text segmentation are mostly at token level. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel framework that incrementally segments natural language sentences at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and recurrent neural networks (RNN) to model the iterations of determining the leftmost segments. We have conducted extensive experiments on syntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets, demonstrating that our methods have significantly outperformed previous all baselines and achieved new stateof-the-art results. Moreover, qualitative analysis and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Question Answering', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Summarization', 'Generation', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0746736228466034, 0.06709177792072296, 0.06674997508525848, 0.06470847129821777, 0.058030713349580765, 0.056910425424575806, 0.052488550543785095, 0.05245032534003258, 0.046738166362047195, 0.04142048582434654, 0.04058564454317093, 0.04058487340807915, 0.040397077798843384, 0.03842705115675926, 0.03828343003988266, 0.03470277786254883, 0.033563148230314255, 0.0327613539993763, 0.030628113076090813, 0.0269077867269516, 0.025269431993365288, 0.025201842188835144, 0.011424827389419079]}",0.0746736228466034,"Syntax: Tagging, Chunking and Parsing",0.0746736228466034
"Syntax: Tagging, Chunking and Parsing",PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols,"Probabilistic context-free grammars (PCFGs) with neural parameterization have been shown to be effective in unsupervised phrasestructure grammar induction. However, due to the cubic computational complexity of PCFG representation and parsing, previous approaches cannot scale up to a relatively large number of (nonterminal and preterminal) symbols. In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in the symbol number and therefore allows us to use a much larger number of symbols. We further use neural parameterization for the new form to improve unsupervised parsing performance. We evaluate our model across ten languages and empirically demonstrate the effectiveness of using more symbols.","{'sequence': 'Probabilistic context-free grammars (PCFGs) with neural parameterization have been shown to be effective in unsupervised phrasestructure grammar induction. However, due to the cubic computational complexity of PCFG representation and parsing, previous approaches cannot scale up to a relatively large number of (nonterminal and preterminal) symbols. In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in the symbol number and therefore allows us to use a much larger number of symbols. We further use neural parameterization for the new form to improve unsupervised parsing performance. We evaluate our model across ten languages and empirically demonstrate the effectiveness of using more symbols.', 'labels': ['Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'NLP Applications', 'Information Extraction', 'Generation', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Speech and Multimodality', 'Summarization', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12925252318382263, 0.08104638755321503, 0.0671578049659729, 0.0649610385298729, 0.062178485095500946, 0.05626546964049339, 0.048018574714660645, 0.04540082439780235, 0.04507704824209213, 0.044135790318250656, 0.04350898042321205, 0.03626050055027008, 0.03276302292943001, 0.031912099570035934, 0.0306248776614666, 0.02764919213950634, 0.027236489579081535, 0.026298372074961662, 0.02308160997927189, 0.02212613634765148, 0.02025487832725048, 0.018188826739788055, 0.016601061448454857]}",0.12925252318382263,Resources and Evaluation,0.08104638755321503
"Syntax: Tagging, Chunking and Parsing",GEMNET: Effective Gated Gazetteer Representations for Recognizing Complex Entities in Low-context Input,"Named Entity Recognition (NER) remains difficult in real-world settings; current challenges include short texts (low context), emerging entities, and complex entities (e.g. movie names). Gazetteer features can help, but results have been mixed due to challenges with adding extra features, and a lack of realistic evaluation data. It has been shown that including gazetteer features can cause models to overuse or underuse them, leading to poor generalization. We propose GEMNET, a novel approach for gazetteer knowledge integration, including (1) a flexible Contextual Gazetteer Representation (CGR) encoder that can be fused with any word-level model; and (2) a Mixture-of-Experts gating network that overcomes the feature overuse issue by learning to conditionally combine the context and gazetteer features, instead of assigning them fixed weights. To comprehensively evaluate our approaches, we create 3 large NER datasets (24M tokens) reflecting current challenges. In an uncased setting, our methods show large gains (up to +49% F1) in recognizing difficult entities compared to existing baselines. On standard benchmarks, we achieve a new uncased SOTA on CoNLL03 and WNUT17. * This research was done during an internship at Amazon.","{'sequence': 'Named Entity Recognition (NER) remains difficult in real-world settings; current challenges include short texts (low context), emerging entities, and complex entities (e.g. movie names). Gazetteer features can help, but results have been mixed due to challenges with adding extra features, and a lack of realistic evaluation data. It has been shown that including gazetteer features can cause models to overuse or underuse them, leading to poor generalization. We propose GEMNET, a novel approach for gazetteer knowledge integration, including (1) a flexible Contextual Gazetteer Representation (CGR) encoder that can be fused with any word-level model; and (2) a Mixture-of-Experts gating network that overcomes the feature overuse issue by learning to conditionally combine the context and gazetteer features, instead of assigning them fixed weights. To comprehensively evaluate our approaches, we create 3 large NER datasets (24M tokens) reflecting current challenges. In an uncased setting, our methods show large gains (up to +49% F1) in recognizing difficult entities compared to existing baselines. On standard benchmarks, we achieve a new uncased SOTA on CoNLL03 and WNUT17. * This research was done during an internship at Amazon.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Generation', 'NLP Applications', 'Information Extraction', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Machine Learning for NLP', 'Summarization', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08699516952037811, 0.07089390605688095, 0.0665590688586235, 0.05982578918337822, 0.05913377180695534, 0.05485453084111214, 0.05304575711488724, 0.04673025384545326, 0.0444943793118, 0.042916662991046906, 0.04049292951822281, 0.039200156927108765, 0.03759552910923958, 0.03757378086447716, 0.03734559938311577, 0.0371219664812088, 0.03535225987434387, 0.03174268826842308, 0.028693821281194687, 0.025234714150428772, 0.023803869262337685, 0.020288575440645218, 0.02010485902428627]}",0.08699516952037811,Resources and Evaluation,0.0371219664812088
Dialogue and Interactive Systems,Generating Negative Samples by Manipulating Golden Responses for Unsupervised Learning of a Response Evaluation Model,"Evaluating the quality of responses generated by open-domain conversation systems is a challenging task. This is partly because there can be multiple appropriate responses to a given dialogue history. Reference-based metrics that rely on comparisons to a set of known correct responses often fail to account for this variety, and consequently correlate poorly with human judgment. To address this problem, researchers have investigated the possibility of assessing response quality without using a set of known correct responses. Tao et al. (2018) demonstrated that an automatic response evaluation model could be made using unsupervised learning for the next-utterance prediction (NUP) task. For unsupervised learning of such a model, we propose a method of manipulating a golden response to create a new negative response that is designed to be inappropriate within the context while maintaining high similarity with the original golden response. We find, from our experiments on English datasets, that using the negative samples generated by our method alongside random negative samples can increase the model's correlation with human evaluations. The process of generating such negative samples is automated and does not rely on human annotation. 1","{'sequence': ""Evaluating the quality of responses generated by open-domain conversation systems is a challenging task. This is partly because there can be multiple appropriate responses to a given dialogue history. Reference-based metrics that rely on comparisons to a set of known correct responses often fail to account for this variety, and consequently correlate poorly with human judgment. To address this problem, researchers have investigated the possibility of assessing response quality without using a set of known correct responses. Tao et al. (2018) demonstrated that an automatic response evaluation model could be made using unsupervised learning for the next-utterance prediction (NUP) task. For unsupervised learning of such a model, we propose a method of manipulating a golden response to create a new negative response that is designed to be inappropriate within the context while maintaining high similarity with the original golden response. We find, from our experiments on English datasets, that using the negative samples generated by our method alongside random negative samples can increase the model's correlation with human evaluations. The process of generating such negative samples is automated and does not rely on human annotation. 1"", 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Generation', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09499922394752502, 0.09044788777828217, 0.07532373815774918, 0.07216430455446243, 0.06524261087179184, 0.04634030535817146, 0.044805675745010376, 0.04432278871536255, 0.04147152975201607, 0.03814338520169258, 0.036222316324710846, 0.03508152440190315, 0.03459203988313675, 0.033326443284749985, 0.032707758247852325, 0.032469797879457474, 0.03199543431401253, 0.029976889491081238, 0.028659066185355186, 0.02704041451215744, 0.025847002863883972, 0.0201744232326746, 0.018645422533154488]}",0.09499922394752502,Dialogue and Interactive Systems,0.09499922394752502
Dialogue and Interactive Systems,How Robust are Fact Checking Systems on Colloquial Claims?,"Knowledge is now starting to power neural dialogue agents. At the same time, the risk of misinformation and disinformation from dialogue agents also rises. Verifying the veracity of information from formal sources are widely studied in computational fact checking. In this work, we ask: How robust are fact checking systems on claims in colloquial style? We aim to open up new discussions in the intersection of fact verification and dialogue safety. In order to investigate how fact checking systems behave on colloquial claims, we transfer the styles of claims from FEVER (Thorne et al., 2018) into colloquialism. We find that existing fact checking systems that perform well on claims in formal style significantly degenerate on colloquial claims with the same semantics. Especially, we show that document retrieval is the weakest spot in the system even vulnerable to filler words, such as ""yeah"" and ""you know"". The document recall of WikiAPI retriever (Hanselowski et al., 2018) which is 90.0% on FEVER, drops to 72.2% on the colloquial claims. We compare the characteristics of colloquial claims to those of claims in formal style, and demonstrate the challenging issues in them.","{'sequence': 'Knowledge is now starting to power neural dialogue agents. At the same time, the risk of misinformation and disinformation from dialogue agents also rises. Verifying the veracity of information from formal sources are widely studied in computational fact checking. In this work, we ask: How robust are fact checking systems on claims in colloquial style? We aim to open up new discussions in the intersection of fact verification and dialogue safety. In order to investigate how fact checking systems behave on colloquial claims, we transfer the styles of claims from FEVER (Thorne et al., 2018) into colloquialism. We find that existing fact checking systems that perform well on claims in formal style significantly degenerate on colloquial claims with the same semantics. Especially, we show that document retrieval is the weakest spot in the system even vulnerable to filler words, such as ""yeah"" and ""you know"". The document recall of WikiAPI retriever (Hanselowski et al., 2018) which is 90.0% on FEVER, drops to 72.2% on the colloquial claims. We compare the characteristics of colloquial claims to those of claims in formal style, and demonstrate the challenging issues in them.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'NLP Applications', 'Resources and Evaluation', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.14008229970932007, 0.06819552183151245, 0.058096930384635925, 0.051899977028369904, 0.05084266886115074, 0.0474870540201664, 0.04724368825554848, 0.045599717646837234, 0.04401414096355438, 0.0433945395052433, 0.04161554574966431, 0.03951466828584671, 0.03806755691766739, 0.037787795066833496, 0.036418382078409195, 0.03371691331267357, 0.03236939385533333, 0.02838394232094288, 0.02831197716295719, 0.02667233906686306, 0.022063113749027252, 0.019558381289243698, 0.018663475289940834]}",0.14008229970932007,Dialogue and Interactive Systems,0.14008229970932007
Dialogue and Interactive Systems,Fine-grained Post-training for Improving Retrieval-based Dialogue Systems,"Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task. 1","{'sequence': 'Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task. 1', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'NLP Applications', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1474538892507553, 0.11104927957057953, 0.0734306052327156, 0.05166259780526161, 0.046759363263845444, 0.04597292095422745, 0.04531392082571983, 0.04250789433717728, 0.03805428743362427, 0.036735594272613525, 0.03647800162434578, 0.03586772084236145, 0.033461809158325195, 0.03181271627545357, 0.03174726292490959, 0.03125033527612686, 0.029255183413624763, 0.028178129345178604, 0.025230661034584045, 0.022788526490330696, 0.02262645587325096, 0.019858814775943756, 0.012503973208367825]}",0.1474538892507553,Resources and Evaluation,0.11104927957057953
Dialogue and Interactive Systems,Adding Chit-Chat to Enhance Task-Oriented Dialogues,"Existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human ↔ AI collaborative data collection approach for generating diverse chitchat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8K dialogues from two popular task-oriented datasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding chit-chat to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can codeswitch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.","{'sequence': 'Existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human ↔ AI collaborative data collection approach for generating diverse chitchat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8K dialogues from two popular task-oriented datasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding chit-chat to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can codeswitch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Information Extraction', 'Discourse and Pragmatics', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Summarization', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.26238250732421875, 0.08505772799253464, 0.06487228721380234, 0.06182849034667015, 0.04267505183815956, 0.042412929236888885, 0.03856557235121727, 0.03610542416572571, 0.03451188653707504, 0.03341897577047348, 0.03327123448252678, 0.03294799104332924, 0.03130066394805908, 0.03038012608885765, 0.02893989533185959, 0.026604047045111656, 0.0207364559173584, 0.020028555765748024, 0.019324373453855515, 0.017380550503730774, 0.014784329570829868, 0.013833166100084782, 0.008637872524559498]}",0.26238250732421875,Dialogue and Interactive Systems,0.26238250732421875
Discourse and Pragmatics,Incorporating Syntax and Semantics in Coreference Resolution with Heterogeneous Graph Attention Network,"External syntactic and semantic information has been largely ignored by existing neural coreference resolution models. In this paper, we present a heterogeneous graph-based model to incorporate syntactic and semantic structures of sentences. The proposed graph contains a syntactic sub-graph where tokens are connected based on a dependency tree, and a semantic sub-graph that contains arguments and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model. 1","{'sequence': 'External syntactic and semantic information has been largely ignored by existing neural coreference resolution models. In this paper, we present a heterogeneous graph-based model to incorporate syntactic and semantic structures of sentences. The proposed graph contains a syntactic sub-graph where tokens are connected based on a dependency tree, and a semantic sub-graph that contains arguments and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model. 1', 'labels': ['Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Machine Learning for NLP', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Summarization', 'Machine Translation and Multilinguality'], 'scores': [0.12192931026220322, 0.11416663974523544, 0.06779246777296066, 0.06404612958431244, 0.057189423590898514, 0.05023060739040375, 0.04273892939090729, 0.04212294518947601, 0.040767353028059006, 0.03672206401824951, 0.03535890206694603, 0.03515460342168808, 0.03420890122652054, 0.032346829771995544, 0.030957452952861786, 0.03083234466612339, 0.028822898864746094, 0.02725362405180931, 0.02495885267853737, 0.024057593196630478, 0.020061451941728592, 0.01931891031563282, 0.018961671739816666]}",0.12192931026220322,Information Extraction,0.020061451941728592
Discourse and Pragmatics,Context Tracking Network: Graph-based Context Modeling for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition (IDRR) aims to identify logical relations between two adjacent sentences in the discourse. Existing models fail to fully utilize the contextual information which plays an important role in interpreting each local sentence. In this paper, we thus propose a novel graph-based Context Tracking Network (CT-Net) to model the discourse context for IDRR. The CT-Net firstly converts the discourse into the paragraph association graph (PAG), where each sentence tracks their closely related context from the intricate discourse through different types of edges. Then, the CT-Net extracts contextual representation from the PAG through a specially designed cross-grained updating mechanism, which can effectively integrate both sentence-level and token-level contextual semantics. Experiments on PDTB 2.0 show that the CT-Net gains better performance than models that roughly model the context.","{'sequence': 'Implicit discourse relation recognition (IDRR) aims to identify logical relations between two adjacent sentences in the discourse. Existing models fail to fully utilize the contextual information which plays an important role in interpreting each local sentence. In this paper, we thus propose a novel graph-based Context Tracking Network (CT-Net) to model the discourse context for IDRR. The CT-Net firstly converts the discourse into the paragraph association graph (PAG), where each sentence tracks their closely related context from the intricate discourse through different types of edges. Then, the CT-Net extracts contextual representation from the PAG through a specially designed cross-grained updating mechanism, which can effectively integrate both sentence-level and token-level contextual semantics. Experiments on PDTB 2.0 show that the CT-Net gains better performance than models that roughly model the context.', 'labels': ['Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Summarization', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09719756990671158, 0.08113023638725281, 0.07814646512269974, 0.056993041187524796, 0.05207511782646179, 0.050373680889606476, 0.04961760714650154, 0.049245964735746384, 0.04578356444835663, 0.04570172727108002, 0.04455144703388214, 0.04379230737686157, 0.04289451241493225, 0.03659043833613396, 0.032489608973264694, 0.03108629398047924, 0.030814170837402344, 0.02812851592898369, 0.026283521205186844, 0.023233672603964806, 0.020274033769965172, 0.01967594400048256, 0.013920515775680542]}",0.09719756990671158,Information Extraction,0.04455144703388214
Discourse and Pragmatics,Improving Neural RST Parsing Model with Silver Agreement Subtrees,"Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST parsing models by exploiting silver data, i.e., automatically annotated data. We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser.","{'sequence': 'Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST parsing models by exploiting silver data, i.e., automatically annotated data. We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Extraction', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Summarization', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10239902883768082, 0.08093053847551346, 0.08001980930566788, 0.06734459102153778, 0.061810143291950226, 0.05402934178709984, 0.04592672362923622, 0.04440749064087868, 0.04374184086918831, 0.04358900338411331, 0.04347860813140869, 0.03919867053627968, 0.03827335685491562, 0.036395370960235596, 0.03427943214774132, 0.0318063385784626, 0.028388353064656258, 0.02713582292199135, 0.024983670562505722, 0.022248227149248123, 0.021498501300811768, 0.01517180260270834, 0.012943366542458534]}",0.10239902883768082,"Syntax: Tagging, Chunking and Parsing",0.028388353064656258
Discourse and Pragmatics,RST Parsing from Scratch,"We introduce a novel top-down end-to-end formulation of document level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high scoring trees. With extensive experiments on the standard English RST discourse treebank, we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. More importantly, it does so without using any handcrafted features, making it faster and easily adaptable to new languages and domains.","{'sequence': 'We introduce a novel top-down end-to-end formulation of document level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high scoring trees. With extensive experiments on the standard English RST discourse treebank, we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. More importantly, it does so without using any handcrafted features, making it faster and easily adaptable to new languages and domains.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10095904767513275, 0.058232784271240234, 0.053972743451595306, 0.05317901447415352, 0.05105428770184517, 0.05011769384145737, 0.04994513839483261, 0.0487651526927948, 0.0474090613424778, 0.04676775634288788, 0.04672958701848984, 0.04418327659368515, 0.03971591964364052, 0.038223929703235626, 0.0368628054857254, 0.03526873514056206, 0.034610722213983536, 0.034404706209897995, 0.03373252972960472, 0.026944782584905624, 0.026138631626963615, 0.023295806720852852, 0.01948585733771324]}",0.10095904767513275,"Syntax: Tagging, Chunking and Parsing",0.05105428770184517
Discourse and Pragmatics,Did they answer? Subjective acts and intents in conversational discourse,"Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/ elisaF/subjective_discourse. Annotator with positive sentiment Annotator with negative sentiment R: Congresswoman, it might be useful to clarify what actually happened. A developer who is a researcher-Resp. Label: shift+correct Expl: Witness wants to clarify what happened. Resp. Label: shift+dodge Expl: Mr. Zuckerberg goes off on a tangent to ""clarify"" the situation. R: We are working through the process. We have never said we would not provide those. Resp. Label: ans+direct Expl: Mr. Koskinen answers and does say factually that they never said they would not provide the emails. Resp. Label: shift+dodge Expl: Koskinen evades the question, by saying that he never said he wouldn't provide the emails.","{'sequence': 'Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/ elisaF/subjective_discourse. Annotator with positive sentiment Annotator with negative sentiment R: Congresswoman, it might be useful to clarify what actually happened. A developer who is a researcher-Resp. Label: shift+correct Expl: Witness wants to clarify what happened. Resp. Label: shift+dodge Expl: Mr. Zuckerberg goes off on a tangent to ""clarify"" the situation. R: We are working through the process. We have never said we would not provide those. Resp. Label: ans+direct Expl: Mr. Koskinen answers and does say factually that they never said they would not provide the emails. Resp. Label: shift+dodge Expl: Koskinen evades the question, by saying that he never said he wouldn\'t provide the emails.', 'labels': ['Question Answering', 'Information Extraction', 'Summarization', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1427198052406311, 0.0975092425942421, 0.08518099039793015, 0.06121902912855148, 0.060244571417570114, 0.056651052087545395, 0.05377287045121193, 0.0472242571413517, 0.040377549827098846, 0.037912242114543915, 0.03476833924651146, 0.03413612022995949, 0.030156614258885384, 0.02946729026734829, 0.029138853773474693, 0.029020503163337708, 0.024684838950634003, 0.022008754312992096, 0.021819908171892166, 0.01800351031124592, 0.01660965383052826, 0.014946364797651768, 0.012427556328475475]}",0.1427198052406311,Question Answering,0.0472242571413517
Discourse and Pragmatics,Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance,"Recent work on entity coreference resolution (CR) follows current trends in Deep Learning applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention.","{'sequence': 'Recent work on entity coreference resolution (CR) follows current trends in Deep Learning applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Generation', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Resources and Evaluation', 'NLP Applications', 'Semantics: Lexical Semantics', 'Question Answering', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09365912526845932, 0.0843343734741211, 0.06443340331315994, 0.05935601517558098, 0.05856624245643616, 0.05721735581755638, 0.05329152196645737, 0.04830361530184746, 0.04640432447195053, 0.04609455540776253, 0.04451705142855644, 0.04407377913594246, 0.03484279289841652, 0.031542032957077026, 0.03142304718494415, 0.03132759407162666, 0.029628589749336243, 0.02652922086417675, 0.026377765461802483, 0.023848427459597588, 0.02250893972814083, 0.02114388532936573, 0.0205763578414917]}",0.09365912526845932,"Syntax: Tagging, Chunking and Parsing",0.04640432447195053
Discourse and Pragmatics,Bridging Resolution: Making Sense of the State of the Art,"While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rulebased approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and ( 2 ) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of their comparative strengths and weaknesses; and (2) perform a manual analysis of the errors made by the MTL model.","{'sequence': 'While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rulebased approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and ( 2 ) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of their comparative strengths and weaknesses; and (2) perform a manual analysis of the errors made by the MTL model.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Lexical Semantics', 'Generation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Information Extraction', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining'], 'scores': [0.08360940217971802, 0.077879898250103, 0.07343348115682602, 0.06446477025747299, 0.058011818677186966, 0.056569527834653854, 0.05538535118103027, 0.05284386873245239, 0.049615297466516495, 0.04538330063223839, 0.041598912328481674, 0.04136896878480911, 0.0372600220143795, 0.03551295027136803, 0.03431599214673042, 0.03422949090600014, 0.03277270495891571, 0.0307152159512043, 0.025690563023090363, 0.020821673795580864, 0.016592275351285934, 0.016460604965686798, 0.01546396128833294]}",0.08360940217971802,Dialogue and Interactive Systems,0.049615297466516495
Machine Learning for NLP,Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle,"Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing and syntactic generalization tests, while using fewer parameters than other models.","{'sequence': 'Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing and syntactic generalization tests, while using fewer parameters than other models.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'NLP Applications', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09678539633750916, 0.07717646658420563, 0.07118754833936691, 0.062064919620752335, 0.05759983882308006, 0.05738263577222824, 0.043904609978199005, 0.0427754670381546, 0.04168561473488808, 0.04151434823870659, 0.04059199243783951, 0.03784356638789177, 0.03640781715512276, 0.03560385853052139, 0.0347251370549202, 0.034385114908218384, 0.03322473540902138, 0.03265642002224922, 0.031121229752898216, 0.027430996298789978, 0.02735186368227005, 0.022330621257424355, 0.01424980629235506]}",0.09678539633750916,Dialogue and Interactive Systems,0.022330621257424355
Machine Learning for NLP,Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation,"Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. ( 2020 ) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counterevidence to these claims.","{'sequence': 'Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. ( 2020 ) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counterevidence to these claims.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Speech and Multimodality', 'Question Answering', 'Generation', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.23570764064788818, 0.10443089157342911, 0.05997772887349129, 0.05669265240430832, 0.0533355176448822, 0.05327988788485527, 0.04417286440730095, 0.04020703211426735, 0.03995509445667267, 0.03810165822505951, 0.03282182291150093, 0.030742265284061432, 0.02996484749019146, 0.02851736731827259, 0.02498055249452591, 0.02319340407848358, 0.01956731267273426, 0.018507545813918114, 0.01838548108935356, 0.014111526310443878, 0.013357406482100487, 0.01163764949887991, 0.008351851254701614]}",0.23570764064788818,NLP Applications,0.0533355176448822
Machine Learning for NLP,Learning to Organize a Bag of Words into Sentences with Neural Networks: An Empirical Study,"Sequential information, a.k.a., orders, is assumed to be essential for processing a sequence with recurrent neural network or convolutional neural network based encoders. However, is it possible to encode natural languages without orders? Given a bag of words from a disordered sentence, humans may still be able to understand what those words mean by reordering or reconstructing them. Inspired by such an intuition, in this paper, we perform a study to investigate how ""order"" information takes effects in natural language learning. By running comprehensive comparisons, we quantitatively compare the ability of several representative neural models to organize sentences from a bag of words under three typical scenarios, and summarize some empirical findings and challenges, which can shed light on future research on this line of work.","{'sequence': 'Sequential information, a.k.a., orders, is assumed to be essential for processing a sequence with recurrent neural network or convolutional neural network based encoders. However, is it possible to encode natural languages without orders? Given a bag of words from a disordered sentence, humans may still be able to understand what those words mean by reordering or reconstructing them. Inspired by such an intuition, in this paper, we perform a study to investigate how ""order"" information takes effects in natural language learning. By running comprehensive comparisons, we quantitatively compare the ability of several representative neural models to organize sentences from a bag of words under three typical scenarios, and summarize some empirical findings and challenges, which can shed light on future research on this line of work.', 'labels': ['Question Answering', 'NLP Applications', 'Dialogue and Interactive Systems', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Generation', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Summarization', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08826366811990738, 0.07961578667163849, 0.07479251176118851, 0.0706251710653305, 0.06322667747735977, 0.05983211100101471, 0.059822484850883484, 0.05974428728222847, 0.053860362619161606, 0.04633006453514099, 0.04293442144989967, 0.042160604149103165, 0.03340107947587967, 0.031294599175453186, 0.02996308170258999, 0.029386883601546288, 0.02793082594871521, 0.024140652269124985, 0.022575609385967255, 0.019808238372206688, 0.016172757372260094, 0.01485702209174633, 0.009261054918169975]}",0.08826366811990738,Question Answering,0.059822484850883484
Machine Learning for NLP,Mask Attention Networks: Rethinking and Strengthen Transformer,"Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer.","{'sequence': 'Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer.', 'labels': ['Summarization', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Generation', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Question Answering', 'Information Extraction', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.40226510167121887, 0.05306427180767059, 0.05010483041405678, 0.04975892975926399, 0.038790855556726456, 0.03742991387844086, 0.0368371345102787, 0.034725844860076904, 0.033341530710458755, 0.029682597145438194, 0.02887687087059021, 0.025707554072141647, 0.025459690019488335, 0.023048805072903633, 0.022432448342442513, 0.019686706364154816, 0.01853140816092491, 0.01762041635811329, 0.014104572124779224, 0.013801014050841331, 0.012192691676318645, 0.007103667128831148, 0.005433159880340099]}",0.40226510167121887,Summarization,0.025707554072141647
Machine Learning for NLP,ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding,"Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT's Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarsegrained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and finetune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github. com/PaddlePaddle/ERNIE.","{'sequence': ""Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT's Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarsegrained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and finetune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github. com/PaddlePaddle/ERNIE."", 'labels': ['Generation', 'Machine Learning for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Information Extraction', 'Resources and Evaluation', 'Question Answering', 'Speech and Multimodality', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1662677526473999, 0.07864293456077576, 0.05751548707485199, 0.05321133881807327, 0.05102911219000816, 0.04964548721909523, 0.04843737185001373, 0.048381395637989044, 0.04554305598139763, 0.042859699577093124, 0.04253761097788811, 0.03972465917468071, 0.031195996329188347, 0.02882438898086548, 0.0281946063041687, 0.027683183550834656, 0.02751953713595867, 0.02751893736422062, 0.02478673867881298, 0.024342326447367668, 0.022303124889731407, 0.02124832011759281, 0.012586887925863266]}",0.1662677526473999,Generation,0.07864293456077576
Machine Learning for NLP,Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models,"Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese -Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multigranularity representations. 1","{'sequence': 'Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese -Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multigranularity representations. 1', 'labels': ['Information Extraction', 'Machine Learning for NLP', 'Speech and Multimodality', 'NLP Applications', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Generation', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10728403925895691, 0.08554413914680481, 0.07508517801761627, 0.06790141761302948, 0.06336091458797455, 0.0620020292699337, 0.05171111598610878, 0.04740682989358902, 0.04687786102294922, 0.04635066166520119, 0.04503205418586731, 0.04204510524868965, 0.039589062333106995, 0.03878692537546158, 0.02860238403081894, 0.02835889160633087, 0.024917125701904297, 0.02162190154194832, 0.020579850301146507, 0.01915920525789261, 0.018123658373951912, 0.012109557166695595, 0.007550096604973078]}",0.10728403925895691,Information Extraction,0.08554413914680481
Semantics: Lexical Semantics,Modeling Event Plausibility with Consistent Conceptual Abstraction,"Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models-most recently pre-trained, Transformer language modelshave demonstrated improvements in modeling event plausibility, their performance still falls short of humans'. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that ""a person breathing"" is plausible while ""a dentist breathing"" is not, for example. We find this inconsistency persists even when models are softly injected with lexical knowledge, and we present a simple post-hoc method of forcing model consistency that improves correlation with human plausibility judgements.","{'sequence': 'Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models-most recently pre-trained, Transformer language modelshave demonstrated improvements in modeling event plausibility, their performance still falls short of humans\'. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that ""a person breathing"" is plausible while ""a dentist breathing"" is not, for example. We find this inconsistency persists even when models are softly injected with lexical knowledge, and we present a simple post-hoc method of forcing model consistency that improves correlation with human plausibility judgements.', 'labels': ['Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Question Answering', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Generation', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09440896660089493, 0.07812463492155075, 0.07028882205486298, 0.05983815714716911, 0.05292282998561859, 0.05237868055701256, 0.049300603568553925, 0.04833235964179039, 0.04771998152136803, 0.04540162533521652, 0.043728094547986984, 0.04157152399420738, 0.041244301944971085, 0.03695907071232796, 0.034665804356336594, 0.03430433198809624, 0.03262307867407799, 0.029944609850645065, 0.028721069917082787, 0.028471555560827255, 0.02414623647928238, 0.015470834448933601, 0.009432868100702763]}",0.09440896660089493,Dialogue and Interactive Systems,0.04540162533521652
Semantics: Lexical Semantics,UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus,"Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration structured expert domain knowledge from a knowledge base. We introduce UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus is performed in two ways: (i) connecting words that have the same underlying 'concept' in UMLS and (ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, Umls-BERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common namedentity recognition (NER) and clinical natural language inference tasks.","{'sequence': ""Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration structured expert domain knowledge from a knowledge base. We introduce UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus is performed in two ways: (i) connecting words that have the same underlying 'concept' in UMLS and (ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, Umls-BERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common namedentity recognition (NER) and clinical natural language inference tasks."", 'labels': ['Question Answering', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'NLP Applications', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Extraction', 'Generation', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.06803522258996964, 0.05886685475707054, 0.05806810036301613, 0.05558255314826965, 0.05542070046067238, 0.05523913726210594, 0.054693542420864105, 0.05256030336022377, 0.04936841130256653, 0.04802302271127701, 0.04563277214765549, 0.045339856296777725, 0.04266183078289032, 0.041552040725946426, 0.03978067636489868, 0.03826627507805824, 0.03743348643183708, 0.03433561697602272, 0.0316225104033947, 0.02788429707288742, 0.023239297792315483, 0.019507797434926033, 0.016885653138160706]}",0.06803522258996964,Question Answering,0.05256030336022377
Semantics: Lexical Semantics,MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories,"Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely metaphor-aware late interaction over BERT (MelBERT). Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is metaphorical. Our empirical results demonstrate that MelBERT outperforms several strong baselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.","{'sequence': 'Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely metaphor-aware late interaction over BERT (MelBERT). Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is metaphorical. Our empirical results demonstrate that MelBERT outperforms several strong baselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Extraction', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Question Answering', 'NLP Applications', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Machine Learning for NLP', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Generation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09281997382640839, 0.0833638608455658, 0.07302199304103851, 0.06794269382953644, 0.0513739213347435, 0.05094460770487785, 0.04955512285232544, 0.04505409300327301, 0.04370337724685669, 0.04266408085823059, 0.0403006337583065, 0.03789998218417168, 0.035488858819007874, 0.03531448915600777, 0.03278621286153793, 0.032163430005311966, 0.03131876140832901, 0.030398409813642502, 0.028938207775354385, 0.027641979977488518, 0.025456899777054787, 0.024239182472229004, 0.017609186470508575]}",0.09281997382640839,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.0833638608455658
Semantics: Lexical Semantics,Non-Parametric Few-Shot Learning for Word Sense Disambiguation,"Word sense disambiguation (WSD) is a longstanding problem in natural language processing. One significant challenge in supervised all-words WSD is to classify among senses for a majority of words that lie in the longtail distribution. For instance, 84% of the annotated words have less than 10 examples in the SemCor training data. This issue is more pronounced as the imbalance occurs in both word and sense distributions. In this work, we propose MetricWSD, a non-parametric few-shot learning approach to mitigate this data imbalance issue. By learning to compute distances among the senses of a given word through episodic training, MetricWSD transfers knowledge (a learned metric space) from high-frequency words to infrequent ones. MetricWSD constructs the training episodes tailored to word frequencies and explicitly addresses the problem of the skewed distribution, as opposed to mixing all the words trained with parametric models in previous work. Without resorting to any lexical resources, MetricWSD obtains strong performance against parametric alternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark (Raganato et al., 2017b) . Our analysis further validates that infrequent words and senses enjoy significant improvement. 1","{'sequence': 'Word sense disambiguation (WSD) is a longstanding problem in natural language processing. One significant challenge in supervised all-words WSD is to classify among senses for a majority of words that lie in the longtail distribution. For instance, 84% of the annotated words have less than 10 examples in the SemCor training data. This issue is more pronounced as the imbalance occurs in both word and sense distributions. In this work, we propose MetricWSD, a non-parametric few-shot learning approach to mitigate this data imbalance issue. By learning to compute distances among the senses of a given word through episodic training, MetricWSD transfers knowledge (a learned metric space) from high-frequency words to infrequent ones. MetricWSD constructs the training episodes tailored to word frequencies and explicitly addresses the problem of the skewed distribution, as opposed to mixing all the words trained with parametric models in previous work. Without resorting to any lexical resources, MetricWSD obtains strong performance against parametric alternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark (Raganato et al., 2017b) . Our analysis further validates that infrequent words and senses enjoy significant improvement. 1', 'labels': ['Resources and Evaluation', 'Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Generation', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Information Extraction', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07684195786714554, 0.0588243193924427, 0.05652172863483429, 0.05562274903059006, 0.05516358092427254, 0.054256342351436615, 0.05004410818219185, 0.048593420535326004, 0.046587247401475906, 0.04316218942403793, 0.04220176860690117, 0.04193674027919769, 0.04095513001084328, 0.04020169749855995, 0.03993856906890869, 0.0360029973089695, 0.0350651815533638, 0.03427377715706825, 0.03325502201914787, 0.0309553574770689, 0.030418217182159424, 0.02826167456805706, 0.02091626264154911]}",0.07684195786714554,Resources and Evaluation,0.054256342351436615
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Why Do Document-Level Polarity Classifiers Fail?,"Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the learning process. This work helps to fill this gap by proposing a methodology to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories: neutrality, where the text does not convey a clear polarity, and discrepancy, where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances, as they are much harder to classify, for both machine and human classifiers. To the best of our knowledge, this is the first systematic analysis of the impact of hard instances in polarity detection from wellformed textual reviews.","{'sequence': 'Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the learning process. This work helps to fill this gap by proposing a methodology to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories: neutrality, where the text does not convey a clear polarity, and discrepancy, where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances, as they are much harder to classify, for both machine and human classifiers. To the best of our knowledge, this is the first systematic analysis of the impact of hard instances in polarity detection from wellformed textual reviews.', 'labels': ['Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Generation', 'Speech and Multimodality', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08085629343986511, 0.07207014411687851, 0.06377114355564117, 0.056463196873664856, 0.05498645454645157, 0.05168864130973816, 0.05080771818757057, 0.04963402450084686, 0.04737820476293564, 0.046666476875543594, 0.044141825288534164, 0.043502237647771835, 0.042398177087306976, 0.04154932498931885, 0.040814317762851715, 0.03728979825973511, 0.03580434247851372, 0.03003944456577301, 0.026528479531407356, 0.025913430377840996, 0.02087407372891903, 0.019665153697133064, 0.01715703494846821]}",0.08085629343986511,Question Answering,0.01715703494846821
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",A Unified Span-Based Approach for Opinion Mining with Syntactic Constituents,"Fine-grained opinion mining (OM) has achieved increasing attraction in the natural language processing (NLP) community, which aims to find the opinion structures of ""Who expressed what opinions towards what"" in one sentence. In this work, motivated by its spanbased representations of opinion expressions and roles, we propose a unified span-based approach for the end-to-end OM setting. Furthermore, inspired by the unified span-based formalism of OM and constituent parsing, we explore two different methods (multi-task learning and graph convolutional neural network) to integrate syntactic constituents into the proposed model to help OM. We conduct experiments on the commonly used MPQA 2.0 dataset. The experimental results show that our proposed unified span-based approach achieves significant improvements over previous works in the exact F1 score and reduces the number of wrongly-predicted opinion expressions and roles, showing the effectiveness of our method. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. * Rui Wang's contributions were carried out while at Alibaba Group.","{'sequence': 'Fine-grained opinion mining (OM) has achieved increasing attraction in the natural language processing (NLP) community, which aims to find the opinion structures of ""Who expressed what opinions towards what"" in one sentence. In this work, motivated by its spanbased representations of opinion expressions and roles, we propose a unified span-based approach for the end-to-end OM setting. Furthermore, inspired by the unified span-based formalism of OM and constituent parsing, we explore two different methods (multi-task learning and graph convolutional neural network) to integrate syntactic constituents into the proposed model to help OM. We conduct experiments on the commonly used MPQA 2.0 dataset. The experimental results show that our proposed unified span-based approach achieves significant improvements over previous works in the exact F1 score and reduces the number of wrongly-predicted opinion expressions and roles, showing the effectiveness of our method. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. * Rui Wang\'s contributions were carried out while at Alibaba Group.', 'labels': ['Machine Learning for NLP', 'Information Extraction', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'NLP Applications', 'Resources and Evaluation', 'Summarization', 'Ethics and NLP', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12683798372745514, 0.05813698098063469, 0.05686619132757187, 0.05532076209783554, 0.05505445599555969, 0.051362618803977966, 0.04967859387397766, 0.045298490673303604, 0.044831011444330215, 0.042933423072099686, 0.03871253505349159, 0.03791316971182823, 0.03757872059941292, 0.036517128348350525, 0.03504616767168045, 0.03502586856484413, 0.034334782510995865, 0.03366206958889961, 0.03098248317837715, 0.030466590076684952, 0.025801043957471848, 0.023147549480199814, 0.014491441659629345]}",0.12683798372745514,Machine Learning for NLP,0.014491441659629345
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Target-specified Sequence Labeling with Multi-head Self-attention for Target-oriented Opinion Words Extraction,"Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis (ABSA). Many recent works on ABSA focus on Targetoriented Opinion Words (or Terms) Extraction (TOWE), which aims at extracting the corresponding opinion words for a given opinion target. TOWE can be further applied to Aspect-Opinion Pair Extraction (AOPE) which aims at extracting aspects (i.e., opinion targets) and opinion terms in pairs. In this paper, we propose Target-Specified sequence labeling with Multi-head Self-Attention (TSMSA) for TOWE, in which any pre-trained language model with multi-head self-attention can be integrated conveniently. As a case study, we also develop a Multi-Task structure named MT-TSMSA for AOPE by combining our TSMSA with an aspect and opinion term extraction module. Experimental results indicate that TSMSA outperforms the benchmark methods on TOWE significantly; meanwhile, the performance of MT-TSMSA is similar or even better than state-of-the-art AOPE baseline models.","{'sequence': 'Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis (ABSA). Many recent works on ABSA focus on Targetoriented Opinion Words (or Terms) Extraction (TOWE), which aims at extracting the corresponding opinion words for a given opinion target. TOWE can be further applied to Aspect-Opinion Pair Extraction (AOPE) which aims at extracting aspects (i.e., opinion targets) and opinion terms in pairs. In this paper, we propose Target-Specified sequence labeling with Multi-head Self-Attention (TSMSA) for TOWE, in which any pre-trained language model with multi-head self-attention can be integrated conveniently. As a case study, we also develop a Multi-Task structure named MT-TSMSA for AOPE by combining our TSMSA with an aspect and opinion term extraction module. Experimental results indicate that TSMSA outperforms the benchmark methods on TOWE significantly; meanwhile, the performance of MT-TSMSA is similar or even better than state-of-the-art AOPE baseline models.', 'labels': ['Speech and Multimodality', 'Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Summarization', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Ethics and NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11306378990411758, 0.08993436396121979, 0.08115346729755402, 0.07289627194404602, 0.05504700541496277, 0.05365984886884689, 0.05040683597326279, 0.04430213198065758, 0.04322341829538345, 0.04308253899216652, 0.03423911705613136, 0.03416096046566963, 0.03337552025914192, 0.0332510732114315, 0.027828024700284004, 0.02744467929005623, 0.026821846142411232, 0.02679164707660675, 0.02587306872010231, 0.023787790909409523, 0.02270933799445629, 0.019986167550086975, 0.016961127519607544]}",0.11306378990411758,Speech and Multimodality,0.016961127519607544
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa,"Aspect-Based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs. In this paper, we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task, showing that the induced tree from finetuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis experiments reveal that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information. 1 * Equal contribution.","{'sequence': 'Aspect-Based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs. In this paper, we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task, showing that the induced tree from finetuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis experiments reveal that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information. 1 * Equal contribution.', 'labels': ['Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Computational Social Science and Social Media', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09418278187513351, 0.0859365314245224, 0.07216212898492813, 0.07164700329303741, 0.06330987066030502, 0.058555781841278076, 0.056859344244003296, 0.04980618879199028, 0.046662747859954834, 0.040856052190065384, 0.03704792261123657, 0.035759180784225464, 0.035539012402296066, 0.03297434747219086, 0.0305501539260149, 0.03026353195309639, 0.028282945975661278, 0.028078678995370865, 0.023006770759820938, 0.021647309884428978, 0.021459732204675674, 0.02042357251048088, 0.014988507144153118]}",0.09418278187513351,Question Answering,0.014988507144153118
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Domain Divergences: A Survey and Empirical Analysis,"Domain divergence plays a significant role in estimating the performance of a model in new domains. While there is a significant literature on divergence measures, researchers find it hard to choose an appropriate divergence for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes -Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications -1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild -and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance -an important aspect of Decisions in the Wild, we perform correlation analysis spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional measures over word distributions still serve as strong baselines, while higher-order measures with CWR are effective.","{'sequence': 'Domain divergence plays a significant role in estimating the performance of a model in new domains. While there is a significant literature on divergence measures, researchers find it hard to choose an appropriate divergence for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes -Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications -1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild -and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance -an important aspect of Decisions in the Wild, we perform correlation analysis spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional measures over word distributions still serve as strong baselines, while higher-order measures with CWR are effective.', 'labels': ['NLP Applications', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Machine Learning for NLP', 'Summarization', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.37815478444099426, 0.04478111490607262, 0.04354027658700943, 0.043187133967876434, 0.042618781328201294, 0.03856959939002991, 0.038528140634298325, 0.03424888476729393, 0.030370013788342476, 0.02641565352678299, 0.026178324595093727, 0.02600385993719101, 0.02522665075957775, 0.02453184314072132, 0.02440449222922325, 0.024278467521071434, 0.023043598979711533, 0.018804896622896194, 0.01872877962887287, 0.0186268649995327, 0.01799776963889599, 0.016744393855333328, 0.015015658922493458]}",0.37815478444099426,NLP Applications,0.015015658922493458
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Target-Aware Data Augmentation for Stance Detection,"The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed methods significantly outperforms previous augmentation methods on 11 targets.","{'sequence': 'The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed methods significantly outperforms previous augmentation methods on 11 targets.', 'labels': ['Generation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'NLP Applications', 'Information Retrieval and Text Mining', 'Information Extraction', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Summarization', 'Resources and Evaluation', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08724983036518097, 0.0810592770576477, 0.08100221306085587, 0.06229648366570473, 0.05098932608962059, 0.04787571355700493, 0.047216787934303284, 0.04666994884610176, 0.04298125207424164, 0.041602153331041336, 0.04157302528619766, 0.040698662400245667, 0.03845716267824173, 0.03556033596396446, 0.034139733761548996, 0.033198095858097076, 0.03259288892149925, 0.032371219247579575, 0.03064587526023388, 0.02641475945711136, 0.024173853918910027, 0.02246742881834507, 0.01876390352845192]}",0.08724983036518097,Generation,0.01876390352845192
Speech and Multimodality,End-to-end ASR to jointly predict transcriptions and linguistic annotations,"We propose a Transformer-based sequence-tosequence model for automatic speech recognition (ASR) capable of simultaneously transcribing and annotating audio with linguistic information such as phonemic transcripts or part-of-speech (POS) tags. Since linguistic information is important in natural language processing (NLP), the proposed ASR is especially useful for speech interface applications, including spoken dialogue systems and speech translation, which combine ASR and NLP. To produce linguistic annotations, we train the ASR system using modified training targets: each grapheme or multi-grapheme unit in the target transcript is followed by an aligned phoneme sequence and/or POS tag. Since our method has access to the underlying audio data, we can estimate linguistic annotations more accurately than pipeline approaches in which NLP-based methods are applied to a hypothesized ASR transcript. Experimental results on Japanese and English datasets show that the proposed ASR system is capable of simultaneously producing highquality transcriptions and linguistic annotations.","{'sequence': 'We propose a Transformer-based sequence-tosequence model for automatic speech recognition (ASR) capable of simultaneously transcribing and annotating audio with linguistic information such as phonemic transcripts or part-of-speech (POS) tags. Since linguistic information is important in natural language processing (NLP), the proposed ASR is especially useful for speech interface applications, including spoken dialogue systems and speech translation, which combine ASR and NLP. To produce linguistic annotations, we train the ASR system using modified training targets: each grapheme or multi-grapheme unit in the target transcript is followed by an aligned phoneme sequence and/or POS tag. Since our method has access to the underlying audio data, we can estimate linguistic annotations more accurately than pipeline approaches in which NLP-based methods are applied to a hypothesized ASR transcript. Experimental results on Japanese and English datasets show that the proposed ASR system is capable of simultaneously producing highquality transcriptions and linguistic annotations.', 'labels': ['NLP Applications', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Generation', 'Resources and Evaluation', 'Summarization', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15959256887435913, 0.09638791531324387, 0.08211851865053177, 0.0533813051879406, 0.049768611788749695, 0.04976493492722511, 0.049119483679533005, 0.04322609677910805, 0.0405363105237484, 0.03923060745000839, 0.03546902909874916, 0.03298371285200119, 0.030300725251436234, 0.02921001799404621, 0.02896254137158394, 0.027677731588482857, 0.026825549080967903, 0.02622882090508938, 0.02590056136250496, 0.022660208866000175, 0.01994469203054905, 0.017118088901042938, 0.013591828756034374]}",0.15959256887435913,NLP Applications,0.08211851865053177
Speech and Multimodality,Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation,"A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequencelevel knowledge distillation (SeqKD) from external text-based NMT models. To leverage the full potential of the source language information, we propose backward SeqKD, SeqKD from a target-to-source backward NMT model. To this end, we train a bilingual E2E-ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder. The paraphrases are generated from the translations in bitext via back-translation. We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined. Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance, and the effectiveness is complementary regardless of the model capacity.","{'sequence': 'A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequencelevel knowledge distillation (SeqKD) from external text-based NMT models. To leverage the full potential of the source language information, we propose backward SeqKD, SeqKD from a target-to-source backward NMT model. To this end, we train a bilingual E2E-ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder. The paraphrases are generated from the translations in bitext via back-translation. We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined. Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance, and the effectiveness is complementary regardless of the model capacity.', 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Information Extraction', 'Question Answering', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Generation', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10149440169334412, 0.07200757414102554, 0.07055783271789551, 0.06648019701242447, 0.06011539325118065, 0.05798342823982239, 0.04410634934902191, 0.043565426021814346, 0.042043495923280716, 0.04024559631943703, 0.040111128240823746, 0.040071241557598114, 0.03908234089612961, 0.036278314888477325, 0.03423779457807541, 0.030107231810688972, 0.029236089438199997, 0.02909851260483265, 0.0279366597533226, 0.025965038686990738, 0.025136370211839676, 0.02508395165205002, 0.01905553787946701]}",0.10149440169334412,Speech and Multimodality,0.10149440169334412
Speech and Multimodality,Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks,"End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance and can also incorporate external models at intermediate stages of the network to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for speech translation that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous state-of-theart by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C. 1","{'sequence': 'End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance and can also incorporate external models at intermediate stages of the network to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for speech translation that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous state-of-theart by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C. 1', 'labels': ['Speech and Multimodality', 'Information Extraction', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Generation', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2270246297121048, 0.08655353635549545, 0.06474436074495316, 0.05521604046225548, 0.05392683297395706, 0.042539071291685104, 0.03886599838733673, 0.035112813115119934, 0.033846281468868256, 0.03298017755150795, 0.032790444791316986, 0.032159898430109024, 0.031521160155534744, 0.030876364558935165, 0.027696333825588226, 0.027585631236433983, 0.025245197117328644, 0.024714810773730278, 0.02356722578406334, 0.020944392308592796, 0.020565932616591454, 0.020258447155356407, 0.011264403350651264]}",0.2270246297121048,Speech and Multimodality,0.2270246297121048
Speech and Multimodality,SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding,"Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models' performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semisupervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous stateof-the-art performance on the Spoken SQuAD dataset by more than 10%. ⇤ Equal contribution. The work was done when Yu-An Chung was interning at Microsoft.","{'sequence': ""Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models' performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semisupervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous stateof-the-art performance on the Spoken SQuAD dataset by more than 10%. ⇤ Equal contribution. The work was done when Yu-An Chung was interning at Microsoft."", 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Question Answering', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12037280201911926, 0.06452891230583191, 0.05658050253987312, 0.05381599813699722, 0.0532737672328949, 0.05124429985880852, 0.05099618807435036, 0.04486765339970589, 0.04473413899540901, 0.04100026190280914, 0.04051750525832176, 0.039509568363428116, 0.03842642903327942, 0.03834596648812294, 0.03785550221800804, 0.035226743668317795, 0.03210679069161415, 0.030725406482815742, 0.028828155249357224, 0.027293870225548744, 0.02616991102695465, 0.02525986172258854, 0.01831989549100399]}",0.12037280201911926,Speech and Multimodality,0.12037280201911926
Speech and Multimodality,Worldly Wise (WoW) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering,"Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW). WoW is shown to perform endto-end cross-lingual FVSQA at same levels of accuracy across 3 languages -English, Hindi, and Turkish.","{'sequence': 'Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW). WoW is shown to perform endto-end cross-lingual FVSQA at same levels of accuracy across 3 languages -English, Hindi, and Turkish.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Generation', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.6327881813049316, 0.05932260677218437, 0.03251747041940689, 0.022340599447488785, 0.021684931591153145, 0.020260367542505264, 0.01731039211153984, 0.01703590154647827, 0.01670490950345993, 0.014850001782178879, 0.014333226718008518, 0.013925665058195591, 0.013212023302912712, 0.013094617053866386, 0.013054145500063896, 0.01284798514097929, 0.012646677903831005, 0.010845142416656017, 0.010065832175314426, 0.009768328629434109, 0.008647691458463669, 0.00719378562644124, 0.00554955517873168]}",0.6327881813049316,Question Answering,0.05932260677218437
Speech and Multimodality,Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment,"Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose iterative realignment, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14× decoding speedup; on LibriSpeech, we reach an LM-free testother WER of 9.0% (19% relative improvement on comparable work) in three iterations. We release our code at https://github.com/ amazon-research/align-refine.","{'sequence': 'Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose iterative realignment, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14× decoding speedup; on LibriSpeech, we reach an LM-free testother WER of 9.0% (19% relative improvement on comparable work) in three iterations. We release our code at https://github.com/ amazon-research/align-refine.', 'labels': ['Generation', 'Question Answering', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Information Extraction', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.20550526678562164, 0.06217849254608154, 0.056208379566669464, 0.05281839519739151, 0.04817619547247887, 0.04684612527489662, 0.045958761125802994, 0.03976285830140114, 0.0387672558426857, 0.038319338113069534, 0.03576109930872917, 0.03567086160182953, 0.03566494956612587, 0.03273888677358627, 0.03252618387341499, 0.030875762924551964, 0.029401026666164398, 0.02780241332948208, 0.026948295533657074, 0.024660252034664154, 0.01935945264995098, 0.017123380675911903, 0.01692645624279976]}",0.20550526678562164,Generation,0.056208379566669464
NLP Applications,Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis,"Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability. Code and data are available at https://github.com/xxxiaol/GCI/.","{'sequence': 'Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability. Code and data are available at https://github.com/xxxiaol/GCI/.', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Question Answering', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.09731961786746979, 0.0846208706498146, 0.08278875797986984, 0.07948943972587585, 0.06387913972139359, 0.05911681801080704, 0.057607654482126236, 0.05140431225299835, 0.04148176684975624, 0.04118217155337334, 0.03853856027126312, 0.0346340648829937, 0.033308420330286026, 0.0332186184823513, 0.02592657133936882, 0.025886813178658485, 0.025569288060069084, 0.025485802441835403, 0.022594382986426353, 0.021586019545793533, 0.019541841000318527, 0.01854556053876877, 0.016273576766252518]}",0.09731961786746979,Dialogue and Interactive Systems,0.03853856027126312
NLP Applications,Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network,"Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from the irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of the EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMRs of the lymphedema demonstrate that our method can diagnose four types of EMRs correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field 1 .","{'sequence': 'Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from the irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of the EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMRs of the lymphedema demonstrate that our method can diagnose four types of EMRs correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field 1 .', 'labels': ['Information Extraction', 'Question Answering', 'Dialogue and Interactive Systems', 'Summarization', 'Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'NLP Applications', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10635314136743546, 0.07745953649282455, 0.07518599927425385, 0.07338517904281616, 0.06876657158136368, 0.057312510907649994, 0.05597474426031113, 0.053662702441215515, 0.05096819996833801, 0.041942156851291656, 0.03824041038751602, 0.03774545341730118, 0.037501368671655655, 0.03542088344693184, 0.03217190131545067, 0.02944006957113743, 0.028121592476963997, 0.020298941060900688, 0.018269747495651245, 0.017926592379808426, 0.016493787989020348, 0.015770860016345978, 0.011587613262236118]}",0.10635314136743546,Information Extraction,0.02944006957113743
NLP Applications,Personalized Response Generation via Generative Split Memory Network,"Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular personality traits to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain single-turn dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines. The dataset and code are available here. * The work is mainly done when YW was a visiting student at Georgia Institute of Technology. Question: Where do you live and what is something you are doing today? Responses: A: I live in Mongolia and I will be making some good sandwiches today. B: Midwest America, I will be skyping my brothers and going to band practice today. Question: What's your ""go to"" when you're sad? Responses: A: I listen to horror stories for some reason. B: I love to read or listen to sad music.","{'sequence': 'Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular personality traits to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain single-turn dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines. The dataset and code are available here. * The work is mainly done when YW was a visiting student at Georgia Institute of Technology. Question: Where do you live and what is something you are doing today? Responses: A: I live in Mongolia and I will be making some good sandwiches today. B: Midwest America, I will be skyping my brothers and going to band practice today. Question: What\'s your ""go to"" when you\'re sad? Responses: A: I listen to horror stories for some reason. B: I love to read or listen to sad music.', 'labels': ['Question Answering', 'Information Extraction', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'NLP Applications', 'Summarization', 'Resources and Evaluation', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13286937773227692, 0.07285629957914352, 0.0703641027212143, 0.06599172204732895, 0.06388045847415924, 0.05232870578765869, 0.04728103429079056, 0.0440569669008255, 0.04346008226275444, 0.04272616654634476, 0.038572896271944046, 0.03526727855205536, 0.03309027850627899, 0.03274703770875931, 0.03273366391658783, 0.030166540294885635, 0.03013145551085472, 0.029905160889029503, 0.024534324184060097, 0.024487385526299477, 0.019593890756368637, 0.01740381121635437, 0.01555144228041172]}",0.13286937773227692,Question Answering,0.0440569669008255
NLP Applications,Towards Few-shot Fact-Checking via Perplexity,"Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in fewshot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in fewshot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19. * * Equal contribution.","{'sequence': ""Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in fewshot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in fewshot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19. * * Equal contribution."", 'labels': ['Computational Social Science and Social Media', 'Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Extraction', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Machine Learning for NLP', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality'], 'scores': [0.06931716203689575, 0.0692736804485321, 0.06913208961486816, 0.06820318847894669, 0.052009981125593185, 0.05106420814990997, 0.050707172602415085, 0.048691295087337494, 0.04736442491412163, 0.04546584561467171, 0.04171214625239372, 0.04119522497057915, 0.04100578650832176, 0.040611643344163895, 0.036613866686820984, 0.036347270011901855, 0.03620925918221474, 0.03376190736889839, 0.030645985156297684, 0.02815895900130272, 0.02283121459186077, 0.020309867337346077, 0.01936788484454155]}",0.06931716203689575,Computational Social Science and Social Media,0.04546584561467171
NLP Applications,Active$^2$ Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation,"While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active 2 Learning (A 2 L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A 2 L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by ≈ 3 − 25% on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.","{'sequence': 'While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active 2 Learning (A 2 L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A 2 L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by ≈ 3 − 25% on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.', 'labels': ['Machine Learning for NLP', 'Question Answering', 'Dialogue and Interactive Systems', 'NLP Applications', 'Resources and Evaluation', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13866782188415527, 0.09365424513816833, 0.0808689296245575, 0.07073696702718735, 0.06512986868619919, 0.0631118044257164, 0.05345623567700386, 0.048709701746702194, 0.04313196241855621, 0.03947538509964943, 0.03945380821824074, 0.0392671637237072, 0.030838703736662865, 0.02755362167954445, 0.027495523914694786, 0.02487519010901451, 0.02475573867559433, 0.019269412383437157, 0.019146382808685303, 0.01400692667812109, 0.01306301448494196, 0.012648411095142365, 0.010683217085897923]}",0.13866782188415527,Machine Learning for NLP,0.07073696702718735
NLP Applications,Generating An Optimal Interview Question Plan Using A Knowledge Graph And Integer Linear Programming,"Given the diversity of the candidates and complexity of job requirements, and since interviewing is an inherently subjective process, it is an important task to ensure consistent, uniform, efficient and objective interviews that result in high quality recruitment. We propose an interview assistant system to automatically, and in an objective manner, select an optimal set of technical questions (from question banks) personalized for a candidate. This set can help a human interviewer to plan for an upcoming interview of that candidate. We formalize the problem of selecting a set of questions as an integer linear programming problem and use standard solvers to get a solution. We use knowledge graph as background knowledge in this formulation, and derive our objective functions and constraints from it. We use candidate's resume to personalize the selection of questions. We propose an intrinsic evaluation to compare a set of suggested questions with actually asked questions. We also use expert interviewers to comparatively evaluate our approach with a set of reasonable baselines.","{'sequence': ""Given the diversity of the candidates and complexity of job requirements, and since interviewing is an inherently subjective process, it is an important task to ensure consistent, uniform, efficient and objective interviews that result in high quality recruitment. We propose an interview assistant system to automatically, and in an objective manner, select an optimal set of technical questions (from question banks) personalized for a candidate. This set can help a human interviewer to plan for an upcoming interview of that candidate. We formalize the problem of selecting a set of questions as an integer linear programming problem and use standard solvers to get a solution. We use knowledge graph as background knowledge in this formulation, and derive our objective functions and constraints from it. We use candidate's resume to personalize the selection of questions. We propose an intrinsic evaluation to compare a set of suggested questions with actually asked questions. We also use expert interviewers to comparatively evaluate our approach with a set of reasonable baselines."", 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Summarization', 'Generation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP'], 'scores': [0.14406085014343262, 0.08235440403223038, 0.06907237321138382, 0.06430210918188095, 0.05997121334075928, 0.05191302299499512, 0.046100787818431854, 0.04570760577917099, 0.04124109447002411, 0.04054604098200798, 0.03967627137899399, 0.034919630736112595, 0.03481389954686165, 0.03402445837855339, 0.03159032389521599, 0.026545142754912376, 0.025552472099661827, 0.024009747430682182, 0.02366083674132824, 0.022438839077949524, 0.019626913592219353, 0.01925150491297245, 0.0186205692589283]}",0.14406085014343262,Resources and Evaluation,0.03481389954686165
Machine Learning for NLP,"Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!","Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.","{'sequence': 'Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.', 'labels': ['Generation', 'NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Question Answering', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10463245958089828, 0.08929843455553055, 0.07563796639442444, 0.06430260092020035, 0.0642542690038681, 0.061072610318660736, 0.060671065002679825, 0.04852182790637016, 0.04808463528752327, 0.03974383696913719, 0.036694664508104324, 0.03610550984740257, 0.03294184431433678, 0.03224048390984535, 0.02934635803103447, 0.02782335877418518, 0.024095386266708374, 0.02405412308871746, 0.02283608168363571, 0.022332487627863884, 0.021586978808045387, 0.018696550279855728, 0.015026379376649857]}",0.10463245958089828,Generation,0.07563796639442444
Machine Learning for NLP,A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models,"Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and further engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance 1 .","{'sequence': 'Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and further engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance 1 .', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Generation', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'NLP Applications', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11052889376878738, 0.07393353432416916, 0.0717504471540451, 0.06229709833860397, 0.05721151828765869, 0.05211784318089485, 0.050153106451034546, 0.047866061329841614, 0.04321274161338806, 0.040853895246982574, 0.03818776458501816, 0.03708481043577194, 0.036288294941186905, 0.035939231514930725, 0.035891540348529816, 0.034870438277721405, 0.03301122412085533, 0.030847277492284775, 0.027884740382432938, 0.023707900196313858, 0.021499058231711388, 0.01923033967614174, 0.015632225200533867]}",0.11052889376878738,Information Extraction,0.040853895246982574
Machine Learning for NLP,Masked Conditional Random Fields for Sequence Labeling,"Conditional Random Field (CRF) based neural models are among the most performant methods for solving sequence labeling problems. Despite its great success, CRF has the shortcoming of occasionally generating illegal sequences of tags, e.g. sequences containing an ""I-"" tag immediately after an ""O"" tag, which is forbidden by the underlying BIO tagging scheme. In this work, we propose Masked Conditional Random Field (MCRF), an easy to implement variant of CRF that impose restrictions on candidate paths during both training and decoding phases. We show that the proposed method thoroughly resolves this issue and brings consistent improvement over existing CRF-based models with near zero additional cost.","{'sequence': 'Conditional Random Field (CRF) based neural models are among the most performant methods for solving sequence labeling problems. Despite its great success, CRF has the shortcoming of occasionally generating illegal sequences of tags, e.g. sequences containing an ""I-"" tag immediately after an ""O"" tag, which is forbidden by the underlying BIO tagging scheme. In this work, we propose Masked Conditional Random Field (MCRF), an easy to implement variant of CRF that impose restrictions on candidate paths during both training and decoding phases. We show that the proposed method thoroughly resolves this issue and brings consistent improvement over existing CRF-based models with near zero additional cost.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Summarization', 'Information Retrieval and Text Mining', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10789242386817932, 0.08517573773860931, 0.06710334867238998, 0.06635607779026031, 0.06586729735136032, 0.0589197538793087, 0.05312933400273323, 0.05042913556098938, 0.04871164262294769, 0.04723692685365677, 0.04294171556830406, 0.038928788155317307, 0.03859846293926239, 0.032796360552310944, 0.028844479471445084, 0.027588993310928345, 0.02753489650785923, 0.025902120396494865, 0.023751776665449142, 0.01868150196969509, 0.015297905541956425, 0.014618747867643833, 0.013692555017769337]}",0.10789242386817932,Dialogue and Interactive Systems,0.038928788155317307
Machine Learning for NLP,Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data,"Prerequisite relations among concepts are crucial for educational applications, such as curriculum planning and intelligent tutoring. In this paper, we propose a novel concept prerequisite relation learning approach, named CPRL, which combines both concept representation learned from a heterogeneous graph and concept pairwise features. Furthermore, we extend CPRL under weakly supervised settings to make our method more practical, including learning prerequisite relations from learning object dependencies and generating training data with data programming. Our experiments on four datasets show that the proposed approach achieves the state-of-the-art results comparing with existing methods. #W and p(i) = #W (i) #W , where #W (i, j) is the number of sliding windows that contain both c i and c j , #W (i) is the number of sliding windows that only contain c i , and #W is the 2038","{'sequence': 'Prerequisite relations among concepts are crucial for educational applications, such as curriculum planning and intelligent tutoring. In this paper, we propose a novel concept prerequisite relation learning approach, named CPRL, which combines both concept representation learned from a heterogeneous graph and concept pairwise features. Furthermore, we extend CPRL under weakly supervised settings to make our method more practical, including learning prerequisite relations from learning object dependencies and generating training data with data programming. Our experiments on four datasets show that the proposed approach achieves the state-of-the-art results comparing with existing methods. #W and p(i) = #W (i) #W , where #W (i, j) is the number of sliding windows that contain both c i and c j , #W (i) is the number of sliding windows that only contain c i , and #W is the 2038', 'labels': ['Resources and Evaluation', 'Generation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Summarization', 'Information Extraction', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.081385537981987, 0.07321416586637497, 0.06670883297920227, 0.0650402158498764, 0.062310487031936646, 0.054643262177705765, 0.05419636517763138, 0.05183522403240204, 0.04628131538629532, 0.044572677463293076, 0.04335663840174675, 0.04216722771525383, 0.035848040133714676, 0.03546968474984169, 0.034483060240745544, 0.034225620329380035, 0.029938509687781334, 0.029841875657439232, 0.029148247092962265, 0.028592310845851898, 0.022988487035036087, 0.020542308688163757, 0.013209933415055275]}",0.081385537981987,Resources and Evaluation,0.029938509687781334
Machine Learning for NLP,Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models,"Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/ lancopku/Embedding-Poisoning.","{'sequence': 'Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/ lancopku/Embedding-Poisoning.', 'labels': ['Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Question Answering', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09848412871360779, 0.08742652088403702, 0.07687483727931976, 0.06662282347679138, 0.059703923761844635, 0.05592673644423485, 0.04900555685162544, 0.04876546189188957, 0.048037804663181305, 0.04494183138012886, 0.043775979429483414, 0.04224773868918419, 0.0365709625184536, 0.034531790763139725, 0.03424591198563576, 0.028769539669156075, 0.028445474803447723, 0.02374834567308426, 0.022875655442476273, 0.018402861431241035, 0.01706518605351448, 0.016963839530944824, 0.01656712032854557]}",0.09848412871360779,Machine Learning for NLP,0.09848412871360779
Machine Learning for NLP,DA-Transformer: Distance-aware Transformer,"Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distanceaware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long-or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting selfattention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the rescaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.","{'sequence': 'Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distanceaware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long-or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting selfattention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the rescaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Information Extraction', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Generation', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Question Answering', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.19776301085948944, 0.17631900310516357, 0.07413987815380096, 0.0702408030629158, 0.06366512179374695, 0.058322224766016006, 0.03745867684483528, 0.03436289355158806, 0.03131277859210968, 0.028136618435382843, 0.027074189856648445, 0.0268947072327137, 0.025375114753842354, 0.02279394492506981, 0.022213004529476166, 0.01869473047554493, 0.01834910362958908, 0.014135154895484447, 0.012346327304840088, 0.012177548371255398, 0.01057359017431736, 0.010442773811519146, 0.0072089508175849915]}",0.19776301085948944,NLP Applications,0.17631900310516357
Resources and Evaluation,ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction,"Sentiment analysis has attracted increasing attention in e-commerce. The sentiment polarities underlying user reviews are of great value for business intelligence. Aspect category sentiment analysis (ACSA) and review rating prediction (RP) are two essential tasks to detect the fine-to-coarse sentiment polarities. ACSA and RP are highly correlated and usually employed jointly in real-world e-commerce scenarios. While most public datasets are constructed for ACSA and RP separately, which may limit the further exploitations of both tasks. To address the problem and advance related researches, we present a large-scale Chinese restaurant review dataset ASAP including 46, 730 genuine reviews from a leading online-to-offline (O2O) e-commerce platform in China. Besides a 5-star scale rating, each review is manually annotated according to its sentiment polarities towards 18 pre-defined aspect categories. We hope the release of the dataset could shed some light on the field of sentiment analysis. Moreover, we propose an intuitive yet effective joint model for ACSA and RP. Experimental results demonstrate that the joint model outperforms state-of-the-art baselines on both tasks.","{'sequence': 'Sentiment analysis has attracted increasing attention in e-commerce. The sentiment polarities underlying user reviews are of great value for business intelligence. Aspect category sentiment analysis (ACSA) and review rating prediction (RP) are two essential tasks to detect the fine-to-coarse sentiment polarities. ACSA and RP are highly correlated and usually employed jointly in real-world e-commerce scenarios. While most public datasets are constructed for ACSA and RP separately, which may limit the further exploitations of both tasks. To address the problem and advance related researches, we present a large-scale Chinese restaurant review dataset ASAP including 46, 730 genuine reviews from a leading online-to-offline (O2O) e-commerce platform in China. Besides a 5-star scale rating, each review is manually annotated according to its sentiment polarities towards 18 pre-defined aspect categories. We hope the release of the dataset could shed some light on the field of sentiment analysis. Moreover, we propose an intuitive yet effective joint model for ACSA and RP. Experimental results demonstrate that the joint model outperforms state-of-the-art baselines on both tasks.', 'labels': ['Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Question Answering', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Information Retrieval and Text Mining', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08169979602098465, 0.08138059824705124, 0.07717611640691757, 0.07031060755252838, 0.06779845058917999, 0.053881097584962845, 0.05334324389696121, 0.05237467214465141, 0.05175339803099632, 0.04991834983229637, 0.037906959652900696, 0.036482490599155426, 0.03351321071386337, 0.03130805119872093, 0.029683176428079605, 0.029009627178311348, 0.02725454606115818, 0.025875361636281013, 0.025283826515078545, 0.02309630811214447, 0.021576160565018654, 0.020632034167647362, 0.01874188333749771]}",0.08169979602098465,Dialogue and Interactive Systems,0.05237467214465141
Resources and Evaluation,Are NLP Models really able to Solve Simple Math Word Problems?,"The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered ""solved"" with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-ofwords can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.","{'sequence': 'The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered ""solved"" with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-ofwords can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Question Answering', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10069719702005386, 0.10026663541793823, 0.08463554084300995, 0.06625791639089584, 0.058029480278491974, 0.05290190503001213, 0.051029421389102936, 0.04504477232694626, 0.04369201511144638, 0.04230860620737076, 0.0399961955845356, 0.03781767934560776, 0.034708645194768906, 0.032153308391571045, 0.03138897940516472, 0.02713909186422825, 0.02704421617090702, 0.02586635760962963, 0.02337043546140194, 0.020579971373081207, 0.020103467628359795, 0.01904279552400112, 0.015925265848636627]}",0.10069719702005386,Interpretability and Analysis of Models for NLP,0.058029480278491974
Resources and Evaluation,WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations,"We annotate 17,000 SNS posts with both the writer's subjective emotional intensity and the reader's objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer's subjective labels than the readers'. The large gap between the subjective and objective emotions implies the complexity of the mapping from a post to the subjective emotional intensities, which also leads to a lower performance with machine learning models. 12 Each writer provided 500 posts for the training set and 100 posts for the validation and test sets. 13 https://taku910.github.io/mecab/","{'sequence': ""We annotate 17,000 SNS posts with both the writer's subjective emotional intensity and the reader's objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer's subjective labels than the readers'. The large gap between the subjective and objective emotions implies the complexity of the mapping from a post to the subjective emotional intensities, which also leads to a lower performance with machine learning models. 12 Each writer provided 500 posts for the training set and 100 posts for the validation and test sets. 13 https://taku910.github.io/mecab/"", 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Summarization', 'Question Answering', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0775650143623352, 0.07394813001155853, 0.07375361770391464, 0.06989625841379166, 0.06241302564740181, 0.05206098034977913, 0.049117594957351685, 0.04690588638186455, 0.04641104117035866, 0.045261550694704056, 0.04351877048611641, 0.04156411066651344, 0.03820548951625824, 0.037848737090826035, 0.036955028772354126, 0.034257885068655014, 0.033121321350336075, 0.027441726997494698, 0.024912983179092407, 0.02464321069419384, 0.022854456678032875, 0.021589266136288643, 0.015753792598843575]}",0.0775650143623352,Dialogue and Interactive Systems,0.07375361770391464
Resources and Evaluation,KPQA: A Metric for Generative Question Answering Using Keyphrase Weights,"In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQAmetric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics. Code for KPQA-metric will be available at https://github.com/ hwanheelee1993/KPQA.","{'sequence': 'In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQAmetric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics. Code for KPQA-metric will be available at https://github.com/ hwanheelee1993/KPQA.', 'labels': ['Question Answering', 'Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Extraction', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'NLP Applications', 'Summarization', 'Machine Learning for NLP', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1492757648229599, 0.10239633917808533, 0.08367487043142319, 0.06165976822376251, 0.05084116756916046, 0.04603928327560425, 0.04351691156625748, 0.04067786782979965, 0.04060696065425873, 0.038234688341617584, 0.03469778224825859, 0.03357084468007088, 0.031298261135816574, 0.030932312831282616, 0.02959745191037655, 0.02932579815387726, 0.028901079669594765, 0.025784792378544807, 0.025539690628647804, 0.023899098858237267, 0.01999564841389656, 0.017121002078056335, 0.012412559241056442]}",0.1492757648229599,Question Answering,0.08367487043142319
Resources and Evaluation,StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer,"Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, STYLEPTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on STYLEPTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, STYLEPTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.","{'sequence': 'Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, STYLEPTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on STYLEPTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, STYLEPTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.', 'labels': ['Generation', 'Speech and Multimodality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Information Extraction', 'Discourse and Pragmatics', 'NLP Applications', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10061842203140259, 0.07323808968067169, 0.06024390086531639, 0.05698869377374649, 0.05570199713110924, 0.04828527197241783, 0.04755214974284172, 0.045577723532915115, 0.04458035156130791, 0.041811853647232056, 0.039470963180065155, 0.03763944283127785, 0.03709610924124718, 0.03630448877811432, 0.03551855683326721, 0.0345676988363266, 0.03448326140642166, 0.03363204374909401, 0.031289394944906235, 0.030894551426172256, 0.026678776368498802, 0.023914316669106483, 0.023911908268928528]}",0.10061842203140259,Generation,0.04755214974284172
Resources and Evaluation,Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge,"Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest that such a task requires deep language understanding, common sense, and world knowledge and thus can be a good testbed for pretrained language models and help models perform better on other tasks. 1","{'sequence': 'Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest that such a task requires deep language understanding, common sense, and world knowledge and thus can be a good testbed for pretrained language models and help models perform better on other tasks. 1', 'labels': ['Resources and Evaluation', 'Computational Social Science and Social Media', 'Generation', 'NLP Applications', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13988572359085083, 0.09274034202098846, 0.08254846185445786, 0.07735216617584229, 0.06893548369407654, 0.06849925220012665, 0.06399910151958466, 0.0460972897708416, 0.035271309316158295, 0.03130374476313591, 0.02885214239358902, 0.027719931676983833, 0.02745603211224079, 0.026168501004576683, 0.02453271858394146, 0.023338988423347473, 0.022501355037093163, 0.02167660742998123, 0.021030742675065994, 0.019116822630167007, 0.018709776923060417, 0.016831528395414352, 0.015432018786668777]}",0.13988572359085083,Resources and Evaluation,0.13988572359085083
Resources and Evaluation,COVID-19 Named Entity Recognition for Vietnamese,"The current COVID-19 pandemic has lead to the creation of many corpora that facilitate NLP research and downstream applications to help fight the pandemic. However, most of these corpora are exclusively for English. As the pandemic is a global problem, it is worth creating COVID-19 related datasets for languages other than English. In this paper, we present the first manuallyannotated COVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is annotated for the named entity recognition (NER) task with newly-defined entity types that can be used in other future epidemics. Our dataset also contains the largest number of entities compared to existing Vietnamese NER datasets. We empirically conduct experiments using strong baselines on our dataset, and find that: automatic Vietnamese word segmentation helps improve the NER results and the highest performances are obtained by finetuning pre-trained language models where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) produces higher results than the multilingual model XLM-R (Conneau et al., 2020) . We publicly release our dataset at: https://github.com/ VinAIResearch/PhoNER_COVID19.","{'sequence': 'The current COVID-19 pandemic has lead to the creation of many corpora that facilitate NLP research and downstream applications to help fight the pandemic. However, most of these corpora are exclusively for English. As the pandemic is a global problem, it is worth creating COVID-19 related datasets for languages other than English. In this paper, we present the first manuallyannotated COVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is annotated for the named entity recognition (NER) task with newly-defined entity types that can be used in other future epidemics. Our dataset also contains the largest number of entities compared to existing Vietnamese NER datasets. We empirically conduct experiments using strong baselines on our dataset, and find that: automatic Vietnamese word segmentation helps improve the NER results and the highest performances are obtained by finetuning pre-trained language models where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) produces higher results than the multilingual model XLM-R (Conneau et al., 2020) . We publicly release our dataset at: https://github.com/ VinAIResearch/PhoNER_COVID19.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Question Answering', 'Discourse and Pragmatics', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Summarization', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15136587619781494, 0.06974581629037857, 0.0636594146490097, 0.05671693757176399, 0.050249602645635605, 0.05007051303982735, 0.0483076274394989, 0.04798155650496483, 0.04571470990777016, 0.04128154367208481, 0.03995652124285698, 0.03941461443901062, 0.035672545433044434, 0.03260224685072899, 0.03194422274827957, 0.030436480417847633, 0.029873445630073547, 0.028939057141542435, 0.025015633553266525, 0.021954042837023735, 0.02160932682454586, 0.02016940526664257, 0.01731881871819496]}",0.15136587619781494,NLP Applications,0.0636594146490097
Computational Social Science and Social Media,Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames,"Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local information about the events and related actors in a news article through an auto-encoding framework, and to leverage this signal for document-level frame classification. Our experiments show that: our model outperforms previous models of frame prediction; we can further improve performance with unlabeled training data leveraging the semisupervised nature of our model; and the learnt event and actor embeddings intuitively corroborate the document-level predictions, providing a nuanced and interpretable article frame representation.","{'sequence': 'Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local information about the events and related actors in a news article through an auto-encoding framework, and to leverage this signal for document-level frame classification. Our experiments show that: our model outperforms previous models of frame prediction; we can further improve performance with unlabeled training data leveraging the semisupervised nature of our model; and the learnt event and actor embeddings intuitively corroborate the document-level predictions, providing a nuanced and interpretable article frame representation.', 'labels': ['Computational Social Science and Social Media', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'NLP Applications', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08150457590818405, 0.07249613106250763, 0.06991945207118988, 0.06231335923075676, 0.0600825697183609, 0.05186405032873154, 0.0491449236869812, 0.045730799436569214, 0.04496700316667557, 0.044116340577602386, 0.04339650273323059, 0.04210054129362106, 0.040956050157547, 0.03897674009203911, 0.0385294072329998, 0.03790756314992905, 0.0317172147333622, 0.026796676218509674, 0.02516748197376728, 0.02467128075659275, 0.023935789242386818, 0.02383379265666008, 0.019871676340699196]}",0.08150457590818405,Computational Social Science and Social Media,0.08150457590818405
Computational Social Science and Social Media,Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism,"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We collect manual annotations of neutralised techniques used in these texts, and explore semi-supervised models to automatically classify them.","{'sequence': 'Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We collect manual annotations of neutralised techniques used in these texts, and explore semi-supervised models to automatically classify them.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Generation', 'Speech and Multimodality', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.06809769570827484, 0.06796538084745407, 0.06535786390304565, 0.06297566741704941, 0.06263257563114166, 0.062372904270887375, 0.05261009931564331, 0.052264656871557236, 0.04805624112486839, 0.04773999750614166, 0.0477350652217865, 0.042997002601623535, 0.04030611738562584, 0.03925112634897232, 0.03509025275707245, 0.033162616193294525, 0.030241547152400017, 0.028307562693953514, 0.026952195912599564, 0.026357438415288925, 0.02274978533387184, 0.021515361964702606, 0.015260884538292885]}",0.06809769570827484,Information Extraction,0.030241547152400017
Computational Social Science and Social Media,Suicide Ideation Detection via Social and Temporal User Representations using Hyperbolic Learning,"Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Personally contextualizing the buildup of such ideation is critical for accurate identification of users at risk. In this work, we propose a framework jointly leveraging a user's emotional history and social information from a user's neighborhood in a network to contextualize the interpretation of the latest tweet of a user on Twitter. Reflecting upon the scale-free nature of social network relationships, we propose the use of Hyperbolic Graph Convolution Networks, in combination with the Hawkes process to learn the historical emotional spectrum of a user in a timesensitive manner. Our system significantly outperforms state-of-the-art methods on this task, showing the benefits of both socially and personally contextualized representations.","{'sequence': ""Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Personally contextualizing the buildup of such ideation is critical for accurate identification of users at risk. In this work, we propose a framework jointly leveraging a user's emotional history and social information from a user's neighborhood in a network to contextualize the interpretation of the latest tweet of a user on Twitter. Reflecting upon the scale-free nature of social network relationships, we propose the use of Hyperbolic Graph Convolution Networks, in combination with the Hawkes process to learn the historical emotional spectrum of a user in a timesensitive manner. Our system significantly outperforms state-of-the-art methods on this task, showing the benefits of both socially and personally contextualized representations."", 'labels': ['Computational Social Science and Social Media', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Information Retrieval and Text Mining', 'Generation', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10369167476892471, 0.08873722702264786, 0.08537723124027252, 0.08061491698026657, 0.07345900684595108, 0.06793468445539474, 0.049887511879205704, 0.046830397099256516, 0.044717904180288315, 0.03947019577026367, 0.03683098033070564, 0.03582552447915077, 0.03152300789952278, 0.030230671167373657, 0.029833225533366203, 0.028175029903650284, 0.02657599374651909, 0.02483999915421009, 0.023518897593021393, 0.01595228724181652, 0.015219511464238167, 0.01171607431024313, 0.009038010612130165]}",0.10369167476892471,Computational Social Science and Social Media,0.10369167476892471
Computational Social Science and Social Media,The structure of online social networks modulates the rate of lexical change,"New words are regularly introduced to communities, yet not all of these words persist in a community's lexicon. Among the many factors contributing to lexical change, we focus on the understudied effect of social networks. We conduct a large-scale analysis of over 80k neologisms in 4420 online communities across a decade. Using Poisson regression and survival analysis, our study demonstrates that the community's network structure plays a significant role in lexical change. Apart from overall size, properties including dense connections, the lack of local clusters and more external contacts promote lexical innovation and retention. Unlike offline communities, these topic-based communities do not experience strong lexical levelling despite increased contact but accommodate more niche words. Our work provides support for the sociolinguistic hypothesis that lexical change is partially shaped by the structure of the underlying network but also uncovers findings specific to online communities.","{'sequence': ""New words are regularly introduced to communities, yet not all of these words persist in a community's lexicon. Among the many factors contributing to lexical change, we focus on the understudied effect of social networks. We conduct a large-scale analysis of over 80k neologisms in 4420 online communities across a decade. Using Poisson regression and survival analysis, our study demonstrates that the community's network structure plays a significant role in lexical change. Apart from overall size, properties including dense connections, the lack of local clusters and more external contacts promote lexical innovation and retention. Unlike offline communities, these topic-based communities do not experience strong lexical levelling despite increased contact but accommodate more niche words. Our work provides support for the sociolinguistic hypothesis that lexical change is partially shaped by the structure of the underlying network but also uncovers findings specific to online communities."", 'labels': ['Computational Social Science and Social Media', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Information Extraction', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Generation', 'Summarization'], 'scores': [0.07521899789571762, 0.06697728484869003, 0.061531756073236465, 0.05958371236920357, 0.05401191487908363, 0.052282579243183136, 0.0494714230298996, 0.04896816611289978, 0.04729852452874184, 0.045910902321338654, 0.04572160542011261, 0.04538555070757866, 0.04049023985862732, 0.0390973724424839, 0.03854657709598541, 0.03432663902640343, 0.03344300389289856, 0.032597631216049194, 0.029501715674996376, 0.02753785438835621, 0.024316614493727684, 0.024065863341093063, 0.023714106529951096]}",0.07521899789571762,Computational Social Science and Social Media,0.07521899789571762
Computational Social Science and Social Media,Modeling Framing in Immigration Discourse on Social Media,"The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigrationrelated tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users' ideology and region impact framing choices, and how a message's framing influences audience responses. We find that the more commonlyused issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.","{'sequence': ""The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigrationrelated tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users' ideology and region impact framing choices, and how a message's framing influences audience responses. We find that the more commonlyused issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research."", 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Question Answering', 'Resources and Evaluation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Summarization', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.07717740535736084, 0.06888172030448914, 0.06610988825559616, 0.06133774667978287, 0.05678171291947365, 0.05634288489818573, 0.05495521426200867, 0.05038592591881752, 0.05006780847907066, 0.04837657883763313, 0.045910343527793884, 0.044932883232831955, 0.04241027683019638, 0.03850609064102173, 0.037509605288505554, 0.037487346678972244, 0.030962180346250534, 0.02819468453526497, 0.02649216540157795, 0.023426033556461334, 0.02213951200246811, 0.016533710062503815, 0.01507836114615202]}",0.07717740535736084,Speech and Multimodality,0.06133774667978287
Computational Social Science and Social Media,Modeling the Severity of Complaints in Social Media,"The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for the first time in computational linguistics. To facilitate this, we enrich a publicly available data set of complaints with four severity categories and train different transformer-based networks combined with linguistic information achieving 55.7 macro F1. We also jointly model binary complaint classification and complaint severity in a multi-task setting achieving new state-of-the-art results on binary complaint detection reaching up to 88.2 macro F1. Finally, we present a qualitative analysis of the behavior of our models in predicting complaint severity levels. 1,2","{'sequence': 'The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for the first time in computational linguistics. To facilitate this, we enrich a publicly available data set of complaints with four severity categories and train different transformer-based networks combined with linguistic information achieving 55.7 macro F1. We also jointly model binary complaint classification and complaint severity in a multi-task setting achieving new state-of-the-art results on binary complaint detection reaching up to 88.2 macro F1. Finally, we present a qualitative analysis of the behavior of our models in predicting complaint severity levels. 1,2', 'labels': ['Discourse and Pragmatics', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Summarization', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Question Answering', 'Generation', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1401471048593521, 0.11778838187456131, 0.06835485994815826, 0.051550351083278656, 0.05151975527405739, 0.04893803969025612, 0.0484650544822216, 0.04727494716644287, 0.045963846147060394, 0.043456993997097015, 0.03794648125767708, 0.03597334027290344, 0.03466139733791351, 0.03085082769393921, 0.029463477432727814, 0.027213314548134804, 0.026803644374012947, 0.022572482004761696, 0.02256171591579914, 0.021348442882299423, 0.01826298236846924, 0.014973658137023449, 0.013908910565078259]}",0.1401471048593521,Discourse and Pragmatics,0.022572482004761696
Computational Social Science and Social Media,What About the Precedent: An Information-Theoretic Analysis of Common Law,"In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury's, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart's, who believes that what matters most is the precedent's facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges' arguments (i.e. the precedent cases). Taking an information-theoretic view, and modeling the question as a case outcome classification task, we find that the precedent's arguments share 0.38 nats of information with the case's outcome, whereas precedent's facts only share 0.18 nats of information (i.e., 58% less); suggesting Halsbury's view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart's view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.","{'sequence': ""In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury's, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart's, who believes that what matters most is the precedent's facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges' arguments (i.e. the precedent cases). Taking an information-theoretic view, and modeling the question as a case outcome classification task, we find that the precedent's arguments share 0.38 nats of information with the case's outcome, whereas precedent's facts only share 0.18 nats of information (i.e., 58% less); suggesting Halsbury's view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart's view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward."", 'labels': ['Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Generation', 'Information Retrieval and Text Mining', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.21790409088134766, 0.10735858231782913, 0.06754285097122192, 0.055765874683856964, 0.05500541999936104, 0.04635036736726761, 0.04587304964661598, 0.044659145176410675, 0.043196529150009155, 0.04275410622358322, 0.04265711456537247, 0.029264844954013824, 0.02921961434185505, 0.028518186882138252, 0.024825263768434525, 0.02450827695429325, 0.022915445268154144, 0.018169036135077477, 0.015058565884828568, 0.014874408021569252, 0.009820480830967426, 0.007778135593980551, 0.005980585236102343]}",0.21790409088134766,Question Answering,0.029264844954013824
Computational Social Science and Social Media,Introducing CAD: the Contextual Abuse Dataset,"Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.","{'sequence': 'Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.', 'labels': ['Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Ethics and NLP', 'Question Answering', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'NLP Applications', 'Information Extraction', 'Summarization', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.09757983684539795, 0.08608050644397736, 0.07018367946147919, 0.05508057028055191, 0.05316374823451042, 0.05227397754788399, 0.05129481479525566, 0.045665234327316284, 0.043705958873033524, 0.043579403311014175, 0.04321131482720375, 0.04087912663817406, 0.03957374021410942, 0.03526158258318901, 0.033832646906375885, 0.032956697046756744, 0.032244354486465454, 0.03022141382098198, 0.02733975276350975, 0.023915225639939308, 0.02143663354218006, 0.02095799334347248, 0.019561830908060074]}",0.09757983684539795,Computational Social Science and Social Media,0.09757983684539795
Computational Social Science and Social Media,Lifelong Learning of Hate Speech Classification on Social Media,"Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the realword application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques.","{'sequence': 'Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the realword application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques.', 'labels': ['Computational Social Science and Social Media', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Generation', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08320964872837067, 0.07640417665243149, 0.07594756036996841, 0.06331854313611984, 0.055955670773983, 0.050881512463092804, 0.048259180039167404, 0.04610811173915863, 0.04225391894578934, 0.042059626430273056, 0.04153014346957207, 0.040483418852090836, 0.03929528221487999, 0.03840818628668785, 0.03756992518901825, 0.036140076816082, 0.03217943757772446, 0.031962111592292786, 0.030585559085011482, 0.02790306881070137, 0.023910721763968468, 0.01868485100567341, 0.016949189826846123]}",0.08320964872837067,Computational Social Science and Social Media,0.08320964872837067
Computational Social Science and Social Media,Learning to Recognize Dialect Features,"Building NLP systems that serve everyone requires accounting for dialect differences. But dialects are not monolithic entities: rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of dialect features in speech and text, such as the deletion of the copula in ""He ∅ running"". In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most dialects, largescale annotated corpora for these features are unavailable, making it difficult to train recognizers. We train our models on a small number of minimal pairs, building on how linguists typically define dialect features. Evaluation on a test set of 22 dialect features of Indian English demonstrates that these models learn to recognize many features with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of dialect density and as a dialect classifier.","{'sequence': 'Building NLP systems that serve everyone requires accounting for dialect differences. But dialects are not monolithic entities: rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of dialect features in speech and text, such as the deletion of the copula in ""He ∅ running"". In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most dialects, largescale annotated corpora for these features are unavailable, making it difficult to train recognizers. We train our models on a small number of minimal pairs, building on how linguists typically define dialect features. Evaluation on a test set of 22 dialect features of Indian English demonstrates that these models learn to recognize many features with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of dialect density and as a dialect classifier.', 'labels': ['Speech and Multimodality', 'NLP Applications', 'Resources and Evaluation', 'Machine Learning for NLP', 'Question Answering', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Summarization', 'Generation', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10321012884378433, 0.09624332934617996, 0.0947137176990509, 0.0715307742357254, 0.06258093565702438, 0.05099193751811981, 0.047258514910936356, 0.04547099396586418, 0.042321667075157166, 0.04060913249850273, 0.03671547397971153, 0.03300263360142708, 0.032132137566804886, 0.03088080883026123, 0.029076363891363144, 0.02824489399790764, 0.027595311403274536, 0.02594585530459881, 0.025246521458029747, 0.022957101464271545, 0.02124345861375332, 0.019849738106131554, 0.01217869482934475]}",0.10321012884378433,Speech and Multimodality,0.047258514910936356
"Language Grounding to Vision, Robotics and Beyond",EaSe: A Diagnostic Tool for VQA based on Answer Diversity,"We propose EASE, a simple diagnostic tool for Visual Question Answering (VQA) which quantifies the difficulty of an image, question sample. EASE is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their Entropy; (ii) their Semantic content. First, we prove the validity of our diagnostic to identify samples that are easy/hard for state-of-art VQA models. Second, we show that EASE can be successfully used to select the most-informative samples for training/fine-tuning. Crucially, only information that is readily available in any VQA dataset is used to compute its scores. 1","{'sequence': 'We propose EASE, a simple diagnostic tool for Visual Question Answering (VQA) which quantifies the difficulty of an image, question sample. EASE is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their Entropy; (ii) their Semantic content. First, we prove the validity of our diagnostic to identify samples that are easy/hard for state-of-art VQA models. Second, we show that EASE can be successfully used to select the most-informative samples for training/fine-tuning. Crucially, only information that is readily available in any VQA dataset is used to compute its scores. 1', 'labels': ['Question Answering', 'Information Extraction', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Semantics: Lexical Semantics', 'Generation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.6549747586250305, 0.047608911991119385, 0.031149858608841896, 0.028056049719452858, 0.019151080399751663, 0.016552912071347237, 0.015235251747071743, 0.015101142227649689, 0.014643502421677113, 0.014150525443255901, 0.014006000943481922, 0.013769065961241722, 0.013731695711612701, 0.01232527568936348, 0.011882219463586807, 0.0117535050958395, 0.011564902029931545, 0.011378375813364983, 0.010881057940423489, 0.010625777766108513, 0.008481052704155445, 0.0066895559430122375, 0.006287531461566687]}",0.6549747586250305,Question Answering,0.0066895559430122375
"Language Grounding to Vision, Robotics and Beyond",DeCEMBERT: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization,"Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many videoand-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DECEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the model performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss. 1 only inputting its associated caption s i , we also in-330 clude captions from its two neighboring clips, i.e., 331 s i 1 and s i+1 . In most cases, the correct matched 332 caption for the clip is from these three captions. 333 We denote X=[X ci ; X si 1 ; X si ; X si+1 ] 2 R l⇥d 334 as the generalized input sequence to each trans-335 former layer, where X ci , X si 1 , X si , X si+1 are 336 the embedding matrices correspond to the input 337 clip and captions. We further simplify the nota-338 tions as X=[X 0 ; X 1 ; X 2 ; X 3 ]. The self-attention 339 operation in the transformer encoder layers can 340 then be expressed as: 342 where softmax(•, dim = 1) denotes performing 343 softmax at the second dimension of the input ma-344 trix. A is the attention output. When multiple 345 attention heads are used, the formulation is similar. 346 We use S to denote the similarity matrix computed 347 by XX T . It can be expressed as a block matrix: 348 S q,r = X q X T r , q,r 2 {0, 1, 2, 3}. (2) 349 Our goal is to encourage the model to focus on the 350 correct matched caption for an input clip, i.e., the 351 attention mass from the video clip to the correct 352 matched caption should be higher than the others. 353 We denote the overall attention score from the input 354 video clip to one of the ASR captions as: The 355 maximum response between the ASR captions to 356 each element in the video can be computed as: 357 q j =max(S 0,j , dim=1), j 2 {1, 2, 3}. (3) 358 We then employ an entropy-based loss: 359 L e = 3 X i=1 qj log( qj ) (4) 360","{'sequence': 'Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many videoand-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DECEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the model performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss. 1 only inputting its associated caption s i , we also in-330 clude captions from its two neighboring clips, i.e., 331 s i 1 and s i+1 . In most cases, the correct matched 332 caption for the clip is from these three captions. 333 We denote X=[X ci ; X si 1 ; X si ; X si+1 ] 2 R l⇥d 334 as the generalized input sequence to each trans-335 former layer, where X ci , X si 1 , X si , X si+1 are 336 the embedding matrices correspond to the input 337 clip and captions. We further simplify the nota-338 tions as X=[X 0 ; X 1 ; X 2 ; X 3 ]. The self-attention 339 operation in the transformer encoder layers can 340 then be expressed as: 342 where softmax(•, dim = 1) denotes performing 343 softmax at the second dimension of the input ma-344 trix. A is the attention output. When multiple 345 attention heads are used, the formulation is similar. 346 We use S to denote the similarity matrix computed 347 by XX T . It can be expressed as a block matrix: 348 S q,r = X q X T r , q,r 2 {0, 1, 2, 3}. (2) 349 Our goal is to encourage the model to focus on the 350 correct matched caption for an input clip, i.e., the 351 attention mass from the video clip to the correct 352 matched caption should be higher than the others. 353 We denote the overall attention score from the input 354 video clip to one of the ASR captions as: The 355 maximum response between the ASR captions to 356 each element in the video can be computed as: 357 q j =max(S 0,j , dim=1), j 2 {1, 2, 3}. (3) 358 We then employ an entropy-based loss: 359 L e = 3 X i=1 qj log( qj ) (4) 360', 'labels': ['Information Extraction', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Resources and Evaluation', 'Summarization', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Generation', 'NLP Applications', 'Machine Translation and Multilinguality'], 'scores': [0.0890033021569252, 0.06549146771430969, 0.06130046024918556, 0.059910986572504044, 0.05741119757294655, 0.05157914385199547, 0.05153550207614899, 0.05095081031322479, 0.05076257139444351, 0.049615513533353806, 0.04358169808983803, 0.04050053656101227, 0.03901803120970726, 0.03453204408288002, 0.03341236338019371, 0.03188798576593399, 0.03045228309929371, 0.029376477003097534, 0.028665924444794655, 0.028460487723350525, 0.027110053226351738, 0.025380020961165428, 0.020061228424310684]}",0.0890033021569252,Information Extraction,0.05076257139444351
"Language Grounding to Vision, Robotics and Beyond",Improving Generation and Evaluation of Visual Stories via Semantic Consistency,Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this,"{'sequence': 'Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this', 'labels': ['Ethics and NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Machine Learning for NLP', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Language Grounding to Vision, Robotics and Beyond', 'Resources and Evaluation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Question Answering', 'Information Retrieval and Text Mining', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1497838944196701, 0.11484367400407791, 0.08042453974485397, 0.06557700037956238, 0.06053153797984123, 0.05738352984189987, 0.055731527507305145, 0.04942977800965309, 0.049167294055223465, 0.04744749516248703, 0.04032277315855026, 0.033296458423137665, 0.02911740355193615, 0.028544312343001366, 0.020590350031852722, 0.020436085760593414, 0.018858905881643295, 0.017993928864598274, 0.017147203907370567, 0.012784583494067192, 0.011476198211312294, 0.009992847219109535, 0.009118839167058468]}",0.1497838944196701,Ethics and NLP,0.049167294055223465
"Language Grounding to Vision, Robotics and Beyond",Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models,"This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-tovideo search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/ Multi-HT100M","{'sequence': 'This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-tovideo search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/ Multi-HT100M', 'labels': ['Generation', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Information Extraction', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07102448493242264, 0.07085845619440079, 0.07085614651441574, 0.06647808104753494, 0.06135115772485733, 0.05464030057191849, 0.050710055977106094, 0.04882630333304405, 0.048325248062610626, 0.0461469329893589, 0.04398984834551811, 0.04153985157608986, 0.04089518263936043, 0.03532710671424866, 0.033382657915353775, 0.03290367126464844, 0.030164312571287155, 0.029975449666380882, 0.027815798297524452, 0.025900883600115776, 0.024729492142796516, 0.02434983104467392, 0.01980888657271862]}",0.07102448493242264,Generation,0.029975449666380882
"Language Grounding to Vision, Robotics and Beyond",Video Question Answering with Phrases via Semantic Roles,"Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models' application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fillin-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language-bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.","{'sequence': ""Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models' application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fillin-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language-bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public."", 'labels': ['Question Answering', 'Resources and Evaluation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.6649693846702576, 0.039934173226356506, 0.024233220145106316, 0.024216940626502037, 0.02126552350819111, 0.019829990342259407, 0.016712771728634834, 0.016597295179963112, 0.016277041286230087, 0.015637051314115524, 0.01554238423705101, 0.01414690911769867, 0.011603515595197678, 0.011573557741940022, 0.011540085077285767, 0.011114043183624744, 0.010977120138704777, 0.01092587597668171, 0.01025393232703209, 0.009664875455200672, 0.008580395951867104, 0.008068776689469814, 0.006335125304758549]}",0.6649693846702576,Question Answering,0.011603515595197678
Resources and Evaluation,From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding,"The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce XSID, a new benchmark for cross-lingual (X) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification. 1","{'sequence': 'The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce XSID, a new benchmark for cross-lingual (X) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification. 1', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Information Extraction', 'Summarization', 'Discourse and Pragmatics', 'Generation', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.17710360884666443, 0.06934419274330139, 0.06127043440937996, 0.056963808834552765, 0.052657030522823334, 0.05210014805197716, 0.04856494069099426, 0.044220421463251114, 0.04374682158231735, 0.04149571806192398, 0.040996160358190536, 0.03673415631055832, 0.032905928790569305, 0.031177138909697533, 0.02923123724758625, 0.027772551402449608, 0.026211168617010117, 0.025294972583651543, 0.02459237352013588, 0.023036101832985878, 0.02005443535745144, 0.017309848219156265, 0.017216859385371208]}",0.17710360884666443,Resources and Evaluation,0.17710360884666443
Resources and Evaluation,WEC: Deriving a Large-scale Cross-document Event Coreference dataset from Wikipedia,"Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modestsize clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a largescale dataset for cross-document event coreference from Wikipedia, where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of stateof-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task.","{'sequence': 'Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modestsize clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a largescale dataset for cross-document event coreference from Wikipedia, where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of stateof-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task.', 'labels': ['NLP Applications', 'Resources and Evaluation', 'Information Extraction', 'Machine Learning for NLP', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Generation', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Summarization', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.29567793011665344, 0.060096923261880875, 0.04960040748119354, 0.043725091964006424, 0.04221879690885544, 0.040442824363708496, 0.03970388323068619, 0.038125403225421906, 0.03674209862947464, 0.03656196594238281, 0.03306557983160019, 0.0329413004219532, 0.03241531923413277, 0.03181412070989609, 0.03072209842503071, 0.026077747344970703, 0.025798121467232704, 0.023676984012126923, 0.0208581555634737, 0.019022082909941673, 0.014748127199709415, 0.013153349980711937, 0.01281168032437563]}",0.29567793011665344,NLP Applications,0.060096923261880875
Resources and Evaluation,Challenging distributional models with a conceptual network of philosophical terms,"Computational linguistic research on language change through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on methods for small data is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional semantic models for analyzing philosophical data by means of a realistic use-case. We provide a ground truth for evaluation created by philosophy experts and a blueprint for using DS models in a sound methodological setup. We compare three methods for creating specialized models from small datasets. Though the models do not perform well enough to directly support philosophers yet, we find that models designed for small data yield promising directions for future work.","{'sequence': 'Computational linguistic research on language change through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on methods for small data is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional semantic models for analyzing philosophical data by means of a realistic use-case. We provide a ground truth for evaluation created by philosophy experts and a blueprint for using DS models in a sound methodological setup. We compare three methods for creating specialized models from small datasets. Though the models do not perform well enough to directly support philosophers yet, we find that models designed for small data yield promising directions for future work.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Speech and Multimodality', 'Summarization', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07212124764919281, 0.0633392408490181, 0.06199180707335472, 0.060364048928022385, 0.060345374047756195, 0.05696888640522957, 0.05630806088447571, 0.05362267419695854, 0.05037708207964897, 0.0501159243285656, 0.048875369131565094, 0.046995971351861954, 0.04594811052083969, 0.04134073108434677, 0.03788173198699951, 0.03617246076464653, 0.033783942461013794, 0.026822593063116074, 0.025015490129590034, 0.023361261934041977, 0.01889231987297535, 0.016421601176261902, 0.012934024445712566]}",0.07212124764919281,Resources and Evaluation,0.07212124764919281
Resources and Evaluation,KILT: a Benchmark for Knowledge Intensive Language Tasks,"Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the reuse of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/ facebookresearch/KILT. 1","{'sequence': 'Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the reuse of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/ facebookresearch/KILT. 1', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Generation', 'Computational Social Science and Social Media', 'NLP Applications', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Information Extraction', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15005281567573547, 0.10536368936300278, 0.07412845641374588, 0.06526772677898407, 0.055931173264980316, 0.04674825444817543, 0.044836025685071945, 0.043572839349508286, 0.0434054471552372, 0.03639812767505646, 0.03606964275240898, 0.03511086106300354, 0.03497462347149849, 0.03257664293050766, 0.031858526170253754, 0.028063073754310608, 0.027199748903512955, 0.024767892435193062, 0.02214985340833664, 0.01912992261350155, 0.017904730513691902, 0.014602595008909702, 0.009887393563985825]}",0.15005281567573547,Question Answering,0.10536368936300278
Machine Learning for NLP,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.","{'sequence': 'Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.', 'labels': ['NLP Applications', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Ethics and NLP', 'Question Answering', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.47627484798431396, 0.04083355888724327, 0.04051881656050682, 0.03843265399336815, 0.03824586793780327, 0.034276023507118225, 0.033748362213373184, 0.027412407100200653, 0.02723926492035389, 0.025732822716236115, 0.024902354925870895, 0.021665723994374275, 0.020441805943846703, 0.020250661298632622, 0.020100729539990425, 0.017087774351239204, 0.01702686958014965, 0.016274861991405487, 0.014767996035516262, 0.01300276629626751, 0.011949512176215649, 0.011502373032271862, 0.008312016725540161]}",0.47627484798431396,NLP Applications,0.03843265399336815
Machine Learning for NLP,Temporal Knowledge Graph Completion using a Linear Temporal Regularizer and Multivector Embeddings,"Representation learning approaches for knowledge graphs have been mostly designed for static data. However, many knowledge graphs involve evolving data, e.g., the fact (The President of the United States is Barack Obama) is valid only from 2009 to 2017. This introduces important challenges for knowledge representation learning since the knowledge graphs change over time. In this paper, we present a novel time-aware knowledge graph embebdding approach, TeLM, which performs 4th-order tensor factorization of a Temporal knowledge graph using a Linear temporal regularizer and Multivector embeddings. Moreover, we investigate the effect of the temporal dataset's time granularity on temporal knowledge graph completion. Experimental results demonstrate that our proposed models trained with the linear temporal regularizer achieve the state-of-the-art performances on link prediction over four well-established temporal knowledge graph completion benchmarks.","{'sequence': ""Representation learning approaches for knowledge graphs have been mostly designed for static data. However, many knowledge graphs involve evolving data, e.g., the fact (The President of the United States is Barack Obama) is valid only from 2009 to 2017. This introduces important challenges for knowledge representation learning since the knowledge graphs change over time. In this paper, we present a novel time-aware knowledge graph embebdding approach, TeLM, which performs 4th-order tensor factorization of a Temporal knowledge graph using a Linear temporal regularizer and Multivector embeddings. Moreover, we investigate the effect of the temporal dataset's time granularity on temporal knowledge graph completion. Experimental results demonstrate that our proposed models trained with the linear temporal regularizer achieve the state-of-the-art performances on link prediction over four well-established temporal knowledge graph completion benchmarks."", 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Summarization', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Information Extraction', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09178247302770615, 0.0741046890616417, 0.07282166928052902, 0.07206740230321884, 0.0662679672241211, 0.059267353266477585, 0.05755927413702011, 0.05129832774400711, 0.04874325543642044, 0.047180045396089554, 0.045999400317668915, 0.0404394194483757, 0.039453402161598206, 0.03560144454240799, 0.03230729699134827, 0.02916785329580307, 0.026866327971220016, 0.023942049592733383, 0.02250117063522339, 0.02233443595468998, 0.015470440499484539, 0.012733853422105312, 0.012090367265045643]}",0.09178247302770615,Dialogue and Interactive Systems,0.02233443595468998
Machine Learning for NLP,UDALM: Unsupervised Domain Adaptation through Language Modeling,"In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74% accuracy, which is an 1.11% absolute improvement over the state-of-the-art.","{'sequence': 'In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74% accuracy, which is an 1.11% absolute improvement over the state-of-the-art.', 'labels': ['Resources and Evaluation', 'Generation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'NLP Applications', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics'], 'scores': [0.1604796051979065, 0.08329378068447113, 0.07207085937261581, 0.06533404439687729, 0.04434102773666382, 0.04335704445838928, 0.04200921952724457, 0.041663024574518204, 0.04077247530221939, 0.04028301686048508, 0.0367189459502697, 0.03389880061149597, 0.03369694575667381, 0.031731005758047104, 0.03017134964466095, 0.028993330895900726, 0.026865024119615555, 0.02612638846039772, 0.02559053525328636, 0.02522711642086506, 0.02382744289934635, 0.022284116595983505, 0.021264778450131416]}",0.1604796051979065,Resources and Evaluation,0.07207085937261581
Machine Learning for NLP,Clustering-based Inference for Biomedical Entity Linking,"Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via clustering and jointly making linking predictions. In experiments we improve the state-of-the-art entity linking accuracy on two biomedical entity linking datasets including on the largest publicly available dataset.","{'sequence': 'Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via clustering and jointly making linking predictions. In experiments we improve the state-of-the-art entity linking accuracy on two biomedical entity linking datasets including on the largest publicly available dataset.', 'labels': ['Dialogue and Interactive Systems', 'Generation', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Question Answering', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'NLP Applications', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08341138064861298, 0.07897525280714035, 0.06608696281909943, 0.05406104400753975, 0.05178450047969818, 0.04709453135728836, 0.046857476234436035, 0.046652358025312424, 0.04652306064963341, 0.046365439891815186, 0.04619471728801727, 0.04583544284105301, 0.043729186058044434, 0.03922395780682564, 0.03682742267847061, 0.03488253057003021, 0.034679897129535675, 0.03291347250342369, 0.029430227354168892, 0.0244162455201149, 0.023138336837291718, 0.022762730717658997, 0.018153686076402664]}",0.08341138064861298,Dialogue and Interactive Systems,0.034679897129535675
Machine Learning for NLP,Variance-reduced First-order Meta-learning for Natural Language Processing Tasks,"First-order meta-learning algorithms have been widely used in practice to learn initial model parameters that can be quickly adapted to new tasks due to their efficiency and effectiveness. However, existing studies find that meta-learner can overfit to some specific adaptation when we have heterogeneous tasks, leading to significantly degraded performance. In Natural Language Processing (NLP) applications, datasets are often diverse and each task has its unique characteristics. Therefore, to address the overfitting issue when applying firstorder meta-learning to NLP applications, we propose to reduce the variance of the gradient estimator used in task adaptation. To this end, we develop a variance-reduced first-order meta-learning algorithm. The core of our algorithm is to introduce a novel variance reduction term to the gradient estimation when performing the task adaptation. Experiments on two NLP applications: few-shot text classification and multi-domain dialog state tracking demonstrate the superior performance of our proposed method.","{'sequence': 'First-order meta-learning algorithms have been widely used in practice to learn initial model parameters that can be quickly adapted to new tasks due to their efficiency and effectiveness. However, existing studies find that meta-learner can overfit to some specific adaptation when we have heterogeneous tasks, leading to significantly degraded performance. In Natural Language Processing (NLP) applications, datasets are often diverse and each task has its unique characteristics. Therefore, to address the overfitting issue when applying firstorder meta-learning to NLP applications, we propose to reduce the variance of the gradient estimator used in task adaptation. To this end, we develop a variance-reduced first-order meta-learning algorithm. The core of our algorithm is to introduce a novel variance reduction term to the gradient estimation when performing the task adaptation. Experiments on two NLP applications: few-shot text classification and multi-domain dialog state tracking demonstrate the superior performance of our proposed method.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Speech and Multimodality', 'Resources and Evaluation', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Generation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Translation and Multilinguality', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.755866289138794, 0.028519583866000175, 0.027482032775878906, 0.01835545524954796, 0.017306027933955193, 0.016589965671300888, 0.012345440685749054, 0.011956978589296341, 0.011723266914486885, 0.010857787914574146, 0.010700314305722713, 0.010314695537090302, 0.00958172231912613, 0.008562516421079636, 0.00840865820646286, 0.007903069257736206, 0.007567481603473425, 0.006926486734300852, 0.005166829563677311, 0.003921006806194782, 0.003511226736009121, 0.0034435619600117207, 0.0029895594343543053]}",0.755866289138794,NLP Applications,0.028519583866000175
Machine Learning for NLP,Diversity-Aware Batch Active Learning for Dependency Parsing,"While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the parsers. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversityaware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation.","{'sequence': 'While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the parsers. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversityaware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Extraction', 'Summarization', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10177236050367355, 0.07311488687992096, 0.07059101015329361, 0.06983157247304916, 0.06836424767971039, 0.06115473061800003, 0.056504763662815094, 0.04430675506591797, 0.041725147515535355, 0.04081151634454727, 0.03997284919023514, 0.03706469014286995, 0.034965477883815765, 0.03197231516242027, 0.031423069536685944, 0.03121846914291382, 0.030384574085474014, 0.0298953577876091, 0.02667553536593914, 0.023348769173026085, 0.018973996862769127, 0.018649417906999588, 0.01727849617600441]}",0.10177236050367355,Dialogue and Interactive Systems,0.031423069536685944
Machine Learning for NLP,Can Latent Alignments Improve Autoregressive Machine Translation?,"Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.","{'sequence': 'Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.', 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Information Extraction', 'Summarization', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10975330322980881, 0.10965877771377563, 0.07905542105436325, 0.07763276249170303, 0.054929185658693314, 0.054745376110076904, 0.05067291855812073, 0.046866897493600845, 0.0455954410135746, 0.0404653362929821, 0.040090061724185944, 0.038661666214466095, 0.034371353685855865, 0.03016303852200508, 0.02863561362028122, 0.027617700397968292, 0.025871258229017258, 0.023321226239204407, 0.02070118859410286, 0.0186738520860672, 0.018477894365787506, 0.012239963747560978, 0.011799761094152927]}",0.10975330322980881,Machine Translation and Multilinguality,0.025871258229017258
Machine Learning for NLP,Smoothing and Shrinking the Sparse Seq2Seq Search Space,"Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax-the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both cross-entropy and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs.","{'sequence': 'Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax-the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both cross-entropy and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs.', 'labels': ['Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Machine Learning for NLP', 'NLP Applications', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining'], 'scores': [0.09057687968015671, 0.08740018308162689, 0.07682950049638748, 0.07239453494548798, 0.048598047345876694, 0.04803948104381561, 0.04640582948923111, 0.04525972530245781, 0.044161003082990646, 0.0423654168844223, 0.042066898196935654, 0.039003048092126846, 0.03656395897269249, 0.03637254238128662, 0.03283771499991417, 0.03252696245908737, 0.03192538768053055, 0.030336715281009674, 0.029548868536949158, 0.024822376668453217, 0.024055270478129387, 0.020946864038705826, 0.016962802037596703]}",0.09057687968015671,Dialogue and Interactive Systems,0.04803948104381561
Machine Learning for NLP,Unified Pre-training for Program Understanding and Generation,"Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART's effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., if block inside an else block is equivalent to else if block) that are crucial to program semantics and thus excels even with limited annotations.","{'sequence': ""Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART's effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., if block inside an else block is equivalent to else if block) that are crucial to program semantics and thus excels even with limited annotations."", 'labels': ['Generation', 'Summarization', 'Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'NLP Applications', 'Information Extraction', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.19147394597530365, 0.09989345818758011, 0.07158820331096649, 0.0652717798948288, 0.05853002518415451, 0.058292075991630554, 0.04316689074039459, 0.04129008576273918, 0.0378345251083374, 0.037249039858579636, 0.03482585772871971, 0.0344177708029747, 0.032392922788858414, 0.02862517535686493, 0.026054713875055313, 0.024364180862903595, 0.022341672331094742, 0.021695781499147415, 0.020239343866705894, 0.013715318404138088, 0.01319243386387825, 0.01286113541573286, 0.010683674365282059]}",0.19147394597530365,Generation,0.0344177708029747
Machine Learning for NLP,Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding,"Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions of old and new data are significantly different. In fact, the key real-world problem is not the absence of old data, but the inefficiency to retrain the model with the whole old dataset. Is it potential to utilize some old data to yield high accuracy and maintain stable performance, while at the same time, without introducing extra hyperparameters? In this paper, we proposed a hyperparameter-free continual learning model for text data that can stably produce high performance under various environments. Specifically, we utilize Fisher information to select exemplars that can ""record"" key information of the original model. Also, a novel scheme called dynamical weight consolidation is proposed to enable hyperparameterfree learning during the retrain process. Extensive experiments demonstrate that baselines suffer from fluctuated performance and therefore useless in practice. On the contrary, our proposed model CCFI significantly and consistently outperforms the best state-of-the-art method by up to 20% in average accuracy, and each component of CCFI contributes effectively to overall performance.","{'sequence': 'Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions of old and new data are significantly different. In fact, the key real-world problem is not the absence of old data, but the inefficiency to retrain the model with the whole old dataset. Is it potential to utilize some old data to yield high accuracy and maintain stable performance, while at the same time, without introducing extra hyperparameters? In this paper, we proposed a hyperparameter-free continual learning model for text data that can stably produce high performance under various environments. Specifically, we utilize Fisher information to select exemplars that can ""record"" key information of the original model. Also, a novel scheme called dynamical weight consolidation is proposed to enable hyperparameterfree learning during the retrain process. Extensive experiments demonstrate that baselines suffer from fluctuated performance and therefore useless in practice. On the contrary, our proposed model CCFI significantly and consistently outperforms the best state-of-the-art method by up to 20% in average accuracy, and each component of CCFI contributes effectively to overall performance.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11109907180070877, 0.10710874199867249, 0.0774315595626831, 0.06915847957134247, 0.06601318717002869, 0.06560596078634262, 0.06306438893079758, 0.06146197021007538, 0.05067680776119232, 0.03869474306702614, 0.03784538432955742, 0.025984646752476692, 0.025573864579200745, 0.025442929938435555, 0.02433108538389206, 0.024226879701018333, 0.022862175479531288, 0.022005485370755196, 0.020368941128253937, 0.019425522536039352, 0.01563430391252041, 0.015125765465199947, 0.010858098976314068]}",0.11109907180070877,NLP Applications,0.10710874199867249
NLP Applications,On the Embeddings of Variables in Recurrent Neural Networks for Source Code,"Source code processing heavily relies on the methods widely used in natural language processing (NLP), but involves specifics that need to be taken into account to achieve higher quality. An example of this specificity is that the semantics of a variable is defined not only by its name but also by the contexts in which the variable occurs. In this work, we develop dynamic embeddings, a recurrent mechanism that adjusts the learned semantics of the variable when it obtains more information about the variable's role in the program. We show that using the proposed dynamic embeddings significantly improves the performance of the recurrent neural network, in code completion and bug fixing tasks.","{'sequence': ""Source code processing heavily relies on the methods widely used in natural language processing (NLP), but involves specifics that need to be taken into account to achieve higher quality. An example of this specificity is that the semantics of a variable is defined not only by its name but also by the contexts in which the variable occurs. In this work, we develop dynamic embeddings, a recurrent mechanism that adjusts the learned semantics of the variable when it obtains more information about the variable's role in the program. We show that using the proposed dynamic embeddings significantly improves the performance of the recurrent neural network, in code completion and bug fixing tasks."", 'labels': ['Machine Learning for NLP', 'Speech and Multimodality', 'NLP Applications', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Information Extraction', 'Generation', 'Question Answering', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0840393602848053, 0.08357057720422745, 0.07842829078435898, 0.07121878117322922, 0.07012676447629929, 0.060808487236499786, 0.0495138093829155, 0.048330388963222504, 0.04792778939008713, 0.04579788073897362, 0.041485149413347244, 0.0388709157705307, 0.03532255068421364, 0.035121191293001175, 0.03504028543829918, 0.03368239849805832, 0.02780616283416748, 0.024783246219158173, 0.024549059569835663, 0.02027214877307415, 0.017253262922167778, 0.013744689524173737, 0.01230683270841837]}",0.0840393602848053,Machine Learning for NLP,0.07842829078435898
NLP Applications,Semantic Frame Forecast,"This paper introduces semantic frame forecast, a task that predicts the semantic frames that will occur in the next 10, 100, or even 1,000 sentences in a running story. Prior work focused on predicting the immediate future of a story, such as one to a few sentences ahead. However, when novelists write long stories, generating a few sentences is not enough to help them gain high-level insight to develop the follow-up story. In this paper, we formulate a long story as a sequence of ""story blocks,"" where each block contains a fixed number of sentences (e.g., 10, 100, or 200). This formulation allows us to predict the follow-up story arc beyond the scope of a few sentences. We represent a story block using the term frequencies (TF) of semantic frames in it, normalized by each frame's inverse document frequency (IDF). We conduct semantic frame forecast experiments on 4,794 books from the Bookcorpus and 7,962 scientific abstracts from CODA-19, with block sizes ranging from 5 to 1,000 sentences. The results show that automated models can forecast the follow-up story blocks better than the random, prior, and replay baselines, indicating the task's feasibility. We also learn that the models using the frame representation as features outperform all the existing approaches when the block size is over 150 sentences. The human evaluation also shows that the proposed frame representation, when visualized as word clouds, is comprehensible, representative, and specific to humans. Our code is available at: https://github.com/ appleternity/FrameForecasting.","{'sequence': 'This paper introduces semantic frame forecast, a task that predicts the semantic frames that will occur in the next 10, 100, or even 1,000 sentences in a running story. Prior work focused on predicting the immediate future of a story, such as one to a few sentences ahead. However, when novelists write long stories, generating a few sentences is not enough to help them gain high-level insight to develop the follow-up story. In this paper, we formulate a long story as a sequence of ""story blocks,"" where each block contains a fixed number of sentences (e.g., 10, 100, or 200). This formulation allows us to predict the follow-up story arc beyond the scope of a few sentences. We represent a story block using the term frequencies (TF) of semantic frames in it, normalized by each frame\'s inverse document frequency (IDF). We conduct semantic frame forecast experiments on 4,794 books from the Bookcorpus and 7,962 scientific abstracts from CODA-19, with block sizes ranging from 5 to 1,000 sentences. The results show that automated models can forecast the follow-up story blocks better than the random, prior, and replay baselines, indicating the task\'s feasibility. We also learn that the models using the frame representation as features outperform all the existing approaches when the block size is over 150 sentences. The human evaluation also shows that the proposed frame representation, when visualized as word clouds, is comprehensible, representative, and specific to humans. Our code is available at: https://github.com/ appleternity/FrameForecasting.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'NLP Applications', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Summarization', 'Ethics and NLP', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07671193778514862, 0.07504095137119293, 0.06379484385251999, 0.06289064139127731, 0.062239840626716614, 0.06036660820245743, 0.05725548416376114, 0.05448124557733536, 0.051213789731264114, 0.049288611859083176, 0.04388454928994179, 0.0415097251534462, 0.037730611860752106, 0.03514210507273674, 0.03364970535039902, 0.03215467184782028, 0.03142286837100983, 0.026263326406478882, 0.025230467319488525, 0.024555975571274757, 0.023692283779382706, 0.022264719009399414, 0.009215048514306545]}",0.07671193778514862,Dialogue and Interactive Systems,0.05725548416376114
NLP Applications,MUSER: MUltimodal Stress detection using Emotion Recognition as an Auxiliary Task,"The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and humancomputer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER -a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-ofthe-art results.","{'sequence': 'The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and humancomputer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER -a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-ofthe-art results.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'Computational Social Science and Social Media', 'Summarization', 'NLP Applications', 'Discourse and Pragmatics', 'Generation', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Machine Learning for NLP', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.09674049913883209, 0.09434081614017487, 0.09200959652662277, 0.06354673206806183, 0.06091167405247688, 0.05845315754413605, 0.052837908267974854, 0.047813531011343, 0.045964889228343964, 0.04534062743186951, 0.042545948177576065, 0.03899487480521202, 0.03701091185212135, 0.03444647789001465, 0.03190961107611656, 0.02889053337275982, 0.027942275628447533, 0.025419389829039574, 0.023365380242466927, 0.023174211382865906, 0.01282036304473877, 0.008103807456791401, 0.007416772190481424]}",0.09674049913883209,Dialogue and Interactive Systems,0.052837908267974854
NLP Applications,Learning to Decompose and Organize Complex Tasks,"People rely on digital task management tools, such as email or to-do apps, to manage their tasks. Some of these tasks are large and complex, leading to action paralysis and feelings of being overwhelmed on the part of the user. The micro-productivity literature has shown that such tasks could benefit from being decomposed and organized, in order to reduce user cognitive load. Thus in this paper, we propose a novel end-to-end pipeline that consumes a complex task and induces a dependency graph from unstructured text to represent sub-tasks and their relationships. Our solution first finds nodes for sub-tasks from multiple 'how-to' articles on the web by injecting a neural text generator with three key desiderata -relevance, abstraction, and consensus. Then we resolve and infer edges between these subtask nodes by learning task dependency relations. We collect a new dataset of complex tasks with their sub-task graph to develop and evaluate our solutions. Both components of our graph induction solution are evaluated in experiments, demonstrating that our models outperform a state-of-the-art text generator significantly. Our generalizable and scalable endto-end solution has important implications for boosting user productivity and assisting with digital task management.","{'sequence': ""People rely on digital task management tools, such as email or to-do apps, to manage their tasks. Some of these tasks are large and complex, leading to action paralysis and feelings of being overwhelmed on the part of the user. The micro-productivity literature has shown that such tasks could benefit from being decomposed and organized, in order to reduce user cognitive load. Thus in this paper, we propose a novel end-to-end pipeline that consumes a complex task and induces a dependency graph from unstructured text to represent sub-tasks and their relationships. Our solution first finds nodes for sub-tasks from multiple 'how-to' articles on the web by injecting a neural text generator with three key desiderata -relevance, abstraction, and consensus. Then we resolve and infer edges between these subtask nodes by learning task dependency relations. We collect a new dataset of complex tasks with their sub-task graph to develop and evaluate our solutions. Both components of our graph induction solution are evaluated in experiments, demonstrating that our models outperform a state-of-the-art text generator significantly. Our generalizable and scalable endto-end solution has important implications for boosting user productivity and assisting with digital task management."", 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Generation', 'Ethics and NLP', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09447965025901794, 0.09445074945688248, 0.07679220288991928, 0.07446411997079849, 0.0674951821565628, 0.06259143352508545, 0.047648292034864426, 0.044983234256505966, 0.04078807309269905, 0.0396045558154583, 0.03612712398171425, 0.03546731919050217, 0.0352594368159771, 0.034835558384656906, 0.034428298473358154, 0.03368985652923584, 0.02543145976960659, 0.022775063291192055, 0.02208597958087921, 0.022070959210395813, 0.019894376397132874, 0.01782272569835186, 0.016814326867461205]}",0.09447965025901794,Dialogue and Interactive Systems,0.04078807309269905
NLP Applications,Continual Learning for Text Classification with Information Disentanglement Based Regularization,"Continual learning has become increasingly important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into representations that are generic to all tasks and representations specific to each individual task, and further regularizes these representations differently to better constrain the knowledge required to generalize. We also introduce two simple auxiliary tasks: next sentence prediction and task-id prediction, for learning better generic and specific representation spaces. Experiments conducted on large-scale benchmarks demonstrate the effectiveness of our method in continual text classification tasks with various sequences and lengths over state-of-the-art baselines. We have publicly released our code at https: //github.com/GT-SALT/IDBR.","{'sequence': 'Continual learning has become increasingly important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into representations that are generic to all tasks and representations specific to each individual task, and further regularizes these representations differently to better constrain the knowledge required to generalize. We also introduce two simple auxiliary tasks: next sentence prediction and task-id prediction, for learning better generic and specific representation spaces. Experiments conducted on large-scale benchmarks demonstrate the effectiveness of our method in continual text classification tasks with various sequences and lengths over state-of-the-art baselines. We have publicly released our code at https: //github.com/GT-SALT/IDBR.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Information Extraction', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Ethics and NLP', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Generation', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.17476172745227814, 0.10095886886119843, 0.07769061625003815, 0.06318141520023346, 0.061193741858005524, 0.044839002192020416, 0.04264536499977112, 0.04220443218946457, 0.04150008410215378, 0.04092136397957802, 0.03908314183354378, 0.03803972527384758, 0.030519159510731697, 0.030019687488675117, 0.027704454958438873, 0.0276706013828516, 0.02203390933573246, 0.0217750184237957, 0.020070374011993408, 0.018669838085770607, 0.01726444810628891, 0.009508092887699604, 0.007744944654405117]}",0.17476172745227814,Machine Learning for NLP,0.10095886886119843
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Learning from Executions for Semantic Parsing,"Semantic parsing aims at translating natural language (NL) utterances onto machineinterpretable programs, which can be executed against a real-world environment. The expensive annotation of utterance-program pairs has long been acknowledged as a major bottleneck for the deployment of contemporary neural models to real-life applications. In this work, we focus on the task of semi-supervised learning where a limited amount of annotated data is available together with many unlabeled NL utterances. Based on the observation that programs which correspond to NL utterances must be always executable, we propose to encourage a parser to generate executable programs for unlabeled utterances. Due to the large search space of executable programs, conventional methods that use approximations based on beam-search such as self-training and top-k marginal likelihood training, do not perform as well. Instead, we view the problem of learning from executions from the perspective of posterior regularization and propose a set of new training objectives. Experimental results on OVERNIGHT and GEOQUERY show that our new objectives outperform conventional methods, bridging the gap between semi-supervised and supervised learning.","{'sequence': 'Semantic parsing aims at translating natural language (NL) utterances onto machineinterpretable programs, which can be executed against a real-world environment. The expensive annotation of utterance-program pairs has long been acknowledged as a major bottleneck for the deployment of contemporary neural models to real-life applications. In this work, we focus on the task of semi-supervised learning where a limited amount of annotated data is available together with many unlabeled NL utterances. Based on the observation that programs which correspond to NL utterances must be always executable, we propose to encourage a parser to generate executable programs for unlabeled utterances. Due to the large search space of executable programs, conventional methods that use approximations based on beam-search such as self-training and top-k marginal likelihood training, do not perform as well. Instead, we view the problem of learning from executions from the perspective of posterior regularization and propose a set of new training objectives. Experimental results on OVERNIGHT and GEOQUERY show that our new objectives outperform conventional methods, bridging the gap between semi-supervised and supervised learning.', 'labels': ['Syntax: Tagging, Chunking and Parsing', 'Generation', 'Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Discourse and Pragmatics', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10968600958585739, 0.07475375384092331, 0.07267102599143982, 0.06809621304273605, 0.0651269480586052, 0.05238892138004303, 0.050674207508563995, 0.04518140107393265, 0.04484003782272339, 0.04366636648774147, 0.03725232556462288, 0.03577440232038498, 0.03400559350848198, 0.032462526112794876, 0.029972977936267853, 0.02943393960595131, 0.02882792055606842, 0.028789155185222626, 0.027652233839035034, 0.027568954974412918, 0.02239074930548668, 0.022160369902849197, 0.01662391610443592]}",0.10968600958585739,"Syntax: Tagging, Chunking and Parsing",0.04518140107393265
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Learning to Synthesize Data for Semantic Parsing,"Synthesizing data for semantic parsing has gained increasing attention recently. However, most methods require handcrafted (highprecision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a generative model which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our generative model can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to a better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-ofdomain settings of text-to-SQL parsing on the standard benchmarks of GEOQUERY and SPI-DER, respectively. Our empirical results show that the synthesized data generated from our model can substantially help a semantic parser achieve better compositional and domain generalization.","{'sequence': 'Synthesizing data for semantic parsing has gained increasing attention recently. However, most methods require handcrafted (highprecision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a generative model which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our generative model can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to a better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-ofdomain settings of text-to-SQL parsing on the standard benchmarks of GEOQUERY and SPI-DER, respectively. Our empirical results show that the synthesized data generated from our model can substantially help a semantic parser achieve better compositional and domain generalization.', 'labels': ['Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Summarization', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11631408333778381, 0.09455369412899017, 0.08261462301015854, 0.06593415886163712, 0.05490069463849068, 0.05110590532422066, 0.04589333385229111, 0.04364798218011856, 0.038335252553224564, 0.03791361674666405, 0.03533247485756874, 0.034418947994709015, 0.03361321985721588, 0.03201528266072273, 0.03181048110127449, 0.030093276873230934, 0.028864558786153793, 0.028834769502282143, 0.02874743565917015, 0.02683989331126213, 0.02485058829188347, 0.020179681479930878, 0.013186134397983551]}",0.11631408333778381,Generation,0.09455369412899017
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Edge: Enriching Knowledge Graph Embeddings with External Text,"Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowledge bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on ""hard"" co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve ""soft"" augmentation by proposing a knowledge graph enrichment and embedding framework named EDGE. Given an original knowledge graph, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a graph alignment term in a shared embedding space between the original and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of EDGE in link prediction and node classification.","{'sequence': 'Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowledge bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on ""hard"" co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve ""soft"" augmentation by proposing a knowledge graph enrichment and embedding framework named EDGE. Given an original knowledge graph, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a graph alignment term in a shared embedding space between the original and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of EDGE in link prediction and node classification.', 'labels': ['Information Extraction', 'Information Retrieval and Text Mining', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Question Answering', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08248793333768845, 0.08129748702049255, 0.07798105478286743, 0.0773717537522316, 0.06785162538290024, 0.06258917599916458, 0.053911808878183365, 0.0521511547267437, 0.04552522674202919, 0.04477609321475029, 0.03843352943658829, 0.03692875802516937, 0.03618916869163513, 0.03569653630256653, 0.03436754271388054, 0.029866134747862816, 0.02938077040016651, 0.025728097185492516, 0.02171792834997177, 0.01984766684472561, 0.01904119737446308, 0.017580155283212662, 0.009279102087020874]}",0.08248793333768845,Information Extraction,0.053911808878183365
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",FLIN: A Flexible Natural Language Interface for Web Navigation,"AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain.","{'sequence': 'AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain.', 'labels': ['Dialogue and Interactive Systems', 'NLP Applications', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Generation', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14473074674606323, 0.10044798254966736, 0.08821482956409454, 0.06961934268474579, 0.04910408705472946, 0.048511531203985214, 0.045695994049310684, 0.042660415172576904, 0.04158886894583702, 0.03883695602416992, 0.03228744491934776, 0.031725771725177765, 0.030954603105783463, 0.029360691085457802, 0.02899846062064171, 0.028589339926838875, 0.028480164706707, 0.025731012225151062, 0.02386869490146637, 0.022483106702566147, 0.021640313789248466, 0.016368603333830833, 0.010100948624312878]}",0.14473074674606323,Dialogue and Interactive Systems,0.028480164706707
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index,"The input vocabulary and their learned representations are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models, with the embedding layer often constituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model's performance. We apply power indices from cooperative game theory, including the Shapley value and Banzhaf index, that measure the relative importance of individual team members in accomplishing a joint task. We approximately compute these indices to identify the most influential words. Our empirical evaluation examines multiple NLP tasks, including sentence and document classification, question answering and textual entailment. We compare to baselines that select words based on frequency, TF-IDF and regression coefficients under L1 regularization, and show that this game-theoretic vocabulary selection outperforms all baselines on a range of different tasks and datasets.","{'sequence': ""The input vocabulary and their learned representations are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models, with the embedding layer often constituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model's performance. We apply power indices from cooperative game theory, including the Shapley value and Banzhaf index, that measure the relative importance of individual team members in accomplishing a joint task. We approximately compute these indices to identify the most influential words. Our empirical evaluation examines multiple NLP tasks, including sentence and document classification, question answering and textual entailment. We compare to baselines that select words based on frequency, TF-IDF and regression coefficients under L1 regularization, and show that this game-theoretic vocabulary selection outperforms all baselines on a range of different tasks and datasets."", 'labels': ['Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Generation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2594761252403259, 0.13584889471530914, 0.08526337146759033, 0.06720723956823349, 0.05662577971816063, 0.043389659374952316, 0.03810763731598854, 0.029207712039351463, 0.026295283809304237, 0.02546822652220726, 0.02456926740705967, 0.022989708930253983, 0.022228647023439407, 0.020446937531232834, 0.02003849856555462, 0.019195683300495148, 0.01885448954999447, 0.01784178800880909, 0.017214585095643997, 0.017153797671198845, 0.014332454651594162, 0.010913379490375519, 0.007330764085054398]}",0.2594761252403259,Question Answering,0.13584889471530914
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Incorporating External Knowledge to Enhance Tabular Reasoning,"Reasoning about tabular information presents unique challenges to modern NLP approaches which largely rely on pre-trained contextualized embeddings of text. In this paper, we study these challenges through the problem of tabular natural language inference. We propose easy and effective modifications to how information is presented to a model for this task. We show via systematic experiments that these strategies substantially improve tabular inference performance.","{'sequence': 'Reasoning about tabular information presents unique challenges to modern NLP approaches which largely rely on pre-trained contextualized embeddings of text. In this paper, we study these challenges through the problem of tabular natural language inference. We propose easy and effective modifications to how information is presented to a model for this task. We show via systematic experiments that these strategies substantially improve tabular inference performance.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Information Extraction', 'Discourse and Pragmatics', 'Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Summarization', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1283445805311203, 0.09251558780670166, 0.06964176148176193, 0.06841118633747101, 0.06490547209978104, 0.06033919379115105, 0.053723566234111786, 0.04755774512887001, 0.04477355629205704, 0.043236277997493744, 0.04189334437251091, 0.038514841347932816, 0.03705942630767822, 0.032511383295059204, 0.032041169703006744, 0.028602777048945427, 0.021818682551383972, 0.01897907443344593, 0.01769420877099037, 0.016532709822058678, 0.016047948971390724, 0.013070850633084774, 0.011784796603024006]}",0.1283445805311203,NLP Applications,0.04189334437251091
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention,"We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.","{'sequence': 'We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Resources and Evaluation', 'Summarization', 'Generation', 'Information Extraction', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0974041074514389, 0.0679389163851738, 0.06392022967338562, 0.06244388967752457, 0.057476889342069626, 0.05598359555006027, 0.05335908383131027, 0.04940500855445862, 0.04637458547949791, 0.046296846121549606, 0.04445755109190941, 0.042362604290246964, 0.04143277928233147, 0.04138646647334099, 0.04133293032646179, 0.033905260264873505, 0.02989053726196289, 0.029410764575004578, 0.02931041643023491, 0.023550305515527725, 0.01759897544980049, 0.013509534299373627, 0.01124861091375351]}",0.0974041074514389,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.0974041074514389
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Domain Adaptation for Arabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding,"Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target domain data. In this paper, we propose a new unsupervised domain adaptation method for Arabic cross-domain and crossdialect sentiment analysis from Contextualized Word Embedding. Several experiments are performed adopting the coarse-grained and the fine-grained taxonomies of Arabic dialects. The obtained results show that our method yields very promising results and outperforms several domain adaptation methods for most of the evaluated datasets. On average, our method increases the performance by an improvement rate of 20.8% over the zero-shot transfer learning from BERT.","{'sequence': 'Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target domain data. In this paper, we propose a new unsupervised domain adaptation method for Arabic cross-domain and crossdialect sentiment analysis from Contextualized Word Embedding. Several experiments are performed adopting the coarse-grained and the fine-grained taxonomies of Arabic dialects. The obtained results show that our method yields very promising results and outperforms several domain adaptation methods for most of the evaluated datasets. On average, our method increases the performance by an improvement rate of 20.8% over the zero-shot transfer learning from BERT.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Resources and Evaluation', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3685513138771057, 0.12202677130699158, 0.06150931119918823, 0.041419561952352524, 0.038829255849123, 0.033465493470430374, 0.03317911922931671, 0.032772257924079895, 0.029496915638446808, 0.02563430555164814, 0.024533217772841454, 0.02250104956328869, 0.020328864455223083, 0.017603302374482155, 0.01664169691503048, 0.01591579243540764, 0.01583576202392578, 0.01530819945037365, 0.01458163931965828, 0.013914462178945541, 0.01384939905256033, 0.012279252521693707, 0.009823202155530453]}",0.3685513138771057,NLP Applications,0.009823202155530453
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Multi-task Learning of Negation and Speculation for Targeted Sentiment Classification,"The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create English-language models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning via language modelling can improve performance on these challenge datasets, but the overall performances indicate that there is still much room for improvement. We release both the datasets and the source code at https://github.com/ jerbarnes/multitask_negation_ for_targeted_sentiment.","{'sequence': 'The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create English-language models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning via language modelling can improve performance on these challenge datasets, but the overall performances indicate that there is still much room for improvement. We release both the datasets and the source code at https://github.com/ jerbarnes/multitask_negation_ for_targeted_sentiment.', 'labels': ['Resources and Evaluation', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09004954993724823, 0.0638304129242897, 0.06374597549438477, 0.06016659736633301, 0.05678822845220566, 0.05622788891196251, 0.05387938395142555, 0.05383925512433052, 0.04989994317293167, 0.046199992299079895, 0.044880736619234085, 0.039117779582738876, 0.03843741491436958, 0.035662468522787094, 0.0343172550201416, 0.033579304814338684, 0.03313230350613594, 0.029668161645531654, 0.029272573068737984, 0.027524761855602264, 0.02260519564151764, 0.020951755344867706, 0.016223028302192688]}",0.09004954993724823,Resources and Evaluation,0.016223028302192688
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification,"Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different parsers. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different parses before applying GNNs over the resulting graph. This allows GNN models to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and overfitting from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble models without adding model parameters.","{'sequence': 'Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different parsers. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different parses before applying GNNs over the resulting graph. This allows GNN models to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and overfitting from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble models without adding model parameters.', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Question Answering', 'Computational Social Science and Social Media', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08033342659473419, 0.07949225604534149, 0.07035120576620102, 0.06944796442985535, 0.06884850561618805, 0.06881795078516006, 0.06801589578390121, 0.05055040493607521, 0.04885898903012276, 0.038744885474443436, 0.03733823820948601, 0.036995768547058105, 0.03407195955514908, 0.03278369456529617, 0.030233675613999367, 0.030179742723703384, 0.02964668720960617, 0.026971299201250076, 0.024666255339980125, 0.021963026374578476, 0.020270533859729767, 0.01972256414592266, 0.011695051565766335]}",0.08033342659473419,Dialogue and Interactive Systems,0.011695051565766335
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Emotion-Infused Models for Explainable Psychological Stress Detection,"The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as emotionbased language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress.","{'sequence': 'The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as emotionbased language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Discourse and Pragmatics', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Generation', 'Information Extraction', 'Question Answering', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12337940186262131, 0.08581937849521637, 0.08052907139062881, 0.05816461890935898, 0.05358976870775223, 0.05136435478925705, 0.05111958459019661, 0.04486734792590141, 0.04458167403936386, 0.04442571476101875, 0.04017408937215805, 0.03834398835897446, 0.03660415858030319, 0.03650348260998726, 0.03608451783657074, 0.030187055468559265, 0.027732063084840775, 0.023411791771650314, 0.02095665968954563, 0.020189814269542694, 0.019981564953923225, 0.017964256927371025, 0.014025747776031494]}",0.12337940186262131,Speech and Multimodality,0.014025747776031494
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble,"It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the graph and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis. 1 * Equal contribution.","{'sequence': 'It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the graph and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis. 1 * Equal contribution.', 'labels': ['Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'NLP Applications', 'Information Extraction', 'Summarization', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Generation', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08304759114980698, 0.07660651206970215, 0.0720154196023941, 0.06760537624359131, 0.05827371031045914, 0.05114288628101349, 0.05065017566084862, 0.04782020300626755, 0.04766630753874779, 0.04579836130142212, 0.044232066720724106, 0.03987108916044235, 0.03952834755182266, 0.039330191910266876, 0.03314808011054993, 0.032602179795503616, 0.03133381903171539, 0.02688286453485489, 0.026676570996642113, 0.02505470998585224, 0.023801149800419807, 0.023633627220988274, 0.013278740458190441]}",0.08304759114980698,Resources and Evaluation,0.013278740458190441
"Syntax: Tagging, Chunking and Parsing",Supertagging-based Parsing with Linear Context-free Rewriting Systems,"We present the first supertagging-based parser for linear context-free rewriting systems (LCFRS). It utilizes neural classifiers and outperforms previous LCFRS-based parsers in both accuracy and parsing speed by a wide margin. Our results keep up with the best (general) discontinuous parsers, particularly the scores for discontinuous constituents establish a new state of the art. The heart of our approach is an efficient lexicalization procedure which induces a lexical LCFRS from any discontinuous treebank. We describe a modification to usual chart-based LCFRS parsing that accounts for supertagging and introduce a procedure that transforms lexical LCFRS derivations into equivalent parse trees of the original treebank. Our approach is evaluated on the English Discontinuous Penn Treebank and the German treebanks Negra and Tiger.","{'sequence': 'We present the first supertagging-based parser for linear context-free rewriting systems (LCFRS). It utilizes neural classifiers and outperforms previous LCFRS-based parsers in both accuracy and parsing speed by a wide margin. Our results keep up with the best (general) discontinuous parsers, particularly the scores for discontinuous constituents establish a new state of the art. The heart of our approach is an efficient lexicalization procedure which induces a lexical LCFRS from any discontinuous treebank. We describe a modification to usual chart-based LCFRS parsing that accounts for supertagging and introduce a procedure that transforms lexical LCFRS derivations into equivalent parse trees of the original treebank. Our approach is evaluated on the English Discontinuous Penn Treebank and the German treebanks Negra and Tiger.', 'labels': ['Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Computational Social Science and Social Media', 'Question Answering', 'Generation', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Summarization', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.1386120766401291, 0.11782396584749222, 0.06856818497180939, 0.05639353394508362, 0.05617757886648178, 0.04379308223724365, 0.04350617527961731, 0.04332783445715904, 0.04139035940170288, 0.039959054440259933, 0.038478441536426544, 0.0379907600581646, 0.03446383401751518, 0.03358319774270058, 0.032867785543203354, 0.025879185646772385, 0.025098655372858047, 0.024184612557291985, 0.022352781146764755, 0.021077563986182213, 0.018236206844449043, 0.018204692751169205, 0.018030358478426933]}",0.1386120766401291,Resources and Evaluation,0.11782396584749222
"Syntax: Tagging, Chunking and Parsing",Outside Computation with Superior Functions,"We show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a subexponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH).","{'sequence': 'We show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a subexponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH).', 'labels': ['Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Speech and Multimodality', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Summarization', 'Generation', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'NLP Applications', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP'], 'scores': [0.10589423030614853, 0.07925592362880707, 0.07045117765665054, 0.06431504338979721, 0.06191355362534523, 0.05285985767841339, 0.05277485400438309, 0.04814428463578224, 0.047844450920820236, 0.045297443866729736, 0.04142466560006142, 0.04042235016822815, 0.036910101771354675, 0.03618939593434334, 0.034367598593235016, 0.030434712767601013, 0.029916761443018913, 0.02629036270081997, 0.024982774630188942, 0.01970924809575081, 0.018565518781542778, 0.017027975991368294, 0.015007773414254189]}",0.10589423030614853,Dialogue and Interactive Systems,0.05285985767841339
"Syntax: Tagging, Chunking and Parsing",Learning Syntax from Naturally-Occurring Bracketings,"Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments demonstrate that our distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems. On the English WSJ corpus, our models achieve an unlabeled F1 score of 68.9 for constituency parsing. 1","{'sequence': 'Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments demonstrate that our distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems. On the English WSJ corpus, our models achieve an unlabeled F1 score of 68.9 for constituency parsing. 1', 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Information Extraction', 'Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0939793735742569, 0.08035755157470703, 0.07347896695137024, 0.07212623208761215, 0.0600467175245285, 0.05199642479419708, 0.05191517993807793, 0.049076568335294724, 0.045566458255052567, 0.038426127284765244, 0.037783749401569366, 0.03748701512813568, 0.03701293095946312, 0.03374441713094711, 0.03143685311079025, 0.030575336888432503, 0.03047773614525795, 0.030165188014507294, 0.028779203072190285, 0.025984250009059906, 0.024815229699015617, 0.02047039195895195, 0.014298193156719208]}",0.0939793735742569,Dialogue and Interactive Systems,0.049076568335294724
Dialogue and Interactive Systems,Bot-Adversarial Dialogue for Safe Conversational Agents,"Warning: this paper contains example data that may be offensive or upsetting. Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which may include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-theloop framework in a two-stage system, or ""baking-in"" safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2 (Radford et al., 2019) , DialoGPT (Zhang et al., 2019)  and BlenderBot (Roller et al., 2020).","{'sequence': 'Warning: this paper contains example data that may be offensive or upsetting. Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which may include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-theloop framework in a two-stage system, or ""baking-in"" safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2 (Radford et al., 2019) , DialoGPT (Zhang et al., 2019)  and BlenderBot (Roller et al., 2020).', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Question Answering', 'Generation', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Information Extraction', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09853661805391312, 0.09370788186788559, 0.08367373049259186, 0.060983870178461075, 0.05123481899499893, 0.05029112100601196, 0.04438168182969093, 0.042891599237918854, 0.042440466582775116, 0.03983965143561363, 0.03739294782280922, 0.03650123253464699, 0.03614314645528793, 0.036113690584897995, 0.03496300056576729, 0.03067619726061821, 0.028738541528582573, 0.028271326795220375, 0.027830762788653374, 0.02766798436641693, 0.023767296224832535, 0.023662973195314407, 0.020289365202188492]}",0.09853661805391312,Resources and Evaluation,0.09370788186788559
Dialogue and Interactive Systems,Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog,"Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining nonautoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models. Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets. Our code is available at https://github. com/facebookresearch/pytext.","{'sequence': 'Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining nonautoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models. Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets. Our code is available at https://github. com/facebookresearch/pytext.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Generation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Question Answering', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Summarization', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07154986262321472, 0.07084431499242783, 0.06837634742259979, 0.06055247783660889, 0.05939316004514694, 0.05841728672385216, 0.056902192533016205, 0.05542059987783432, 0.05482405424118042, 0.049074072390794754, 0.04800163581967354, 0.046935491263866425, 0.04254894703626633, 0.04003461077809334, 0.03402915224432945, 0.03308180347084999, 0.03246912732720375, 0.029727235436439514, 0.02866123430430889, 0.0259100291877985, 0.017290204763412476, 0.010561253875494003, 0.005394888576120138]}",0.07154986262321472,Dialogue and Interactive Systems,0.07154986262321472
Dialogue and Interactive Systems,Example-Driven Intent Prediction with Observers,"A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2) example-driven training. Prior work has shown that BERT-like models tend to attribute a significant amount of attention to the [CLS] token, which we hypothesize results in diluted representations. Observers are tokens that are not attended to, and are an alternative to the [CLS] token as a semantic representation of utterances. Example-driven training learns to classify utterances by comparing to examples, thereby using the underlying encoder as a sentence similarity model. These methods are complementary; improving the representation through observers allows the example-driven model to better measure sentence similarities. When combined, the proposed methods attain state-of-the-art results on three intent prediction datasets (BANKING77, CLINC150, HWU64) in both the full data and few-shot (10 examples per intent) settings. Furthermore, we demonstrate that the proposed approach can transfer to new intents and across datasets without any additional training.","{'sequence': 'A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2) example-driven training. Prior work has shown that BERT-like models tend to attribute a significant amount of attention to the [CLS] token, which we hypothesize results in diluted representations. Observers are tokens that are not attended to, and are an alternative to the [CLS] token as a semantic representation of utterances. Example-driven training learns to classify utterances by comparing to examples, thereby using the underlying encoder as a sentence similarity model. These methods are complementary; improving the representation through observers allows the example-driven model to better measure sentence similarities. When combined, the proposed methods attain state-of-the-art results on three intent prediction datasets (BANKING77, CLINC150, HWU64) in both the full data and few-shot (10 examples per intent) settings. Furthermore, we demonstrate that the proposed approach can transfer to new intents and across datasets without any additional training.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Semantics: Lexical Semantics', 'Summarization', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'NLP Applications', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Generation', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08827253431081772, 0.0752992108464241, 0.07334326207637787, 0.06427818536758423, 0.06216684728860855, 0.05343586206436157, 0.05265207588672638, 0.05059961974620819, 0.04882418364286423, 0.04425980523228645, 0.04405112937092781, 0.038256704807281494, 0.03797377645969391, 0.03593844920396805, 0.032928671687841415, 0.030860958620905876, 0.030145714059472084, 0.02963600866496563, 0.02732427977025509, 0.02307959645986557, 0.02199465222656727, 0.019933881238102913, 0.01474456861615181]}",0.08827253431081772,Dialogue and Interactive Systems,0.08827253431081772
Dialogue and Interactive Systems,Imperfect also Deserves Reward: Multi-Level and Sequential Reward Modeling for Better Dialog Management,"For task-oriented dialog systems, training a Reinforcement Learning (RL) based Dialog Management module suffers from low sample efficiency and slow convergence speed due to the sparse rewards in RL. To solve this problem, many strategies have been proposed to give proper rewards when training RL, but their rewards lack interpretability and cannot accurately estimate the distribution of stateaction pairs in real dialogs. In this paper, we propose a multi-level reward modeling approach that factorizes a reward into a threelevel hierarchy: domain, act, and slot. Based on inverse adversarial reinforcement learning, our designed reward model can provide more accurate and explainable reward signals for state-action pairs. Extensive evaluations show that our approach can be applied to a wide range of reinforcement learning-based dialog systems and significantly improves both the performance and the speed of convergence.","{'sequence': 'For task-oriented dialog systems, training a Reinforcement Learning (RL) based Dialog Management module suffers from low sample efficiency and slow convergence speed due to the sparse rewards in RL. To solve this problem, many strategies have been proposed to give proper rewards when training RL, but their rewards lack interpretability and cannot accurately estimate the distribution of stateaction pairs in real dialogs. In this paper, we propose a multi-level reward modeling approach that factorizes a reward into a threelevel hierarchy: domain, act, and slot. Based on inverse adversarial reinforcement learning, our designed reward model can provide more accurate and explainable reward signals for state-action pairs. Extensive evaluations show that our approach can be applied to a wide range of reinforcement learning-based dialog systems and significantly improves both the performance and the speed of convergence.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'NLP Applications', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Summarization', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining'], 'scores': [0.08907746523618698, 0.0763203576207161, 0.06919094175100327, 0.05523120239377022, 0.05474191531538963, 0.05017048120498657, 0.049021802842617035, 0.04570427164435387, 0.045312460511922836, 0.04434022307395935, 0.044069740921258926, 0.04050238057971001, 0.0386803075671196, 0.03711820766329765, 0.035861268639564514, 0.03318950533866882, 0.03233709931373596, 0.03149547055363655, 0.03048928827047348, 0.02691609598696232, 0.024592136964201927, 0.023840315639972687, 0.021797161549329758]}",0.08907746523618698,Resources and Evaluation,0.0763203576207161
Dialogue and Interactive Systems,Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems,"Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8% absolute accuracy) still exists to reach human-level performance on ABCD. 1","{'sequence': 'Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8% absolute accuracy) still exists to reach human-level performance on ABCD. 1', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Ethics and NLP', 'NLP Applications', 'Information Extraction', 'Summarization', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Question Answering'], 'scores': [0.12386559695005417, 0.07225623726844788, 0.06664736568927765, 0.05832147225737572, 0.05792422965168953, 0.05137614905834198, 0.04656201973557472, 0.04579617455601692, 0.040357768535614014, 0.039714690297842026, 0.038656871765851974, 0.03678377717733383, 0.03640871122479439, 0.03635229915380478, 0.033844735473394394, 0.03334508836269379, 0.03227115422487259, 0.02843918465077877, 0.026874667033553123, 0.026484280824661255, 0.02556408941745758, 0.023636505007743835, 0.018516937270760536]}",0.12386559695005417,Dialogue and Interactive Systems,0.12386559695005417
Dialogue and Interactive Systems,Controlling Dialogue Generation with Semantic Exemplars,"Dialogue systems pretrained with large language models generate locally coherent responses, but lack the fine-grained control over responses necessary to achieve specific goals. A promising method for controlling generated responses is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue GEneration model, EDGE, that uses the semantic frames present in exemplar responses to guide response generation. We show that controlling dialogue generation based on the semantic frames of exemplars improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar re-1 sponses.","{'sequence': 'Dialogue systems pretrained with large language models generate locally coherent responses, but lack the fine-grained control over responses necessary to achieve specific goals. A promising method for controlling generated responses is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue GEneration model, EDGE, that uses the semantic frames present in exemplar responses to guide response generation. We show that controlling dialogue generation based on the semantic frames of exemplars improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar re-1 sponses.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Extraction', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'NLP Applications', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.25756189227104187, 0.11922703683376312, 0.07244624197483063, 0.05166928097605705, 0.04847737029194832, 0.04482264444231987, 0.0420098751783371, 0.03742356598377228, 0.03406454995274544, 0.03405201435089111, 0.030431659892201424, 0.029217049479484558, 0.027096420526504517, 0.023126058280467987, 0.02258487604558468, 0.022145776078104973, 0.021927200257778168, 0.01754912920296192, 0.017219550907611847, 0.016908226534724236, 0.01650840789079666, 0.009155570529401302, 0.004375559743493795]}",0.25756189227104187,Generation,0.11922703683376312
Information Retrieval and Text Mining,COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List,"Classical information retrieval systems such as BM25 rely on exact lexical match and carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft semantic matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture that brings semantic lexical matching. COIL scoring is based on overlapping query document tokens' contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency. 1","{'sequence': ""Classical information retrieval systems such as BM25 rely on exact lexical match and carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft semantic matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture that brings semantic lexical matching. COIL scoring is based on overlapping query document tokens' contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency. 1"", 'labels': ['Information Extraction', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Resources and Evaluation', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Summarization', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14178630709648132, 0.08387263119220734, 0.08088760077953339, 0.06361421197652817, 0.05683792382478714, 0.05445276200771332, 0.04604563117027283, 0.041347160935401917, 0.03902054950594902, 0.03856261819601059, 0.037188928574323654, 0.035303156822919846, 0.034710220992565155, 0.032697904855012894, 0.031161509454250336, 0.028327634558081627, 0.02726934663951397, 0.0260743647813797, 0.025422919541597366, 0.024722957983613014, 0.022415796294808388, 0.01560008991509676, 0.012677783146500587]}",0.14178630709648132,Information Extraction,0.03902054950594902
Information Retrieval and Text Mining,X-Class: Text Classification with Extremely Weak Supervision,"In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective-ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations should be adaptive to the given class names. We propose a novel framework X-Class to realize the adaptive representations. Specifically, we first estimate class representations by incrementally adding the most similar word to each class until inconsistency arises. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. With the prior of each document assigned to its nearest class, we then cluster and align the documents to classes. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets.","{'sequence': 'In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective-ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations should be adaptive to the given class names. We propose a novel framework X-Class to realize the adaptive representations. Specifically, we first estimate class representations by incrementally adding the most similar word to each class until inconsistency arises. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. With the prior of each document assigned to its nearest class, we then cluster and align the documents to classes. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets.', 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Summarization', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'NLP Applications', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11412771791219711, 0.07570915669202805, 0.06278945505619049, 0.0545593798160553, 0.052378907799720764, 0.0523311011493206, 0.04816439002752304, 0.04568378999829292, 0.04339443892240524, 0.04335690289735794, 0.041529517620801926, 0.03953311964869499, 0.03718806058168411, 0.032359275966882706, 0.032187312841415405, 0.031052768230438232, 0.030416812747716904, 0.030347924679517746, 0.029351726174354553, 0.029304534196853638, 0.02890688180923462, 0.02483317442238331, 0.020493704825639725]}",0.11412771791219711,Dialogue and Interactive Systems,0.03953311964869499
Information Retrieval and Text Mining,Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling,"Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic modeling. Thus, we propose several methods for fine-tuning encoders to improve both monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on auxiliary tasks, constructing a new topic classification task, integrating the topic classification objective directly into topic model training, and continued pre-training. We find that fine-tuning encoder representations on topic classification and integrating the topic classification task directly into topic modeling improves topic quality, and that fine-tuning encoder representations on any task is the most important factor for facilitating cross-lingual transfer.","{'sequence': 'Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic modeling. Thus, we propose several methods for fine-tuning encoders to improve both monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on auxiliary tasks, constructing a new topic classification task, integrating the topic classification objective directly into topic model training, and continued pre-training. We find that fine-tuning encoder representations on topic classification and integrating the topic classification task directly into topic modeling improves topic quality, and that fine-tuning encoder representations on any task is the most important factor for facilitating cross-lingual transfer.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Question Answering', 'Generation', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'NLP Applications', 'Information Extraction', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09321049600839615, 0.08883003890514374, 0.07191526889801025, 0.06700346618890762, 0.057633426040410995, 0.05673627555370331, 0.054886605590581894, 0.05393434688448906, 0.0473666712641716, 0.046533990651369095, 0.04066235572099686, 0.037342000752687454, 0.03401341289281845, 0.030265673995018005, 0.030229495838284492, 0.030200764536857605, 0.029015837237238884, 0.027680430561304092, 0.026397164911031723, 0.025622930377721786, 0.02389681525528431, 0.01666274666786194, 0.009959734044969082]}",0.09321049600839615,Speech and Multimodality,0.01666274666786194
Information Retrieval and Text Mining,"Exploring the Relationship Between Algorithm Performance, Vocabulary, and Run-Time in Text Classification","Text classification is a significant branch of natural language processing, and has many applications including document classification and sentiment analysis. Unsurprisingly, those who do text classification are concerned with the run-time of their algorithms, many of which depend on the size of the corpus' vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and accuracy, none have examined how these methods affect a model's run-time. To fill this gap, we provide a comprehensive study that examines how preprocessing techniques affect the vocabulary size, model performance, and model run-time, evaluating ten techniques over four models and two datasets. We show that some individual methods can reduce run-time with no loss of accuracy, while some combinations of methods can trade 2-5% of the accuracy for up to a 65% reduction of run-time. Furthermore, some combinations of preprocessing techniques can even provide a 15% reduction in run-time while simultaneously improving model accuracy. 1 Group Method Vocab Size ↓ Train Time ↓ Test Time ↓ Accuracy ↑ stop 99.8 ± 0.2 69.5 ± 1.4 79.0 ± 3.5 97.4 ± 2.2 rare 1.0 ± 0.0 80.6 ± 3.0 70.3 ± 3.2 99.3 ± 2.8 seg 24.6 ± 0.2 93.7 ± 2.4 80.3 ± 2.5 100.6 ± 1.6 spell 57.8 ± 0.2 95.1 ± 2.6 89.7 ± 2.6 99.4 ± 2.3 Individual Methods hash 10.1 ± 0.0 97.1 ± 4.0 75.7 ± 4.0 99.2 ± 1.1 nopunct 61.9 ± 0.2 97.5 ± 2.2 89.5 ± 2.0 100.","{'sequence': ""Text classification is a significant branch of natural language processing, and has many applications including document classification and sentiment analysis. Unsurprisingly, those who do text classification are concerned with the run-time of their algorithms, many of which depend on the size of the corpus' vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and accuracy, none have examined how these methods affect a model's run-time. To fill this gap, we provide a comprehensive study that examines how preprocessing techniques affect the vocabulary size, model performance, and model run-time, evaluating ten techniques over four models and two datasets. We show that some individual methods can reduce run-time with no loss of accuracy, while some combinations of methods can trade 2-5% of the accuracy for up to a 65% reduction of run-time. Furthermore, some combinations of preprocessing techniques can even provide a 15% reduction in run-time while simultaneously improving model accuracy. 1 Group Method Vocab Size ↓ Train Time ↓ Test Time ↓ Accuracy ↑ stop 99.8 ± 0.2 69.5 ± 1.4 79.0 ± 3.5 97.4 ± 2.2 rare 1.0 ± 0.0 80.6 ± 3.0 70.3 ± 3.2 99.3 ± 2.8 seg 24.6 ± 0.2 93.7 ± 2.4 80.3 ± 2.5 100.6 ± 1.6 spell 57.8 ± 0.2 95.1 ± 2.6 89.7 ± 2.6 99.4 ± 2.3 Individual Methods hash 10.1 ± 0.0 97.1 ± 4.0 75.7 ± 4.0 99.2 ± 1.1 nopunct 61.9 ± 0.2 97.5 ± 2.2 89.5 ± 2.0 100."", 'labels': ['NLP Applications', 'Ethics and NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Question Answering', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0955585464835167, 0.09245447814464569, 0.08741769194602966, 0.058910470455884933, 0.05162319913506508, 0.04947926476597786, 0.04819682240486145, 0.04376108944416046, 0.0422976016998291, 0.038634274154901505, 0.03747125715017319, 0.036937177181243896, 0.03608893230557442, 0.032349396497011185, 0.03230084106326103, 0.03003724105656147, 0.02833656221628189, 0.02829561196267605, 0.027792885899543762, 0.027415167540311813, 0.026026630774140358, 0.024533463642001152, 0.02408137172460556]}",0.0955585464835167,NLP Applications,0.027415167540311813
Information Retrieval and Text Mining,Faithfully Explainable Recommendation via Neural Logic Reasoning,"Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decisionmaking process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural logic reasoning for explainable recommendation (LOGER) by drawing on interpretable logical rules to guide the pathreasoning process for explanation generation. We experiment on three large-scale datasets in the e-commerce domain, demonstrating the effectiveness of our method in delivering highquality recommendations as well as ascertaining the faithfulness of the derived explanation.","{'sequence': 'Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decisionmaking process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural logic reasoning for explainable recommendation (LOGER) by drawing on interpretable logical rules to guide the pathreasoning process for explanation generation. We experiment on three large-scale datasets in the e-commerce domain, demonstrating the effectiveness of our method in delivering highquality recommendations as well as ascertaining the faithfulness of the derived explanation.', 'labels': ['Resources and Evaluation', 'Generation', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Speech and Multimodality', 'Ethics and NLP', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Information Extraction', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining'], 'scores': [0.113228440284729, 0.09020835906267166, 0.08421947807073593, 0.06507211178541183, 0.06242920830845833, 0.05733842775225639, 0.05143645778298378, 0.04678279533982277, 0.042890992015600204, 0.0428185909986496, 0.041465140879154205, 0.03816372528672218, 0.036062560975551605, 0.03566017374396324, 0.034086957573890686, 0.0334891714155674, 0.03327880799770355, 0.018723754212260246, 0.01540480274707079, 0.01499231532216072, 0.014761491678655148, 0.014747083187103271, 0.01273919828236103]}",0.113228440284729,Resources and Evaluation,0.01273919828236103
Information Retrieval and Text Mining,You Sound Like Someone Who Watches Drama Movies: Towards Predicting Movie Preferences from Conversational Interactions,"The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation: mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user's interest vector, and adapting collaborative filtering techniques to estimate the current user's preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user's sentiment towards an entity from the conversation context, and 2) transforms the ratings of ""similar"" external reviewers to predict the current user's preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent . Our results demonstrate that Con-vExtr can improve the accuracy of predicting users' ratings for new movies by exploiting conversation content and external data.","{'sequence': 'The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation: mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user\'s interest vector, and adapting collaborative filtering techniques to estimate the current user\'s preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user\'s sentiment towards an entity from the conversation context, and 2) transforms the ratings of ""similar"" external reviewers to predict the current user\'s preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent . Our results demonstrate that Con-vExtr can improve the accuracy of predicting users\' ratings for new movies by exploiting conversation content and external data.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Summarization', 'NLP Applications', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12058644741773605, 0.11723998188972473, 0.08772091567516327, 0.07896827161312103, 0.07318637520074844, 0.05774226039648056, 0.05635841563344002, 0.053698182106018066, 0.03816826641559601, 0.03747514635324478, 0.03579948842525482, 0.028215711936354637, 0.027416571974754333, 0.024400563910603523, 0.023680899292230606, 0.022922975942492485, 0.02125181443989277, 0.020126253366470337, 0.018485531210899353, 0.018108291551470757, 0.013782423920929432, 0.012448931112885475, 0.012216323055326939]}",0.12058644741773605,Resources and Evaluation,0.02125181443989277
"Language Grounding to Vision, Robotics and Beyond",Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents,"Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including ZORK I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.","{'sequence': 'Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including ZORK I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Computational Social Science and Social Media', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Information Extraction', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07506115734577179, 0.06883984804153442, 0.06638254225254059, 0.06584914773702621, 0.05730597302317619, 0.05352453887462616, 0.05246793478727341, 0.04842056706547737, 0.047740574926137924, 0.047005124390125275, 0.04492863640189171, 0.044218286871910095, 0.03506913781166077, 0.03490075841546059, 0.033142298460006714, 0.03291436284780502, 0.03232768550515175, 0.03225347772240639, 0.029300816357135773, 0.029167182743549347, 0.027062805369496346, 0.023710452020168304, 0.018406786024570465]}",0.07506115734577179,Dialogue and Interactive Systems,0.03490075841546059
"Language Grounding to Vision, Robotics and Beyond",SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency,"Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world -they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the reasoning question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant subquestions higher than irrelevant questions for an <image, reasoning-question> pair. We show that SOrT improves model consistency by up to 6.5% points over existing approaches, while also improving visual grounding and robustness to rephrasings of questions.","{'sequence': 'Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world -they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the reasoning question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant subquestions higher than irrelevant questions for an <image, reasoning-question> pair. We show that SOrT improves model consistency by up to 6.5% points over existing approaches, while also improving visual grounding and robustness to rephrasings of questions.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Lexical Semantics', 'Generation', 'NLP Applications', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.15436024963855743, 0.09727692604064941, 0.070278599858284, 0.06573375314474106, 0.04975241795182228, 0.04942144453525543, 0.044203661382198334, 0.03950023278594017, 0.03858426958322525, 0.037278469651937485, 0.03632798790931702, 0.03473516181111336, 0.03238547593355179, 0.03216886520385742, 0.031359702348709106, 0.03131401911377907, 0.02811935544013977, 0.02680620364844799, 0.02495553158223629, 0.021920038387179375, 0.021741347387433052, 0.016512461006641388, 0.015263871289789677]}",0.15436024963855743,Question Answering,0.02680620364844799
"Language Grounding to Vision, Robotics and Beyond",Semi-Supervised Policy Initialization for Playing Games with Language Hints,"Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI not only helps to learn faster (1.2x) but also has a higher success rate (11% relative improvement) of the final policy.","{'sequence': 'Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI not only helps to learn faster (1.2x) but also has a higher success rate (11% relative improvement) of the final policy.', 'labels': ['Dialogue and Interactive Systems', 'Ethics and NLP', 'NLP Applications', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Generation', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Information Extraction', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10858576744794846, 0.0834970474243164, 0.08219192177057266, 0.07301256060600281, 0.06325167417526245, 0.061764128506183624, 0.05216093361377716, 0.04855751991271973, 0.04541097581386566, 0.042948123067617416, 0.03686447814106941, 0.032434333115816116, 0.03142368048429489, 0.03097042627632618, 0.02986897900700569, 0.027860406786203384, 0.027619101107120514, 0.025012778118252754, 0.023958241567015648, 0.021690718829631805, 0.01805725134909153, 0.017191065475344658, 0.015667904168367386]}",0.10858576744794846,Dialogue and Interactive Systems,0.032434333115816116
"Language Grounding to Vision, Robotics and Beyond",Revisiting Document Representations for Large-Scale Zero-Shot Learning,"Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences, after a novel weighting scheme to distinguish similar classes, essentially form semantic representations like visual attributes but need much less human effort. On the Ima-geNet dataset with over 10,000 unseen classes, our representations lead to a 64% relative improvement against the commonly used ones.","{'sequence': 'Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences, after a novel weighting scheme to distinguish similar classes, essentially form semantic representations like visual attributes but need much less human effort. On the Ima-geNet dataset with over 10,000 unseen classes, our representations lead to a 64% relative improvement against the commonly used ones.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'NLP Applications', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10486240684986115, 0.07180734723806381, 0.062258075922727585, 0.058598119765520096, 0.055923134088516235, 0.05382484942674637, 0.052142538130283356, 0.05071967840194702, 0.048708103597164154, 0.04692770540714264, 0.04376114904880524, 0.040757205337285995, 0.04014754295349121, 0.0346914567053318, 0.0345134362578392, 0.031021589413285255, 0.030718639492988586, 0.02777964435517788, 0.02775816060602665, 0.024652669206261635, 0.02237258478999138, 0.019941100850701332, 0.016112735494971275]}",0.10486240684986115,Information Extraction,0.031021589413285255
Resources and Evaluation,Negative language transfer in learner English: A new dataset,"Automatic personalized corrective feedback can help language learners from different backgrounds better acquire a new language. This paper introduces a learner English dataset in which learner errors are accompanied by information about possible error sources. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset will enable second language acquisition researchers to computationally analyze a large quantity of learner errors that are related to language transfer from the learners' first language. The dataset can also be applied in personalizing grammatical error correction systems according to the learners' first language and in providing feedback that is informed by the cause of an error.","{'sequence': ""Automatic personalized corrective feedback can help language learners from different backgrounds better acquire a new language. This paper introduces a learner English dataset in which learner errors are accompanied by information about possible error sources. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset will enable second language acquisition researchers to computationally analyze a large quantity of learner errors that are related to language transfer from the learners' first language. The dataset can also be applied in personalizing grammatical error correction systems according to the learners' first language and in providing feedback that is informed by the cause of an error."", 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Machine Translation and Multilinguality', 'Generation', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Information Retrieval and Text Mining', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09565658867359161, 0.08088240027427673, 0.07447810471057892, 0.06671171635389328, 0.05392910912632942, 0.0519893541932106, 0.04982631281018257, 0.04875356703996658, 0.04823576286435127, 0.046565499156713486, 0.04613853618502617, 0.04478078335523605, 0.039203789085149765, 0.038442086428403854, 0.03330209106206894, 0.030345270410180092, 0.028899969533085823, 0.027791503816843033, 0.024095667526125908, 0.023337965831160545, 0.01861373707652092, 0.0154509786516428, 0.012569168582558632]}",0.09565658867359161,Speech and Multimodality,0.07447810471057892
Resources and Evaluation,SentSim: Crosslingual Semantic Evaluation of Machine Translation,"Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approachesnamely BERTScore and Word Mover's Distance -by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information.","{'sequence': ""Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approachesnamely BERTScore and Word Mover's Distance -by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information."", 'labels': ['Machine Translation and Multilinguality', 'Information Extraction', 'Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Generation', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Retrieval and Text Mining', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2151724398136139, 0.09260059148073196, 0.08677228540182114, 0.06412912160158157, 0.0542263463139534, 0.04860515892505646, 0.04101162403821945, 0.03470342233777046, 0.0334663949906826, 0.033330488950014114, 0.03223367780447006, 0.030809195712208748, 0.029319828376173973, 0.027585094794631004, 0.027046220377087593, 0.02335965819656849, 0.02286382205784321, 0.022724950686097145, 0.019436189904808998, 0.018793322145938873, 0.018719656392931938, 0.014972739852964878, 0.008117818273603916]}",0.2151724398136139,Machine Translation and Multilinguality,0.08677228540182114
Resources and Evaluation,Quality Estimation for Image Captions Based on Large-scale Human Evaluations,"Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and without access to groundtruth references, so that it can be applied at prediction time to detect low-quality captions produced on previously unseen images. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out lowquality image captions, thereby improving the user experience from captioning systems.","{'sequence': 'Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and without access to groundtruth references, so that it can be applied at prediction time to detect low-quality captions produced on previously unseen images. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out lowquality image captions, thereby improving the user experience from captioning systems.', 'labels': ['Resources and Evaluation', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Information Extraction', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Summarization', 'Speech and Multimodality', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09039708971977234, 0.07534381002187729, 0.06802123039960861, 0.0657721534371376, 0.06545260548591614, 0.06506974995136261, 0.062150225043296814, 0.04835477098822594, 0.04574265703558922, 0.04520947113633156, 0.041611623018980026, 0.040574681013822556, 0.037151038646698, 0.036303821951150894, 0.03628312051296234, 0.031001169234514236, 0.023899409919977188, 0.02380620874464512, 0.022349955514073372, 0.02227482944726944, 0.020739244297146797, 0.01952247880399227, 0.012968662194907665]}",0.09039708971977234,Resources and Evaluation,0.09039708971977234
Resources and Evaluation,CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems,"Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closeddomain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https:// github.com/kushalchawla/CaSiNo.","{'sequence': 'Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closeddomain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https:// github.com/kushalchawla/CaSiNo.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11674025654792786, 0.09028830379247665, 0.08073612302541733, 0.07830990850925446, 0.04393427073955536, 0.042736101895570755, 0.03712322935461998, 0.0370192676782608, 0.036632563918828964, 0.03644612058997154, 0.03602399304509163, 0.035955287516117096, 0.03547706827521324, 0.035165753215551376, 0.034365661442279816, 0.03304522857069969, 0.031816449016332626, 0.03157144784927368, 0.029830986633896828, 0.02954799495637417, 0.0264972485601902, 0.023436637595295906, 0.017300020903348923]}",0.11674025654792786,Dialogue and Interactive Systems,0.08073612302541733
Resources and Evaluation,News Headline Grouping as a Challenging NLU Task,"Recent progress in Natural Language Understanding (NLU) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze highperforming models with consistency tests, and find that models are not consistent in their predictions, revealing modeling limits of current architectures.","{'sequence': 'Recent progress in Natural Language Understanding (NLU) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze highperforming models with consistency tests, and find that models are not consistent in their predictions, revealing modeling limits of current architectures.', 'labels': ['Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Information Extraction', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2341914027929306, 0.08231443166732788, 0.07383657246828079, 0.0636531189084053, 0.06331171095371246, 0.059918660670518875, 0.04497866705060005, 0.03217575326561928, 0.031897176057100296, 0.03178111091256142, 0.030566465109586716, 0.028025582432746887, 0.02788865752518177, 0.02626408077776432, 0.02563181333243847, 0.023918401449918747, 0.02287430316209793, 0.02023054100573063, 0.020168239250779152, 0.01815931312739849, 0.017488596960902214, 0.010636993683874607, 0.010088400915265083]}",0.2341914027929306,Machine Learning for NLP,0.059918660670518875
Machine Learning for NLP,Grouping Words with Semantic Diversity,"Deep Learning-based NLP systems can be sensitive to unseen tokens and hard to learn with high-dimensional inputs, which critically hinder learning generalization. We introduce an approach by grouping input words based on their semantic diversity to simplify input language representation with low ambiguity. Since the semantically diverse words reside in different contexts, we are able to substitute words with their groups and still distinguish word meanings relying on their contexts. We design several algorithms that compute diverse groupings based on random sampling, geometric distances, and entropy maximization, and we prove formal guarantees for the entropy-based algorithms. Experimental results show that our methods generalize NLP models and demonstrate enhanced accuracy on POS tagging and LM tasks and significant improvements on medium-scale machine translation tasks, up to +6.5 BLEU points. Our source code is available at https://github.com/abdulrafae/dg.","{'sequence': 'Deep Learning-based NLP systems can be sensitive to unseen tokens and hard to learn with high-dimensional inputs, which critically hinder learning generalization. We introduce an approach by grouping input words based on their semantic diversity to simplify input language representation with low ambiguity. Since the semantically diverse words reside in different contexts, we are able to substitute words with their groups and still distinguish word meanings relying on their contexts. We design several algorithms that compute diverse groupings based on random sampling, geometric distances, and entropy maximization, and we prove formal guarantees for the entropy-based algorithms. Experimental results show that our methods generalize NLP models and demonstrate enhanced accuracy on POS tagging and LM tasks and significant improvements on medium-scale machine translation tasks, up to +6.5 BLEU points. Our source code is available at https://github.com/abdulrafae/dg.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Generation', 'Computational Social Science and Social Media', 'Question Answering', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2356100231409073, 0.07380875200033188, 0.06573659926652908, 0.04279809817671776, 0.04144781082868576, 0.040994301438331604, 0.04022219032049179, 0.039792511612176895, 0.03732531890273094, 0.03706027567386627, 0.036820992827415466, 0.03523288294672966, 0.034366585314273834, 0.03291286155581474, 0.030391573905944824, 0.029256489127874374, 0.027037324383854866, 0.024392567574977875, 0.02292989008128643, 0.022382568567991257, 0.020330531522631645, 0.0181819386780262, 0.010967916809022427]}",0.2356100231409073,Machine Learning for NLP,0.2356100231409073
Machine Learning for NLP,Noise Stability Regularization for Improving BERT Fine-tuning,"Fine-tuning pre-trained language models such as BERT has become a common practice dominating leaderboards across various NLP tasks. Despite its recent success and wide adoption, this process is unstable when there are only a small number of training samples available. The brittleness of this process is often reflected by the sensitivity to random seeds. In this paper, we propose to tackle this problem based on the noise stability property of deep nets, which is investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020) . Specifically, we introduce a novel and effective regularization method to improve fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability Regularization (LNSR). We extend the theories about adding noise to the input and prove that our method gives a stabler regularization effect. We provide supportive evidence by experimentally confirming that well-performing models show a low sensitivity to noise and fine-tuning with LNSR exhibits clearly higher generalizability and stability. Furthermore, our method also demonstrates advantages over other state-of-the-art algorithms including L 2 -","{'sequence': 'Fine-tuning pre-trained language models such as BERT has become a common practice dominating leaderboards across various NLP tasks. Despite its recent success and wide adoption, this process is unstable when there are only a small number of training samples available. The brittleness of this process is often reflected by the sensitivity to random seeds. In this paper, we propose to tackle this problem based on the noise stability property of deep nets, which is investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020) . Specifically, we introduce a novel and effective regularization method to improve fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability Regularization (LNSR). We extend the theories about adding noise to the input and prove that our method gives a stabler regularization effect. We provide supportive evidence by experimentally confirming that well-performing models show a low sensitivity to noise and fine-tuning with LNSR exhibits clearly higher generalizability and stability. Furthermore, our method also demonstrates advantages over other state-of-the-art algorithms including L 2 -', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Question Answering', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.36006879806518555, 0.08484672755002975, 0.06662958860397339, 0.055530697107315063, 0.04634122923016548, 0.04127538576722145, 0.03859034553170204, 0.03311946988105774, 0.03157978877425194, 0.02886606752872467, 0.02574607916176319, 0.02256651036441326, 0.02087344601750374, 0.019690275192260742, 0.018549209460616112, 0.017134614288806915, 0.016253899782896042, 0.015195815823972225, 0.013768907636404037, 0.01298451703041792, 0.012613155879080296, 0.009848836809396744, 0.007926622405648232]}",0.36006879806518555,Machine Learning for NLP,0.36006879806518555
Machine Learning for NLP,FlowPrior: Learning Expressive Priors for Latent Variable Sentence Models,"Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we add the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for sentence VAEs based on normalizing flows (NF). Priors parameterized with NF are no longer constrained to a specific distribution family, allowing a more flexible way to encode the data distribution. Our model, which we call FlowPrior, shows a substantial improvement in language modeling tasks compared to strong baselines. We demonstrate that FlowPrior learns an expressive prior with analysis and several forms of evaluation involving generation.","{'sequence': 'Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we add the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for sentence VAEs based on normalizing flows (NF). Priors parameterized with NF are no longer constrained to a specific distribution family, allowing a more flexible way to encode the data distribution. Our model, which we call FlowPrior, shows a substantial improvement in language modeling tasks compared to strong baselines. We demonstrate that FlowPrior learns an expressive prior with analysis and several forms of evaluation involving generation.', 'labels': ['Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Information Extraction', 'Semantics: Lexical Semantics', 'Question Answering', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Summarization', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3468295633792877, 0.07033860683441162, 0.05083779618144035, 0.04860244691371918, 0.04263744130730629, 0.037876080721616745, 0.036678116768598557, 0.03365597128868103, 0.03330618143081665, 0.03231523185968399, 0.030846433714032173, 0.029090622439980507, 0.028042472898960114, 0.027168702334165573, 0.021455205976963043, 0.018702302128076553, 0.018289532512426376, 0.017746221274137497, 0.01759701780974865, 0.016626443713903427, 0.0156851839274168, 0.014925675466656685, 0.010746750980615616]}",0.3468295633792877,Generation,0.01759701780974865
Machine Learning for NLP,HTCInfoMax: A Global Model for Hierarchical Text Classification via Information Maximization,"The current state-of-the-art model HiAGM for hierarchical text classification has two limitations. First, it correlates each text sample with all labels in the dataset which contains irrelevant information. Second, it does not consider any statistical constraint on the label representations learned by the structure encoder, while constraints for representation learning are proved to be helpful in previous work. In this paper, we propose HTCInfoMax to address these issues by introducing information maximization which includes two modules: text-label mutual information maximization and label prior matching. The first module can model the interaction between each text sample and its ground truth labels explicitly which filters out irrelevant information. The second one encourages the structure encoder to learn better representations with desired characteristics for all labels which can better handle label imbalance in hierarchical text classification. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed HTCInfoMax.","{'sequence': 'The current state-of-the-art model HiAGM for hierarchical text classification has two limitations. First, it correlates each text sample with all labels in the dataset which contains irrelevant information. Second, it does not consider any statistical constraint on the label representations learned by the structure encoder, while constraints for representation learning are proved to be helpful in previous work. In this paper, we propose HTCInfoMax to address these issues by introducing information maximization which includes two modules: text-label mutual information maximization and label prior matching. The first module can model the interaction between each text sample and its ground truth labels explicitly which filters out irrelevant information. The second one encourages the structure encoder to learn better representations with desired characteristics for all labels which can better handle label imbalance in hierarchical text classification. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed HTCInfoMax.', 'labels': ['Information Extraction', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Summarization', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09642024338245392, 0.0730941966176033, 0.07223160564899445, 0.06407062709331512, 0.06095357611775398, 0.05959649756550789, 0.05944087728857994, 0.057980045676231384, 0.054821569472551346, 0.0469752736389637, 0.044436633586883545, 0.04397731274366379, 0.034107498824596405, 0.03126704692840576, 0.030343707650899887, 0.028879929333925247, 0.026537304744124413, 0.02611149474978447, 0.020924950018525124, 0.019147964194417, 0.01682182587683201, 0.016360996291041374, 0.015498755499720573]}",0.09642024338245392,Information Extraction,0.0469752736389637
Machine Learning for NLP,Knowledge Guided Metric Learning for Few-Shot Text Classification,"Humans can distinguish new categories very efficiently with few examples, largely due to the fact that human beings can leverage knowledge obtained from relevant tasks. However, deep learning based text classification model tends to struggle to achieve satisfactory performance when labeled data are scarce. Inspired by human intelligence, we propose to introduce external knowledge into few-shot learning to imitate human knowledge. A novel parameter generator network is investigated to this end, which is able to use the external knowledge to generate different metrics for different tasks. Armed with this network, similar tasks can use similar metrics while different tasks use different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models.","{'sequence': 'Humans can distinguish new categories very efficiently with few examples, largely due to the fact that human beings can leverage knowledge obtained from relevant tasks. However, deep learning based text classification model tends to struggle to achieve satisfactory performance when labeled data are scarce. Inspired by human intelligence, we propose to introduce external knowledge into few-shot learning to imitate human knowledge. A novel parameter generator network is investigated to this end, which is able to use the external knowledge to generate different metrics for different tasks. Armed with this network, similar tasks can use similar metrics while different tasks use different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Resources and Evaluation', 'Summarization', 'NLP Applications', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09682401269674301, 0.0963994488120079, 0.08067536354064941, 0.05985439941287041, 0.058350417762994766, 0.05538645759224892, 0.051499005407094955, 0.04546819254755974, 0.042700786143541336, 0.04183873534202576, 0.03891127184033394, 0.03679167479276657, 0.03591959550976753, 0.0347590297460556, 0.03472984582185745, 0.03136733919382095, 0.03079797513782978, 0.030460461974143982, 0.022623544558882713, 0.019741889089345932, 0.01973513327538967, 0.019007714465260506, 0.016157811507582664]}",0.09682401269674301,Generation,0.05538645759224892
Dialogue and Interactive Systems,Ensemble of MRR and NDCG models for Visual Dialog,"Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single humanderived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., 'yeah' and 'yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as 'I don't know.' Crafting a model that excels on both MRR and NDCG metrics is challenging (Murahari et al., 2020). Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a twostep non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https: //github.com/idansc/mrr-ndcg.","{'sequence': ""Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single humanderived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., 'yeah' and 'yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as 'I don't know.' Crafting a model that excels on both MRR and NDCG metrics is challenging (Murahari et al., 2020). Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a twostep non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https: //github.com/idansc/mrr-ndcg."", 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining', 'NLP Applications', 'Machine Translation and Multilinguality'], 'scores': [0.11637803167104721, 0.08164691925048828, 0.07881037890911102, 0.0582883358001709, 0.05788623169064522, 0.05171164870262146, 0.048887915909290314, 0.042134519666433334, 0.04172799736261368, 0.0415058359503746, 0.03950361907482147, 0.03877272456884384, 0.037478119134902954, 0.03514345735311508, 0.03360205888748169, 0.031199652701616287, 0.028494780883193016, 0.027378138154745102, 0.026493314653635025, 0.022295817732810974, 0.02097865752875805, 0.02084343694150448, 0.01883838139474392]}",0.11637803167104721,Generation,0.08164691925048828
Dialogue and Interactive Systems,Supervised Neural Clustering via Latent Structured Output Learning: Application to Question Intents,"Previous pre-neural work on structured prediction has produced very effective supervised clustering algorithms using linear classifiers, e.g., structured SVM or perceptron. However, these cannot exploit the representation learning ability of neural networks, which would make supervised clustering even more powerful, i.e., general clustering patterns can be learned automatically. In this paper, we design neural networks based on latent structured prediction loss and Transformer models to approach supervised clustering. We tested our methods on the task of automatically recreating categories of intents from publicly available question intent corpora. The results show that our approach delivers 95.65% of F1, outperforming the state of the art by 17.24%.","{'sequence': 'Previous pre-neural work on structured prediction has produced very effective supervised clustering algorithms using linear classifiers, e.g., structured SVM or perceptron. However, these cannot exploit the representation learning ability of neural networks, which would make supervised clustering even more powerful, i.e., general clustering patterns can be learned automatically. In this paper, we design neural networks based on latent structured prediction loss and Transformer models to approach supervised clustering. We tested our methods on the task of automatically recreating categories of intents from publicly available question intent corpora. The results show that our approach delivers 95.65% of F1, outperforming the state of the art by 17.24%.', 'labels': ['Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Information Extraction', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality'], 'scores': [0.1294933408498764, 0.0824393779039383, 0.07873130589723587, 0.06643154472112656, 0.0463404580950737, 0.04627431929111481, 0.046113740652799606, 0.04387396574020386, 0.04240522161126137, 0.04003642126917839, 0.038469020277261734, 0.03743768483400345, 0.037017881870269775, 0.036690063774585724, 0.0362146832048893, 0.03415875509381294, 0.03390222042798996, 0.026698686182498932, 0.025068223476409912, 0.020261384546756744, 0.019569041207432747, 0.01781676895916462, 0.014555948786437511]}",0.1294933408498764,Generation,0.07873130589723587
Dialogue and Interactive Systems,ConVEx: Data-Efficient and Few-Shot Slot Labeling,"We propose ConVEx (Conversational Value Extractor), an efficient pretraining and finetuning neural approach for slot-labeling dialog tasks. Instead of relying on more general pretraining objectives from prior work (e.g., language modeling, response selection), Con-VEx's pretraining objective, a novel pairwise cloze task using Reddit data, is well aligned with its intended usage on sequence labeling tasks. This enables learning domain-specific slot labelers by simply fine-tuning decoding layers of the pretrained general-purpose sequence labeling model, while the majority of the pretrained model's parameters are kept frozen. We report state-of-the-art performance of ConVEx across a range of diverse domains and data sets for dialog slot-labeling, with the largest gains in the most challenging, few-shot setups. We believe that ConVEx's reduced pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its efficient finetuning and strong performance, promise wider portability and scalability for data-efficient sequence-labeling tasks in general.","{'sequence': ""We propose ConVEx (Conversational Value Extractor), an efficient pretraining and finetuning neural approach for slot-labeling dialog tasks. Instead of relying on more general pretraining objectives from prior work (e.g., language modeling, response selection), Con-VEx's pretraining objective, a novel pairwise cloze task using Reddit data, is well aligned with its intended usage on sequence labeling tasks. This enables learning domain-specific slot labelers by simply fine-tuning decoding layers of the pretrained general-purpose sequence labeling model, while the majority of the pretrained model's parameters are kept frozen. We report state-of-the-art performance of ConVEx across a range of diverse domains and data sets for dialog slot-labeling, with the largest gains in the most challenging, few-shot setups. We believe that ConVEx's reduced pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its efficient finetuning and strong performance, promise wider portability and scalability for data-efficient sequence-labeling tasks in general."", 'labels': ['Resources and Evaluation', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Generation', 'Information Extraction', 'Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0879703238606453, 0.07593444734811783, 0.07030437886714935, 0.06108344346284866, 0.06006253883242607, 0.05669092386960983, 0.05428646132349968, 0.054005563259124756, 0.05118541792035103, 0.046910177916288376, 0.04243090748786926, 0.039587266743183136, 0.039377715438604355, 0.03847195580601692, 0.03731755167245865, 0.03655204176902771, 0.03286144137382507, 0.024873413145542145, 0.024251488968729973, 0.02165365032851696, 0.017340218648314476, 0.01703769899904728, 0.009810916148126125]}",0.0879703238606453,Resources and Evaluation,0.07030437886714935
Dialogue and Interactive Systems,CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues,"Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a selfcontained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset.","{'sequence': 'Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a selfcontained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Generation', 'Computational Social Science and Social Media', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'NLP Applications', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12420915067195892, 0.10840307176113129, 0.08444643020629883, 0.05586474388837814, 0.05342516675591469, 0.05073396861553192, 0.04608733579516411, 0.044147755950689316, 0.03996247053146362, 0.03913869336247444, 0.037475112825632095, 0.03652673959732056, 0.03534470871090889, 0.031741369515657425, 0.030229829251766205, 0.028630327433347702, 0.02624019794166088, 0.025325125083327293, 0.022839518263936043, 0.022455204278230667, 0.021829092875123024, 0.017623266205191612, 0.017320698127150536]}",0.12420915067195892,Resources and Evaluation,0.10840307176113129
Dialogue and Interactive Systems,Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems,"In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of ""cheese pizza"" (a menu item) and ""oreo cookies"" (a topping) from an input utterance ""Can I order a cheese pizza with oreo cookies on top?"" exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow execution of validation rules as a post-processing step after slots have been filled which can lead to error accumulation. In this paper, we formalize knowledge-driven slot constraints and present a new task of constraint violation detection accompanied with benchmarking data. Then, we propose methods to integrate the external knowledge into the system and model constraint violation detection as an end-to-end classification task and compare it to the traditional rule-based pipeline approach. Experiments on two domains of the MultiDoGO dataset reveal challenges of constraint violation detection and sets the stage for future work and improvements.","{'sequence': 'In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of ""cheese pizza"" (a menu item) and ""oreo cookies"" (a topping) from an input utterance ""Can I order a cheese pizza with oreo cookies on top?"" exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow execution of validation rules as a post-processing step after slots have been filled which can lead to error accumulation. In this paper, we formalize knowledge-driven slot constraints and present a new task of constraint violation detection accompanied with benchmarking data. Then, we propose methods to integrate the external knowledge into the system and model constraint violation detection as an end-to-end classification task and compare it to the traditional rule-based pipeline approach. Experiments on two domains of the MultiDoGO dataset reveal challenges of constraint violation detection and sets the stage for future work and improvements.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1422858089208603, 0.11172964423894882, 0.05741102620959282, 0.05473574623465538, 0.05124752223491669, 0.04891413450241089, 0.04694489762187004, 0.04559953510761261, 0.04474940150976181, 0.04363930597901344, 0.037739697843790054, 0.03449774533510208, 0.034363508224487305, 0.034248024225234985, 0.03330744057893753, 0.03246508166193962, 0.028193766251206398, 0.027066273614764214, 0.022593757137656212, 0.019710976630449295, 0.017979657277464867, 0.017544103786349297, 0.013032865710556507]}",0.1422858089208603,Dialogue and Interactive Systems,0.1422858089208603
Dialogue and Interactive Systems,Clipping Loops for Sample-Efficient Dialogue Policy Optimisation,"Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguishes useless actions from others and reduces the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80% success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in human evaluation where the agent interactively collects 100 real-user dialogues in the training phase.","{'sequence': 'Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguishes useless actions from others and reduces the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80% success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in human evaluation where the agent interactively collects 100 real-user dialogues in the training phase.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Information Extraction', 'Ethics and NLP', 'Question Answering', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10035071521997452, 0.06849875301122665, 0.062301818281412125, 0.05617496371269226, 0.05601042881608009, 0.055309586226940155, 0.05352514609694481, 0.05151556059718132, 0.04671851918101311, 0.04116186499595642, 0.0401967316865921, 0.038084033876657486, 0.03777240589261055, 0.03564883768558502, 0.034659504890441895, 0.03439539298415184, 0.029570946469902992, 0.029245775192975998, 0.028810210525989532, 0.028465986251831055, 0.026324180886149406, 0.02437664195895195, 0.02088194712996483]}",0.10035071521997452,Dialogue and Interactive Systems,0.10035071521997452
Information Extraction,Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction,"Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi (Zhang et al., 2019) addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entityrelation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information. https://github.com/drevicko/OpenKI","{'sequence': 'Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi (Zhang et al., 2019) addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entityrelation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information. https://github.com/drevicko/OpenKI', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Generation', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.201976478099823, 0.10992132872343063, 0.0833757296204567, 0.046875178813934326, 0.04445716366171837, 0.044321149587631226, 0.042946070432662964, 0.03673422709107399, 0.0348636619746685, 0.03286619111895561, 0.032139286398887634, 0.0319071039557457, 0.031187353655695915, 0.028713271021842957, 0.028017373755574226, 0.02785344049334526, 0.023576853796839714, 0.022919204086065292, 0.022760828956961632, 0.021488677710294724, 0.02016894891858101, 0.017407234758138657, 0.013523130677640438]}",0.201976478099823,Information Extraction,0.201976478099823
Information Extraction,Noisy-Labeled NER with Confidence Estimation,"Recent studies in deep learning have shown significant progress in named entity recognition (NER). Most existing works assume clean data annotation, yet a fundamental challenge in real-world scenarios is the large amount of noise from a variety of sources (e.g., pseudo, weak, or distant annotations). This work studies NER under a noisy labeled setting with calibrated confidence estimation. Based on empirical observations of different training dynamics of noisy and clean labels, we propose strategies for estimating confidence scores based on local and global independence assumptions. We partially marginalize out labels of low confidence with a CRF model. We further propose a calibration method for confidence scores based on the structure of entity labels. We integrate our approach into a self-training framework for boosting performance. Experiments in general noisy settings with four languages and distantly labeled settings demonstrate the effectiveness of our method 1 .","{'sequence': 'Recent studies in deep learning have shown significant progress in named entity recognition (NER). Most existing works assume clean data annotation, yet a fundamental challenge in real-world scenarios is the large amount of noise from a variety of sources (e.g., pseudo, weak, or distant annotations). This work studies NER under a noisy labeled setting with calibrated confidence estimation. Based on empirical observations of different training dynamics of noisy and clean labels, we propose strategies for estimating confidence scores based on local and global independence assumptions. We partially marginalize out labels of low confidence with a CRF model. We further propose a calibration method for confidence scores based on the structure of entity labels. We integrate our approach into a self-training framework for boosting performance. Experiments in general noisy settings with four languages and distantly labeled settings demonstrate the effectiveness of our method 1 .', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Question Answering', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.0786878690123558, 0.07450692355632782, 0.06914136558771133, 0.06874068081378937, 0.06617508828639984, 0.05647118762135506, 0.05574622377753258, 0.05202014371752739, 0.049400292336940765, 0.04534145072102547, 0.04173031449317932, 0.04116446152329445, 0.035553522408008575, 0.03477766737341881, 0.034390490502119064, 0.031446732580661774, 0.026437919586896896, 0.025331012904644012, 0.024317633360624313, 0.024159008637070656, 0.022890662774443626, 0.02169180102646351, 0.019877582788467407]}",0.0786878690123558,Generation,0.06874068081378937
Information Extraction,TABBIE: Pretrained Representations of Tabular Data,"Existing work on tabular representationlearning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of tablebased prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model's learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.","{'sequence': ""Existing work on tabular representationlearning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of tablebased prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model's learned cell, column, and row representations shows that it understands complex table semantics and numerical trends."", 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Generation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Information Retrieval and Text Mining', 'NLP Applications', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2952580451965332, 0.08141861855983734, 0.06159576401114464, 0.05343235284090042, 0.05339264124631882, 0.05159364268183708, 0.044384825974702835, 0.042795464396476746, 0.03876565024256706, 0.03484851494431496, 0.029556777328252792, 0.02364434115588665, 0.022693218663334846, 0.021502507850527763, 0.019474372267723083, 0.018800746649503708, 0.018749265000224113, 0.018673943355679512, 0.018163125962018967, 0.01514250785112381, 0.015105539001524448, 0.01160477101802826, 0.009403395466506481]}",0.2952580451965332,Question Answering,0.05343235284090042
Information Extraction,Better Feature Integration for Named Entity Recognition,"It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. We believe this is because both types of features -the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks (GCNs) for building improved NER models, where the exact interaction mechanism between the two different types of features is not very clear, and the performance gain does not appear to be significant. In this work, we propose a simple and robust solution to incorporate both types of features with our Synergized-LSTM (Syn-LSTM), which clearly captures how the two types of features interact. We conduct extensive experiments on several standard datasets across four languages. The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. Our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines. 1","{'sequence': 'It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. We believe this is because both types of features -the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks (GCNs) for building improved NER models, where the exact interaction mechanism between the two different types of features is not very clear, and the performance gain does not appear to be significant. In this work, we propose a simple and robust solution to incorporate both types of features with our Synergized-LSTM (Syn-LSTM), which clearly captures how the two types of features interact. We conduct extensive experiments on several standard datasets across four languages. The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. Our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines. 1', 'labels': ['Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Question Answering', 'Machine Learning for NLP', 'Information Extraction', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07779760658740997, 0.0728917121887207, 0.07240469008684158, 0.06955081224441528, 0.06774397194385529, 0.06475720554590225, 0.05888962000608444, 0.05586769059300423, 0.04558194428682327, 0.044657401740550995, 0.04158230870962143, 0.03843238204717636, 0.033535946160554886, 0.03320803865790367, 0.029706796631217003, 0.02967151626944542, 0.029028546065092087, 0.027898501604795456, 0.02708260528743267, 0.026160454377532005, 0.020876513794064522, 0.019703112542629242, 0.012970514595508575]}",0.07779760658740997,Interpretability and Analysis of Models for NLP,0.06774397194385529
Information Extraction,ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning,"While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zeroshot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to directly predict unseen relations without handcrafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their relations, ZS-BERT learns two functions that project sentences and relation descriptions into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and newcoming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54% improvement on F1 score.","{'sequence': 'While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zeroshot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to directly predict unseen relations without handcrafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their relations, ZS-BERT learns two functions that project sentences and relation descriptions into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and newcoming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54% improvement on F1 score.', 'labels': ['Generation', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Summarization', 'NLP Applications', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1520884782075882, 0.0889885351061821, 0.07327397167682648, 0.056038305163383484, 0.05332929641008377, 0.0498390756547451, 0.043538156896829605, 0.04334605485200882, 0.04074641317129135, 0.04047846794128418, 0.04003209248185158, 0.039303213357925415, 0.035344552248716354, 0.03452169522643089, 0.03358549624681473, 0.028787756338715553, 0.02854166366159916, 0.028117569163441658, 0.027356956154108047, 0.018726488575339317, 0.017146117985248566, 0.014645193703472614, 0.012224303558468819]}",0.1520884782075882,Generation,0.0889885351061821
Information Extraction,Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures,"We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept intersentence event mention pairs. As such, we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI, featuring discourse, syntax, and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets.","{'sequence': 'We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept intersentence event mention pairs. As such, we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI, featuring discourse, syntax, and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Question Answering', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Generation', 'Discourse and Pragmatics', 'Summarization', 'Resources and Evaluation', 'Speech and Multimodality', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.10586076229810715, 0.09316077083349228, 0.08552338927984238, 0.07188781350851059, 0.06269727647304535, 0.05072847381234169, 0.047262515872716904, 0.04539578780531883, 0.039016954600811005, 0.038926299661397934, 0.03451590985059738, 0.03447316586971283, 0.034329794347286224, 0.03383060544729233, 0.03290913254022598, 0.028575915843248367, 0.02847198024392128, 0.02809940092265606, 0.024070750921964645, 0.021697424352169037, 0.02118564210832119, 0.019247353076934814, 0.0181328933686018]}",0.10586076229810715,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.09316077083349228
Information Extraction,A Context-Dependent Gated Module for Incorporating Symbolic Semantics into Event Coreference Resolution,"Event coreference resolution is an important research problem with many applications. Despite the recent remarkable success of pretrained language models, we argue that it is still highly beneficial to utilize symbolic features for the task. However, as the input for coreference resolution typically comes from upstream components in the information extraction pipeline, the automatically extracted symbolic features can be noisy and contain errors. Also, depending on the specific context, some features can be more informative than others. Motivated by these observations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016. 1","{'sequence': 'Event coreference resolution is an important research problem with many applications. Despite the recent remarkable success of pretrained language models, we argue that it is still highly beneficial to utilize symbolic features for the task. However, as the input for coreference resolution typically comes from upstream components in the information extraction pipeline, the automatically extracted symbolic features can be noisy and contain errors. Also, depending on the specific context, some features can be more informative than others. Motivated by these observations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016. 1', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality'], 'scores': [0.20204462110996246, 0.07662977278232574, 0.06925090402364731, 0.06671208888292313, 0.04583675041794777, 0.04255719855427742, 0.04208962246775627, 0.039754148572683334, 0.03868513181805611, 0.036418844014406204, 0.03361718729138374, 0.03351427614688873, 0.030298558995127678, 0.029300693422555923, 0.02872942015528679, 0.02696167677640915, 0.026742273941636086, 0.025267286226153374, 0.025210315361618996, 0.02448228932917118, 0.02101524919271469, 0.018833590671420097, 0.01604805700480938]}",0.20204462110996246,Information Extraction,0.20204462110996246
Generation,Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus,"Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformerbased language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.","{'sequence': 'Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformerbased language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.', 'labels': ['Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.15912850201129913, 0.10658237338066101, 0.06745757907629013, 0.059622932225465775, 0.052609872072935104, 0.04968488961458206, 0.04930342733860016, 0.04732874780893326, 0.0445721298456192, 0.04248984158039093, 0.03910871595144272, 0.037936706095933914, 0.03229116275906563, 0.030338497832417488, 0.027896294370293617, 0.02492767572402954, 0.02070939540863037, 0.019997427240014076, 0.019648488610982895, 0.019064124673604965, 0.018271101638674736, 0.016640447080135345, 0.014389626681804657]}",0.15912850201129913,Generation,0.15912850201129913
Generation,FUDGE: Controlled Text Generation With Future Discriminators,"We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a preexisting model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks -couplet completion in poetry, topic control in language generation, and formality change in machine translation -and observe gains in all three tasks.","{'sequence': ""We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a preexisting model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks -couplet completion in poetry, topic control in language generation, and formality change in machine translation -and observe gains in all three tasks."", 'labels': ['Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Question Answering', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization'], 'scores': [0.4725063741207123, 0.06546091288328171, 0.05368701368570328, 0.05200456455349922, 0.037728942930698395, 0.02530362270772457, 0.022710522636771202, 0.022432414814829826, 0.020868638530373573, 0.020487314090132713, 0.019866351038217545, 0.019580788910388947, 0.018940333276987076, 0.018807051703333855, 0.01753149926662445, 0.017095068469643593, 0.016027823090553284, 0.014735927805304527, 0.014100881293416023, 0.013589521870017052, 0.012735261581838131, 0.011973192915320396, 0.011825982481241226]}",0.4725063741207123,Generation,0.4725063741207123
Generation,Controllable Text Simplification with Explicit Paraphrasing,"Text Simplification improves the readability of sentences through several rewriting transformations, such as lexical paraphrasing, deletion, and splitting. Current simplification systems are predominantly sequence-to-sequence models that are trained end-to-end to perform all these operations simultaneously. However, such systems limit themselves to mostly deleting words and cannot easily adapt to the requirements of different target audiences. In this paper, we propose a novel hybrid approach that leverages linguistically-motivated rules for splitting and deletion, and couples them with a neural paraphrasing model to produce varied rewriting styles. We introduce a new data augmentation method to improve the paraphrasing capability of our model. Through automatic and manual evaluations, we show that our proposed model establishes a new state-ofthe art for the task, paraphrasing more often than the existing systems, and can control the degree of each simplification operation applied to the input texts. 1","{'sequence': 'Text Simplification improves the readability of sentences through several rewriting transformations, such as lexical paraphrasing, deletion, and splitting. Current simplification systems are predominantly sequence-to-sequence models that are trained end-to-end to perform all these operations simultaneously. However, such systems limit themselves to mostly deleting words and cannot easily adapt to the requirements of different target audiences. In this paper, we propose a novel hybrid approach that leverages linguistically-motivated rules for splitting and deletion, and couples them with a neural paraphrasing model to produce varied rewriting styles. We introduce a new data augmentation method to improve the paraphrasing capability of our model. Through automatic and manual evaluations, we show that our proposed model establishes a new state-ofthe art for the task, paraphrasing more often than the existing systems, and can control the degree of each simplification operation applied to the input texts. 1', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Generation', 'Ethics and NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09967880696058273, 0.09516508877277374, 0.07694008201360703, 0.050835296511650085, 0.04721252992749214, 0.04438978433609009, 0.04364525154232979, 0.04293462261557579, 0.04007413983345032, 0.03983980789780617, 0.03924063593149185, 0.037228431552648544, 0.036564476788043976, 0.03481929749250412, 0.034795548766851425, 0.033114802092313766, 0.03291427716612816, 0.03166934475302696, 0.03153835982084274, 0.03133055567741394, 0.02944161370396614, 0.02684062346816063, 0.019786730408668518]}",0.09967880696058273,Resources and Evaluation,0.036564476788043976
Generation,Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training,"Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domainspecific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, largescale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.","{'sequence': 'Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domainspecific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, largescale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.', 'labels': ['Generation', 'Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'NLP Applications', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.4241771996021271, 0.07232105731964111, 0.06533227860927582, 0.046578649431467056, 0.03305577486753464, 0.03258497640490532, 0.0304275956004858, 0.02777780406177044, 0.0265315193682909, 0.026113852858543396, 0.024077750742435455, 0.02295874059200287, 0.02117997221648693, 0.020874015986919403, 0.01978377252817154, 0.01749776117503643, 0.016711223870515823, 0.014341541565954685, 0.014020299538969994, 0.013652976602315903, 0.010558852925896645, 0.010549815371632576, 0.008892571553587914]}",0.4241771996021271,Generation,0.4241771996021271
Generation,Choose Your Own Adventure: Paired Suggestions in Collaborative Writing for Evaluating Story Generation Models,"Story generation is an open-ended and subjective task, which poses a challenge for evaluating story generation models. We present CHOOSE YOUR OWN ADVENTURE, a collaborative writing setup for pairwise model evaluation. Two models generate suggestions to people as they write a short story; we ask writers to choose one of the two suggestions, and we observe which model's suggestions they prefer. The setup also allows further analysis based on the revisions people make to the suggestions. We show that these measures, combined with automatic metrics, provide an informative picture of the models' performance, both in cases where the differences in generation methods are small (nucleus vs. top-k sampling) and large (GPT2 vs. Fusion models).","{'sequence': ""Story generation is an open-ended and subjective task, which poses a challenge for evaluating story generation models. We present CHOOSE YOUR OWN ADVENTURE, a collaborative writing setup for pairwise model evaluation. Two models generate suggestions to people as they write a short story; we ask writers to choose one of the two suggestions, and we observe which model's suggestions they prefer. The setup also allows further analysis based on the revisions people make to the suggestions. We show that these measures, combined with automatic metrics, provide an informative picture of the models' performance, both in cases where the differences in generation methods are small (nucleus vs. top-k sampling) and large (GPT2 vs. Fusion models)."", 'labels': ['Generation', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining'], 'scores': [0.17718948423862457, 0.1307373344898224, 0.07579980045557022, 0.07414388656616211, 0.05778402090072632, 0.04637157544493675, 0.0428859144449234, 0.04168952256441116, 0.03940758481621742, 0.03619964420795441, 0.03238023445010185, 0.031991634517908096, 0.03186795487999916, 0.03070433996617794, 0.023006075993180275, 0.02184390276670456, 0.01760239154100418, 0.017468836158514023, 0.01573917642235756, 0.015539220534265041, 0.013918855227530003, 0.012887807562947273, 0.012840726412832737]}",0.17718948423862457,Generation,0.17718948423862457
Machine Translation and Multilinguality,InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training,"In this work, we present an informationtheoretic framework that formulates crosslingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pretraining task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.","{'sequence': 'In this work, we present an informationtheoretic framework that formulates crosslingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pretraining task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.', 'labels': ['Speech and Multimodality', 'Machine Translation and Multilinguality', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Question Answering', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08515948057174683, 0.08082649111747742, 0.07349330186843872, 0.06902189552783966, 0.06137585639953613, 0.05762868747115135, 0.051966339349746704, 0.048703670501708984, 0.04685205593705177, 0.046403925865888596, 0.04134479537606239, 0.038368839770555496, 0.037686072289943695, 0.036130551248788834, 0.03050411492586136, 0.028924712911248207, 0.02797648124396801, 0.025789549574255943, 0.025468047708272934, 0.023996863514184952, 0.02338198572397232, 0.023041386157274246, 0.01595490612089634]}",0.08515948057174683,Speech and Multimodality,0.08082649111747742
Machine Translation and Multilinguality,Context-Interactive Pre-Training for Document Machine Translation,"Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information. However, it typically suffers from a lack of doc-level bilingual data. To remedy this, here we propose a simple yet effective context-interactive pre-training approach, which targets benefiting from external largescale corpora. The proposed model performs inter sentence generation to capture the crosssentence dependency within the target document, and cross sentence translation to make better use of valuable contextual information. Comprehensive experiments illustrate that our approach can achieve state-of-the-art performance on three benchmark datasets, which significantly outperforms a variety of baselines.","{'sequence': 'Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information. However, it typically suffers from a lack of doc-level bilingual data. To remedy this, here we propose a simple yet effective context-interactive pre-training approach, which targets benefiting from external largescale corpora. The proposed model performs inter sentence generation to capture the crosssentence dependency within the target document, and cross sentence translation to make better use of valuable contextual information. Comprehensive experiments illustrate that our approach can achieve state-of-the-art performance on three benchmark datasets, which significantly outperforms a variety of baselines.', 'labels': ['Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Generation', 'Information Extraction', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'NLP Applications', 'Question Answering', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Summarization', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.22608299553394318, 0.09238740056753159, 0.0920693501830101, 0.07760462164878845, 0.049988050013780594, 0.04508160427212715, 0.0425235778093338, 0.039917733520269394, 0.038631871342659, 0.037235505878925323, 0.03400300070643425, 0.03285600244998932, 0.032398298382759094, 0.028630031272768974, 0.021446524187922478, 0.019390560686588287, 0.018882151693105698, 0.017611391842365265, 0.016366802155971527, 0.011251029558479786, 0.010680142790079117, 0.008399028331041336, 0.006562161725014448]}",0.22608299553394318,Machine Translation and Multilinguality,0.22608299553394318
Machine Translation and Multilinguality,Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots,"Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong blackbox adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter directly aligns the clean example with its translations before extracting phrases as perturbations. Our phrase-level attack has a success rate of 89.75% against XLM-R large , bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme that trains in the same number of steps as the original model and show that it improves model accuracy. 1  Original P: The girl that can help me is all the way across town. H: There is no one who can help me. Adversary P: olan girl that can help me is all the way across town. H: one who can help me. Prediction Before: Contradiction After: Entailment Original P: We didn't know where they were going. H: We didn't know where the people were traveling to. Adversary P: We didn't know where they were going. H: We didn't know where les gens allaient. Prediction Before: Entailment After: Neutral Original P: Well it got to where there's two or three aircraft arrive in a week and I didn't know where they're flying to. H: There are never any aircraft arriving. Adversary P: общем, дошло до mahali there's two or three aircraft arrive in a week and I didn't know where they're flying to.","{'sequence': ""Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong blackbox adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter directly aligns the clean example with its translations before extracting phrases as perturbations. Our phrase-level attack has a success rate of 89.75% against XLM-R large , bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme that trains in the same number of steps as the original model and show that it improves model accuracy. 1  Original P: The girl that can help me is all the way across town. H: There is no one who can help me. Adversary P: olan girl that can help me is all the way across town. H: one who can help me. Prediction Before: Contradiction After: Entailment Original P: We didn't know where they were going. H: We didn't know where the people were traveling to. Adversary P: We didn't know where they were going. H: We didn't know where les gens allaient. Prediction Before: Entailment After: Neutral Original P: Well it got to where there's two or three aircraft arrive in a week and I didn't know where they're flying to. H: There are never any aircraft arriving. Adversary P: общем, дошло до mahali there's two or three aircraft arrive in a week and I didn't know where they're flying to."", 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Information Extraction', 'Question Answering', 'Generation', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Resources and Evaluation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10834300518035889, 0.0683203861117363, 0.06457122415304184, 0.06001337617635727, 0.055145829916000366, 0.051432546228170395, 0.048667289316654205, 0.0460052564740181, 0.04358012229204178, 0.04145041108131409, 0.04044477641582489, 0.03993606939911842, 0.0372965931892395, 0.03624990954995155, 0.033721551299095154, 0.033317528665065765, 0.03323454037308693, 0.03106570430099964, 0.030750781297683716, 0.02964920923113823, 0.027740245684981346, 0.02263355441391468, 0.016429996117949486]}",0.10834300518035889,Machine Translation and Multilinguality,0.10834300518035889
Machine Translation and Multilinguality,X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering,"Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for crosslingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.","{'sequence': 'Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for crosslingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.', 'labels': ['Question Answering', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Speech and Multimodality', 'Generation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.5606027841567993, 0.06583248823881149, 0.044770292937755585, 0.035979751497507095, 0.03455544263124466, 0.029334668070077896, 0.02893761172890663, 0.024653222411870956, 0.0221363864839077, 0.020197592675685883, 0.016854342073202133, 0.015381503850221634, 0.014005879871547222, 0.012546677142381668, 0.011073574423789978, 0.010650398209691048, 0.010540129616856575, 0.009965662844479084, 0.009828899055719376, 0.008299673907458782, 0.0056589460000395775, 0.004102097824215889, 0.0040919058956205845]}",0.5606027841567993,Question Answering,0.06583248823881149
Machine Translation and Multilinguality,Explicit Alignment Objectives for Multilingual Bidirectional Encoders,"Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020a) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zeroshot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AM-BER. Our code and models are available at http://github.com/junjiehu/amber.","{'sequence': 'Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020a) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zeroshot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AM-BER. Our code and models are available at http://github.com/junjiehu/amber.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11744530498981476, 0.11520232260227203, 0.062497228384017944, 0.05960604175925255, 0.05887250229716301, 0.05475630611181259, 0.049570582807064056, 0.047834329307079315, 0.04690735787153244, 0.04515508934855461, 0.03761739283800125, 0.036158476024866104, 0.03370637446641922, 0.03348211571574211, 0.03254272788763046, 0.030536025762557983, 0.027982575818896294, 0.027128227055072784, 0.020275548100471497, 0.019435934722423553, 0.016990460455417633, 0.015198983252048492, 0.011098146438598633]}",0.11744530498981476,Machine Learning for NLP,0.062497228384017944
Machine Translation and Multilinguality,Cross-lingual Cross-modal Pretraining for Multimodal Retrieval,"Recent pretrained vision-language models have achieved impressive performance on cross-modal retrieval tasks in English. Their success, however, heavily depends on the availability of many annotated image-caption datasets for pretraining, where the texts are not necessarily in English. Although we can utilize machine translation (MT) tools to translate non-English text to English, the performance still largely relies on MT's quality and may suffer from high latency problems in realworld applications. This paper proposes a new approach to learn cross-lingual cross-modal representations for matching images and their relevant captions in multiple languages. We seamlessly combine cross-lingual pretraining objectives and cross-modal pretraining objectives in a unified framework to learn image and text in a joint embedding space from available English image-caption data, monolingual and parallel corpus. We show that our approach achieves SOTA performance in retrieval tasks on two multimodal multilingual image caption benchmarks: Multi30k with German captions and MSCOCO with Japanese captions.","{'sequence': ""Recent pretrained vision-language models have achieved impressive performance on cross-modal retrieval tasks in English. Their success, however, heavily depends on the availability of many annotated image-caption datasets for pretraining, where the texts are not necessarily in English. Although we can utilize machine translation (MT) tools to translate non-English text to English, the performance still largely relies on MT's quality and may suffer from high latency problems in realworld applications. This paper proposes a new approach to learn cross-lingual cross-modal representations for matching images and their relevant captions in multiple languages. We seamlessly combine cross-lingual pretraining objectives and cross-modal pretraining objectives in a unified framework to learn image and text in a joint embedding space from available English image-caption data, monolingual and parallel corpus. We show that our approach achieves SOTA performance in retrieval tasks on two multimodal multilingual image caption benchmarks: Multi30k with German captions and MSCOCO with Japanese captions."", 'labels': ['Machine Translation and Multilinguality', 'Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Information Extraction', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Summarization', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP'], 'scores': [0.0821252092719078, 0.07775156199932098, 0.07755470275878906, 0.05869812145829201, 0.056935928761959076, 0.0508197657763958, 0.050478823482990265, 0.049382809549570084, 0.04584532231092453, 0.04432893916964531, 0.043631069362163544, 0.04303639009594917, 0.040233027189970016, 0.0400809682905674, 0.03751285746693611, 0.03468717634677887, 0.03107048012316227, 0.03092358075082302, 0.022354070097208023, 0.02161453105509281, 0.021283073350787163, 0.020240463316440582, 0.019410984590649605]}",0.0821252092719078,Machine Translation and Multilinguality,0.0821252092719078
Machine Translation and Multilinguality,Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks,"Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a languageindependent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexicalsemantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.","{'sequence': 'Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a languageindependent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexicalsemantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.', 'labels': ['Semantics: Lexical Semantics', 'Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Generation', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Question Answering', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10705222189426422, 0.08514279872179031, 0.08466833084821701, 0.061534781008958817, 0.06145588681101799, 0.05841173231601715, 0.04841435328125954, 0.04444336146116257, 0.04377356916666031, 0.043458446860313416, 0.043024610728025436, 0.03785311058163643, 0.03613009303808212, 0.03586878255009651, 0.03419804573059082, 0.03342179208993912, 0.03099275380373001, 0.026481661945581436, 0.023195097222924232, 0.017987295985221863, 0.01686280407011509, 0.014704618602991104, 0.010923818685114384]}",0.10705222189426422,Semantics: Lexical Semantics,0.04377356916666031
Question Answering,multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,"We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PROVER (Saha et al., 2020) , performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, MULTIPROVER. Our first model, Multilabel-MULTIPROVER, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-MULTIPROVER, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both MULTIPROVER models significantly outperform PROVER on datasets containing multiple gold proofs. Iterative-MULTIPROVER obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent. 1 Our code and models are publicly available at https: //github.com/swarnaHub/multiPRover. Facts: F1: Bob is quiet. F2: Bob is young. F3: Charlie is quiet. F4: Charlie is young. F5: Fiona is nice. F6: Fiona is quiet. F7: Fiona is round. F8: Fiona is white. F9: Gary is green. F10: Gary is nice. F11: Gary is quiet. F12: Gary is young. Facts: F1: Anne is cold. F2: Bob is cold. F3: Bob is young. F4: Fiona is big. F5: Fiona is young. F6: Harry is big. F7: Harry is blue. F8: Harry is cold. F9: Harry is furry. F10: Harry is quite. F11: Harry is red. F12: Harry is young. Rules: R1: Cold, young people are red. R2: Furry people are young. R3: Young, big people are blue. R4: Red, big people are quiet. R5: Quiet people are furry. R6: Blue people are red. R7: Young people are big. R8: All quiet, big people are furry. R9: If someone is blue and furry then they are cold. Q2: Bob is not cold. [Answer : F ] Rules: R1: All green, white people are round. R2: Quiet people are white. R3: All green, young people are nice. R4: If someone is quiet and green then they are kind. R5: White people are nice. R6: Quiet people are young. R7: All green, white people are nice. R8: If someone is kind and white then they are green. R9: All nice, quiet people are kind.","{'sequence': 'We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PROVER (Saha et al., 2020) , performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, MULTIPROVER. Our first model, Multilabel-MULTIPROVER, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-MULTIPROVER, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both MULTIPROVER models significantly outperform PROVER on datasets containing multiple gold proofs. Iterative-MULTIPROVER obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent. 1 Our code and models are publicly available at https: //github.com/swarnaHub/multiPRover. Facts: F1: Bob is quiet. F2: Bob is young. F3: Charlie is quiet. F4: Charlie is young. F5: Fiona is nice. F6: Fiona is quiet. F7: Fiona is round. F8: Fiona is white. F9: Gary is green. F10: Gary is nice. F11: Gary is quiet. F12: Gary is young. Facts: F1: Anne is cold. F2: Bob is cold. F3: Bob is young. F4: Fiona is big. F5: Fiona is young. F6: Harry is big. F7: Harry is blue. F8: Harry is cold. F9: Harry is furry. F10: Harry is quite. F11: Harry is red. F12: Harry is young. Rules: R1: Cold, young people are red. R2: Furry people are young. R3: Young, big people are blue. R4: Red, big people are quiet. R5: Quiet people are furry. R6: Blue people are red. R7: Young people are big. R8: All quiet, big people are furry. R9: If someone is blue and furry then they are cold. Q2: Bob is not cold. [Answer : F ] Rules: R1: All green, white people are round. R2: Quiet people are white. R3: All green, young people are nice. R4: If someone is quiet and green then they are kind. R5: White people are nice. R6: Quiet people are young. R7: All green, white people are nice. R8: If someone is kind and white then they are green. R9: All nice, quiet people are kind.', 'labels': ['Speech and Multimodality', 'Generation', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Question Answering', 'Information Extraction', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Machine Learning for NLP'], 'scores': [0.07251368463039398, 0.07148584723472595, 0.06833645701408386, 0.0646924078464508, 0.060282934457063675, 0.059590525925159454, 0.05926482379436493, 0.04635252058506012, 0.04496794566512108, 0.043489471077919006, 0.03941292688250542, 0.03823814168572426, 0.035750627517700195, 0.03531067445874214, 0.03509264439344406, 0.03434694558382034, 0.03330298513174057, 0.03192674368619919, 0.02774510718882084, 0.027681365609169006, 0.02600439451634884, 0.02568156085908413, 0.018529171124100685]}",0.07251368463039398,Speech and Multimodality,0.059590525925159454
Question Answering,Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge,"Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a ""fact memory"". Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re-training, by updating the fact memory.","{'sequence': 'Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a ""fact memory"". Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re-training, by updating the fact memory.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Generation', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2168879210948944, 0.06448652595281601, 0.06205866113305092, 0.05979559198021889, 0.051614366471767426, 0.047857869416475296, 0.04686415567994118, 0.04654194414615631, 0.04418005794286728, 0.04277269169688225, 0.03560256585478783, 0.03212084248661995, 0.030104581266641617, 0.0296035073697567, 0.027254650369286537, 0.027108456939458847, 0.02373230643570423, 0.021909309551119804, 0.021818654611706734, 0.021460209041833878, 0.018843645229935646, 0.015242466703057289, 0.01213906705379486]}",0.2168879210948944,Question Answering,0.2168879210948944
Question Answering,Refining Targeted Syntactic Evaluation of Language Models,"Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models' syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb's conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a language model's syntactic knowledge: given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a model's likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new metrics to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that models score up to 40% better on verbs that they predict are likely in context.","{'sequence': ""Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models' syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb's conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a language model's syntactic knowledge: given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a model's likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new metrics to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that models score up to 40% better on verbs that they predict are likely in context."", 'labels': ['Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Information Extraction', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Summarization', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09523939341306686, 0.0778273493051529, 0.0704507902264595, 0.06088531017303467, 0.06076589226722717, 0.05986300855875015, 0.048256199806928635, 0.04661338031291962, 0.043736428022384644, 0.04346689581871033, 0.04327990487217903, 0.04105069115757942, 0.04020030423998833, 0.037871744483709335, 0.03248843178153038, 0.032134078443050385, 0.029966752976179123, 0.028524572029709816, 0.024072112515568733, 0.023867525160312653, 0.02260066196322441, 0.02057635597884655, 0.01626225747168064]}",0.09523939341306686,Speech and Multimodality,0.05986300855875015
Question Answering,Universal Adversarial Attacks with Natural Triggers for Text Classification,"Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an adversarially regularized autoencoder (ARAE) (Zhao et al., 2018a) to generate triggers and propose a gradient-based search that aims to maximize the downstream classifier's prediction loss. Our attacks effectively reduce model accuracy on classification tasks while being less identifiable than prior models as per automatic detection metrics and humansubject studies. Our aim is to demonstrate that adversarial attacks can be made harder to detect than previously thought and to enable the development of appropriate defenses. 1","{'sequence': ""Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an adversarially regularized autoencoder (ARAE) (Zhao et al., 2018a) to generate triggers and propose a gradient-based search that aims to maximize the downstream classifier's prediction loss. Our attacks effectively reduce model accuracy on classification tasks while being less identifiable than prior models as per automatic detection metrics and humansubject studies. Our aim is to demonstrate that adversarial attacks can be made harder to detect than previously thought and to enable the development of appropriate defenses. 1"", 'labels': ['Dialogue and Interactive Systems', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'NLP Applications', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization'], 'scores': [0.0961528792977333, 0.06011856719851494, 0.055741697549819946, 0.05443193390965462, 0.053988829255104065, 0.051789771765470505, 0.051184747368097305, 0.051089443266391754, 0.050377774983644485, 0.045066751539707184, 0.038403116166591644, 0.037854988127946854, 0.036270346492528915, 0.03619955852627754, 0.034417785704135895, 0.034201279282569885, 0.03367847949266434, 0.03323681280016899, 0.03187411278486252, 0.0317278690636158, 0.03145371749997139, 0.02659100480377674, 0.02414855547249317]}",0.0961528792977333,Dialogue and Interactive Systems,0.051089443266391754
Question Answering,QuadrupletBERT: An Efficient Model For Embedding-Based Large-Scale Retrieval,"The embedding-based large-scale querydocument retrieval problem is a hot topic in the information retrieval (IR) field. Considering that pre-trained language models like BERT have achieved great success in a wide variety of NLP tasks, we present a Quadru-pletBERT model for effective and efficient retrieval in this paper. Unlike most existing BERT-style retrieval models, which only focus on the ranking phase in retrieval systems, our model makes considerable improvements to the retrieval phase and leverages the distances between simple negative and hard negative instances to obtaining better embeddings. Experimental results demonstrate that our QuadrupletBERT achieves state-of-the-art results in embedding-based large-scale retrieval tasks.","{'sequence': 'The embedding-based large-scale querydocument retrieval problem is a hot topic in the information retrieval (IR) field. Considering that pre-trained language models like BERT have achieved great success in a wide variety of NLP tasks, we present a Quadru-pletBERT model for effective and efficient retrieval in this paper. Unlike most existing BERT-style retrieval models, which only focus on the ranking phase in retrieval systems, our model makes considerable improvements to the retrieval phase and leverages the distances between simple negative and hard negative instances to obtaining better embeddings. Experimental results demonstrate that our QuadrupletBERT achieves state-of-the-art results in embedding-based large-scale retrieval tasks.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Generation', 'Information Extraction', 'Ethics and NLP', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Question Answering', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.18490386009216309, 0.13534705340862274, 0.06667041778564453, 0.06539653986692429, 0.062022119760513306, 0.05179228261113167, 0.04759344458580017, 0.039328914135694504, 0.037585388869047165, 0.033924400806427, 0.030633846297860146, 0.028332484886050224, 0.027843881398439407, 0.027539795264601707, 0.023465661332011223, 0.021483775228261948, 0.021468548104166985, 0.021280281245708466, 0.019223926588892937, 0.017530063167214394, 0.015134396031498909, 0.011990368366241455, 0.009508593939244747]}",0.18490386009216309,Interpretability and Analysis of Models for NLP,0.033924400806427
Ethics and NLP,Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack,"Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.","{'sequence': 'Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.', 'labels': ['Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Ethics and NLP', 'Generation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Question Answering', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13182653486728668, 0.11547614634037018, 0.09787900745868683, 0.055340997874736786, 0.05051961913704872, 0.044987209141254425, 0.04421307146549225, 0.042706508189439774, 0.042617395520210266, 0.04112842679023743, 0.038162749260663986, 0.03501246124505997, 0.03414588049054146, 0.0327097587287426, 0.027599846944212914, 0.025443103164434433, 0.023402288556098938, 0.02106056734919548, 0.020924154669046402, 0.020404882729053497, 0.02004929445683956, 0.01937895081937313, 0.015011236071586609]}",0.13182653486728668,Interpretability and Analysis of Models for NLP,0.044987209141254425
Ethics and NLP,An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls,"Volatility prediction is complex due to the stock market's stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives' speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous stateof-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gendersensitive audio features and fewer female executives in earnings calls of one of the world's biggest stock indexes, the S&P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes. 1","{'sequence': ""Volatility prediction is complex due to the stock market's stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives' speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous stateof-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gendersensitive audio features and fewer female executives in earnings calls of one of the world's biggest stock indexes, the S&P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes. 1"", 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Information Extraction', 'Question Answering', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.16747428476810455, 0.059997960925102234, 0.05893540754914284, 0.05823627486824989, 0.057155340909957886, 0.051317598670721054, 0.05037298426032066, 0.04628876969218254, 0.04607317969202995, 0.03926074877381325, 0.03895602375268936, 0.037594132125377655, 0.03570900484919548, 0.03426597639918327, 0.034010980278253555, 0.030484337359666824, 0.027752669528126717, 0.025669727474451065, 0.02403068169951439, 0.021353309974074364, 0.021180612966418266, 0.020079417154192924, 0.01380056794732809]}",0.16747428476810455,Speech and Multimodality,0.020079417154192924
Ethics and NLP,Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing,"The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.","{'sequence': 'The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.', 'labels': ['Ethics and NLP', 'Discourse and Pragmatics', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'NLP Applications', 'Generation', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.18433703482151031, 0.06686782091856003, 0.06440090388059616, 0.057193316519260406, 0.050782401114702225, 0.050576306879520416, 0.04239335656166077, 0.04087148234248161, 0.03973303735256195, 0.03827522695064545, 0.037215761840343475, 0.036605071276426315, 0.03435077518224716, 0.034340936690568924, 0.034314870834350586, 0.032470982521772385, 0.027392862364649773, 0.024569883942604065, 0.023233843967318535, 0.02284838818013668, 0.02022557333111763, 0.01980751007795334, 0.01719256304204464]}",0.18433703482151031,Ethics and NLP,0.18433703482151031
Ethics and NLP,On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning,"Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring additional data collection and customized annotation of sensitive attributes, and re-evaluation of appropriate fairness metrics. We explore the feasibility and benefits of upstream bias mitigation (UBM) for reducing bias on downstream tasks, by first applying bias mitigation to an upstream model through fine-tuning and subsequently using it for downstream finetuning. We find, in extensive experiments across hate speech detection, toxicity detection, occupation prediction, and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via finetuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model. Though challenges remain, we show that UBM promises more efficient and accessible bias mitigation in LM fine-tuning. 12","{'sequence': 'Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring additional data collection and customized annotation of sensitive attributes, and re-evaluation of appropriate fairness metrics. We explore the feasibility and benefits of upstream bias mitigation (UBM) for reducing bias on downstream tasks, by first applying bias mitigation to an upstream model through fine-tuning and subsequently using it for downstream finetuning. We find, in extensive experiments across hate speech detection, toxicity detection, occupation prediction, and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via finetuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model. Though challenges remain, we show that UBM promises more efficient and accessible bias mitigation in LM fine-tuning. 12', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Ethics and NLP', 'NLP Applications', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07868468016386032, 0.06924395263195038, 0.0657438188791275, 0.06502768397331238, 0.05918798968195915, 0.055194590240716934, 0.054333098232746124, 0.04573153704404831, 0.04562593996524811, 0.04447821155190468, 0.044040944427251816, 0.043579742312431335, 0.0414259135723114, 0.040943443775177, 0.03714517503976822, 0.035993851721286774, 0.03014981560409069, 0.02909931167960167, 0.026580987498164177, 0.02520890347659588, 0.023712560534477234, 0.021897688508033752, 0.016970181837677956]}",0.07868468016386032,Question Answering,0.044040944427251816
Ethics and NLP,Case Study: Deontological Ethics in NLP,"Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.","{'sequence': 'Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.', 'labels': ['Ethics and NLP', 'NLP Applications', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Resources and Evaluation', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Machine Translation and Multilinguality', 'Information Extraction', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3832191824913025, 0.058556318283081055, 0.04338567331433296, 0.04059581086039543, 0.03974943235516548, 0.03870529308915138, 0.03801911324262619, 0.03583737090229988, 0.03440464660525322, 0.03204004094004631, 0.02746453508734703, 0.026253491640090942, 0.02498539909720421, 0.02434021607041359, 0.02121453359723091, 0.020778775215148926, 0.019078725948929787, 0.01745162531733513, 0.017374033108353615, 0.01600019633769989, 0.015090031549334526, 0.013921741396188736, 0.011533843353390694]}",0.3832191824913025,Ethics and NLP,0.3832191824913025
Ethics and NLP,Privacy Regularization: Joint Privacy-Utility Optimization in LanguageModels,"Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy implications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacypreserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups. * Work done as part of an MSR internship. 1 A naïve example is an attacker querying ""My account number is"" and hoping to receive a user's account number.","{'sequence': 'Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy implications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacypreserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups. * Work done as part of an MSR internship. 1 A naïve example is an attacker querying ""My account number is"" and hoping to receive a user\'s account number.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Speech and Multimodality', 'Information Extraction', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Summarization', 'Machine Learning for NLP', 'Generation', 'Computational Social Science and Social Media', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1254844069480896, 0.07239481806755066, 0.06551624834537506, 0.05853457376360893, 0.0584644190967083, 0.052228640764951706, 0.05133382976055145, 0.04551547393202782, 0.04252251237630844, 0.0403578020632267, 0.0396655909717083, 0.038879428058862686, 0.03468978404998779, 0.03457334637641907, 0.03212985023856163, 0.03189036622643471, 0.030410947278141975, 0.029626643285155296, 0.029217302799224854, 0.022724531590938568, 0.02256893739104271, 0.020859742537140846, 0.02041085809469223]}",0.1254844069480896,Resources and Evaluation,0.05853457376360893
Interpretability and Analysis of Models for NLP,Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures,"When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models' generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.","{'sequence': ""When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models' generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data."", 'labels': ['Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Computational Social Science and Social Media', 'NLP Applications', 'Summarization', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.24873961508274078, 0.07549034804105759, 0.06859340518712997, 0.0635194480419159, 0.04815287888050079, 0.04788711667060852, 0.043017324060201645, 0.04080228507518768, 0.03986089304089546, 0.03481690213084221, 0.0301821269094944, 0.028203802183270454, 0.027397891506552696, 0.025367554277181625, 0.023258475586771965, 0.02301262877881527, 0.022961802780628204, 0.02140139415860176, 0.020754944533109665, 0.0201612189412117, 0.018169954419136047, 0.014764784835278988, 0.013483187183737755]}",0.24873961508274078,Question Answering,0.043017324060201645
Interpretability and Analysis of Models for NLP,Discourse Probing of Pretrained Language Models,"Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse -but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.","{'sequence': 'Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse -but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.', 'labels': ['Speech and Multimodality', 'Discourse and Pragmatics', 'Information Extraction', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Question Answering', 'Generation', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08462809026241302, 0.08219088613986969, 0.07770927995443344, 0.07209105044603348, 0.06653055548667908, 0.05950409546494484, 0.04972057789564133, 0.04909440129995346, 0.045364875346422195, 0.04449419677257538, 0.044135693460702896, 0.04351145774126053, 0.04161325469613075, 0.03688238188624382, 0.031860046088695526, 0.031125731766223907, 0.03055259771645069, 0.027817323803901672, 0.02326265349984169, 0.0189145989716053, 0.017686501145362854, 0.012604394927620888, 0.008705359883606434]}",0.08462809026241302,Speech and Multimodality,0.04909440129995346
Interpretability and Analysis of Models for NLP,UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost,"Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the Transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of Transformer models. Specifically, we propose an approach named UniDrop to unite three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout. Theoretically, we demonstrate that these three dropouts play different roles from regularization perspectives. Empirically, we conduct experiments on both neural machine translation and text classification benchmark datasets. Extensive results indicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement on IWSLT14 translation tasks, and better accuracy for the classification even using strong pre-trained RoBERTa as backbone.","{'sequence': 'Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the Transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of Transformer models. Specifically, we propose an approach named UniDrop to unite three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout. Theoretically, we demonstrate that these three dropouts play different roles from regularization perspectives. Empirically, we conduct experiments on both neural machine translation and text classification benchmark datasets. Extensive results indicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement on IWSLT14 translation tasks, and better accuracy for the classification even using strong pre-trained RoBERTa as backbone.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Generation', 'Speech and Multimodality', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.19854062795639038, 0.13135768473148346, 0.07183491438627243, 0.0706266462802887, 0.060351695865392685, 0.04394565522670746, 0.039947014302015305, 0.03664722293615341, 0.0348316989839077, 0.03463342413306236, 0.03072497248649597, 0.028645653277635574, 0.02799942158162594, 0.026639893651008606, 0.02622002549469471, 0.02548789419233799, 0.020693402737379074, 0.019415032118558884, 0.018080484122037888, 0.017235124483704567, 0.016561461612582207, 0.012422537431120872, 0.007157524581998587]}",0.19854062795639038,Machine Learning for NLP,0.07183491438627243
Interpretability and Analysis of Models for NLP,Learning to Learn to be Right for the Right Reasons,"Improving model generalization on held-out data is one of the core objectives in commonsense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy instances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.","{'sequence': 'Improving model generalization on held-out data is one of the core objectives in commonsense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy instances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Semantics: Lexical Semantics', 'NLP Applications', 'Discourse and Pragmatics', 'Ethics and NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.11200612783432007, 0.0853690505027771, 0.06999499350786209, 0.06716687977313995, 0.058736588805913925, 0.05661781132221222, 0.05209887772798538, 0.050132159143686295, 0.04562484100461006, 0.044329699128866196, 0.03977283835411072, 0.03756452351808548, 0.0373048298060894, 0.03622308373451233, 0.02930835261940956, 0.027282487601041794, 0.027109868824481964, 0.025796961039304733, 0.024405037984251976, 0.02099611423909664, 0.017994500696659088, 0.017268089577555656, 0.01689627580344677]}",0.11200612783432007,Resources and Evaluation,0.027109868824481964
Interpretability and Analysis of Models for NLP,Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation,"Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a ""double perturbation"" framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models' robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/ nlp-second-order-attack.","{'sequence': 'Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a ""double perturbation"" framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models\' robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/ nlp-second-order-attack.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Machine Learning for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Summarization', 'Generation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.08566651493310928, 0.06091240420937538, 0.05925946310162544, 0.05712365731596947, 0.056097693741321564, 0.05165497958660126, 0.04882889986038208, 0.047058992087841034, 0.046181488782167435, 0.0446491502225399, 0.04431943595409393, 0.04268541932106018, 0.04134953394532204, 0.038710854947566986, 0.037703219801187515, 0.03498028218746185, 0.034748535603284836, 0.034260667860507965, 0.02917160838842392, 0.028584754094481468, 0.027151167392730713, 0.02698429860174656, 0.021916896104812622]}",0.08566651493310928,Resources and Evaluation,0.046181488782167435
Interpretability and Analysis of Models for NLP,Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks,"Explaining neural network models is important for increasing their trustworthiness in realworld applications. Most existing methods generate post-hoc explanations for neural network models by identifying individual feature attributions or detecting interactions between adjacent features. However, for models with text pairs as inputs (e.g., paraphrase identification), existing methods are not sufficient to capture feature interactions between two texts and their simple extension of computing all word-pair interactions between two texts is computationally inefficient. In this work, we propose the Group Mask (GMASK) method to implicitly detect word correlations by grouping correlated words from the input text pair together and measure their contribution to the corresponding NLP tasks as a whole. The proposed method is evaluated with two different model architectures (decomposable attention model and BERT) across four datasets, including natural language inference and paraphrase identification tasks. Experiments show the effectiveness of GMASK in providing faithful explanations to these models 1 .","{'sequence': 'Explaining neural network models is important for increasing their trustworthiness in realworld applications. Most existing methods generate post-hoc explanations for neural network models by identifying individual feature attributions or detecting interactions between adjacent features. However, for models with text pairs as inputs (e.g., paraphrase identification), existing methods are not sufficient to capture feature interactions between two texts and their simple extension of computing all word-pair interactions between two texts is computationally inefficient. In this work, we propose the Group Mask (GMASK) method to implicitly detect word correlations by grouping correlated words from the input text pair together and measure their contribution to the corresponding NLP tasks as a whole. The proposed method is evaluated with two different model architectures (decomposable attention model and BERT) across four datasets, including natural language inference and paraphrase identification tasks. Experiments show the effectiveness of GMASK in providing faithful explanations to these models 1 .', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Machine Learning for NLP', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Summarization', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2706431448459625, 0.10595638304948807, 0.06961149722337723, 0.06370698660612106, 0.06346111744642258, 0.04450737312436104, 0.04385630413889885, 0.03510667011141777, 0.03296526148915291, 0.032358087599277496, 0.0302505474537611, 0.026330308988690376, 0.02575450763106346, 0.02448529191315174, 0.023151442408561707, 0.022105956450104713, 0.021311147138476372, 0.0203720610588789, 0.012206711806356907, 0.010122902691364288, 0.010122736915946007, 0.006554058287292719, 0.005059455521404743]}",0.2706431448459625,NLP Applications,0.10595638304948807
Machine Translation and Multilinguality,Almost Free Semantic Draft for Neural Machine Translation,"Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible semantic from the semantic space. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.","{'sequence': 'Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible semantic from the semantic space. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.', 'labels': ['Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Generation', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Question Answering', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09442480653524399, 0.08516906946897507, 0.0842868983745575, 0.08072778582572937, 0.07734549045562744, 0.061788078397512436, 0.051573362201452255, 0.04086922109127045, 0.038950953632593155, 0.03736349567770958, 0.03727472200989723, 0.03354020416736603, 0.03221834823489189, 0.030912669375538826, 0.03071199543774128, 0.030327176675200462, 0.025936661288142204, 0.024811584502458572, 0.023920873180031776, 0.02300439216196537, 0.02227625995874405, 0.016675328835844994, 0.01589064486324787]}",0.09442480653524399,Speech and Multimodality,0.07734549045562744
Machine Translation and Multilinguality,Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation,"Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of ""divide and conquer"" which is based on the importance of neurons or parameters for the translation model. In this method, we first prune the model and only keep the important neurons or parameters, making them responsible for both generaldomain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the model to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines.","{'sequence': 'Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of ""divide and conquer"" which is based on the importance of neurons or parameters for the translation model. In this method, we first prune the model and only keep the important neurons or parameters, making them responsible for both generaldomain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the model to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Information Extraction', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Speech and Multimodality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.0938740074634552, 0.088529072701931, 0.07430614531040192, 0.0634906068444252, 0.06203414127230644, 0.06054913252592087, 0.05911485478281975, 0.05536537244915962, 0.04593832418322563, 0.03682185709476471, 0.03668227791786194, 0.034571316093206406, 0.034219805151224136, 0.03353407606482506, 0.03230297937989235, 0.030623523518443108, 0.028034817427396774, 0.027058573439717293, 0.02564050443470478, 0.02371302992105484, 0.02156229130923748, 0.017147529870271683, 0.014885791577398777]}",0.0938740074634552,Resources and Evaluation,0.0634906068444252
Machine Translation and Multilinguality,Multi-Hop Transformer for Document-Level Machine Translation,"Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior -human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve documentlevel translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.","{'sequence': 'Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior -human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve documentlevel translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.', 'labels': ['Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'NLP Applications', 'Machine Learning for NLP', 'Resources and Evaluation', 'Information Extraction', 'Generation', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Speech and Multimodality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13935452699661255, 0.07593648880720139, 0.0645100399851799, 0.0606091171503067, 0.05943476781249046, 0.057816628366708755, 0.054039064794778824, 0.051405902951955795, 0.05033227056264877, 0.04961549490690231, 0.03954663127660751, 0.037487395107746124, 0.034689273685216904, 0.0329631082713604, 0.03209124132990837, 0.0287714134901762, 0.026681222021579742, 0.021600689738988876, 0.021008197218179703, 0.020112719386816025, 0.015768904238939285, 0.014358428306877613, 0.011866484768688679]}",0.13935452699661255,Machine Translation and Multilinguality,0.13935452699661255
Machine Translation and Multilinguality,Continual Learning for Neural Machine Translation,"Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings. 1","{'sequence': 'Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings. 1', 'labels': ['Resources and Evaluation', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Question Answering', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12831798195838928, 0.11401297897100449, 0.10341694205999374, 0.08383245766162872, 0.06241944432258606, 0.04664384946227074, 0.043036721646785736, 0.042153894901275635, 0.04167396202683449, 0.04141458496451378, 0.0402291901409626, 0.03298773616552353, 0.029999738559126854, 0.02826196700334549, 0.025756334885954857, 0.023196415975689888, 0.020987866446375847, 0.020237324759364128, 0.019980009645223618, 0.017061321064829826, 0.013096804730594158, 0.011013464070856571, 0.010269006714224815]}",0.12831798195838928,Resources and Evaluation,0.11401297897100449
Machine Translation and Multilinguality,Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios,"Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. * Part of this work was done when Haipeng Sun and Rui Wang were an internship research fellow and a researcher at NICT, respectively.","{'sequence': 'Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. * Part of this work was done when Haipeng Sun and Rui Wang were an internship research fellow and a researcher at NICT, respectively.', 'labels': ['Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Question Answering', 'Resources and Evaluation', 'NLP Applications', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Extraction', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09824857115745544, 0.0717860609292984, 0.06164192780852318, 0.059635575860738754, 0.05638495460152626, 0.05067478492856026, 0.04818352684378624, 0.04575493931770325, 0.045445166528224945, 0.04423562064766884, 0.042987797409296036, 0.04065681993961334, 0.03787095472216606, 0.03523591160774231, 0.03303782269358635, 0.03282071277499199, 0.0313447006046772, 0.031173348426818848, 0.02980402298271656, 0.02929508686065674, 0.02837713435292244, 0.025928093120455742, 0.01947641931474209]}",0.09824857115745544,Machine Translation and Multilinguality,0.09824857115745544
Machine Translation and Multilinguality,Smart-Start Decoding for Neural Machine Translation,"Most current neural machine translation models adopt a monotonic decoding order of either left-to-right or right-to-left. In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. More specifically, our method first predicts a median word. It starts to decode the words on the right side of the median word and then generates words on the left. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental results show that the proposed method can significantly outperform strong baseline models.","{'sequence': 'Most current neural machine translation models adopt a monotonic decoding order of either left-to-right or right-to-left. In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. More specifically, our method first predicts a median word. It starts to decode the words on the right side of the median word and then generates words on the left. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental results show that the proposed method can significantly outperform strong baseline models.', 'labels': ['Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Resources and Evaluation', 'Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Computational Social Science and Social Media', 'Summarization', 'NLP Applications', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10337559133768082, 0.08279212564229965, 0.08276788890361786, 0.08271584659814835, 0.07326412200927734, 0.05596502125263214, 0.04654358699917793, 0.0450744703412056, 0.04070301353931427, 0.039291154593229294, 0.035186197608709335, 0.03370087221264839, 0.03343202546238899, 0.03049357607960701, 0.02888769470155239, 0.02842964418232441, 0.02800997532904148, 0.02648853324353695, 0.02541009522974491, 0.025129146873950958, 0.02295304462313652, 0.017406051978468895, 0.011980315670371056]}",0.10337559133768082,Machine Translation and Multilinguality,0.10337559133768082
Machine Translation and Multilinguality,Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation,"Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties of source sentences. Therefore, we propose to adopt multi-task learning to transfer the AT knowledge to NAT models through encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 English⇔German and WMT16 English⇔Romanian datasets show that the proposed MULTI-TASK NAT achieves significant improvements over the baseline NAT models. Furthermore, the performance on large-scale WMT19 and WMT20 English⇔German datasets confirm the consistency of our proposed method. In addition, experimental results demonstrate that our MULTI-TASK NAT is complementary to knowledge distillation, the standard knowledge transfer method for NAT. 1","{'sequence': 'Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties of source sentences. Therefore, we propose to adopt multi-task learning to transfer the AT knowledge to NAT models through encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 English⇔German and WMT16 English⇔Romanian datasets show that the proposed MULTI-TASK NAT achieves significant improvements over the baseline NAT models. Furthermore, the performance on large-scale WMT19 and WMT20 English⇔German datasets confirm the consistency of our proposed method. In addition, experimental results demonstrate that our MULTI-TASK NAT is complementary to knowledge distillation, the standard knowledge transfer method for NAT. 1', 'labels': ['Speech and Multimodality', 'Machine Translation and Multilinguality', 'Question Answering', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'NLP Applications'], 'scores': [0.1035478338599205, 0.08315121382474899, 0.08159732073545456, 0.07344304025173187, 0.0563167929649353, 0.05134991928935051, 0.049555011093616486, 0.04473938047885895, 0.04354623705148697, 0.043020863085985184, 0.04207340255379677, 0.036454930901527405, 0.03316258639097214, 0.03240472823381424, 0.030669638887047768, 0.029871180653572083, 0.027251077815890312, 0.026489077135920525, 0.026178983971476555, 0.025532593950629234, 0.02172325737774372, 0.019459959119558334, 0.018461043015122414]}",0.1035478338599205,Speech and Multimodality,0.08315121382474899
NLP Applications,ER-AE: Differentially Private Text Generation for Authorship Anonymization,"Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preserving text mining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed model on the reallife peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation.","{'sequence': 'Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preserving text mining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed model on the reallife peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation.', 'labels': ['Generation', 'Information Retrieval and Text Mining', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Question Answering', 'NLP Applications', 'Discourse and Pragmatics', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13987822830677032, 0.09101506322622299, 0.07192271947860718, 0.05811005085706711, 0.05557917058467865, 0.05518468841910362, 0.05017116293311119, 0.04241284728050232, 0.04222549498081207, 0.04150111600756645, 0.040402766317129135, 0.035798732191324234, 0.033473506569862366, 0.02941373735666275, 0.029246097430586815, 0.028354618698358536, 0.026620402932167053, 0.023781614378094673, 0.023213760927319527, 0.023193132132291794, 0.022885996848344803, 0.019358476623892784, 0.016256583854556084]}",0.13987822830677032,Generation,0.033473506569862366
NLP Applications,Distantly Supervised Transformers For E-Commerce Product QA,"We propose a practical instant question answering (QA) system on product pages of ecommerce services, where for each user query, relevant community question answer (CQA) pairs are retrieved. User queries and CQA pairs differ significantly in language characteristics making relevance learning difficult. Our proposed transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations without the need for human labeled data. This is achieved by distantly supervising our model by distilling from predictions of a syntactic matching system on user queries and simultaneously training with CQA pairs. Training with CQA pairs helps our model learning semantic QA relevance and distant supervision enables learning of syntactic features as well as the nuances of user querying language. Additionally, our model encodes queries and candidate responses independently allowing offline candidate embedding generation thereby minimizing the need for real-time transformer model execution. Consequently, our framework is able to scale to large e-commerce QA traffic. Extensive evaluation on user queries shows that our framework significantly outperforms both syntactic and semantic baselines in offline as well as large scale online A/B setups of a popular e-commerce service.","{'sequence': 'We propose a practical instant question answering (QA) system on product pages of ecommerce services, where for each user query, relevant community question answer (CQA) pairs are retrieved. User queries and CQA pairs differ significantly in language characteristics making relevance learning difficult. Our proposed transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations without the need for human labeled data. This is achieved by distantly supervising our model by distilling from predictions of a syntactic matching system on user queries and simultaneously training with CQA pairs. Training with CQA pairs helps our model learning semantic QA relevance and distant supervision enables learning of syntactic features as well as the nuances of user querying language. Additionally, our model encodes queries and candidate responses independently allowing offline candidate embedding generation thereby minimizing the need for real-time transformer model execution. Consequently, our framework is able to scale to large e-commerce QA traffic. Extensive evaluation on user queries shows that our framework significantly outperforms both syntactic and semantic baselines in offline as well as large scale online A/B setups of a popular e-commerce service.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Summarization', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2072993814945221, 0.12212050706148148, 0.06623152643442154, 0.0565093457698822, 0.04927369952201843, 0.04884018376469612, 0.03806168958544731, 0.03792452812194824, 0.0357973650097847, 0.035613372921943665, 0.03149613365530968, 0.03133905678987503, 0.02981574460864067, 0.026073461398482323, 0.023769591003656387, 0.02376367710530758, 0.022093409672379494, 0.02162923850119114, 0.020611409097909927, 0.02052622102200985, 0.018700242042541504, 0.01764819025993347, 0.014862151816487312]}",0.2072993814945221,Question Answering,0.01764819025993347
NLP Applications,Quantitative Day Trading from Natural Language using Reinforcement Learning,"It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-ofthe-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading.","{'sequence': 'It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-ofthe-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading.', 'labels': ['NLP Applications', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Question Answering', 'Information Retrieval and Text Mining', 'Information Extraction', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Machine Learning for NLP', 'Summarization', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.08971469104290009, 0.06364872306585312, 0.061466339975595474, 0.05998929589986801, 0.054152779281139374, 0.05301159992814064, 0.051183637231588364, 0.050480447709560394, 0.04700995981693268, 0.04500782489776611, 0.04496411606669426, 0.041966479271650314, 0.04118042811751366, 0.039053209125995636, 0.03817833214998245, 0.03789732977747917, 0.03755873814225197, 0.031337786465883255, 0.029820118099451065, 0.028707005083560944, 0.02371031418442726, 0.017212290316820145, 0.012748518027365208]}",0.08971469104290009,NLP Applications,0.08971469104290009
NLP Applications,Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation,"Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we present a multi-task learning approach to restore and translate historical documents based on a selfattention mechanism, specifically utilizing two Korean historical records, ones of the most voluminous historical records in the world. Experimental results show that our approach significantly improves the accuracy of the translation task than baselines without multi-task learning. In addition, we present an in-depth exploratory analysis on our translated results via topic modeling, uncovering several significant historical events.","{'sequence': 'Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we present a multi-task learning approach to restore and translate historical documents based on a selfattention mechanism, specifically utilizing two Korean historical records, ones of the most voluminous historical records in the world. Experimental results show that our approach significantly improves the accuracy of the translation task than baselines without multi-task learning. In addition, we present an in-depth exploratory analysis on our translated results via topic modeling, uncovering several significant historical events.', 'labels': ['Speech and Multimodality', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Summarization', 'Computational Social Science and Social Media', 'NLP Applications', 'Generation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.08856663852930069, 0.08639294654130936, 0.07561077177524567, 0.07084642350673676, 0.06724458187818527, 0.063084676861763, 0.05546090379357338, 0.04913612827658653, 0.04060409963130951, 0.04037150368094444, 0.03642408549785614, 0.03487255796790123, 0.03464382886886597, 0.03300836682319641, 0.03156642988324165, 0.03127294033765793, 0.031200559809803963, 0.03102198988199234, 0.026861675083637238, 0.024578861892223358, 0.01921718381345272, 0.016689378768205643, 0.011323342099785805]}",0.08856663852930069,Speech and Multimodality,0.04060409963130951
NLP Applications,Modeling Diagnostic Label Correlation for Automatic ICD Coding,"Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD coding by capturing the label correlation. Specifically, we train a label set distribution estimator to rescore the probability of each label set candidate generated by a base predictor. This paper is the first attempt at learning the label set distribution as a reranking module for medical code prediction. In the experiments, our proposed framework is able to improve upon best-performing predictors on the benchmark MIMIC datasets. 1","{'sequence': 'Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD coding by capturing the label correlation. Specifically, we train a label set distribution estimator to rescore the probability of each label set candidate generated by a base predictor. This paper is the first attempt at learning the label set distribution as a reranking module for medical code prediction. In the experiments, our proposed framework is able to improve upon best-performing predictors on the benchmark MIMIC datasets. 1', 'labels': ['Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Question Answering', 'Summarization', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09589923918247223, 0.07717277854681015, 0.0641133189201355, 0.058561891317367554, 0.058028049767017365, 0.05307312682271004, 0.05210112780332565, 0.04788864776492119, 0.046770211309194565, 0.04662064462900162, 0.0463070273399353, 0.04439222812652588, 0.041313186287879944, 0.03594444692134857, 0.031111404299736023, 0.030573656782507896, 0.030057745054364204, 0.02820739336311817, 0.027695918455719948, 0.023729346692562103, 0.020977918058633804, 0.020623475313186646, 0.018837181851267815]}",0.09589923918247223,Dialogue and Interactive Systems,0.030057745054364204
NLP Applications,Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents,"Turn-level user satisfaction is one of the most important performance metrics for conversational agents. It can be used to monitor the agent's performance and provide insights about defective user experiences. While endto-end deep learning has shown promising results, having access to a large number of reliable annotated samples required by these methods remains challenging. In a large-scale conversational system, there is a growing number of newly developed skills, making the traditional data collection, annotation, and modeling process impractical due to the required annotation costs and the turnaround times. In this paper, we suggest a self-supervised contrastive learning approach that leverages the pool of unlabeled data to learn user-agent interactions. We show that the pre-trained models using the self-supervised objective are transferable to the user satisfaction prediction. In addition, we propose a novel few-shot transfer learning approach that ensures better transferability for very small sample sizes. The suggested few-shot method does not require any inner loop optimization process and is scalable to very large datasets and complex models. Based on our experiments using real data from a large-scale commercial system, the suggested approach is able to significantly reduce the required number of annotations, while improving the generalization on unseen skills.","{'sequence': ""Turn-level user satisfaction is one of the most important performance metrics for conversational agents. It can be used to monitor the agent's performance and provide insights about defective user experiences. While endto-end deep learning has shown promising results, having access to a large number of reliable annotated samples required by these methods remains challenging. In a large-scale conversational system, there is a growing number of newly developed skills, making the traditional data collection, annotation, and modeling process impractical due to the required annotation costs and the turnaround times. In this paper, we suggest a self-supervised contrastive learning approach that leverages the pool of unlabeled data to learn user-agent interactions. We show that the pre-trained models using the self-supervised objective are transferable to the user satisfaction prediction. In addition, we propose a novel few-shot transfer learning approach that ensures better transferability for very small sample sizes. The suggested few-shot method does not require any inner loop optimization process and is scalable to very large datasets and complex models. Based on our experiments using real data from a large-scale commercial system, the suggested approach is able to significantly reduce the required number of annotations, while improving the generalization on unseen skills."", 'labels': ['Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Resources and Evaluation', 'Ethics and NLP', 'NLP Applications', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08546742796897888, 0.07177448272705078, 0.06496627628803253, 0.058715078979730606, 0.058519914746284485, 0.05249430984258652, 0.047572795301675797, 0.047030236572027206, 0.04471195489168167, 0.042447492480278015, 0.040954526513814926, 0.040696881711483, 0.039760489016771317, 0.03960258886218071, 0.03944212943315506, 0.03447588533163071, 0.03371041268110275, 0.03303159028291702, 0.02871115319430828, 0.02690097503364086, 0.026631278917193413, 0.025403160601854324, 0.016978992149233818]}",0.08546742796897888,Dialogue and Interactive Systems,0.05249430984258652
Discourse and Pragmatics,Predicting Discourse Trees from Transformer-based Neural Summarizers,"Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency-and constituencystyle discourse information, which is typically encoded in a single head, covering long-and short-distance discourse dependencies. Overall, the experimental results suggest that the learned discourse information is general and transferable inter-domain 1 . Recent neural summarization models are typically based on transformers (Liu and Lapata, 2019a; Zhang et al., 2019) . One advantage of these mod-","{'sequence': 'Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency-and constituencystyle discourse information, which is typically encoded in a single head, covering long-and short-distance discourse dependencies. Overall, the experimental results suggest that the learned discourse information is general and transferable inter-domain 1 . Recent neural summarization models are typically based on transformers (Liu and Lapata, 2019a; Zhang et al., 2019) . One advantage of these mod-', 'labels': ['Summarization', 'Generation', 'Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Machine Learning for NLP', 'NLP Applications'], 'scores': [0.41663238406181335, 0.07181305438280106, 0.05836262181401253, 0.042715415358543396, 0.03851132094860077, 0.03375763073563576, 0.03172923997044563, 0.028104452416300774, 0.023947931826114655, 0.022806446999311447, 0.022705011069774628, 0.02266433835029602, 0.02190881036221981, 0.02097437158226967, 0.020314445719122887, 0.019747506827116013, 0.019121352583169937, 0.016319086775183678, 0.015109831467270851, 0.01387742068618536, 0.01366401743143797, 0.01267990656197071, 0.01253347098827362]}",0.41663238406181335,Summarization,0.028104452416300774
Discourse and Pragmatics,Probing for Bridging Inference in Transformer Language Models,"We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations incomparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of-Cloze test). Our formulation produces optimistic results without any finetuning, which indicates that pre-trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference.","{'sequence': 'We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations incomparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of-Cloze test). Our formulation produces optimistic results without any finetuning, which indicates that pre-trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference.', 'labels': ['Resources and Evaluation', 'Generation', 'Question Answering', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Summarization', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.07795452326536179, 0.07287480682134628, 0.06333217024803162, 0.05894586443901062, 0.05791042000055313, 0.0537121407687664, 0.053510721772909164, 0.052040159702301025, 0.050570372492074966, 0.04848974943161011, 0.04770185425877571, 0.04420764744281769, 0.03777641803026199, 0.035633429884910583, 0.03454406186938286, 0.03173697739839554, 0.02971818298101425, 0.029052814468741417, 0.028518350794911385, 0.025918520987033844, 0.02452838607132435, 0.022400114685297012, 0.018922368064522743]}",0.07795452326536179,Resources and Evaluation,0.03173697739839554
Discourse and Pragmatics,Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models,"Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, allowing for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective do encode. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence.","{'sequence': 'Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, allowing for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective do encode. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Question Answering', 'Discourse and Pragmatics', 'NLP Applications', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.12193030118942261, 0.0720532014966011, 0.0677078366279602, 0.0659811794757843, 0.061683591455221176, 0.05499374493956566, 0.05367565155029297, 0.046252526342868805, 0.04400543123483658, 0.042706556618213654, 0.0376037061214447, 0.03621605038642883, 0.03620804846286774, 0.03541065752506256, 0.03313339874148369, 0.031286511570215225, 0.028569484129548073, 0.026354700326919556, 0.02635253220796585, 0.021296929568052292, 0.020014215260744095, 0.01847798191010952, 0.018085746094584465]}",0.12193030118942261,Resources and Evaluation,0.0659811794757843
Discourse and Pragmatics,Stay Together: A System for Single and Split-antecedent Anaphora Resolution,"The state-of-the-art on basic, singleantecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in Time-Warner is considering a legal challenge to Telecommunications Inc's plan to buy half of Showtime Networks Inc-a move that could lead to all-out war between the two powerful companies. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora; as a result, it is not annotated in many datasets designed to test coreference, and previous work on resolving this type of anaphora was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics. 1","{'sequence': ""The state-of-the-art on basic, singleantecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in Time-Warner is considering a legal challenge to Telecommunications Inc's plan to buy half of Showtime Networks Inc-a move that could lead to all-out war between the two powerful companies. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora; as a result, it is not annotated in many datasets designed to test coreference, and previous work on resolving this type of anaphora was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics. 1"", 'labels': ['Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Extraction', 'Semantics: Lexical Semantics', 'Generation', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Summarization', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.08562853932380676, 0.07971668243408203, 0.07280100136995316, 0.06661516427993774, 0.0607936792075634, 0.05437535047531128, 0.053368307650089264, 0.04940441995859146, 0.04323909431695938, 0.0430094413459301, 0.042310044169425964, 0.04061102867126465, 0.04010305181145668, 0.03946390375494957, 0.037975750863552094, 0.03324862942099571, 0.02878338284790516, 0.025906965136528015, 0.02291215769946575, 0.022508548572659492, 0.02101827599108219, 0.018104972317814827, 0.01810169592499733]}",0.08562853932380676,Resources and Evaluation,0.042310044169425964
Information Retrieval and Text Mining,Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness,"Neural keyphrase generation models have recently attracted much interest due to their ability to output absent keyphrases, that is, keyphrases that do not appear in the source text. In this paper, we discuss the usefulness of absent keyphrases from an Information Retrieval (IR) perspective, and show that the commonly drawn distinction between present and absent keyphrases is not made explicit enough. We introduce a finer-grained categorization scheme that sheds more light on the impact of absent keyphrases on scientific document retrieval. Under this scheme, we find that only a fraction (around 20%) of the words that make up keyphrases actually serves as document expansion, but that this small fraction of words is behind much of the gains observed in retrieval effectiveness. We also discuss how the proposed scheme can offer a new angle to evaluate the output of neural keyphrase generation models.","{'sequence': 'Neural keyphrase generation models have recently attracted much interest due to their ability to output absent keyphrases, that is, keyphrases that do not appear in the source text. In this paper, we discuss the usefulness of absent keyphrases from an Information Retrieval (IR) perspective, and show that the commonly drawn distinction between present and absent keyphrases is not made explicit enough. We introduce a finer-grained categorization scheme that sheds more light on the impact of absent keyphrases on scientific document retrieval. Under this scheme, we find that only a fraction (around 20%) of the words that make up keyphrases actually serves as document expansion, but that this small fraction of words is behind much of the gains observed in retrieval effectiveness. We also discuss how the proposed scheme can offer a new angle to evaluate the output of neural keyphrase generation models.', 'labels': ['Information Retrieval and Text Mining', 'Generation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0777762159705162, 0.0740734115242958, 0.07243738323450089, 0.06515700370073318, 0.05920228362083435, 0.05558519437909126, 0.04884403198957443, 0.04829854890704155, 0.0473330095410347, 0.044084932655096054, 0.040413256734609604, 0.03990741819143295, 0.039532747119665146, 0.035871054977178574, 0.03384928032755852, 0.03237398341298103, 0.031693704426288605, 0.02918144501745701, 0.028438523411750793, 0.026934228837490082, 0.026836473494768143, 0.025064874440431595, 0.017111126333475113]}",0.0777762159705162,Information Retrieval and Text Mining,0.0777762159705162
Information Retrieval and Text Mining,CoRT: Complementary Rankings from Transformers,"Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as BM25. Although BM25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing BM25 with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using CoRT can be realized with surprisingly low latencies.","{'sequence': 'Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as BM25. Although BM25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing BM25 with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using CoRT can be realized with surprisingly low latencies.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Summarization', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Ethics and NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.2033659964799881, 0.092546746134758, 0.0671195313334465, 0.06311098486185074, 0.048971522599458694, 0.04048975929617882, 0.039907924830913544, 0.03663943335413933, 0.03633750230073929, 0.03511061146855354, 0.034345753490924835, 0.03416416049003601, 0.03180360049009323, 0.03140103071928024, 0.03046535886824131, 0.02674959972500801, 0.025504080578684807, 0.02459053508937359, 0.02451225183904171, 0.021700138226151466, 0.01905643194913864, 0.01770022138953209, 0.014406801201403141]}",0.2033659964799881,Information Extraction,0.048971522599458694
Information Retrieval and Text Mining,Multi-source Neural Topic Modeling in Multi-view Embedding Spaces,"Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embedding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (contextinsensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (highresource) and 5 target (low-resource) corpora. * : equal contribution Topic Topic Words Topic Label Z 1 (S 1 ) profit, growth, stocks, apple, fall, Trading consumer, buy, billion, shares Z 2 (S 2 ) smartphone, ipad, apple, app, Product Line iphone, devices, phone, tablet Z 3 (S 3 ) microsoft, mac, linux, ibm, ios, Operating System apple, xp, windows, software Z 4 (T ) apple, talk, computers, shares, ? disease, driver, electronics, profit, ios","{'sequence': 'Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embedding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (contextinsensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (highresource) and 5 target (low-resource) corpora. * : equal contribution Topic Topic Words Topic Label Z 1 (S 1 ) profit, growth, stocks, apple, fall, Trading consumer, buy, billion, shares Z 2 (S 2 ) smartphone, ipad, apple, app, Product Line iphone, devices, phone, tablet Z 3 (S 3 ) microsoft, mac, linux, ibm, ios, Operating System apple, xp, windows, software Z 4 (T ) apple, talk, computers, shares, ? disease, driver, electronics, profit, ios', 'labels': ['Summarization', 'Information Retrieval and Text Mining', 'Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Question Answering', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'NLP Applications', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP'], 'scores': [0.07864145189523697, 0.069124236702919, 0.06816970556974411, 0.06807249039411545, 0.0649409294128418, 0.061312947422266006, 0.05323098972439766, 0.05003224313259125, 0.049163177609443665, 0.04755749925971031, 0.042019858956336975, 0.038889117538928986, 0.03588234260678291, 0.03318941965699196, 0.03283950686454773, 0.032346513122320175, 0.03202301636338234, 0.030772510915994644, 0.023851286619901657, 0.023309839889407158, 0.02201199159026146, 0.021342044696211815, 0.021276870742440224]}",0.07864145189523697,Summarization,0.069124236702919
Information Retrieval and Text Mining,Inductive Topic Variational Graph Auto-Encoder for Text Classification,"Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE), which incorporates a topic model into variational graph-auto-encoder (VGAE) to capture the hidden semantic information between documents and words. T-VGAE inherits the interpretability of the topic model and the efficient information propagation mechanism of VGAE. It learns probabilistic representations of words and documents by jointly encoding and reconstructing the global wordlevel graph and bipartite graphs of documents, where each document is considered individually and decoupled from the global correlation graph so as to enable inductive learning. Our experiments on several benchmark datasets show that our method outperforms the existing competitive models on supervised and semi-supervised text classification, as well as unsupervised text representation learning. In addition, it has higher interpretability and is able to deal with unseen documents.","{'sequence': 'Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE), which incorporates a topic model into variational graph-auto-encoder (VGAE) to capture the hidden semantic information between documents and words. T-VGAE inherits the interpretability of the topic model and the efficient information propagation mechanism of VGAE. It learns probabilistic representations of words and documents by jointly encoding and reconstructing the global wordlevel graph and bipartite graphs of documents, where each document is considered individually and decoupled from the global correlation graph so as to enable inductive learning. Our experiments on several benchmark datasets show that our method outperforms the existing competitive models on supervised and semi-supervised text classification, as well as unsupervised text representation learning. In addition, it has higher interpretability and is able to deal with unseen documents.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Generation', 'Summarization', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09597046673297882, 0.079810731112957, 0.07370395958423615, 0.07330478727817535, 0.06696910411119461, 0.06331764906644821, 0.05389046669006348, 0.047955650836229324, 0.04066116362810135, 0.03873659297823906, 0.03798675164580345, 0.03796311840415001, 0.03500508517026901, 0.03285307437181473, 0.03187079727649689, 0.029979076236486435, 0.028859838843345642, 0.025632765144109726, 0.024743445217609406, 0.023404547944664955, 0.02170431800186634, 0.021310875192284584, 0.014365682378411293]}",0.09597046673297882,Information Extraction,0.03796311840415001
Information Retrieval and Text Mining,Self-Alignment Pretraining for Biomedical Entity Representations,"Despite the widespread success of selfsupervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SAPBERT, a pretraining scheme that selfaligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipelinebased hybrid systems, SAPBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without taskspecific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BIOBERT, SCIBERT and PUB-MEDBERT, our pretraining scheme proves to be both effective and robust. 1","{'sequence': 'Despite the widespread success of selfsupervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SAPBERT, a pretraining scheme that selfaligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipelinebased hybrid systems, SAPBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without taskspecific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BIOBERT, SCIBERT and PUB-MEDBERT, our pretraining scheme proves to be both effective and robust. 1', 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'NLP Applications', 'Discourse and Pragmatics', 'Information Extraction', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.11209363490343094, 0.06987074017524719, 0.06497273594141006, 0.06439883261919022, 0.05799811705946922, 0.05514272302389145, 0.05426767095923424, 0.0508284866809845, 0.0439688079059124, 0.041345011442899704, 0.04124104976654053, 0.037445068359375, 0.033473141491413116, 0.03329635038971901, 0.03107820451259613, 0.030856210738420486, 0.030558189377188683, 0.03041112795472145, 0.02962237037718296, 0.02289951965212822, 0.02185342274606228, 0.02122635394334793, 0.021152254194021225]}",0.11209363490343094,Dialogue and Interactive Systems,0.030558189377188683
Information Retrieval and Text Mining,TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names,"Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its ""core classes"", and then check core classes' ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document's core classes and utilizes confident core classes to train a taxonomyenhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%.","{'sequence': 'Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its ""core classes"", and then check core classes\' ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document\'s core classes and utilizes confident core classes to train a taxonomyenhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%.', 'labels': ['Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Generation', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1002659872174263, 0.08145547658205032, 0.07567426562309265, 0.07262995094060898, 0.0646054595708847, 0.0547826774418354, 0.04717515781521797, 0.04664294794201851, 0.0428621880710125, 0.04276818037033081, 0.03935062512755394, 0.03818070515990257, 0.0356810986995697, 0.033154863864183426, 0.03154262527823448, 0.02954975888133049, 0.029180431738495827, 0.028585072606801987, 0.028034070506691933, 0.023999100551009178, 0.023292483761906624, 0.01601186767220497, 0.014574882574379444]}",0.1002659872174263,Dialogue and Interactive Systems,0.03818070515990257
Generation,MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding,"Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (Jacobs, 2018) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model finetuned on our parallel data to generate high quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. Moreover, a task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.","{'sequence': 'Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (Jacobs, 2018) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model finetuned on our parallel data to generate high quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. Moreover, a task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.', 'labels': ['Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'NLP Applications', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.18635793030261993, 0.08675075322389603, 0.07819034159183502, 0.05939392000436783, 0.048231735825538635, 0.04776495695114136, 0.0471358597278595, 0.04184979945421219, 0.0404231920838356, 0.035878561437129974, 0.035335723310709, 0.03433994576334953, 0.032661065459251404, 0.03257748484611511, 0.031657468527555466, 0.029251277446746826, 0.02750290557742119, 0.026088183745741844, 0.019679514691233635, 0.016289399936795235, 0.014602667652070522, 0.014571668580174446, 0.01346571370959282]}",0.18635793030261993,Generation,0.18635793030261993
Generation,On Learning Text Style Transfer with Direct Rewards,"In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the styletransferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose efficient strategies of using these metrics for training. The experimental results show that our model provides significant gains in both automatic and human evaluation over strong baselines, indicating the effectiveness of our proposed methods and training strategies. 1","{'sequence': 'In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the styletransferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose efficient strategies of using these metrics for training. The experimental results show that our model provides significant gains in both automatic and human evaluation over strong baselines, indicating the effectiveness of our proposed methods and training strategies. 1', 'labels': ['Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Ethics and NLP', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Discourse and Pragmatics', 'Generation', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Information Extraction', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Summarization'], 'scores': [0.0902533307671547, 0.06894876062870026, 0.06708871573209763, 0.05847117677330971, 0.05791148170828819, 0.052887480705976486, 0.05135519430041313, 0.0502113476395607, 0.05000244826078415, 0.049089252948760986, 0.04791790619492531, 0.04266263544559479, 0.039482954889535904, 0.03713184595108032, 0.034261226654052734, 0.03360472992062569, 0.032593466341495514, 0.026574982330203056, 0.0230473093688488, 0.02293449454009533, 0.02234460972249508, 0.021026821807026863, 0.020197845995426178]}",0.0902533307671547,Resources and Evaluation,0.039482954889535904
Generation,Focused Attention Improves Document-Grounded Generation,"Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks.","{'sequence': 'Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks.', 'labels': ['Generation', 'Resources and Evaluation', 'Information Extraction', 'Computational Social Science and Social Media', 'NLP Applications', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.22467930614948273, 0.08236420154571533, 0.07591819763183594, 0.05183473229408264, 0.0493229515850544, 0.04814041778445244, 0.045492012053728104, 0.04316647723317146, 0.041066259145736694, 0.03747158870100975, 0.03576049208641052, 0.035069067031145096, 0.031011151149868965, 0.03035830706357956, 0.02823547087609768, 0.026992706581950188, 0.022142410278320312, 0.01936132274568081, 0.01904371567070484, 0.01641198620200157, 0.01458682119846344, 0.011221042834222317, 0.010349319316446781]}",0.22467930614948273,Generation,0.22467930614948273
Generation,NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints,"Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of taskspecific examples. We propose NEUROLOGIC DECODING, a simple yet effective algorithm that enables neural language models -supervised or not -to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NEUROLOGIC DECODING outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NEUROLOGIC DECODING often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.","{'sequence': ""Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of taskspecific examples. We propose NEUROLOGIC DECODING, a simple yet effective algorithm that enables neural language models -supervised or not -to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NEUROLOGIC DECODING outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NEUROLOGIC DECODING often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms."", 'labels': ['Generation', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'NLP Applications', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Extraction', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1069667711853981, 0.06872899830341339, 0.06486893445253372, 0.06436532735824585, 0.06276742368936539, 0.05707775428891182, 0.04927237331867218, 0.045977406203746796, 0.04490283504128456, 0.043629299849271774, 0.04241194576025009, 0.04204457998275757, 0.03876752406358719, 0.03622065857052803, 0.03246152028441429, 0.032353129237890244, 0.03223443776369095, 0.028830096125602722, 0.026976700872182846, 0.026575788855552673, 0.024297280237078667, 0.02068919688463211, 0.007580118253827095]}",0.1069667711853981,Generation,0.1069667711853981
Generation,Progressive Generation of Long Text with Pretrained Language Models,"Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent. 1","{'sequence': 'Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent. 1', 'labels': ['Generation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Question Answering', 'Resources and Evaluation', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.1379176825284958, 0.08053215593099594, 0.07848647981882095, 0.06980626285076141, 0.06425527483224869, 0.05755108594894409, 0.05022416263818741, 0.0435808040201664, 0.03942616656422615, 0.03546065837144852, 0.03479841351509094, 0.03332067281007767, 0.03154529258608818, 0.03056652657687664, 0.02888028509914875, 0.02824183739721775, 0.02665621228516102, 0.025370536372065544, 0.02349339984357357, 0.022942259907722473, 0.02107587270438671, 0.019368113949894905, 0.016499919816851616]}",0.1379176825284958,Generation,0.1379176825284958
Resources and Evaluation,SOCCER: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain,"In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to simplified, fully observable systems that show some of these properties: Sports events. We curated 2,263 soccer matches including timestamped natural language commentary accompanied by discrete events such as a team scoring goals, switching players or being penalized with cards. We propose a new task formulation where, given paragraphs of commentary of a game at different timestamps, the system is asked to recognize the occurrence of in-game events. This domain allows for rich descriptions of state while avoiding the complexities of many other real-world settings. As an initial point of performance measurement, we include two baseline methods from the perspectives of sentence classification with temporal dependence and current state-of-the-art generative model, respectively, and demonstrate that even sophisticated existing methods struggle on the state tracking task when the definition of state broadens or non-event chatter becomes prevalent.","{'sequence': 'In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to simplified, fully observable systems that show some of these properties: Sports events. We curated 2,263 soccer matches including timestamped natural language commentary accompanied by discrete events such as a team scoring goals, switching players or being penalized with cards. We propose a new task formulation where, given paragraphs of commentary of a game at different timestamps, the system is asked to recognize the occurrence of in-game events. This domain allows for rich descriptions of state while avoiding the complexities of many other real-world settings. As an initial point of performance measurement, we include two baseline methods from the perspectives of sentence classification with temporal dependence and current state-of-the-art generative model, respectively, and demonstrate that even sophisticated existing methods struggle on the state tracking task when the definition of state broadens or non-event chatter becomes prevalent.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Question Answering', 'Generation', 'Information Extraction', 'Dialogue and Interactive Systems', 'NLP Applications', 'Discourse and Pragmatics', 'Summarization', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09187155216932297, 0.08522167056798935, 0.07858758419752121, 0.07388832420110703, 0.06805723905563354, 0.06692194193601608, 0.05204622074961662, 0.04631916061043739, 0.04264012351632118, 0.04101340100169182, 0.039962369948625565, 0.037126071751117706, 0.037123940885066986, 0.036726996302604675, 0.033932946622371674, 0.03201707825064659, 0.026150794699788094, 0.025556528940796852, 0.021906306967139244, 0.01910216733813286, 0.018847204744815826, 0.01610725373029709, 0.008873159997165203]}",0.09187155216932297,Resources and Evaluation,0.09187155216932297
Resources and Evaluation,Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation,"With the recent advances of open-domain story generation, the lack of reliable automatic evaluation metrics becomes an increasingly imperative issue that hinders the fast development of story generation. According to conducted researches in this regard, learnable evaluation metrics have promised more accurate assessments by having higher correlations with human judgments. A critical bottleneck of obtaining a reliable learnable evaluation metric is the lack of high-quality training data for classifiers to efficiently distinguish plausible and implausible machine-generated stories. Previous works relied on heuristically manipulated plausible examples to mimic possible system drawbacks such as repetition, contradiction, or irrelevant content in the text level, which can be unnatural and oversimplify the characteristics of implausible machine-generated stories. We propose to tackle these issues by generating a more comprehensive set of implausible stories using plots, which are structured representations of controllable factors used to generate stories. Since these plots are compact and structured, it is easier to manipulate them to generate text with targeted undesirable properties, while at the same time maintain the grammatical correctness and naturalness of the generated sentences. To improve the quality of generated implausible stories, we further apply the adversarial filtering procedure presented by Zellers et al. (2018) to select a more nuanced set of implausible texts. Experiments show that the evaluation metrics trained on our generated data result in more reliable automatic assessments that correlate remarkably better with human judgments compared to the baselines. Human Written Story: jenny liked fresh fish. she decided to go fishing to catch her own. she brought her worms and pole and a chair. she sat there all day but didn't catch anything. she packed it up and went home disappointed. Sentence Manipulation: jenny liked fresh fish. she decided to go fishing to catch her own. she wrote songs every single day. she sat there all day but didn't catch anything. she packed it up and went home disappointed. Keyword Manipulation: jenny liked fresh fish. she decided to go fishing to catch her own. she brought her worms and pole and a chair. she sat there all day but didn't catch anything. she unpacked it up and went home disappointed. UNION: jenny liked fresh fish. jim has a very structured workout program to help him achieve goals. she brought her worms and pole and a relaxer. she sat there all day but didn't catch anything. she unpack it up and went home disappointed. Plot: jenny fresh fish -> decided Manipulated Plot: jenny fresh fish -> tasha fishing catch -> brought worms chair offered woman store -> brought worms chair -> -> sat -> packed home disappointed sat -> got wet packed home disappointed Manipulated Plot Guided Generation (Ours): jenny was out of fresh fish. tasha offered to buy her some from the woman at the store. she brought her worms and a chair and decided to play with them. jenny sat down and laid down on the chair. when she got wet, she packed up and went home disappointed.","{'sequence': ""With the recent advances of open-domain story generation, the lack of reliable automatic evaluation metrics becomes an increasingly imperative issue that hinders the fast development of story generation. According to conducted researches in this regard, learnable evaluation metrics have promised more accurate assessments by having higher correlations with human judgments. A critical bottleneck of obtaining a reliable learnable evaluation metric is the lack of high-quality training data for classifiers to efficiently distinguish plausible and implausible machine-generated stories. Previous works relied on heuristically manipulated plausible examples to mimic possible system drawbacks such as repetition, contradiction, or irrelevant content in the text level, which can be unnatural and oversimplify the characteristics of implausible machine-generated stories. We propose to tackle these issues by generating a more comprehensive set of implausible stories using plots, which are structured representations of controllable factors used to generate stories. Since these plots are compact and structured, it is easier to manipulate them to generate text with targeted undesirable properties, while at the same time maintain the grammatical correctness and naturalness of the generated sentences. To improve the quality of generated implausible stories, we further apply the adversarial filtering procedure presented by Zellers et al. (2018) to select a more nuanced set of implausible texts. Experiments show that the evaluation metrics trained on our generated data result in more reliable automatic assessments that correlate remarkably better with human judgments compared to the baselines. Human Written Story: jenny liked fresh fish. she decided to go fishing to catch her own. she brought her worms and pole and a chair. she sat there all day but didn't catch anything. she packed it up and went home disappointed. Sentence Manipulation: jenny liked fresh fish. she decided to go fishing to catch her own. she wrote songs every single day. she sat there all day but didn't catch anything. she packed it up and went home disappointed. Keyword Manipulation: jenny liked fresh fish. she decided to go fishing to catch her own. she brought her worms and pole and a chair. she sat there all day but didn't catch anything. she unpacked it up and went home disappointed. UNION: jenny liked fresh fish. jim has a very structured workout program to help him achieve goals. she brought her worms and pole and a relaxer. she sat there all day but didn't catch anything. she unpack it up and went home disappointed. Plot: jenny fresh fish -> decided Manipulated Plot: jenny fresh fish -> tasha fishing catch -> brought worms chair offered woman store -> brought worms chair -> -> sat -> packed home disappointed sat -> got wet packed home disappointed Manipulated Plot Guided Generation (Ours): jenny was out of fresh fish. tasha offered to buy her some from the woman at the store. she brought her worms and a chair and decided to play with them. jenny sat down and laid down on the chair. when she got wet, she packed up and went home disappointed."", 'labels': ['Generation', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Summarization', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Information Retrieval and Text Mining', 'NLP Applications', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.29777127504348755, 0.05562128499150276, 0.05301804095506668, 0.048949599266052246, 0.04364326596260071, 0.03695708513259888, 0.03571484610438347, 0.03471440076828003, 0.034317851066589355, 0.03255473077297211, 0.03228110447525978, 0.031023215502500534, 0.029140958562493324, 0.028651662170886993, 0.0277885589748621, 0.026433264836668968, 0.02607014589011669, 0.02449115179479122, 0.022862495854496956, 0.022021958604454994, 0.019811222329735756, 0.019089864566922188, 0.017071988433599472]}",0.29777127504348755,Generation,0.05562128499150276
Resources and Evaluation,MultiOpEd: A Corpus of Multi-Perspective News Editorials,"We propose MULTIOPED 1 , an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MULTIOPED aims at supporting the study of multiple tasks relevant to automatic perspective discovery, where a system is expected to produce a singlesentence thesis statement summarizing the arguments presented. We argue that identifying and abstracting such natural language perspectives from editorials is a crucial step toward studying the implicit argumentation structure in news editorials. We first discuss the challenges and define a few conceptual tasks towards our goal. To demonstrate the utility of MULTIOPED and the induced tasks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that MULTIOPED will be a useful resource for future studies on argumentation in the news editorial domain.","{'sequence': 'We propose MULTIOPED 1 , an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MULTIOPED aims at supporting the study of multiple tasks relevant to automatic perspective discovery, where a system is expected to produce a singlesentence thesis statement summarizing the arguments presented. We argue that identifying and abstracting such natural language perspectives from editorials is a crucial step toward studying the implicit argumentation structure in news editorials. We first discuss the challenges and define a few conceptual tasks towards our goal. To demonstrate the utility of MULTIOPED and the induced tasks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that MULTIOPED will be a useful resource for future studies on argumentation in the news editorial domain.', 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Generation', 'Computational Social Science and Social Media', 'Summarization', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'NLP Applications', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08122921735048294, 0.07895676046609879, 0.06259462982416153, 0.06157553568482399, 0.06099562346935272, 0.05886654555797577, 0.053135864436626434, 0.052757129073143005, 0.04664682224392891, 0.04547325149178505, 0.04464082419872284, 0.044624317437410355, 0.04177403450012207, 0.03299525007605553, 0.03279360011219978, 0.030244560912251472, 0.030229784548282623, 0.02660691738128662, 0.025479236617684364, 0.024036545306444168, 0.022951345890760422, 0.022887390106916428, 0.01850474439561367]}",0.08122921735048294,Resources and Evaluation,0.08122921735048294
Resources and Evaluation,Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality,"We release a new benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. For writing, lexical substitution systems can assist humans by suggesting words that humans cannot easily think of. However, existing benchmarks depend on human recall as the only source of data, and therefore lack coverage of the substitutes that would be most helpful to humans. Furthermore, annotators often provide substitutes of low quality, which are not actually appropriate in the given context. We collect higher-coverage and higher-quality data by framing lexical substitution as a classification problem, guided by the intuition that it is easier for humans to judge the appropriateness of candidate substitutes than conjure them from memory. To this end, we use a contextfree thesaurus to produce candidates and rely on human judgement to determine contextual appropriateness. Compared to the previous largest benchmark, our SWORDS benchmark has 3x as many substitutes per target word for the same level of quality, and its substitutes are 1.4x more appropriate (based on human judgement) for the same number of substitutes.","{'sequence': 'We release a new benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. For writing, lexical substitution systems can assist humans by suggesting words that humans cannot easily think of. However, existing benchmarks depend on human recall as the only source of data, and therefore lack coverage of the substitutes that would be most helpful to humans. Furthermore, annotators often provide substitutes of low quality, which are not actually appropriate in the given context. We collect higher-coverage and higher-quality data by framing lexical substitution as a classification problem, guided by the intuition that it is easier for humans to judge the appropriateness of candidate substitutes than conjure them from memory. To this end, we use a contextfree thesaurus to produce candidates and rely on human judgement to determine contextual appropriateness. Compared to the previous largest benchmark, our SWORDS benchmark has 3x as many substitutes per target word for the same level of quality, and its substitutes are 1.4x more appropriate (based on human judgement) for the same number of substitutes.', 'labels': ['Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Resources and Evaluation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09677597880363464, 0.07065154612064362, 0.07038959115743637, 0.05869121849536896, 0.057122621685266495, 0.05335412919521332, 0.05192434787750244, 0.04582645744085312, 0.04359032213687897, 0.041228439658880234, 0.04104074463248253, 0.040780987590551376, 0.03957146778702736, 0.03732328861951828, 0.03421461209654808, 0.03403303772211075, 0.0339648574590683, 0.03352275863289833, 0.03192553669214249, 0.027176503092050552, 0.019980110228061676, 0.01851058565080166, 0.018400827422738075]}",0.09677597880363464,Dialogue and Interactive Systems,0.03192553669214249
Resources and Evaluation,Identifying Medical Self-Disclosure in Online Communities,"Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical selfdisclosure in online communities is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (NO SELF-DISCLOSURE, POSSI-BLE SELF-DISCLOSURE, and CLEAR SELF-DISCLOSURE) with high inter-annotator agreement (κ = 0.88). We make this data available to the research community. We also release a predictive model trained on this dataset that achieves an accuracy of 81.02%, establishing a strong performance benchmark for this task.","{'sequence': 'Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical selfdisclosure in online communities is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (NO SELF-DISCLOSURE, POSSI-BLE SELF-DISCLOSURE, and CLEAR SELF-DISCLOSURE) with high inter-annotator agreement (κ = 0.88). We make this data available to the research community. We also release a predictive model trained on this dataset that achieves an accuracy of 81.02%, establishing a strong performance benchmark for this task.', 'labels': ['Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Information Extraction', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Speech and Multimodality', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09083814918994904, 0.0903409793972969, 0.08664088696241379, 0.07848791033029556, 0.07510901242494583, 0.06416937708854675, 0.04842798039317131, 0.04554439336061478, 0.042787209153175354, 0.04003874212503433, 0.03964286297559738, 0.036059506237506866, 0.035192880779504776, 0.03508780151605606, 0.031019343063235283, 0.030134961009025574, 0.02694949321448803, 0.026676446199417114, 0.018573645502328873, 0.016437359154224396, 0.015624932944774628, 0.01319399569183588, 0.013022084720432758]}",0.09083814918994904,Computational Social Science and Social Media,0.07510901242494583
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction,"We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits compositional properties while being fully learnable without any explicit labelling. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as word2vec and BERT.","{'sequence': 'We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits compositional properties while being fully learnable without any explicit labelling. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as word2vec and BERT.', 'labels': ['Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Machine Learning for NLP', 'Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.08777877688407898, 0.08617022633552551, 0.08115077018737793, 0.07048246264457703, 0.06585320830345154, 0.056025102734565735, 0.05437912791967392, 0.050501447170972824, 0.04362575709819794, 0.04120106250047684, 0.0404241606593132, 0.0401209220290184, 0.03649839758872986, 0.03326015546917915, 0.03294134885072708, 0.03122744709253311, 0.028942283242940903, 0.02671186253428459, 0.025862302631139755, 0.0207783505320549, 0.01940567046403885, 0.01808924600481987, 0.008569919504225254]}",0.08777877688407898,Dialogue and Interactive Systems,0.03294134885072708
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",How (Non-)Optimal is the Lexicon?,"The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf's law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world's languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon's optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes-as measured by code length.","{'sequence': ""The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf's law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world's languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon's optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes-as measured by code length."", 'labels': ['Question Answering', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Generation', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Resources and Evaluation', 'Summarization', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Learning for NLP'], 'scores': [0.08373615145683289, 0.06948336958885193, 0.06888166069984436, 0.06859715282917023, 0.05936335772275925, 0.055835168808698654, 0.05383018031716347, 0.04856271296739578, 0.046198468655347824, 0.04010922089219093, 0.03978227823972702, 0.039744727313518524, 0.03913436457514763, 0.03511154279112816, 0.034911926835775375, 0.033448003232479095, 0.03296489641070366, 0.03198792040348053, 0.02873915806412697, 0.02670515701174736, 0.022325562313199043, 0.0218335073441267, 0.01871350407600403]}",0.08373615145683289,Question Answering,0.02670515701174736
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Word Complexity is in the Eye of the Beholder,"Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a ""one-size-fits-all"" approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and nonnative speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach and foster further research into these aspects, we release a dataset of complex words annotated by readers with different backgrounds.","{'sequence': 'Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a ""one-size-fits-all"" approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and nonnative speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach and foster further research into these aspects, we release a dataset of complex words annotated by readers with different backgrounds.', 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Generation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction'], 'scores': [0.1320701539516449, 0.08380511403083801, 0.08319901674985886, 0.06477196514606476, 0.05934843793511391, 0.058226145803928375, 0.05764853209257126, 0.05429607257246971, 0.04603848606348038, 0.04518267512321472, 0.041255734860897064, 0.036788661032915115, 0.02907516248524189, 0.028725137934088707, 0.02864757366478443, 0.028142070397734642, 0.024945657700300217, 0.022364329546689987, 0.018833104521036148, 0.01609138771891594, 0.014044346287846565, 0.013418233022093773, 0.013081950135529041]}",0.1320701539516449,Speech and Multimodality,0.04518267512321472
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Linguistic Complexity Loss in Text-Based Therapy,"The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of mental health in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced lexical diversity as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, syntactic complexity, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, therapists, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for anxiety, an exciting prospect in a time of both increased online communication and increased mental health issues.","{'sequence': 'The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of mental health in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced lexical diversity as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, syntactic complexity, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, therapists, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for anxiety, an exciting prospect in a time of both increased online communication and increased mental health issues.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Discourse and Pragmatics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP'], 'scores': [0.08491881936788559, 0.07635243237018585, 0.07314223051071167, 0.06277056783437729, 0.06222958862781525, 0.05714299902319908, 0.05334958806633949, 0.04942476004362106, 0.0477074570953846, 0.04659103974699974, 0.04551524296402931, 0.04373887926340103, 0.039806488901376724, 0.039780668914318085, 0.03617187216877937, 0.03452848643064499, 0.029833225533366203, 0.025827275589108467, 0.019083678722381592, 0.018932772800326347, 0.01817246526479721, 0.018164459615945816, 0.016815096139907837]}",0.08491881936788559,Question Answering,0.03617187216877937
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",Ab Antiquo: Neural Proto-language Reconstruction,"Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics.","{'sequence': 'Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Information Extraction', 'Generation', 'Computational Social Science and Social Media', 'NLP Applications', 'Summarization', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11478252708911896, 0.10153932869434357, 0.08462691307067871, 0.05658078193664551, 0.05569348484277725, 0.053398750722408295, 0.04805131256580353, 0.04329759627580643, 0.042309850454330444, 0.039578087627887726, 0.03813769668340683, 0.03514210879802704, 0.03456629812717438, 0.03305197134613991, 0.03280794993042946, 0.03263537213206291, 0.027757450938224792, 0.027250219136476517, 0.024552244693040848, 0.0230507031083107, 0.018536768853664398, 0.016390180215239525, 0.01626242883503437]}",0.11478252708911896,Question Answering,0.024552244693040848
"Linguistic Theories, Cognitive Modeling and Psycholinguistics",On Biasing Transformer Attention Towards Monotonicity,"Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has facilitated or enforced learning of monotonic attention behavior via specialized attention functions or pretraining. In this work, we introduce a monotonicity loss function that is compatible with standard attention mechanisms and test it on several sequence-to-sequence tasks: grapheme-to-phoneme conversion, morphological inflection, transliteration, and dialect normalization. Experiments show that we can achieve largely monotonic behavior. Performance is mixed, with larger gains on top of RNN baselines. General monotonicity does not benefit transformer multihead attention, however, we see isolated improvements when only a subset of heads is biased towards monotonic behavior.","{'sequence': 'Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has facilitated or enforced learning of monotonic attention behavior via specialized attention functions or pretraining. In this work, we introduce a monotonicity loss function that is compatible with standard attention mechanisms and test it on several sequence-to-sequence tasks: grapheme-to-phoneme conversion, morphological inflection, transliteration, and dialect normalization. Experiments show that we can achieve largely monotonic behavior. Performance is mixed, with larger gains on top of RNN baselines. General monotonicity does not benefit transformer multihead attention, however, we see isolated improvements when only a subset of heads is biased towards monotonic behavior.', 'labels': ['NLP Applications', 'Dialogue and Interactive Systems', 'Phonology, Morphology and Word Segmentation', 'Speech and Multimodality', 'Question Answering', 'Resources and Evaluation', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07895583659410477, 0.07258977741003036, 0.07020018249750137, 0.060747385025024414, 0.058102525770664215, 0.05791981890797615, 0.05132565274834633, 0.047212086617946625, 0.04532304033637047, 0.04470385983586311, 0.044198885560035706, 0.04352425038814545, 0.040008459240198135, 0.038129501044750214, 0.03447536751627922, 0.033019062131643295, 0.033006101846694946, 0.028636110946536064, 0.02788248285651207, 0.02723810449242592, 0.024162182584404945, 0.01972351409494877, 0.018915710970759392]}",0.07895583659410477,NLP Applications,0.01972351409494877
NLP Applications,Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,"The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms-a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID-19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available.","{'sequence': 'The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms-a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID-19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'NLP Applications', 'Summarization', 'Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.13550284504890442, 0.08937953412532806, 0.07032395154237747, 0.06420964002609253, 0.05450061708688736, 0.04735974222421646, 0.046342793852090836, 0.04631265252828598, 0.04578309878706932, 0.044499292969703674, 0.04425212740898132, 0.03851477801799774, 0.03807476907968521, 0.036599207669496536, 0.03453900292515755, 0.030251383781433105, 0.028647413477301598, 0.024525700137019157, 0.02198781445622444, 0.017491662874817848, 0.017371539026498795, 0.01558672171086073, 0.007943612523376942]}",0.13550284504890442,Information Extraction,0.04631265252828598
NLP Applications,Constrained Multi-Task Learning for Event Coreference Resolution,"We propose a neural event coreference model in which event coreference is jointly trained with five tasks: trigger detection, entity coreference, anaphoricity determination, realis detection, and argument extraction. To guide the learning of this complex model, we incorporate cross-task consistency constraints into the learning process as soft constraints via designing penalty functions. In addition, we propose the novel idea of viewing entity coreference and event coreference as a single coreference task, which we believe is a step towards a unified model of coreference resolution. The resulting model achieves state-of-the-art results on the KBP 2017 event coreference dataset.","{'sequence': 'We propose a neural event coreference model in which event coreference is jointly trained with five tasks: trigger detection, entity coreference, anaphoricity determination, realis detection, and argument extraction. To guide the learning of this complex model, we incorporate cross-task consistency constraints into the learning process as soft constraints via designing penalty functions. In addition, we propose the novel idea of viewing entity coreference and event coreference as a single coreference task, which we believe is a step towards a unified model of coreference resolution. The resulting model achieves state-of-the-art results on the KBP 2017 event coreference dataset.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Summarization', 'Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Discourse and Pragmatics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'NLP Applications', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.09739502519369125, 0.07817716151475906, 0.07520908117294312, 0.05827651917934418, 0.05790501460433006, 0.05674610286951065, 0.05609115958213806, 0.048953086137771606, 0.047622714191675186, 0.047255706042051315, 0.0440225750207901, 0.03844835236668587, 0.03652161732316017, 0.03392421826720238, 0.03378843516111374, 0.03378233313560486, 0.029051538556814194, 0.02902502752840519, 0.026900675147771835, 0.02386562153697014, 0.01755230501294136, 0.015070922672748566, 0.014414748176932335]}",0.09739502519369125,Dialogue and Interactive Systems,0.03652161732316017
NLP Applications,Empirical Evaluation of Pre-trained Transformers for Human-Level NLP: The Role of Sample Size and Dimensionality,"In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in humanlevel tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 1 12 of the embedding dimensions.","{'sequence': 'In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in humanlevel tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 1 12 of the embedding dimensions.', 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Question Answering', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.29363691806793213, 0.20516057312488556, 0.10549695044755936, 0.037805818021297455, 0.036849118769168854, 0.03495342656970024, 0.02768651582300663, 0.025563005357980728, 0.022490013390779495, 0.021214483305811882, 0.0211462564766407, 0.019587865099310875, 0.019373634830117226, 0.01784648932516575, 0.01746671088039875, 0.01591302827000618, 0.014538516290485859, 0.014073415659368038, 0.013893098570406437, 0.010027973912656307, 0.009928504936397076, 0.009145500138401985, 0.006202194839715958]}",0.29363691806793213,NLP Applications,0.29363691806793213
NLP Applications,Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality,"Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at https://github.com/bionlplab/ heart_failure_mortality.","{'sequence': 'Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at https://github.com/bionlplab/ heart_failure_mortality.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Generation', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13029639422893524, 0.09410563111305237, 0.08139690011739731, 0.05581994354724884, 0.05500726401805878, 0.05136769264936447, 0.049387961626052856, 0.04611818492412567, 0.0441504642367363, 0.04077436774969101, 0.0405297577381134, 0.04049307480454445, 0.034929681569337845, 0.03272460773587227, 0.03197849169373512, 0.030096853151917458, 0.027568882331252098, 0.024582678452134132, 0.024208584800362587, 0.021149853244423866, 0.017836375162005424, 0.01383566576987505, 0.011640697717666626]}",0.13029639422893524,Information Extraction,0.05500726401805878
NLP Applications,On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles,"In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness.","{'sequence': 'In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Generation', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08406005054712296, 0.07376361638307571, 0.06944943964481354, 0.06292058527469635, 0.05963779240846634, 0.0571305938065052, 0.056621745228767395, 0.049895960837602615, 0.04964461177587509, 0.04701637476682663, 0.04630633443593979, 0.03733331337571144, 0.03715302050113678, 0.0365523099899292, 0.035287171602249146, 0.034304432570934296, 0.03415442258119583, 0.029415827244520187, 0.02499772608280182, 0.02214977703988552, 0.019833194091916084, 0.01818939670920372, 0.014182403683662415]}",0.08406005054712296,NLP Applications,0.08406005054712296
NLP Applications,Data and Model Distillation as a Solution for Domain-transferable Fact Verification,"While neural networks produce state-of-theart performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. We present a combination of two strategies to mitigate this dependence on lexicalized information in fact verification tasks. We present a data distillation technique for delexicalization, which we then combine with a model distillation method to prevent aggressive data distillation. We show that by using our solution, not only does the performance of an existing state-of-the-art model remain at par with that of the model trained on a fully lexicalized data, but it also performs better than it when tested out of domain. We show that the technique we present encourages models to extract transferable facts from a given fact verification dataset.","{'sequence': 'While neural networks produce state-of-theart performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. We present a combination of two strategies to mitigate this dependence on lexicalized information in fact verification tasks. We present a data distillation technique for delexicalization, which we then combine with a model distillation method to prevent aggressive data distillation. We show that by using our solution, not only does the performance of an existing state-of-the-art model remain at par with that of the model trained on a fully lexicalized data, but it also performs better than it when tested out of domain. We show that the technique we present encourages models to extract transferable facts from a given fact verification dataset.', 'labels': ['Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Information Extraction', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Generation', 'Question Answering', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1825386881828308, 0.12377685308456421, 0.08996851742267609, 0.08228596299886703, 0.05064401775598526, 0.04648997262120247, 0.043595943599939346, 0.04313768073916435, 0.04305112361907959, 0.030442489311099052, 0.029756668955087662, 0.02651982568204403, 0.026403551921248436, 0.025198964402079582, 0.022636963054537773, 0.0195737536996603, 0.01882472075521946, 0.01775161549448967, 0.017570581287145615, 0.017419464886188507, 0.01637171395123005, 0.014802174642682076, 0.01123867742717266]}",0.1825386881828308,Machine Learning for NLP,0.08996851742267609
NLP Applications,Adapting Coreference Resolution for Processing Violent Death Narratives,"Coreference resolution is an important component in analyzing narrative text from administrative data (e.g., clinical or police sources). However, existing coreference models trained on general language corpora suffer from poor transferability due to domain gaps, especially when they are applied to gender-inclusive data with lesbian, gay, bisexual, and transgender (LGBT) individuals. In this paper, we analyzed the challenges of coreference resolution in an exemplary form of administrative text written in English: violent death narratives from the USA's Centers for Disease Control's (CDC) National Violent Death Reporting System. We developed a set of data augmentation rules to improve model performance using a probabilistic data programming framework. Experiments on narratives from an administrative database, as well as existing gender-inclusive coreference datasets, demonstrate the effectiveness of data augmentation in training coreference models that can better handle text data about LGBT individuals.","{'sequence': ""Coreference resolution is an important component in analyzing narrative text from administrative data (e.g., clinical or police sources). However, existing coreference models trained on general language corpora suffer from poor transferability due to domain gaps, especially when they are applied to gender-inclusive data with lesbian, gay, bisexual, and transgender (LGBT) individuals. In this paper, we analyzed the challenges of coreference resolution in an exemplary form of administrative text written in English: violent death narratives from the USA's Centers for Disease Control's (CDC) National Violent Death Reporting System. We developed a set of data augmentation rules to improve model performance using a probabilistic data programming framework. Experiments on narratives from an administrative database, as well as existing gender-inclusive coreference datasets, demonstrate the effectiveness of data augmentation in training coreference models that can better handle text data about LGBT individuals."", 'labels': ['Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Information Extraction', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'NLP Applications', 'Generation', 'Question Answering', 'Ethics and NLP', 'Summarization', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07532402127981186, 0.07328884303569794, 0.07017069309949875, 0.06939908862113953, 0.06343578547239304, 0.06323102861642838, 0.06082692742347717, 0.0557272806763649, 0.04915318638086319, 0.04574897140264511, 0.045494284480810165, 0.041361723095178604, 0.04059654846787453, 0.035493724048137665, 0.03332017362117767, 0.029755298048257828, 0.02506949193775654, 0.02350699156522751, 0.022555259987711906, 0.021418819203972816, 0.019926372915506363, 0.019173603504896164, 0.016021855175495148]}",0.07532402127981186,Dialogue and Interactive Systems,0.04915318638086319
Question Answering,Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events,"Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformerbased language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model (TSLM model) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published stateof-the-art results with a 3.1% increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general.","{'sequence': 'Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformerbased language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model (TSLM model) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published stateof-the-art results with a 3.1% increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general.', 'labels': ['Question Answering', 'Machine Learning for NLP', 'Speech and Multimodality', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Information Extraction', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.592781662940979, 0.02859320119023323, 0.02296070195734501, 0.022142542526125908, 0.021723415702581406, 0.02154855616390705, 0.021528882905840874, 0.02144695818424225, 0.02113175019621849, 0.020825475454330444, 0.020621750503778458, 0.019822141155600548, 0.019116077572107315, 0.018315453082323074, 0.01740502193570137, 0.016053682193160057, 0.0160059854388237, 0.014540658332407475, 0.01368983555585146, 0.01367501262575388, 0.013359444215893745, 0.013269958086311817, 0.009442015551030636]}",0.592781662940979,Question Answering,0.592781662940979
Question Answering,If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering,"Multi-hop reasoning requires aggregation and inference from multiple facts. To retrieve such facts, we propose a simple approach that retrieves and reranks set of evidence facts jointly. Our approach first generates unsupervised clusters of sentences as candidate evidence by accounting links between sentences and coverage with the given query. Then, a RoBERTa-based reranker is trained to bring the most representative evidence cluster to the top. We specifically emphasize on the importance of retrieving evidence jointly by showing several comparative analyses to other methods that retrieve and rerank evidence sentences individually. First, we introduce several attention-and embedding-based analyses, which indicate that jointly retrieving and reranking approaches can learn compositional knowledge required for multi-hop reasoning. Second, our experiments show that jointly retrieving candidate evidence leads to substantially higher evidence retrieval performance when fed to the same supervised reranker. In particular, our joint retrieval and then reranking approach achieves new state-of-the-art evidence retrieval performance on two multi-hop question answering (QA) datasets: 30.5 Recall@2 on QASC, and 67.6% F1 on MultiRC. When the evidence text from our joint retrieval approach is fed to a RoBERTa-based answer selection classifier, we achieve new state-ofthe-art QA performance on MultiRC and second best result on QASC.","{'sequence': 'Multi-hop reasoning requires aggregation and inference from multiple facts. To retrieve such facts, we propose a simple approach that retrieves and reranks set of evidence facts jointly. Our approach first generates unsupervised clusters of sentences as candidate evidence by accounting links between sentences and coverage with the given query. Then, a RoBERTa-based reranker is trained to bring the most representative evidence cluster to the top. We specifically emphasize on the importance of retrieving evidence jointly by showing several comparative analyses to other methods that retrieve and rerank evidence sentences individually. First, we introduce several attention-and embedding-based analyses, which indicate that jointly retrieving and reranking approaches can learn compositional knowledge required for multi-hop reasoning. Second, our experiments show that jointly retrieving candidate evidence leads to substantially higher evidence retrieval performance when fed to the same supervised reranker. In particular, our joint retrieval and then reranking approach achieves new state-of-the-art evidence retrieval performance on two multi-hop question answering (QA) datasets: 30.5 Recall@2 on QASC, and 67.6% F1 on MultiRC. When the evidence text from our joint retrieval approach is fed to a RoBERTa-based answer selection classifier, we achieve new state-ofthe-art QA performance on MultiRC and second best result on QASC.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Speech and Multimodality', 'Resources and Evaluation', 'Summarization', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.24960565567016602, 0.0745634064078331, 0.07023612409830093, 0.0562211312353611, 0.050254955887794495, 0.04677125811576843, 0.04151489585638046, 0.03826124966144562, 0.037424229085445404, 0.03200840950012207, 0.03056519292294979, 0.027975022792816162, 0.026960987597703934, 0.02641597017645836, 0.024774940684437752, 0.02443847805261612, 0.024019457399845123, 0.02350345067679882, 0.022697655484080315, 0.0216864924877882, 0.02146882750093937, 0.015796611085534096, 0.012835643254220486]}",0.24960565567016602,Question Answering,0.24960565567016602
Question Answering,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,"This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.","{'sequence': ""This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text."", 'labels': ['Question Answering', 'NLP Applications', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Information Extraction', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.8133508563041687, 0.019964726641774178, 0.01534776110202074, 0.013285670429468155, 0.012578990310430527, 0.010656150057911873, 0.010300415568053722, 0.00958974938839674, 0.009193547070026398, 0.00895126722753048, 0.00864390004426241, 0.008008517324924469, 0.007894644513726234, 0.006934354081749916, 0.006684275809675455, 0.006675056181848049, 0.00606170017272234, 0.005433839745819569, 0.005387783981859684, 0.004270599689334631, 0.0042323162779212, 0.003586704609915614, 0.002967157168313861]}",0.8133508563041687,Question Answering,0.8133508563041687
Question Answering,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,"Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.","{'sequence': 'Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'NLP Applications', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Machine Learning for NLP', 'Speech and Multimodality', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.6935930848121643, 0.028901293873786926, 0.026898302137851715, 0.024565739557147026, 0.020348375663161278, 0.018981121480464935, 0.017104171216487885, 0.0158233605325222, 0.014534772373735905, 0.014092467725276947, 0.013296253979206085, 0.01304596196860075, 0.012325174175202847, 0.011862574145197868, 0.01057461928576231, 0.010469797067344189, 0.010292977094650269, 0.008971916511654854, 0.008732321672141552, 0.007178578991442919, 0.006929973606020212, 0.006491520907729864, 0.004985609091818333]}",0.6935930848121643,Question Answering,0.6935930848121643
Question Answering,Differentiable Open-Ended Commonsense Reasoning,"Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic and useful, we propose to study open-ended commonsense reasoning (OpenCSR) -the task of answering a commonsense question without any predefined choices -using as a resource only a knowledge corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DRFACT, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt three popular multiple-choice datasets, and collect multiple new answers to each test question via crowd-sourcing. Experiments show that DRFACT outperforms strong baseline methods by a large margin. 1","{'sequence': 'Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic and useful, we propose to study open-ended commonsense reasoning (OpenCSR) -the task of answering a commonsense question without any predefined choices -using as a resource only a knowledge corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DRFACT, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt three popular multiple-choice datasets, and collect multiple new answers to each test question via crowd-sourcing. Experiments show that DRFACT outperforms strong baseline methods by a large margin. 1', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Information Extraction', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1564394235610962, 0.11226712167263031, 0.09715770184993744, 0.05828535184264183, 0.056288011372089386, 0.050996601581573486, 0.05017085373401642, 0.044594183564186096, 0.04017356038093567, 0.037385471165180206, 0.03661588206887245, 0.03381667286157608, 0.032932646572589874, 0.02966477908194065, 0.02890598215162754, 0.025467930361628532, 0.02277478203177452, 0.019103489816188812, 0.01869373954832554, 0.01775209978222847, 0.013756143860518932, 0.009774120524525642, 0.006983461324125528]}",0.1564394235610962,Resources and Evaluation,0.09715770184993744
Question Answering,Does Structure Matter? Encoding Documents for Machine Reading Comprehension,"Machine reading comprehension is a challenging task especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as tree slices. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two datasets from varied domains.","{'sequence': 'Machine reading comprehension is a challenging task especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as tree slices. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two datasets from varied domains.', 'labels': ['Resources and Evaluation', 'Question Answering', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Summarization', 'Speech and Multimodality', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12851274013519287, 0.08536678552627563, 0.07824546098709106, 0.05880152806639671, 0.05689206346869469, 0.052185796201229095, 0.05174528807401657, 0.045183222740888596, 0.04457911476492882, 0.04402194172143936, 0.04380568116903305, 0.041165824979543686, 0.03475423529744148, 0.03460369631648064, 0.029229504987597466, 0.028563719242811203, 0.02771702967584133, 0.021182095631957054, 0.020453648641705513, 0.019721724092960358, 0.019684886559844017, 0.01692507229745388, 0.016659073531627655]}",0.12851274013519287,Resources and Evaluation,0.08536678552627563
Question Answering,Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval,"Complex question answering often requires finding a reasoning chain that consists of multiple evidence pieces. Current approaches incorporate the strengths of structured knowledge and unstructured text, assuming text corpora is semi-structured. Building on dense retrieval methods, we propose a new multi-step retrieval approach (BEAMDR) that iteratively forms an evidence chain through beam search in dense representations. When evaluated on multi-hop question answering, BEAMDR is competitive to state-of-the-art systems, without using any semi-structured information. Through query composition in dense space, BEAMDR captures the implicit relationships between evidence in the reasoning chain. The code is available at https://github.com/ henryzhao5852/BeamDR.","{'sequence': 'Complex question answering often requires finding a reasoning chain that consists of multiple evidence pieces. Current approaches incorporate the strengths of structured knowledge and unstructured text, assuming text corpora is semi-structured. Building on dense retrieval methods, we propose a new multi-step retrieval approach (BEAMDR) that iteratively forms an evidence chain through beam search in dense representations. When evaluated on multi-hop question answering, BEAMDR is competitive to state-of-the-art systems, without using any semi-structured information. Through query composition in dense space, BEAMDR captures the implicit relationships between evidence in the reasoning chain. The code is available at https://github.com/ henryzhao5852/BeamDR.', 'labels': ['Question Answering', 'Information Extraction', 'Speech and Multimodality', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Ethics and NLP', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.4619603455066681, 0.06169012933969498, 0.05013810470700264, 0.04651590809226036, 0.04309748485684395, 0.03518898785114288, 0.031188413500785828, 0.021870020776987076, 0.021159663796424866, 0.02092481590807438, 0.02060117945075035, 0.01930570974946022, 0.018812280148267746, 0.01852569170296192, 0.018229743465781212, 0.017631014809012413, 0.016363475471735, 0.016253584995865822, 0.016168635338544846, 0.013093321584165096, 0.012477567419409752, 0.010651940479874611, 0.008152129128575325]}",0.4619603455066681,Question Answering,0.4619603455066681
Semantics: Lexical Semantics,Scalable and Interpretable Semantic Change Detection,"Several cluster-based methods for semantic change detection with contextual embeddings emerged recently. They allow a fine-grained analysis of word use change by aggregating embeddings into clusters that reflect the different usages of the word. However, these methods are unscalable in terms of memory consumption and computation time. Therefore, they require a limited set of target words to be picked in advance. This drastically limits the usability of these methods in open exploratory tasks, where each word from the vocabulary can be considered as a potential target. We propose a novel scalable method for word usagechange detection that offers large gains in processing time and significant memory savings while offering the same interpretability and better performance than unscalable methods. We demonstrate the applicability of the proposed method by analysing a large corpus of news articles about COVID-19.","{'sequence': 'Several cluster-based methods for semantic change detection with contextual embeddings emerged recently. They allow a fine-grained analysis of word use change by aggregating embeddings into clusters that reflect the different usages of the word. However, these methods are unscalable in terms of memory consumption and computation time. Therefore, they require a limited set of target words to be picked in advance. This drastically limits the usability of these methods in open exploratory tasks, where each word from the vocabulary can be considered as a potential target. We propose a novel scalable method for word usagechange detection that offers large gains in processing time and significant memory savings while offering the same interpretability and better performance than unscalable methods. We demonstrate the applicability of the proposed method by analysing a large corpus of news articles about COVID-19.', 'labels': ['Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Information Extraction', 'NLP Applications', 'Summarization', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07135431468486786, 0.07006087899208069, 0.0648537278175354, 0.06353054940700531, 0.0635027065873146, 0.05653394013643265, 0.05577017366886139, 0.055548254400491714, 0.05466752499341965, 0.05142068490386009, 0.049387458711862564, 0.0422835648059845, 0.03913882374763489, 0.037384551018476486, 0.03732718899846077, 0.03556560352444649, 0.032909054309129715, 0.02570132538676262, 0.024577459320425987, 0.019903667271137238, 0.01694449596107006, 0.016883043572306633, 0.01475100964307785]}",0.07135431468486786,Dialogue and Interactive Systems,0.06353054940700531
Semantics: Lexical Semantics,Scalar Adjective Identification and Multilingual Ranking,"The intensity relationship that holds between scalar adjectives (e.g., nice < great < wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using monolingual and multilingual contextual language models. Additionally, we introduce a new binary classification task for English scalar adjective identification which examines the models' ability to distinguish scalar from relational adjectives. We probe contextualised representations and report baseline results for future comparison on this task.","{'sequence': ""The intensity relationship that holds between scalar adjectives (e.g., nice < great < wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using monolingual and multilingual contextual language models. Additionally, we introduce a new binary classification task for English scalar adjective identification which examines the models' ability to distinguish scalar from relational adjectives. We probe contextualised representations and report baseline results for future comparison on this task."", 'labels': ['Resources and Evaluation', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Information Extraction', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Generation', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.10702577978372574, 0.10427643358707428, 0.07077039033174515, 0.06714680790901184, 0.058840569108724594, 0.051845915615558624, 0.049393270164728165, 0.04412240535020828, 0.04298635944724083, 0.0416942723095417, 0.03992000222206116, 0.03365446999669075, 0.03335937112569809, 0.03153269737958908, 0.031016845256090164, 0.03082948364317417, 0.03079609014093876, 0.027376918122172356, 0.02431773580610752, 0.023662591353058815, 0.02210337482392788, 0.018332205712795258, 0.014996013604104519]}",0.10702577978372574,Resources and Evaluation,0.031016845256090164
Semantics: Lexical Semantics,ESC: Redesigning WSD with Extractive Sense Comprehension,"Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by reframing WSD as a span extraction problem -which we called Extractive Sense Comprehension (ESC) -and propose ESCHER, a transformer-based neural architecture for this new formulation. By means of an extensive array of experiments, we show that ESC unleashes the full potential of our model, leading it to outdo all of its competitors and to set a new state of the art on the English WSD task. In the few-shot scenario, ESCHER proves to exploit training data efficiently, attaining the same performance as its closest competitor while relying on almost three times fewer annotations. Furthermore, ESCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone's reach. The model along with data is available at https://github.com/ SapienzaNLP/esc.","{'sequence': ""Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by reframing WSD as a span extraction problem -which we called Extractive Sense Comprehension (ESC) -and propose ESCHER, a transformer-based neural architecture for this new formulation. By means of an extensive array of experiments, we show that ESC unleashes the full potential of our model, leading it to outdo all of its competitors and to set a new state of the art on the English WSD task. In the few-shot scenario, ESCHER proves to exploit training data efficiently, attaining the same performance as its closest competitor while relying on almost three times fewer annotations. Furthermore, ESCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone's reach. The model along with data is available at https://github.com/ SapienzaNLP/esc."", 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Generation', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.15813903510570526, 0.08648259937763214, 0.08434328436851501, 0.061969105154275894, 0.05736333876848221, 0.05238034576177597, 0.0470772348344326, 0.04398268833756447, 0.03808841109275818, 0.037187252193689346, 0.03447466716170311, 0.03321942687034607, 0.03195955604314804, 0.0313427709043026, 0.03081391006708145, 0.030348757281899452, 0.026958774775266647, 0.02673931233584881, 0.02417895197868347, 0.0198854710906744, 0.018191784620285034, 0.013136286288499832, 0.011736962012946606]}",0.15813903510570526,Machine Learning for NLP,0.030348757281899452
Semantics: Lexical Semantics,"Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective","Metaphor is an indispensable part of human cognition and everyday communication. Much research has been conducted elucidating metaphor processing in the mind/brain and the role it plays in communication. In recent years, metaphor processing systems have benefited greatly from these studies, as well as the rapid advances in deep learning for natural language processing (NLP). This paper provides a comprehensive review and discussion of recent developments in automated metaphor processing, in light of the findings about metaphor in the mind, language, and communication, and from the perspective of downstream NLP tasks.","{'sequence': 'Metaphor is an indispensable part of human cognition and everyday communication. Much research has been conducted elucidating metaphor processing in the mind/brain and the role it plays in communication. In recent years, metaphor processing systems have benefited greatly from these studies, as well as the rapid advances in deep learning for natural language processing (NLP). This paper provides a comprehensive review and discussion of recent developments in automated metaphor processing, in light of the findings about metaphor in the mind, language, and communication, and from the perspective of downstream NLP tasks.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'NLP Applications', 'Dialogue and Interactive Systems', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Resources and Evaluation', 'Generation', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Summarization', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11122222244739532, 0.09220267087221146, 0.07889409363269806, 0.06359083205461502, 0.05432913452386856, 0.05363793671131134, 0.05320854112505913, 0.048881739377975464, 0.041477784514427185, 0.03937868773937225, 0.03578506410121918, 0.03570101037621498, 0.03417987748980522, 0.033893585205078125, 0.033560145646333694, 0.030477628111839294, 0.030424097552895546, 0.02998759225010872, 0.028975261375308037, 0.026555893942713737, 0.015347818844020367, 0.01476925890892744, 0.013519009575247765]}",0.11122222244739532,Interpretability and Analysis of Models for NLP,0.03417987748980522
Semantics: Lexical Semantics,Constructing Taxonomies from Pretrained Language Models,"We present a method for constructing taxonomic trees (e.g., WORDNET) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WORDNET, and test on nonoverlapping WORDNET subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WORDNET, the model achieves 66.7 ancestor F 1 , a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using OPEN MULTILINGUAL WORDNET and extend our results across these languages.","{'sequence': 'We present a method for constructing taxonomic trees (e.g., WORDNET) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WORDNET, and test on nonoverlapping WORDNET subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WORDNET, the model achieves 66.7 ancestor F 1 , a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using OPEN MULTILINGUAL WORDNET and extend our results across these languages.', 'labels': ['Speech and Multimodality', 'Generation', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Question Answering', 'Ethics and NLP', 'Information Extraction', 'Information Retrieval and Text Mining', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09415898472070694, 0.07915671169757843, 0.07669994980096817, 0.06673096865415573, 0.06508056819438934, 0.0591822974383831, 0.05445878580212593, 0.05209159851074219, 0.04867047443985939, 0.04438316822052002, 0.04178697615861893, 0.03539618104696274, 0.03344493731856346, 0.02998805232346058, 0.0288199782371521, 0.028382349759340286, 0.027638496831059456, 0.026529889553785324, 0.02454083040356636, 0.023455845192074776, 0.02049010619521141, 0.020229604095220566, 0.01868312992155552]}",0.09415898472070694,Speech and Multimodality,0.02998805232346058
Semantics: Lexical Semantics,"Event Representation with Sequential, Semi-Supervised Discrete Variables","Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semisupervised external discrete knowledge to guide, but not restrict, the variational latent parameters during training. Our experiments indicate that our approach not only outperforms multiple baselines and the state-of-the-art in narrative script induction, but also converges more quickly.","{'sequence': 'Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semisupervised external discrete knowledge to guide, but not restrict, the variational latent parameters during training. Our experiments indicate that our approach not only outperforms multiple baselines and the state-of-the-art in narrative script induction, but also converges more quickly.', 'labels': ['Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.09389559924602509, 0.08965257555246353, 0.08666399866342545, 0.07723797857761383, 0.072050079703331, 0.06127415597438812, 0.055099185556173325, 0.05107094347476959, 0.04703677073121071, 0.04362485185265541, 0.03617076948285103, 0.031209172680974007, 0.030684446915984154, 0.028492597863078117, 0.02846560813486576, 0.027346376329660416, 0.026831170544028282, 0.026513732969760895, 0.022737843915820122, 0.02119322679936886, 0.017492717131972313, 0.014310195110738277, 0.010945986956357956]}",0.09389559924602509,Resources and Evaluation,0.055099185556173325
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Seq2Emo: A Sequence to Multi-Label Emotion Classification Model,"Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval'18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting. 1","{'sequence': ""Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval'18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting. 1"", 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.37347280979156494, 0.0879194363951683, 0.06684385240077972, 0.0572967566549778, 0.04744946211576462, 0.03506579250097275, 0.03289562091231346, 0.0318513885140419, 0.027707716450095177, 0.026715608313679695, 0.026126347482204437, 0.026104524731636047, 0.02435595728456974, 0.021290699020028114, 0.018895629793405533, 0.017606845125555992, 0.017461488023400307, 0.016352064907550812, 0.015870630741119385, 0.010903669521212578, 0.007794050499796867, 0.006251156330108643, 0.003768528811633587]}",0.37347280979156494,NLP Applications,0.006251156330108643
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Knowledge Enhanced Masked Language Model for Stance Detection,"Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Finetuned language models using large-scale indomain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election.","{'sequence': 'Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Finetuned language models using large-scale indomain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Generation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Question Answering', 'Summarization', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11727139353752136, 0.11169752478599548, 0.0826675221323967, 0.06184341385960579, 0.06012774258852005, 0.057424627244472504, 0.048226404935121536, 0.04531918838620186, 0.043424468487501144, 0.03976927697658539, 0.036904044449329376, 0.03436022251844406, 0.03410167247056961, 0.03300616890192032, 0.028471501544117928, 0.02767796628177166, 0.0237771887332201, 0.02143658883869648, 0.02139679715037346, 0.020535072311758995, 0.02001035027205944, 0.020006610080599785, 0.010544203221797943]}",0.11727139353752136,Machine Learning for NLP,0.010544203221797943
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Learning Paralinguistic Features from Audiobooks through Style Voice Conversion,"Paralinguistics, the non-lexical components of speech, play a crucial role in human-human interaction. Models designed to recognize paralinguistic information, particularly speech emotion and style, are difficult to train because of the limited labeled datasets available. In this work, we present a new framework that enables a neural network to learn to extract paralinguistic attributes from speech using data that are not annotated for emotion. We assess the utility of the learned embeddings on the downstream tasks of emotion recognition and speaking style detection, demonstrating significant improvements over surface acoustic features as well as over embeddings extracted from other unsupervised approaches. Our work enables future systems to leverage the learned embedding extractor as a separate component capable of highlighting the paralinguistic components of speech.","{'sequence': 'Paralinguistics, the non-lexical components of speech, play a crucial role in human-human interaction. Models designed to recognize paralinguistic information, particularly speech emotion and style, are difficult to train because of the limited labeled datasets available. In this work, we present a new framework that enables a neural network to learn to extract paralinguistic attributes from speech using data that are not annotated for emotion. We assess the utility of the learned embeddings on the downstream tasks of emotion recognition and speaking style detection, demonstrating significant improvements over surface acoustic features as well as over embeddings extracted from other unsupervised approaches. Our work enables future systems to leverage the learned embedding extractor as a separate component capable of highlighting the paralinguistic components of speech.', 'labels': ['Information Extraction', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Generation', 'Question Answering', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.12966927886009216, 0.08519791811704636, 0.0793728455901146, 0.056475844234228134, 0.05499190092086792, 0.05002310127019882, 0.04462740942835808, 0.04364219680428505, 0.042232949286699295, 0.041102334856987, 0.04041806980967522, 0.037560123950242996, 0.036620453000068665, 0.036354970186948776, 0.032978545874357224, 0.03256341814994812, 0.02706352435052395, 0.026155266910791397, 0.025931745767593384, 0.0246988907456398, 0.023958878591656685, 0.014256028458476067, 0.014104343019425869]}",0.12966927886009216,Information Extraction,0.014256028458476067
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks,"This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments. 1","{'sequence': 'This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments. 1', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Generation', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.11535977572202682, 0.0636630728840828, 0.062324799597263336, 0.05891253426671028, 0.05761818587779999, 0.05618415027856827, 0.05594267696142197, 0.0551687628030777, 0.04959867522120476, 0.04639512673020363, 0.04380995035171509, 0.04173886030912399, 0.040769655257463455, 0.03905612230300903, 0.03760245814919472, 0.03054296225309372, 0.028319723904132843, 0.027638304978609085, 0.027349527925252914, 0.027139438316226006, 0.01877816766500473, 0.008758224546909332, 0.007328948006033897]}",0.11535977572202682,Dialogue and Interactive Systems,0.008758224546909332
"Sentiment Analysis, Stylistic Analysis, and Argument Mining",Adversarial Learning for Zero-Shot Stance Detection on Social Media,"Stance detection on social media can help to identify and understand slanted news or commentary in everyday life. In this work, we propose a new model for zero-shot stance detection on Twitter that uses adversarial learning to generalize across topics. Our model achieves state-of-the-art performance on a number of unseen test topics with minimal computational costs. In addition, we extend zero-shot stance detection to new topics, highlighting future directions for zero-shot transfer.","{'sequence': 'Stance detection on social media can help to identify and understand slanted news or commentary in everyday life. In this work, we propose a new model for zero-shot stance detection on Twitter that uses adversarial learning to generalize across topics. Our model achieves state-of-the-art performance on a number of unseen test topics with minimal computational costs. In addition, we extend zero-shot stance detection to new topics, highlighting future directions for zero-shot transfer.', 'labels': ['Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Question Answering', 'NLP Applications', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'Resources and Evaluation', 'Summarization', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.3041009306907654, 0.07757111638784409, 0.05608696490526199, 0.050694819539785385, 0.04899474233388901, 0.0474252812564373, 0.043436646461486816, 0.04248544201254845, 0.03173794969916344, 0.030652735382318497, 0.030463913455605507, 0.026918187737464905, 0.02653508633375168, 0.02387523651123047, 0.02387377806007862, 0.021003827452659607, 0.020612506195902824, 0.01984807848930359, 0.017200279980897903, 0.015013437718153, 0.014646889641880989, 0.014466924592852592, 0.012355171144008636]}",0.3041009306907654,Computational Social Science and Social Media,0.017200279980897903
Summarization,Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters,"This paper presents an efficient graphenhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis  et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents. 1","{'sequence': 'This paper presents an efficient graphenhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis  et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents. 1', 'labels': ['Summarization', 'Information Extraction', 'Information Retrieval and Text Mining', 'Generation', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.2648736238479614, 0.06941279023885727, 0.05055031552910805, 0.04648425430059433, 0.04287751019001007, 0.04134305566549301, 0.041302748024463654, 0.039151791483163834, 0.0363055020570755, 0.035971883684396744, 0.03520561754703522, 0.0338786356151104, 0.030502136796712875, 0.028835354372859, 0.02786765806376934, 0.02580699510872364, 0.02479548379778862, 0.023858385160565376, 0.023328978568315506, 0.02189740724861622, 0.020581334829330444, 0.017636777833104134, 0.017531823366880417]}",0.2648736238479614,Summarization,0.2648736238479614
Summarization,Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization,"Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019) , an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-TRANSFORMER outperforms the Transformer and the original TP-TRANSFORMER significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and improved syntactic interpretability in the TPR layer outputs. 1","{'sequence': 'Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019) , an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-TRANSFORMER outperforms the Transformer and the original TP-TRANSFORMER significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and improved syntactic interpretability in the TPR layer outputs. 1', 'labels': ['Summarization', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Generation', 'Question Answering', 'Computational Social Science and Social Media', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.298215389251709, 0.06929630786180496, 0.0630531758069992, 0.05709462985396385, 0.05161250755190849, 0.044721540063619614, 0.04163163900375366, 0.03319448605179787, 0.03283589333295822, 0.029958123341202736, 0.028495416045188904, 0.027961188927292824, 0.026996256783604622, 0.024965081363916397, 0.023520981892943382, 0.022902395576238632, 0.022290604189038277, 0.022136265411973, 0.021825481206178665, 0.01966290920972824, 0.016731783747673035, 0.012787044048309326, 0.008110933937132359]}",0.298215389251709,Summarization,0.298215389251709
Summarization,Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics,"Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses. 1","{'sequence': 'Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses. 1', 'labels': ['Summarization', 'Generation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Information Extraction', 'Resources and Evaluation', 'Question Answering', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.20224671065807343, 0.06317171454429626, 0.06293867528438568, 0.05921383574604988, 0.055334124714136124, 0.053463079035282135, 0.04461748152971268, 0.044515710324048996, 0.044098008424043655, 0.041474826633930206, 0.040132712572813034, 0.03579258918762207, 0.03289550170302391, 0.028496967628598213, 0.02694365568459034, 0.024662889540195465, 0.024492228403687477, 0.021900152787566185, 0.02104279212653637, 0.020077036693692207, 0.020064566284418106, 0.017598610371351242, 0.014826123602688313]}",0.20224671065807343,Summarization,0.20224671065807343
Summarization,GSum: A General Framework for Guided Neural Abstractive Summarization,"Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models. 1","{'sequence': 'Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models. 1', 'labels': ['Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Question Answering', 'NLP Applications', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.27568575739860535, 0.062043339014053345, 0.04911409318447113, 0.049016110599040985, 0.04805862158536911, 0.04083399102091789, 0.03847634792327881, 0.03814180567860603, 0.03495815768837929, 0.033573903143405914, 0.0331864133477211, 0.032927218824625015, 0.02970767393708229, 0.029379012063145638, 0.02823338471353054, 0.027208395302295685, 0.027043310925364494, 0.026035306975245476, 0.024893613532185555, 0.02372710220515728, 0.019365357235074043, 0.016861697658896446, 0.011529287323355675]}",0.27568575739860535,Summarization,0.27568575739860535
Computational Social Science and Social Media,Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures,"The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning. Our results demonstrate that positive knowledge transfer via context-specific shared representations of a flexible cross-stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act (refutation, justification, allegation) classification, homogeneous multitask learning is helpful, whereas for more general tasks such as stance and hate speech detection, heterogeneous multitask learning with emotion classification works better. 1","{'sequence': 'The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning. Our results demonstrate that positive knowledge transfer via context-specific shared representations of a flexible cross-stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act (refutation, justification, allegation) classification, homogeneous multitask learning is helpful, whereas for more general tasks such as stance and hate speech detection, heterogeneous multitask learning with emotion classification works better. 1', 'labels': ['NLP Applications', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Information Extraction', 'Speech and Multimodality', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Question Answering', 'Machine Translation and Multilinguality', 'Summarization', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09656491130590439, 0.08331632614135742, 0.07055147737264633, 0.0686722993850708, 0.06286220997571945, 0.0554049015045166, 0.05257333442568779, 0.049824777990579605, 0.0414542593061924, 0.04026360064744949, 0.04000864923000336, 0.0395653061568737, 0.03815171495079994, 0.036660291254520416, 0.03287162259221077, 0.03183070197701454, 0.031549353152513504, 0.027492541819810867, 0.02692297473549843, 0.024201739579439163, 0.022337982431054115, 0.013825815171003342, 0.013093160465359688]}",0.09656491130590439,NLP Applications,0.07055147737264633
Computational Social Science and Social Media,Self Promotion in US Congressional Tweets,"Prior studies have found that women selfpromote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in Congress, number of daily tweets, and number of followers, we found that women in Congress actually perform more self-promotion on Twitter, indicating a reversal of traditional gender norms where women self-promote less than men.","{'sequence': 'Prior studies have found that women selfpromote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in Congress, number of daily tweets, and number of followers, we found that women in Congress actually perform more self-promotion on Twitter, indicating a reversal of traditional gender norms where women self-promote less than men.', 'labels': ['Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Ethics and NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Information Extraction', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Generation', 'Semantics: Lexical Semantics', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07970086485147476, 0.07786452770233154, 0.07258287072181702, 0.07176683843135834, 0.07140328735113144, 0.06180046126246452, 0.05777797847986221, 0.0535617470741272, 0.05054235830903053, 0.04220730811357498, 0.040369611233472824, 0.03863731399178505, 0.0346449539065361, 0.034418582916259766, 0.03242219239473343, 0.032094042748212814, 0.030480841174721718, 0.02897193655371666, 0.028085732832551003, 0.018931202590465546, 0.018727637827396393, 0.01447330228984356, 0.008534510619938374]}",0.07970086485147476,Machine Learning for NLP,0.06180046126246452
Computational Social Science and Social Media,Profiling of Intertextuality in Latin Literature Using Word Embeddings,"Identifying intertextual relationships between authors is of central importance to the study of literature. We report an empirical analysis of intertextuality in classical Latin literature using word embedding models. To enable quantitative evaluation of intertextual search methods, we curate a new dataset of 945 known parallels drawn from traditional scholarship on Latin epic poetry. We train an optimized word2vec model on a large corpus of lemmatized Latin, which achieves state-of-the-art performance for synonym detection and outperforms a widely used lexical method for intertextual search. We then demonstrate that training embeddings on very small corpora can capture salient aspects of literary style and apply this approach to replicate a previous intertextual study of the Roman historian Livy, which relied on hand-crafted stylometric features. Our results advance the development of core computational resources for a major premodern language and highlight a productive avenue for cross-disciplinary collaboration between the study of literature and NLP. 1","{'sequence': 'Identifying intertextual relationships between authors is of central importance to the study of literature. We report an empirical analysis of intertextuality in classical Latin literature using word embedding models. To enable quantitative evaluation of intertextual search methods, we curate a new dataset of 945 known parallels drawn from traditional scholarship on Latin epic poetry. We train an optimized word2vec model on a large corpus of lemmatized Latin, which achieves state-of-the-art performance for synonym detection and outperforms a widely used lexical method for intertextual search. We then demonstrate that training embeddings on very small corpora can capture salient aspects of literary style and apply this approach to replicate a previous intertextual study of the Roman historian Livy, which relied on hand-crafted stylometric features. Our results advance the development of core computational resources for a major premodern language and highlight a productive avenue for cross-disciplinary collaboration between the study of literature and NLP. 1', 'labels': ['Resources and Evaluation', 'NLP Applications', 'Machine Learning for NLP', 'Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Summarization', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10009265691041946, 0.09121786057949066, 0.07159118354320526, 0.06724169850349426, 0.06713083386421204, 0.05689172446727753, 0.051044125109910965, 0.04752010107040405, 0.04073764756321907, 0.04007900133728981, 0.03837927058339119, 0.038118407130241394, 0.03762366995215416, 0.03573428839445114, 0.03265765309333801, 0.031892504543066025, 0.029223114252090454, 0.02510414645075798, 0.025093749165534973, 0.022757768630981445, 0.01845577172935009, 0.01693630777299404, 0.01447659358382225]}",0.10009265691041946,Resources and Evaluation,0.03837927058339119
Computational Social Science and Social Media,Identifying inherent disagreement in natural language inference,"Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that AAs learn linguistic patterns and context-dependent reasoning.","{'sequence': 'Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that AAs learn linguistic patterns and context-dependent reasoning.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Question Answering', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Generation', 'Summarization', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.09112116694450378, 0.08352372795343399, 0.0666145384311676, 0.059696510434150696, 0.05309179425239563, 0.052473034709692, 0.052470967173576355, 0.05012873560190201, 0.044544730335474014, 0.04068129137158394, 0.03964681923389435, 0.03947296366095543, 0.03942053020000458, 0.03766731917858124, 0.03613046929240227, 0.03446844220161438, 0.0323312021791935, 0.02897852659225464, 0.024831928312778473, 0.024158863350749016, 0.02362871915102005, 0.023164909332990646, 0.02175275981426239]}",0.09112116694450378,NLP Applications,0.05309179425239563
Computational Social Science and Social Media,Modeling Human Mental States with an Entity-based Narrative Graph,"Understanding narrative text requires capturing characters' motivations, goals, and mental states. This paper proposes an Entity-based Narrative Graph (ENG) to model the internalstates of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks: predicting character mental states, and desire fulfillment, and conduct a qualitative analysis.","{'sequence': ""Understanding narrative text requires capturing characters' motivations, goals, and mental states. This paper proposes an Entity-based Narrative Graph (ENG) to model the internalstates of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks: predicting character mental states, and desire fulfillment, and conduct a qualitative analysis."", 'labels': ['Resources and Evaluation', 'NLP Applications', 'Information Extraction', 'Generation', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Summarization', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.17824327945709229, 0.07618153840303421, 0.06321513652801514, 0.06273078918457031, 0.061079759150743484, 0.04832088574767113, 0.0479770302772522, 0.045681651681661606, 0.044491562992334366, 0.044343966990709305, 0.041987258940935135, 0.0372636616230011, 0.03339892253279686, 0.03287040442228317, 0.027632305398583412, 0.025345558300614357, 0.025192277505993843, 0.023107051849365234, 0.020692475140094757, 0.02045063115656376, 0.019573651254177094, 0.011071646586060524, 0.009148523211479187]}",0.17824327945709229,Resources and Evaluation,0.045681651681661606
Generation,A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation,"Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer -conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the stateof-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.","{'sequence': 'Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer -conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the stateof-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.', 'labels': ['Speech and Multimodality', 'Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Information Extraction', 'Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Summarization', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.159504696726799, 0.11626369506120682, 0.07045786082744598, 0.058894623070955276, 0.04772248491644859, 0.047192055732011795, 0.04634001851081848, 0.0453060045838356, 0.04374809190630913, 0.03622222691774368, 0.03618522360920906, 0.03556353971362114, 0.03409899026155472, 0.034067604690790176, 0.03395049646496773, 0.02520722523331642, 0.023646963760256767, 0.02355276606976986, 0.023465270176529884, 0.019163167104125023, 0.016067057847976685, 0.013845201581716537, 0.00953467097133398]}",0.159504696726799,Speech and Multimodality,0.11626369506120682
Generation,Hurdles to Progress in Long-form Question Answering,"The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future. 1 * Work done during an internship at Google Research. 1 Resources accompanying our paper can be found in https://github.com/martiansideofthemoon/ hurdles-longform-qa Val Q: Can you protect electronics from EMPs/solar flares? If so, how? Generation using random retrievals, 24.8 ROUGE-L Yes, you absolutely can, in fact you can build a Faraday cage around your electronics, and protect them from solar flares... This is what is done with the Faraday cage around your electronics, which is the problem. The reason it is expensive is because it requires a huge amount of power and is expensive to replace... designed to shield your electronics from solar flares, you will have to pay for the protection. This is because you have to buy a piece of equipment that is designed to shield your electronics from solar flares, and that is expensive. ... This is also expensive, but not as expensive as the protection you need to shield your electronics from solar flares... designed to be as cheap as possible... Generation using predicted retrievals, 19.0 ROUGE-L Yes, you can shield them. But it's a slow process... Also, the equipment that's powered by them is a lot more expensive than you'd think, so it's hard to make sure that you're not just shielding them from your remote control. Train Q1: How does an EMP ruin electronics? What does it do? How would they be fixed? Can It be protected against? How? Train Q2: If Earth were hit with a massive EMP, would all of our currently technology be completely unusable permanently? Train Q3: Whenever a electromagnetic pulse (EMP) is released what does it do to electronics to disable them? Train Q4: If earth was hit with an EMP, could we ever restore electricity? If not, why? Train Q5: What are solar flares and why does it impact our electronics? Train Q6. When an EMP goes off, can the electronics affected be replaced? Gold Answer, 18.6 ROUGE-L I'll start with the grounding question, because that's the easiest to answer: Doesn't help a bit. All that matters is that the metal container is conductive and doesn't have gaps...completely seal your Faraday cage. Consider soldering the lid on to that paint can... look at little baggie it comes in. Sealed mylar. That protected that chip from air travel at 35,000 feet, land travel through rural, urban, and suburban areas, and all the electromagnetic radiation that the trip entails... No lead shielding. No safes.... Random Train Ans, 19.4 ROUGE-L The fast lane/slow lane is a bit of a misnomer. It gives the impression that new, faster lanes are being built. In reality, normal speed will be...","{'sequence': ""The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future. 1 * Work done during an internship at Google Research. 1 Resources accompanying our paper can be found in https://github.com/martiansideofthemoon/ hurdles-longform-qa Val Q: Can you protect electronics from EMPs/solar flares? If so, how? Generation using random retrievals, 24.8 ROUGE-L Yes, you absolutely can, in fact you can build a Faraday cage around your electronics, and protect them from solar flares... This is what is done with the Faraday cage around your electronics, which is the problem. The reason it is expensive is because it requires a huge amount of power and is expensive to replace... designed to shield your electronics from solar flares, you will have to pay for the protection. This is because you have to buy a piece of equipment that is designed to shield your electronics from solar flares, and that is expensive. ... This is also expensive, but not as expensive as the protection you need to shield your electronics from solar flares... designed to be as cheap as possible... Generation using predicted retrievals, 19.0 ROUGE-L Yes, you can shield them. But it's a slow process... Also, the equipment that's powered by them is a lot more expensive than you'd think, so it's hard to make sure that you're not just shielding them from your remote control. Train Q1: How does an EMP ruin electronics? What does it do? How would they be fixed? Can It be protected against? How? Train Q2: If Earth were hit with a massive EMP, would all of our currently technology be completely unusable permanently? Train Q3: Whenever a electromagnetic pulse (EMP) is released what does it do to electronics to disable them? Train Q4: If earth was hit with an EMP, could we ever restore electricity? If not, why? Train Q5: What are solar flares and why does it impact our electronics? Train Q6. When an EMP goes off, can the electronics affected be replaced? Gold Answer, 18.6 ROUGE-L I'll start with the grounding question, because that's the easiest to answer: Doesn't help a bit. All that matters is that the metal container is conductive and doesn't have gaps...completely seal your Faraday cage. Consider soldering the lid on to that paint can... look at little baggie it comes in. Sealed mylar. That protected that chip from air travel at 35,000 feet, land travel through rural, urban, and suburban areas, and all the electromagnetic radiation that the trip entails... No lead shielding. No safes.... Random Train Ans, 19.4 ROUGE-L The fast lane/slow lane is a bit of a misnomer. It gives the impression that new, faster lanes are being built. In reality, normal speed will be..."", 'labels': ['Generation', 'Resources and Evaluation', 'Speech and Multimodality', 'Question Answering', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'NLP Applications', 'Ethics and NLP', 'Machine Learning for NLP', 'Computational Social Science and Social Media'], 'scores': [0.11937262117862701, 0.07397667318582535, 0.057757165282964706, 0.05126699060201645, 0.04498474299907684, 0.04455268010497093, 0.04433544725179672, 0.04431789368391037, 0.042095035314559937, 0.04102381691336632, 0.04030480235815048, 0.03961385041475296, 0.038102611899375916, 0.03772236034274101, 0.03619327396154404, 0.035917144268751144, 0.03507222607731819, 0.03417730703949928, 0.03375917673110962, 0.03181642293930054, 0.02844276651740074, 0.02293059602379799, 0.022264283150434494]}",0.11937262117862701,Generation,0.11937262117862701
Generation,ENTRUST: Argument Reframing with Language Models and Entailment,"Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Entman, 1983) . Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for connotations to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a postdecoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.","{'sequence': ""Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Entman, 1983) . Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for connotations to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a postdecoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear."", 'labels': ['Speech and Multimodality', 'Generation', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Extraction', 'Machine Translation and Multilinguality', 'Summarization', 'Question Answering', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.11911842972040176, 0.09108265489339828, 0.08778230845928192, 0.08687420934438705, 0.05868925526738167, 0.05083736404776573, 0.043943457305431366, 0.04217749834060669, 0.03545856848359108, 0.035129740834236145, 0.03499718755483627, 0.034729115664958954, 0.0335988849401474, 0.032826587557792664, 0.03272314369678497, 0.031951237469911575, 0.03009819984436035, 0.025086065754294395, 0.022601839154958725, 0.021765708923339844, 0.01849186420440674, 0.015917520970106125, 0.01411913800984621]}",0.11911842972040176,Speech and Multimodality,0.09108265489339828
Summarization,Paragraph-level Simplification of Medical Texts,"We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing 'jargon' terms; we find that this yields improvements over baselines in terms of readability.","{'sequence': ""We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing 'jargon' terms; we find that this yields improvements over baselines in terms of readability."", 'labels': ['Summarization', 'Information Extraction', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.15075869858264923, 0.07702045142650604, 0.07701296359300613, 0.062060970813035965, 0.055424898862838745, 0.052142102271318436, 0.05093318223953247, 0.049108803272247314, 0.04601069912314415, 0.04310686141252518, 0.040836017578840256, 0.03656318038702011, 0.0326601006090641, 0.028581498190760612, 0.028522752225399017, 0.02797444723546505, 0.02521696127951145, 0.023074734956026077, 0.022959508001804352, 0.019215602427721024, 0.01866372674703598, 0.01645655557513237, 0.01569531112909317]}",0.15075869858264923,Summarization,0.15075869858264923
Generation,An Empirical Study on Neural Keyphrase Generation,"Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system's generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic.","{'sequence': ""Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system's generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic."", 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Summarization', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.15923349559307098, 0.06936929374933243, 0.05817523971199989, 0.055952005088329315, 0.052179787307977676, 0.049697957932949066, 0.04935547709465027, 0.04654531553387642, 0.044473931193351746, 0.04242702201008797, 0.038912270218133926, 0.038860585540533066, 0.038047000765800476, 0.03792375326156616, 0.0353507362306118, 0.026154939085245132, 0.02607887051999569, 0.024775682017207146, 0.024427739903330803, 0.023533690720796585, 0.021225638687610626, 0.01913982257246971, 0.01815958134829998]}",0.15923349559307098,Generation,0.15923349559307098
Summarization,Attention Head Masking for Inference Time Content Selection in Abstractive Summarization,"How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoderdecoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our models outperform prior state-ofthe-art models on CNN/Daily Mail and New York Times datasets. Moreover, our inferencetime masking technique is also data-efficient, requiring less than 20% of the training samples to outperform BART fine-tuned on the full CNN/DailyMail dataset.","{'sequence': 'How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoderdecoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our models outperform prior state-ofthe-art models on CNN/Daily Mail and New York Times datasets. Moreover, our inferencetime masking technique is also data-efficient, requiring less than 20% of the training samples to outperform BART fine-tuned on the full CNN/DailyMail dataset.', 'labels': ['Summarization', 'Information Extraction', 'Question Answering', 'Dialogue and Interactive Systems', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.1582447737455368, 0.07282993942499161, 0.07000502943992615, 0.05873958766460419, 0.0547298863530159, 0.0509130135178566, 0.049959078431129456, 0.04369182139635086, 0.04299840331077576, 0.0391043983399868, 0.03907761350274086, 0.03751599043607712, 0.0348832905292511, 0.032611485570669174, 0.03259569779038429, 0.03044419176876545, 0.030337374657392502, 0.027854161337018013, 0.024448135867714882, 0.02171429991722107, 0.01935087889432907, 0.015613892115652561, 0.012337069027125835]}",0.1582447737455368,Summarization,0.1582447737455368
Interpretability and Analysis of Models for NLP,Factual Probing Is [MASK]: Learning vs. Learning to Recall,"Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pretrained language model by expressing them as cloze-style prompts and interpret the model's prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OPTIPROMPT, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle ""learning"" from ""learning to recall"", providing a more detailed picture of what different prompts can reveal about pre-trained language models. 1","{'sequence': 'Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pretrained language model by expressing them as cloze-style prompts and interpret the model\'s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OPTIPROMPT, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle ""learning"" from ""learning to recall"", providing a more detailed picture of what different prompts can reveal about pre-trained language models. 1', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Generation', 'Summarization', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08157608658075333, 0.07365784049034119, 0.06131081283092499, 0.06031274050474167, 0.053436748683452606, 0.051670610904693604, 0.0493912510573864, 0.04845349118113518, 0.047836124897003174, 0.04698970541357994, 0.04126356542110443, 0.040513940155506134, 0.0372982993721962, 0.03728535771369934, 0.03595684468746185, 0.0341377779841423, 0.034061938524246216, 0.0330667681992054, 0.03013814613223076, 0.02779303304851055, 0.027658723294734955, 0.0269376952201128, 0.019252462312579155]}",0.08157608658075333,Information Extraction,0.0341377779841423
Interpretability and Analysis of Models for NLP,Evaluating Saliency Methods for Neural Language Models,"Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentencelevel and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.","{'sequence': 'Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentencelevel and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Machine Learning for NLP', 'NLP Applications', 'Question Answering', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Summarization', 'Machine Translation and Multilinguality', 'Generation', 'Computational Social Science and Social Media', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.17969270050525665, 0.13793282210826874, 0.0703614205121994, 0.06704734265804291, 0.05190792679786682, 0.051696375012397766, 0.0474865660071373, 0.042420677840709686, 0.03853873163461685, 0.03821132704615593, 0.031723495572805405, 0.031025029718875885, 0.0279234629124403, 0.027871020138263702, 0.025041799992322922, 0.02465808019042015, 0.023408666253089905, 0.020476119592785835, 0.017667483538389206, 0.014628850854933262, 0.014277550391852856, 0.008167608641088009, 0.007834874093532562]}",0.17969270050525665,Interpretability and Analysis of Models for NLP,0.17969270050525665
Interpretability and Analysis of Models for NLP,Contextualized Perturbation for Textual Adversarial Attack,"Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a contextaware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.","{'sequence': 'Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a contextaware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.', 'labels': ['Generation', 'NLP Applications', 'Resources and Evaluation', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Question Answering', 'Information Extraction', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.23527409136295319, 0.09811357408761978, 0.08817687630653381, 0.06478167325258255, 0.05617577210068703, 0.05193176493048668, 0.03854788467288017, 0.03826521709561348, 0.03685319423675537, 0.035578418523073196, 0.030985651537775993, 0.030119001865386963, 0.029675904661417007, 0.025618622079491615, 0.02559640072286129, 0.018028130754828453, 0.017699124291539192, 0.017228610813617706, 0.016814375296235085, 0.015084221959114075, 0.012549183331429958, 0.008483431302011013, 0.008418881334364414]}",0.23527409136295319,Generation,0.05193176493048668
Interpretability and Analysis of Models for NLP,DirectProbe: Studying Representations without Classifiers,"Understanding how linguistic structure is encoded in contextualized embedding could help explain their impressive performance across NLP. Existing approaches for probing them usually call for training classifiers and use the accuracy, mutual information, or complexity as a proxy for the representation's goodness. In this work, we argue that doing so can be unreliable because different representations may need different classifiers. We develop a heuristic, DIRECTPROBE, that directly studies the geometry of a representation by building upon the notion of a version space for a task. Experiments with several linguistic tasks and contextualized embeddings show that, even without training classifiers, DIRECTPROBE can shine light into how an embedding space represents labels, and also anticipate classifier performance for the representation.","{'sequence': ""Understanding how linguistic structure is encoded in contextualized embedding could help explain their impressive performance across NLP. Existing approaches for probing them usually call for training classifiers and use the accuracy, mutual information, or complexity as a proxy for the representation's goodness. In this work, we argue that doing so can be unreliable because different representations may need different classifiers. We develop a heuristic, DIRECTPROBE, that directly studies the geometry of a representation by building upon the notion of a version space for a task. Experiments with several linguistic tasks and contextualized embeddings show that, even without training classifiers, DIRECTPROBE can shine light into how an embedding space represents labels, and also anticipate classifier performance for the representation."", 'labels': ['Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Information Extraction', 'Question Answering', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12167809158563614, 0.11963743716478348, 0.071193628013134, 0.06492801755666733, 0.06421312689781189, 0.06351029127836227, 0.047268856316804886, 0.04565637186169624, 0.04527237266302109, 0.036572858691215515, 0.03493226319551468, 0.0321219377219677, 0.028019990772008896, 0.025842318311333656, 0.025068731978535652, 0.02466989867389202, 0.02371729351580143, 0.023491540923714638, 0.023468565195798874, 0.02327258698642254, 0.021511444821953773, 0.017919134348630905, 0.016033150255680084]}",0.12167809158563614,Interpretability and Analysis of Models for NLP,0.12167809158563614
Interpretability and Analysis of Models for NLP,Evaluating the Values of Sources in Transfer Learning,"Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop SEAL-Shap, an efficient source valuation framework for quantifying the usefulness of the sources (e.g., domains/languages) in transfer learning based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.","{'sequence': 'Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop SEAL-Shap, an efficient source valuation framework for quantifying the usefulness of the sources (e.g., domains/languages) in transfer learning based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Machine Learning for NLP', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Generation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.23700444400310516, 0.10121848434209824, 0.07149959355592728, 0.05953047797083855, 0.0513976626098156, 0.045399852097034454, 0.04163740202784538, 0.03754661977291107, 0.03738226741552353, 0.0349588505923748, 0.03435904160141945, 0.03247296065092087, 0.031153813004493713, 0.028635568916797638, 0.026759402826428413, 0.02339680679142475, 0.020941663533449173, 0.01997854933142662, 0.015184466727077961, 0.01454169861972332, 0.013307776302099228, 0.011900641024112701, 0.009791858494281769]}",0.23700444400310516,NLP Applications,0.10121848434209824
Interpretability and Analysis of Models for NLP,Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications,"The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation. 1","{'sequence': 'The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation. 1', 'labels': ['Resources and Evaluation', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Dialogue and Interactive Systems', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Information Extraction', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Summarization', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07929431647062302, 0.07278531044721603, 0.07022503018379211, 0.06504150480031967, 0.059768058359622955, 0.052349381148815155, 0.05191967636346817, 0.04734990373253822, 0.04392390698194504, 0.04341519996523857, 0.0414125919342041, 0.0413508303463459, 0.038820456713438034, 0.03766757622361183, 0.037250857800245285, 0.03622117266058922, 0.03345487639307976, 0.030412310734391212, 0.027382371947169304, 0.027292994782328606, 0.02586038038134575, 0.020233675837516785, 0.0165676511824131]}",0.07929431647062302,Resources and Evaluation,0.07022503018379211
Machine Learning for NLP,On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies,"We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the masked language model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).","{'sequence': 'We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the masked language model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Computational Social Science and Social Media', 'NLP Applications', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07915513217449188, 0.07201015204191208, 0.07165990024805069, 0.05724911019206047, 0.05698903650045395, 0.05510148033499718, 0.051979053765535355, 0.051742687821388245, 0.05142596736550331, 0.04636378213763237, 0.045826103538274765, 0.04571712017059326, 0.039335791021585464, 0.03585148975253105, 0.03584787994623184, 0.03533032536506653, 0.03399281203746796, 0.03308287635445595, 0.027231216430664062, 0.02616133727133274, 0.019978847354650497, 0.01720045693218708, 0.010767476633191109]}",0.07915513217449188,Dialogue and Interactive Systems,0.03533032536506653
Machine Learning for NLP,Limitations of Autoregressive Models and Their Alternatives,"Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol. While this is attractive, it means they cannot model distributions whose next-symbol probability is hard to compute. Indeed, they cannot even model them well enough to solve associated easy decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.","{'sequence': 'Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol. While this is attractive, it means they cannot model distributions whose next-symbol probability is hard to compute. Indeed, they cannot even model them well enough to solve associated easy decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.', 'labels': ['NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Information Extraction', 'Generation', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12126979976892471, 0.09669079631567001, 0.06977570801973343, 0.0671989694237709, 0.06277715414762497, 0.056731928139925, 0.04723317548632622, 0.042851295322179794, 0.04282025992870331, 0.03914017975330353, 0.037154000252485275, 0.035272128880023956, 0.03493479639291763, 0.03393827751278877, 0.030460629612207413, 0.029074514284729958, 0.028340160846710205, 0.023705389350652695, 0.02346666157245636, 0.02258264645934105, 0.020135696977376938, 0.017226791009306908, 0.01721906289458275]}",0.12126979976892471,NLP Applications,0.0671989694237709
Machine Learning for NLP,On the Transformer Growth for Progressive BERT Training,"Due to the excessive cost of large-scale language model pre-training, considerable efforts have been made to train BERT progressivelystart from an inferior but low-cost model and gradually grow the model to increase the computational complexity. Our objective is to advance the understanding of Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture search, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give operator selection practical guidance. In light of our analyses, the proposed method CompoundGrow speeds up BERT pretraining by 73.6% and 82.2% for the base and large models respectively, while achieving comparable performances 1 .","{'sequence': 'Due to the excessive cost of large-scale language model pre-training, considerable efforts have been made to train BERT progressivelystart from an inferior but low-cost model and gradually grow the model to increase the computational complexity. Our objective is to advance the understanding of Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture search, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give operator selection practical guidance. In light of our analyses, the proposed method CompoundGrow speeds up BERT pretraining by 73.6% and 82.2% for the base and large models respectively, while achieving comparable performances 1 .', 'labels': ['Question Answering', 'Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Information Extraction', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.08738851547241211, 0.07935899496078491, 0.07226641476154327, 0.07179535180330276, 0.053727660328149796, 0.05234191566705704, 0.04438849911093712, 0.04431907460093498, 0.042054831981658936, 0.04106364771723747, 0.03836613893508911, 0.03763864189386368, 0.0372091606259346, 0.03594466671347618, 0.033714376389980316, 0.032792430371046066, 0.03235931694507599, 0.032013628631830215, 0.030967598780989647, 0.029021916911005974, 0.027099939063191414, 0.02330663986504078, 0.020860666409134865]}",0.08738851547241211,Question Answering,0.053727660328149796
Machine Learning for NLP,Revisiting Simple Neural Probabilistic Language Models,"Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003) , which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first selfattention layer with the NPLM's local concatenation layer, which results in small but consistent perplexity decreases across three wordlevel language modeling datasets.","{'sequence': ""Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003) , which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first selfattention layer with the NPLM's local concatenation layer, which results in small but consistent perplexity decreases across three wordlevel language modeling datasets."", 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Computational Social Science and Social Media', 'Question Answering', 'Speech and Multimodality', 'Generation', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.0841698944568634, 0.07946942001581192, 0.07335097342729568, 0.06630530953407288, 0.06350146234035492, 0.05803290754556656, 0.05779070407152176, 0.0502362996339798, 0.045145973563194275, 0.04488199204206467, 0.0409611277282238, 0.03980622440576553, 0.03952264040708542, 0.03893439099192619, 0.03552224114537239, 0.033376097679138184, 0.030896088108420372, 0.02696610800921917, 0.023929625749588013, 0.02143193408846855, 0.018699046224355698, 0.018583869561553, 0.008485618978738785]}",0.0841698944568634,Dialogue and Interactive Systems,0.07335097342729568
Machine Learning for NLP,ReadTwice: Reading Very Large Documents with Memories,"Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose READTWICE 1 , a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.","{'sequence': 'Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose READTWICE 1 , a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.', 'labels': ['Question Answering', 'Information Extraction', 'Summarization', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.49863365292549133, 0.10001190751791, 0.05109851062297821, 0.03869813680648804, 0.03481881693005562, 0.02931242063641548, 0.023371614515781403, 0.02135823294520378, 0.02047516219317913, 0.018484050408005714, 0.017707744613289833, 0.016765417531132698, 0.01593058370053768, 0.015361486934125423, 0.014483730308711529, 0.013458379544317722, 0.01306067407131195, 0.01273813284933567, 0.011141247116029263, 0.010137381963431835, 0.009279953315854073, 0.006966766901314259, 0.00670588156208396]}",0.49863365292549133,Question Answering,0.01273813284933567
Machine Learning for NLP,SCRIPT: Self-Critic PreTraining of Transformers,"We introduce Self-CRItic Pretraining Transformers (SCRIPT) for representation learning of text. The popular masked language modeling (MLM) pretraining methods like BERT replace some tokens with [MASK] and an encoder is trained to recover them, while ELEC-TRA trains a discriminator to detect replaced tokens proposed by a generator. In contrast, we train a language model as in MLM and further derive a discriminator or critic on top of the encoder without using any additional parameters. That is, the model itself is a critic. SCRIPT combines MLM training and discriminative training for learning rich representations and compute-and sample-efficiency. We demonstrate improved sample-efficiency in pretraining and enhanced representations evidenced by improved downstream task performance on GLUE and SQuAD over strong baselines. Also, the self-critic scores can be directly used as pseudo-log-likelihood for efficient scoring.","{'sequence': 'We introduce Self-CRItic Pretraining Transformers (SCRIPT) for representation learning of text. The popular masked language modeling (MLM) pretraining methods like BERT replace some tokens with [MASK] and an encoder is trained to recover them, while ELEC-TRA trains a discriminator to detect replaced tokens proposed by a generator. In contrast, we train a language model as in MLM and further derive a discriminator or critic on top of the encoder without using any additional parameters. That is, the model itself is a critic. SCRIPT combines MLM training and discriminative training for learning rich representations and compute-and sample-efficiency. We demonstrate improved sample-efficiency in pretraining and enhanced representations evidenced by improved downstream task performance on GLUE and SQuAD over strong baselines. Also, the self-critic scores can be directly used as pseudo-log-likelihood for efficient scoring.', 'labels': ['Generation', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Resources and Evaluation', 'Question Answering', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'NLP Applications', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.11270900815725327, 0.06956381350755692, 0.062187403440475464, 0.06030135229229927, 0.058799292892217636, 0.05562518164515495, 0.05266701057553291, 0.050571635365486145, 0.04842870309948921, 0.04723671078681946, 0.04274274408817291, 0.0390264131128788, 0.034666962921619415, 0.03461555764079094, 0.03235789015889168, 0.03123091347515583, 0.029269197955727577, 0.029093872755765915, 0.023853378370404243, 0.023679647594690323, 0.02219063602387905, 0.01982373185455799, 0.019358940422534943]}",0.11270900815725327,Generation,0.04842870309948921
NLP Applications,Nutri-bullets Hybrid: Consensual Multi-document Summarization,"We present a method for generating comparative summaries that highlights similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization, we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health -rife with inconsistencies. Compared to conventional methods, our framework leads to more faithful, relevant and aggregation-sensitive summarization -while being equally fluent. 1 Transformer (baseline) * Whole -grain cereals may protect against obesity , diabetes and certain cancers. However , more research is needed . * Whole grains , such as mozambican grass , are safe to eat with no serious side effects . * Whole -grain cereals may protect against obesity , diabetes and certain cancers. However , more research is needed . * Whole grains , such as blueberries , are likely safe to eat with no serious side effects . * Whole grains are safe to eat. However , people with type 2 diabetes should avoid whole grains . * Whole grains are lower in carbs than whole grains , making them a good choice for people with type 2 diabetes. Our Method * Whole grains has been shown to lower weight gain and improve various type 2 diabetes risk factors . * Whole grains has been shown to lower insulin resistance and improve various cancer risk factors . * Whole grains has been linked to several other potential health benefits , such as improved CVD risk , eyesight , and memory. However , more studies are needed to draw stronger conclusions. * There is some evidence , in both animals and humans , that whole grains can reduce mortality by regulating the hormone ghrelin.","{'sequence': 'We present a method for generating comparative summaries that highlights similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization, we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health -rife with inconsistencies. Compared to conventional methods, our framework leads to more faithful, relevant and aggregation-sensitive summarization -while being equally fluent. 1 Transformer (baseline) * Whole -grain cereals may protect against obesity , diabetes and certain cancers. However , more research is needed . * Whole grains , such as mozambican grass , are safe to eat with no serious side effects . * Whole -grain cereals may protect against obesity , diabetes and certain cancers. However , more research is needed . * Whole grains , such as blueberries , are likely safe to eat with no serious side effects . * Whole grains are safe to eat. However , people with type 2 diabetes should avoid whole grains . * Whole grains are lower in carbs than whole grains , making them a good choice for people with type 2 diabetes. Our Method * Whole grains has been shown to lower weight gain and improve various type 2 diabetes risk factors . * Whole grains has been shown to lower insulin resistance and improve various cancer risk factors . * Whole grains has been linked to several other potential health benefits , such as improved CVD risk , eyesight , and memory. However , more studies are needed to draw stronger conclusions. * There is some evidence , in both animals and humans , that whole grains can reduce mortality by regulating the hormone ghrelin.', 'labels': ['Generation', 'Summarization', 'Speech and Multimodality', 'Question Answering', 'Semantics: Lexical Semantics', 'Information Extraction', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Resources and Evaluation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.20348873734474182, 0.20159925520420074, 0.0906190276145935, 0.048695191740989685, 0.04833070933818817, 0.04662863165140152, 0.03630426526069641, 0.031947989016771317, 0.03155035525560379, 0.03017459623515606, 0.028847509995102882, 0.026049617677927017, 0.02517589181661606, 0.020955964922904968, 0.01741614378988743, 0.017194930464029312, 0.015650367364287376, 0.014455746859312057, 0.014072490856051445, 0.013494469225406647, 0.0131588876247406, 0.012185175903141499, 0.01200406439602375]}",0.20348873734474182,Generation,0.017194930464029312
NLP Applications,AVA: an Automatic eValuation Approach for Question Answering Systems,"We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers (references), can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics. To design, train, and test AVA, we built multiple large training, development, and test sets on public and industrial benchmarks. Our innovative solutions achieve up to 74.7% F1 score in predicting human judgment for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an error lower than 7% at 95% of confidence when measured on several QA systems.","{'sequence': 'We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers (references), can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics. To design, train, and test AVA, we built multiple large training, development, and test sets on public and industrial benchmarks. Our innovative solutions achieve up to 74.7% F1 score in predicting human judgment for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an error lower than 7% at 95% of confidence when measured on several QA systems.', 'labels': ['Question Answering', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Generation', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Computational Social Science and Social Media', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Summarization'], 'scores': [0.9012632966041565, 0.010175074450671673, 0.008131285198032856, 0.007759318221360445, 0.006630602292716503, 0.006060081068426371, 0.005340016912668943, 0.005277176387608051, 0.005069422535598278, 0.00410288292914629, 0.004000892397016287, 0.003970513120293617, 0.0038643034640699625, 0.0037604980170726776, 0.003694747341796756, 0.003430385375395417, 0.003195621771737933, 0.003155939280986786, 0.0025723425205796957, 0.0022922076750546694, 0.002269407967105508, 0.0020841246005147696, 0.0018998592859134078]}",0.9012632966041565,Question Answering,0.003155939280986786
NLP Applications,SpanPredict: Extraction of Predictive Document Spans with Neural Attention,"In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance.","{'sequence': 'In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance.', 'labels': ['NLP Applications', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.1957274228334427, 0.06990132480859756, 0.0635443776845932, 0.05482572317123413, 0.053961098194122314, 0.049667298793792725, 0.047918133437633514, 0.042696088552474976, 0.0424918457865715, 0.038586728274822235, 0.036044783890247345, 0.03583903983235359, 0.03492652624845505, 0.034233562648296356, 0.032847583293914795, 0.028806209564208984, 0.027078887447714806, 0.024493912234902382, 0.02309977263212204, 0.021958665922284126, 0.016762377694249153, 0.012904364615678787, 0.011684286408126354]}",0.1957274228334427,NLP Applications,0.1957274228334427
NLP Applications,Text Editing by Command,"A prevailing paradigm in neural text generation is one-shot generation, where text is produced in a single step. The one-shot setting is inadequate, however, when the constraints the user wishes to impose on the generated text are dynamic, especially when authoring longer documents. We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text. To this end, we propose a novel text editing task, and introduce WikiDocEdits, a dataset of singlesentence edits extracted from Wikipedia revision histories. We show that our Interactive Editor, a transformer-based model trained on this dataset, outperforms baselines and obtains positive results in both automatic and human evaluations. We present empirical and qualitative analyses of this model's performance. 1","{'sequence': ""A prevailing paradigm in neural text generation is one-shot generation, where text is produced in a single step. The one-shot setting is inadequate, however, when the constraints the user wishes to impose on the generated text are dynamic, especially when authoring longer documents. We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text. To this end, we propose a novel text editing task, and introduce WikiDocEdits, a dataset of singlesentence edits extracted from Wikipedia revision histories. We show that our Interactive Editor, a transformer-based model trained on this dataset, outperforms baselines and obtains positive results in both automatic and human evaluations. We present empirical and qualitative analyses of this model's performance. 1"", 'labels': ['Dialogue and Interactive Systems', 'Generation', 'Speech and Multimodality', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Discourse and Pragmatics', 'Information Extraction', 'NLP Applications', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Question Answering', 'Phonology, Morphology and Word Segmentation', 'Syntax: Tagging, Chunking and Parsing', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization'], 'scores': [0.12488838285207748, 0.1247512623667717, 0.0706598088145256, 0.06676457077264786, 0.059524886310100555, 0.0573791041970253, 0.04178733378648758, 0.040167175233364105, 0.0398443229496479, 0.0387209877371788, 0.03731474280357361, 0.036549389362335205, 0.034843456000089645, 0.0341750793159008, 0.031085480004549026, 0.027015479281544685, 0.025445440784096718, 0.024699896574020386, 0.02321292646229267, 0.017427882179617882, 0.016195517033338547, 0.015322920866310596, 0.012223935686051846]}",0.12488838285207748,Dialogue and Interactive Systems,0.040167175233364105
NLP Applications,A Deep Metric Learning Approach to Account Linking,"We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and metadata of their corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity-ranging from single posts to entire months of activity-to a vector space, where samples by the same author map to nearby points. The approach does not require humanannotated data for training purposes, which allows us to leverage large amounts of social media content. The proposed model outperforms several competitive baselines under a novel evaluation framework modeled after established recognition benchmarks in other domains. Our method achieves high linking accuracy, even with small samples from accounts not seen at training time, a prerequisite for practical applications of the proposed linking framework.","{'sequence': 'We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and metadata of their corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity-ranging from single posts to entire months of activity-to a vector space, where samples by the same author map to nearby points. The approach does not require humanannotated data for training purposes, which allows us to leverage large amounts of social media content. The proposed model outperforms several competitive baselines under a novel evaluation framework modeled after established recognition benchmarks in other domains. Our method achieves high linking accuracy, even with small samples from accounts not seen at training time, a prerequisite for practical applications of the proposed linking framework.', 'labels': ['Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Information Extraction', 'Summarization', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12033042311668396, 0.08947186917066574, 0.08852224051952362, 0.07655667513608932, 0.07084847241640091, 0.06829744577407837, 0.05154892057180405, 0.04789421334862709, 0.04720581695437431, 0.04210205003619194, 0.0391768217086792, 0.031405430287122726, 0.029551753774285316, 0.02901943027973175, 0.02669939026236534, 0.025226641446352005, 0.024996552616357803, 0.021903835237026215, 0.01941906102001667, 0.014984656125307083, 0.013445272110402584, 0.011541461572051048, 0.009851625189185143]}",0.12033042311668396,Computational Social Science and Social Media,0.031405430287122726
NLP Applications,Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation,"Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or BLEU, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports: one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these rewards via reinforcement learning. On two open radiology report datasets, our system substantially improved the F 1 score of a clinical information extraction performance by +22.1 (∆ + 63.9%). We further show via a human evaluation and a qualitative analysis that our system leads to generations that are more factually complete and consistent compared to the baselines.","{'sequence': 'Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or BLEU, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports: one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these rewards via reinforcement learning. On two open radiology report datasets, our system substantially improved the F 1 score of a clinical information extraction performance by +22.1 (∆ + 63.9%). We further show via a human evaluation and a qualitative analysis that our system leads to generations that are more factually complete and consistent compared to the baselines.', 'labels': ['Information Extraction', 'Generation', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Question Answering', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Summarization', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.12727338075637817, 0.0920226126909256, 0.07193481922149658, 0.06709559261798859, 0.05282704532146454, 0.047502148896455765, 0.047447286546230316, 0.04117441549897194, 0.039793070405721664, 0.0395718589425087, 0.03792031854391098, 0.0379110686480999, 0.037553198635578156, 0.033759135752916336, 0.032720401883125305, 0.03065539337694645, 0.030405648052692413, 0.025267701596021652, 0.024961700662970543, 0.02199014276266098, 0.021148279309272766, 0.020880645141005516, 0.018184280022978783]}",0.12727338075637817,Information Extraction,0.047447286546230316
"Language Grounding to Vision, Robotics and Beyond",Multimodal End-to-End Sparse Model for Emotion Recognition,"Existing works on multimodal affective computing tasks, such as emotion recognition, generally adopt a two-phase pipeline, first extracting feature representations for each single modality with hand-crafted algorithms and then performing end-to-end learning with the extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extraction algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-theart models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain performance with around half the computation in the feature extraction part.","{'sequence': 'Existing works on multimodal affective computing tasks, such as emotion recognition, generally adopt a two-phase pipeline, first extracting feature representations for each single modality with hand-crafted algorithms and then performing end-to-end learning with the extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extraction algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-theart models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain performance with around half the computation in the feature extraction part.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Question Answering', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10403703153133392, 0.07804951071739197, 0.07737870514392853, 0.07060403376817703, 0.062409237027168274, 0.05753634497523308, 0.05279099568724632, 0.05214477330446243, 0.04493289813399315, 0.04013478383421898, 0.03956834226846695, 0.035820286720991135, 0.03358824551105499, 0.0322795994579792, 0.03155231475830078, 0.031095022335648537, 0.030926261097192764, 0.029802359640598297, 0.028798876330256462, 0.024533983319997787, 0.0178117286413908, 0.013637229800224304, 0.010567449033260345]}",0.10403703153133392,Resources and Evaluation,0.029802359640598297
"Language Grounding to Vision, Robotics and Beyond",MIMOQA: Multimodal Input Multimodal Output Question Answering,"Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task -MIMOQA -Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.","{'sequence': 'Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task -MIMOQA -Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Generation', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.8568921685218811, 0.01430300623178482, 0.013552805408835411, 0.010277229361236095, 0.009494753554463387, 0.007846501655876637, 0.007672468665987253, 0.007505515124648809, 0.006865058094263077, 0.006677273195236921, 0.006288961973041296, 0.006203406490385532, 0.0060251071117818356, 0.005567817948758602, 0.00523021537810564, 0.004779967945069075, 0.0047663538716733456, 0.0042765187099576, 0.0041613769717514515, 0.0034979269839823246, 0.003138372441753745, 0.002596419071778655, 0.0023807131219655275]}",0.8568921685218811,Question Answering,0.0060251071117818356
"Language Grounding to Vision, Robotics and Beyond",OCID-Ref: A 3D Robotic Dataset With Embodied Language For Clutter Scene Grounding,"To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (VG) can affect machine performance on occluded objects. However, current VG works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel OCID-Ref dataset featuring a referring expression segmentation task with referring expressions of occluded objects. OCID-Ref consists of 305,694 referring expressions from 2,300 scenes with providing RGB image and point cloud inputs. To resolve challenging occlusion issues, we argue that it's crucial to take advantage of both 2D and 3D signals to resolve challenging occlusion issues. Our experimental results demonstrate the effectiveness of aggregating 2D and 3D signals but referring to occluded objects still remains challenging for the modern visual grounding systems. OCID-Ref is publicly available at https://github.com/ lluma/OCID-Ref * * Equal contribution.","{'sequence': ""To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (VG) can affect machine performance on occluded objects. However, current VG works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel OCID-Ref dataset featuring a referring expression segmentation task with referring expressions of occluded objects. OCID-Ref consists of 305,694 referring expressions from 2,300 scenes with providing RGB image and point cloud inputs. To resolve challenging occlusion issues, we argue that it's crucial to take advantage of both 2D and 3D signals to resolve challenging occlusion issues. Our experimental results demonstrate the effectiveness of aggregating 2D and 3D signals but referring to occluded objects still remains challenging for the modern visual grounding systems. OCID-Ref is publicly available at https://github.com/ lluma/OCID-Ref * * Equal contribution."", 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Information Extraction', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'NLP Applications', 'Summarization', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09326053410768509, 0.06971678882837296, 0.05971936136484146, 0.05958915501832962, 0.051209744065999985, 0.05016806349158287, 0.049213334918022156, 0.04918859899044037, 0.04620062932372093, 0.04591544717550278, 0.04550996050238609, 0.0434345081448555, 0.042992956936359406, 0.04018879309296608, 0.03970176354050636, 0.03620832785964012, 0.031015101820230484, 0.029647301882505417, 0.027855252847075462, 0.024616368114948273, 0.02442079409956932, 0.02327895350754261, 0.01694837585091591]}",0.09326053410768509,Resources and Evaluation,0.05016806349158287
"Language Grounding to Vision, Robotics and Beyond",Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions,"Pre-trained contextual vision-and-language (V&L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct ""mask-and-predict"" pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a model pre-trained with aligned data, on four English V&L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&L pre-training, while significantly reducing the amount of supervision needed for V&L models.","{'sequence': 'Pre-trained contextual vision-and-language (V&L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct ""mask-and-predict"" pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a model pre-trained with aligned data, on four English V&L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&L pre-training, while significantly reducing the amount of supervision needed for V&L models.', 'labels': ['Information Extraction', 'Machine Learning for NLP', 'Resources and Evaluation', 'Question Answering', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07617761194705963, 0.07406976819038391, 0.0737566202878952, 0.07212317734956741, 0.06527551263570786, 0.06256136298179626, 0.0548814982175827, 0.05137183517217636, 0.048977307975292206, 0.04809604957699776, 0.048026829957962036, 0.044281478971242905, 0.039884913712739944, 0.03820718824863434, 0.03647315129637718, 0.029576776549220085, 0.0292759221047163, 0.029104316607117653, 0.02272525615990162, 0.01906450279057026, 0.017182862386107445, 0.011661975644528866, 0.007244117558002472]}",0.07617761194705963,Information Extraction,0.044281478971242905
"Language Grounding to Vision, Robotics and Beyond",Multitasking Inhibits Semantic Drift,"When intelligent agents communicate to accomplish shared goals, how do these goals shape the agents' language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to lowlevel actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying taskoriented language use. But previous work has found that LLP training is prone to semantic drift (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem: we prove that multitask training eliminates semantic drift in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency.","{'sequence': ""When intelligent agents communicate to accomplish shared goals, how do these goals shape the agents' language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to lowlevel actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying taskoriented language use. But previous work has found that LLP training is prone to semantic drift (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem: we prove that multitask training eliminates semantic drift in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency."", 'labels': ['Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Computational Social Science and Social Media', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Phonology, Morphology and Word Segmentation', 'Generation', 'Summarization', 'Resources and Evaluation', 'Information Extraction', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.11126002669334412, 0.07794263958930969, 0.07186111062765121, 0.06157586723566055, 0.05736374482512474, 0.0535258911550045, 0.04906267672777176, 0.04657072201371193, 0.04365343973040581, 0.042113110423088074, 0.040935344994068146, 0.0405128076672554, 0.035970062017440796, 0.03510916978120804, 0.034485992044210434, 0.03321165218949318, 0.03217771276831627, 0.029447734355926514, 0.02779785357415676, 0.021967044100165367, 0.01998494751751423, 0.01732381246984005, 0.016146598383784294]}",0.11126002669334412,Speech and Multimodality,0.016146598383784294
"Language Grounding to Vision, Robotics and Beyond",Probing Contextual Language Models for Common Ground with Visual Representations,"The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.","{'sequence': 'The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.', 'labels': ['Information Extraction', 'Question Answering', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'NLP Applications', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07027506083250046, 0.06834905594587326, 0.06707888841629028, 0.065138079226017, 0.06103602424263954, 0.05550062656402588, 0.054937783628702164, 0.051186833530664444, 0.04946203902363777, 0.04601603001356125, 0.044665735214948654, 0.04344474524259567, 0.03975199908018112, 0.039322610944509506, 0.03446190059185028, 0.03387477993965149, 0.030892295762896538, 0.028841016814112663, 0.02799505740404129, 0.02619246393442154, 0.02545822598040104, 0.020080775022506714, 0.016038009896874428]}",0.07027506083250046,Information Extraction,0.03446190059185028
Machine Learning for NLP,BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification,"Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the everincreasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work.","{'sequence': 'Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the everincreasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Question Answering', 'NLP Applications', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Summarization', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.4803920388221741, 0.05187487602233887, 0.045825615525245667, 0.03424685820937157, 0.033764466643333435, 0.032961826771497726, 0.030025603249669075, 0.02610636316239834, 0.02550441026687622, 0.02541545033454895, 0.023588892072439194, 0.023334262892603874, 0.023159952834248543, 0.02185520902276039, 0.020355330780148506, 0.01995348185300827, 0.019903218373656273, 0.013296233490109444, 0.011730839498341084, 0.010925632901489735, 0.008730566129088402, 0.00857599638402462, 0.00847293809056282]}",0.4803920388221741,Generation,0.023334262892603874
Machine Learning for NLP,Targeted Adversarial Training for Natural Language Understanding,We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released at: https://github. com/namisan/mt-dnn.,"{'sequence': 'We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released at: https://github. com/namisan/mt-dnn.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Question Answering', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.3229154944419861, 0.12508906424045563, 0.07007529586553574, 0.05826033651828766, 0.050792396068573, 0.04998204857110977, 0.03754770755767822, 0.029690150171518326, 0.029332682490348816, 0.02177109569311142, 0.021557068452239037, 0.020881786942481995, 0.019805876538157463, 0.019251348450779915, 0.018761347979307175, 0.01814080961048603, 0.018108638003468513, 0.017683884128928185, 0.017594173550605774, 0.009773345664143562, 0.009730576537549496, 0.006746199913322926, 0.006508702877908945]}",0.3229154944419861,Machine Learning for NLP,0.3229154944419861
Machine Learning for NLP,Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection,"The existence of multiple datasets for sarcasm detection prompts us to apply transfer learning to exploit their commonality. The adversarial neural transfer (ANT) framework utilizes multiple loss terms that encourage the source-domain and the target-domain feature distributions to be similar while optimizing for domain-specific performance. However, these objectives may be in conflict, which can lead to optimization difficulties and sometimes diminished transfer. We propose a generalized latent optimization strategy that allows different losses to accommodate each other and improves training dynamics. The proposed method outperforms transfer learning and meta-learning baselines. In particular, we achieve 10.02% absolute performance gain over the previous state of the art on the iSarcasm dataset.","{'sequence': 'The existence of multiple datasets for sarcasm detection prompts us to apply transfer learning to exploit their commonality. The adversarial neural transfer (ANT) framework utilizes multiple loss terms that encourage the source-domain and the target-domain feature distributions to be similar while optimizing for domain-specific performance. However, these objectives may be in conflict, which can lead to optimization difficulties and sometimes diminished transfer. We propose a generalized latent optimization strategy that allows different losses to accommodate each other and improves training dynamics. The proposed method outperforms transfer learning and meta-learning baselines. In particular, we achieve 10.02% absolute performance gain over the previous state of the art on the iSarcasm dataset.', 'labels': ['Dialogue and Interactive Systems', 'Generation', 'NLP Applications', 'Resources and Evaluation', 'Ethics and NLP', 'Machine Learning for NLP', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08750836551189423, 0.08362915366888046, 0.06681888550519943, 0.06485648453235626, 0.06439489871263504, 0.061792392283678055, 0.060785990208387375, 0.05303342267870903, 0.048594880849123, 0.03977479785680771, 0.039600271731615067, 0.0394832044839859, 0.03750557079911232, 0.03367309644818306, 0.03255594149231911, 0.03147190064191818, 0.030282096937298775, 0.027961861342191696, 0.026187866926193237, 0.021528759971261024, 0.02043800987303257, 0.016653073951601982, 0.0114690400660038]}",0.08750836551189423,Dialogue and Interactive Systems,0.061792392283678055
Machine Learning for NLP,Self-training Improves Pre-training for Natural Language Understanding,"Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semisupervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.","{'sequence': 'Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semisupervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.', 'labels': ['Dialogue and Interactive Systems', 'NLP Applications', 'Resources and Evaluation', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Question Answering', 'Discourse and Pragmatics', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11084417998790741, 0.09865452349185944, 0.09703433513641357, 0.06980915367603302, 0.06811214238405228, 0.056045349687337875, 0.05576826259493828, 0.054226215928792953, 0.04517121985554695, 0.03755023702979088, 0.035422418266534805, 0.03286519646644592, 0.03165937215089798, 0.03041110560297966, 0.029639584943652153, 0.026551255956292152, 0.022473152726888657, 0.02102675847709179, 0.020829400047659874, 0.015562022104859352, 0.01547215972095728, 0.014766637235879898, 0.01010528951883316]}",0.11084417998790741,Dialogue and Interactive Systems,0.06980915367603302
Machine Learning for NLP,Supporting Clustering with Contrastive Learning,"Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) -a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering and show that SCCL significantly advances the state-of-the-art results on most benchmark datasets with 3%−11% improvement on Accuracy and 4% − 15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and top-down clustering to achieve better intracluster and inter-cluster distances when evaluated with the ground truth cluster labels 1 .","{'sequence': 'Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) -a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering and show that SCCL significantly advances the state-of-the-art results on most benchmark datasets with 3%−11% improvement on Accuracy and 4% − 15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and top-down clustering to achieve better intracluster and inter-cluster distances when evaluated with the ground truth cluster labels 1 .', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Summarization', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Machine Learning for NLP', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality'], 'scores': [0.09617584198713303, 0.08640821278095245, 0.07799240946769714, 0.07229359447956085, 0.06118880584836006, 0.06069910153746605, 0.05865103378891945, 0.057004738599061966, 0.04120283201336861, 0.040769509971141815, 0.04008495435118675, 0.03580467030405998, 0.03478861600160599, 0.03068356029689312, 0.030422721058130264, 0.029693616554141045, 0.029606297612190247, 0.028962820768356323, 0.021503908559679985, 0.02016427367925644, 0.018400657922029495, 0.01418341789394617, 0.013314394280314445]}",0.09617584198713303,Resources and Evaluation,0.029606297612190247
NLP Applications,TITA: A Two-stage Interaction and Topic-Aware Text Matching Model,"In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than partially-relevance ones in this application. However, previous tasks never define topic-aware relevance clearly. To tackle this problem, we define a three-level relevance in keyword-document matching task: topicaware relevance, partially-relevance and irrelevance. To capture the relevance between the short keyword and the document at abovementioned three levels, we should not only combine the latent topic of the document with its deep neural representation, but also model complex interactions between the keyword and the document. To this end, we propose a Two-stage Interaction and Topic-Aware text matching model (TITA). In terms of ""topicaware"", we introduce neural topic model to analyze the topic of the document and then use it to further encode the document. In terms of ""two-stage interaction"", we propose two successive stages to model complex interactions between the keyword and the document. Extensive experiments reveal that TITA outperforms other well-designed baselines and shows excellent performance in our recommendation system. 044 query-document matching is an important feature 045 in the ranking models. (2) as for the task of ques-046 tion answering (Yang et al., 2016), query-document 047 matching method can be used to find document can-048 didates or to help predict the answer span. (3) it 049 is also widely applied to recommendation systems 050 (Jiang et al., 2019). 051 In many scenarios, we need to distinguish dif-052 ferent keyword-document (query-document) rele-053 vance levels. For instance, in our recommendation 054 system, we need to attach documents to some hot 055 keywords and then distribute the documents to the 056 people who follow the keywords. In this circum-057 stance, the document and the keyword should better 058 have the same topic, which we call topic-aware rel-059 evance. As shown in Table 1, for the hot keyword 060 ""cherry blossoms"", the document (labeled 2) should 061 be the ideal document which should be attached be-062 cause it has the same topic with the keyword while 063 the document (labeled 1) should be a secondary 064 choice, because only several words or phrases in 065 this document match the keyword but the topics of 066 the document mismatch the keyword. 067 To tackle this problem, we define a three-068 level relevance: topic-aware relevance, partially-069 relevance and irrelevance. The topic-aware rele-070 vance means the keyword and the document have 071 the same topic while the partially-relevance means 072 only part of the document matches with the key-073 word. Our task is more challenging than previous 074 query-document matching tasks. To capture the rel-075 evance between the keyword and the document at 076 above-mentioned three levels, we should not only 077 combine the latent topic of the document with its 078 deep representation, but also model complex inter-079 actions between the keyword and the document. 080 Previous neural query-document matching mod-081 els (similar as keyword-document matching) can 082 be divided into two categories according to their 083 model architectures (Guo et al., 2016). One is the 084 5432 Keyword: cherry blossoms Original Keyword: 樱花 Label: 0 Irrelevance Case Translated Document: There was a flower shop which has opened for a few months. I bought some flowers to decorate my house. The shop had common flowers such as lilies and carnations, but there were not many colors to be chosen... Original Document: 这家花店开了有几个月了。我买了一些花来装饰我的房子。店里有百合 花、康乃馨等普通花卉，但可供选择的颜色不多... Label: 1 Partially-Relevance Case Translated Document: The food in this restaurant is very delicious. I tried some dishes, such as foie gras, steak, squid, noodles, desserts, etc. All the dishes are really yummy, especially the filet mignon... By the way, there is a cherry blossoms exhibition near this restaurant.","{'sequence': 'In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than partially-relevance ones in this application. However, previous tasks never define topic-aware relevance clearly. To tackle this problem, we define a three-level relevance in keyword-document matching task: topicaware relevance, partially-relevance and irrelevance. To capture the relevance between the short keyword and the document at abovementioned three levels, we should not only combine the latent topic of the document with its deep neural representation, but also model complex interactions between the keyword and the document. To this end, we propose a Two-stage Interaction and Topic-Aware text matching model (TITA). In terms of ""topicaware"", we introduce neural topic model to analyze the topic of the document and then use it to further encode the document. In terms of ""two-stage interaction"", we propose two successive stages to model complex interactions between the keyword and the document. Extensive experiments reveal that TITA outperforms other well-designed baselines and shows excellent performance in our recommendation system. 044 query-document matching is an important feature 045 in the ranking models. (2) as for the task of ques-046 tion answering (Yang et al., 2016), query-document 047 matching method can be used to find document can-048 didates or to help predict the answer span. (3) it 049 is also widely applied to recommendation systems 050 (Jiang et al., 2019). 051 In many scenarios, we need to distinguish dif-052 ferent keyword-document (query-document) rele-053 vance levels. For instance, in our recommendation 054 system, we need to attach documents to some hot 055 keywords and then distribute the documents to the 056 people who follow the keywords. In this circum-057 stance, the document and the keyword should better 058 have the same topic, which we call topic-aware rel-059 evance. As shown in Table 1, for the hot keyword 060 ""cherry blossoms"", the document (labeled 2) should 061 be the ideal document which should be attached be-062 cause it has the same topic with the keyword while 063 the document (labeled 1) should be a secondary 064 choice, because only several words or phrases in 065 this document match the keyword but the topics of 066 the document mismatch the keyword. 067 To tackle this problem, we define a three-068 level relevance: topic-aware relevance, partially-069 relevance and irrelevance. The topic-aware rele-070 vance means the keyword and the document have 071 the same topic while the partially-relevance means 072 only part of the document matches with the key-073 word. Our task is more challenging than previous 074 query-document matching tasks. To capture the rel-075 evance between the keyword and the document at 076 above-mentioned three levels, we should not only 077 combine the latent topic of the document with its 078 deep representation, but also model complex inter-079 actions between the keyword and the document. 080 Previous neural query-document matching mod-081 els (similar as keyword-document matching) can 082 be divided into two categories according to their 083 model architectures (Guo et al., 2016). One is the 084 5432 Keyword: cherry blossoms Original Keyword: 樱花 Label: 0 Irrelevance Case Translated Document: There was a flower shop which has opened for a few months. I bought some flowers to decorate my house. The shop had common flowers such as lilies and carnations, but there were not many colors to be chosen... Original Document: 这家花店开了有几个月了。我买了一些花来装饰我的房子。店里有百合 花、康乃馨等普通花卉，但可供选择的颜色不多... Label: 1 Partially-Relevance Case Translated Document: The food in this restaurant is very delicious. I tried some dishes, such as foie gras, steak, squid, noodles, desserts, etc. All the dishes are really yummy, especially the filet mignon... By the way, there is a cherry blossoms exhibition near this restaurant.', 'labels': ['Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'NLP Applications', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Generation', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Information Extraction', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media', 'Machine Learning for NLP'], 'scores': [0.09174686670303345, 0.07823945581912994, 0.06386558711528778, 0.0612085685133934, 0.060812827199697495, 0.0556788295507431, 0.054192934185266495, 0.049593061208724976, 0.04866816848516464, 0.0462745800614357, 0.0426662415266037, 0.04252827540040016, 0.038891423493623734, 0.03687378019094467, 0.03574143722653389, 0.03399508818984032, 0.030777232721447945, 0.027652757242321968, 0.025491349399089813, 0.02505510300397873, 0.018324781209230423, 0.017519110813736916, 0.014202425256371498]}",0.09174686670303345,Question Answering,0.0556788295507431
NLP Applications,Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction,"Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/ thunlp/VERNet.","{'sequence': 'Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/ thunlp/VERNet.', 'labels': ['Resources and Evaluation', 'Generation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'NLP Applications', 'Question Answering', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Machine Translation and Multilinguality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08831984549760818, 0.08473870158195496, 0.07855000346899033, 0.0693342313170433, 0.05467831715941429, 0.05058843269944191, 0.04815172031521797, 0.046894680708646774, 0.04645950719714165, 0.044917620718479156, 0.04449136182665825, 0.04400628060102463, 0.0413396880030632, 0.038339804857969284, 0.031251613050699234, 0.030066395178437233, 0.0269921887665987, 0.02637471817433834, 0.026328062638640404, 0.024737557396292686, 0.024685703217983246, 0.017820069566369057, 0.010933544486761093]}",0.08831984549760818,Resources and Evaluation,0.05058843269944191
NLP Applications,Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects,"Side effects during neural network tuning are typically measured by overall accuracy changes. However, we find that even with similar overall accuracy, existing tuning methods result in non-negligible instance-wise side effects. Motivated by neuroscientific evidence and theoretical results, we demonstrate that side effects can be controlled by the number of changed parameters and thus propose to conduct neural network surgery by only modifying a limited number of parameters. Neural network surgery can be realized using diverse techniques, and we investigate three lines of methods. Experimental results on representative tuning problems validate the effectiveness of the surgery approach. The dynamic selecting method achieves the best overall performance that not only satisfies the tuning goal but also induces fewer instance-wise side effects by changing only 10 −5 of the parameters.","{'sequence': 'Side effects during neural network tuning are typically measured by overall accuracy changes. However, we find that even with similar overall accuracy, existing tuning methods result in non-negligible instance-wise side effects. Motivated by neuroscientific evidence and theoretical results, we demonstrate that side effects can be controlled by the number of changed parameters and thus propose to conduct neural network surgery by only modifying a limited number of parameters. Neural network surgery can be realized using diverse techniques, and we investigate three lines of methods. Experimental results on representative tuning problems validate the effectiveness of the surgery approach. The dynamic selecting method achieves the best overall performance that not only satisfies the tuning goal but also induces fewer instance-wise side effects by changing only 10 −5 of the parameters.', 'labels': ['Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Resources and Evaluation', 'Question Answering', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Speech and Multimodality', 'Summarization', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10429245978593826, 0.09088543802499771, 0.07844328880310059, 0.07198913395404816, 0.060587648302316666, 0.051436543464660645, 0.05064051225781441, 0.045869868248701096, 0.045156899839639664, 0.03607171028852463, 0.035547349601984024, 0.0355079248547554, 0.034074656665325165, 0.03359825909137726, 0.033493880182504654, 0.029175320640206337, 0.028941655531525612, 0.02619067393243313, 0.025636009871959686, 0.02537238597869873, 0.022074123844504356, 0.02183801680803299, 0.013176285661756992]}",0.10429245978593826,Dialogue and Interactive Systems,0.034074656665325165
NLP Applications,Discrete Argument Representation Learning for Interactive Argument Pair Identification,"In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental results on the large-scale dataset collected from CMV show that our proposed framework can significantly outperform the competitive baselines. Further analyses reveal why our model yields superior performance and prove the usefulness of our learned representations.","{'sequence': 'In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental results on the large-scale dataset collected from CMV show that our proposed framework can significantly outperform the competitive baselines. Further analyses reveal why our model yields superior performance and prove the usefulness of our learned representations.', 'labels': ['Dialogue and Interactive Systems', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Information Extraction', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Summarization', 'Machine Translation and Multilinguality', 'Generation', 'Discourse and Pragmatics', 'Question Answering', 'Resources and Evaluation', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.11046309769153595, 0.07426335662603378, 0.07197362929582596, 0.06565359234809875, 0.06418447941541672, 0.05120442807674408, 0.050240471959114075, 0.048966843634843826, 0.04418399930000305, 0.041854288429021835, 0.040306102484464645, 0.039033371955156326, 0.035094521939754486, 0.034815602004528046, 0.03258251026272774, 0.030761560425162315, 0.0301901176571846, 0.028352389112114906, 0.026870757341384888, 0.02517394907772541, 0.02254832535982132, 0.020653879269957542, 0.010628717020154]}",0.11046309769153595,Dialogue and Interactive Systems,0.04418399930000305
NLP Applications,On Unifying Misinformation Detection,"In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and model's generalizability to unseen events.","{'sequence': ""In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and model's generalizability to unseen events."", 'labels': ['Speech and Multimodality', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'NLP Applications', 'Resources and Evaluation', 'Question Answering', 'Generation', 'Ethics and NLP', 'Summarization', 'Discourse and Pragmatics', 'Interpretability and Analysis of Models for NLP', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.11708389222621918, 0.08685380965471268, 0.0811786949634552, 0.05942622572183609, 0.04923761263489723, 0.048137154430150986, 0.04738399013876915, 0.04714709520339966, 0.046647556126117706, 0.04519869014620781, 0.043948519974946976, 0.04250262305140495, 0.04210452735424042, 0.03574628010392189, 0.034444790333509445, 0.03224029764533043, 0.031615786254405975, 0.025527557358145714, 0.02242836356163025, 0.021679993718862534, 0.01649601384997368, 0.013127011246979237, 0.0098434928804636]}",0.11708389222621918,Speech and Multimodality,0.05942622572183609
NLP Applications,Frustratingly Easy Edit-based Linguistic Steganography with a Masked Language Model,"With advances in neural language models, the focus of linguistic steganography has shifted from edit-based approaches to generationbased ones. While the latter's payload capacity is impressive, generating genuine-looking texts remains challenging. In this paper, we revisit edit-based linguistic steganography, with the idea that a masked language model offers an off-the-shelf solution. The proposed method eliminates painstaking rule construction and has a high payload capacity for an edit-based model. It is also shown to be more secure against automatic detection than a generation-based method while offering better control of the security/payload capacity tradeoff.","{'sequence': ""With advances in neural language models, the focus of linguistic steganography has shifted from edit-based approaches to generationbased ones. While the latter's payload capacity is impressive, generating genuine-looking texts remains challenging. In this paper, we revisit edit-based linguistic steganography, with the idea that a masked language model offers an off-the-shelf solution. The proposed method eliminates painstaking rule construction and has a high payload capacity for an edit-based model. It is also shown to be more secure against automatic detection than a generation-based method while offering better control of the security/payload capacity tradeoff."", 'labels': ['Speech and Multimodality', 'Generation', 'Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08648255467414856, 0.07471369206905365, 0.06846541911363602, 0.06824430078268051, 0.06451529264450073, 0.05654985085129738, 0.04945828765630722, 0.04854552447795868, 0.047106511890888214, 0.047073524445295334, 0.0464431568980217, 0.045868776738643646, 0.038417235016822815, 0.037593889981508255, 0.03169417753815651, 0.028968125581741333, 0.028465650975704193, 0.027628600597381592, 0.026392994448542595, 0.02441548928618431, 0.02240244671702385, 0.016266243532299995, 0.014288361184298992]}",0.08648255467414856,Speech and Multimodality,0.06824430078268051
NLP Applications,"Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning","Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation-a technique particularly suitable for training with limited data-for this few-shot, highlymulticlass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a twostage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.","{'sequence': 'Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation-a technique particularly suitable for training with limited data-for this few-shot, highlymulticlass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a twostage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.', 'labels': ['Machine Learning for NLP', 'NLP Applications', 'Ethics and NLP', 'Question Answering', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.22508108615875244, 0.17090369760990143, 0.07332467287778854, 0.05937318503856659, 0.05173564702272415, 0.051031697541475296, 0.049509938806295395, 0.04618459567427635, 0.03149658441543579, 0.027081269770860672, 0.024337857961654663, 0.02107926644384861, 0.019492357969284058, 0.01872324012219906, 0.0184042826294899, 0.017393099144101143, 0.017328308895230293, 0.015476889908313751, 0.014702272601425648, 0.014499585144221783, 0.012991303578019142, 0.010318250395357609, 0.009530890733003616]}",0.22508108615875244,Machine Learning for NLP,0.17090369760990143
"Phonology, Morphology and Word Segmentation",Do RNN States Encode Abstract Phonological Alternations?,"Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized or whether there is some level of generalization across related sound changes in a sequence-to-sequence model, we perform several experiments on Finnish consonant gradation-a complex set of sound changes triggered in some words by certain suffixes. We find that our models oftenthough not always-encode 17 different consonant gradation processes in a handful of dimensions in the RNN. We also show that by scaling the activations in these dimensions we can control whether consonant gradation occurs and the direction of the gradation.","{'sequence': 'Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized or whether there is some level of generalization across related sound changes in a sequence-to-sequence model, we perform several experiments on Finnish consonant gradation-a complex set of sound changes triggered in some words by certain suffixes. We find that our models oftenthough not always-encode 17 different consonant gradation processes in a handful of dimensions in the RNN. We also show that by scaling the activations in these dimensions we can control whether consonant gradation occurs and the direction of the gradation.', 'labels': ['Dialogue and Interactive Systems', 'Question Answering', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07697740942239761, 0.07499252259731293, 0.06206536293029785, 0.060137130320072174, 0.05909101292490959, 0.055353451520204544, 0.050527606159448624, 0.050415899604558945, 0.04907795041799545, 0.045391250401735306, 0.04058866947889328, 0.03832319751381874, 0.03790818527340889, 0.03761514648795128, 0.036671023815870285, 0.035325199365615845, 0.03502880409359932, 0.03410598263144493, 0.033363863825798035, 0.03173631429672241, 0.02387371100485325, 0.016278084367513657, 0.015152182430028915]}",0.07697740942239761,Dialogue and Interactive Systems,0.055353451520204544
"Phonology, Morphology and Word Segmentation",Pre-training with Meta Learning for Chinese Word Segmentation,"Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model METASEG, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that METASEG could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, METASEG can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in lowresource settings.","{'sequence': 'Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model METASEG, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that METASEG could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, METASEG can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in lowresource settings.', 'labels': ['Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Machine Learning for NLP', 'Generation', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'NLP Applications', 'Question Answering', 'Ethics and NLP', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Summarization', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08656217902898788, 0.07921729981899261, 0.0717768743634224, 0.06145922839641571, 0.05896778032183647, 0.056201279163360596, 0.04956454411149025, 0.04866122826933861, 0.047520894557237625, 0.04492201283574104, 0.04375443980097771, 0.03887896239757538, 0.03519143909215927, 0.03292936086654663, 0.03148924931883812, 0.030872387811541557, 0.030588777735829353, 0.02933146432042122, 0.028506694361567497, 0.026780689135193825, 0.026089271530508995, 0.02298230491578579, 0.01775161363184452]}",0.08656217902898788,Interpretability and Analysis of Models for NLP,0.04375443980097771
"Phonology, Morphology and Word Segmentation","Decompose, Fuse and Generate: A Formation-Informed Method for Chinese Definition Generation","In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word (""桃 花"", peachblossom) is formed by formation components (""桃"", peach; ""花"", flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 * Equal contribution.","{'sequence': 'In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word (""桃 花"", peachblossom) is formed by formation components (""桃"", peach; ""花"", flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 * Equal contribution.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Information Extraction', 'Discourse and Pragmatics', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.19246506690979004, 0.06905803829431534, 0.06414832174777985, 0.05905075743794441, 0.05330977961421013, 0.05224296450614929, 0.05028730258345604, 0.046104028820991516, 0.04596152529120445, 0.045651067048311234, 0.044269923120737076, 0.03612230718135834, 0.034912142902612686, 0.03320567309856415, 0.02740577422082424, 0.026412272825837135, 0.021561000496149063, 0.020570209249854088, 0.019194230437278748, 0.018434111028909683, 0.01751374453306198, 0.0120951933786273, 0.010024560615420341]}",0.19246506690979004,Generation,0.021561000496149063
"Phonology, Morphology and Word Segmentation",User-Generated Text Corpus for Evaluating Japanese Morphological Analysis and Lexical Normalization,"Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGTspecific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT.","{'sequence': 'Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGTspecific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT.', 'labels': ['Resources and Evaluation', 'Generation', 'Phonology, Morphology and Word Segmentation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Question Answering', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Machine Learning for NLP', 'Speech and Multimodality', 'Ethics and NLP', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13996291160583496, 0.08098340779542923, 0.07225289195775986, 0.06099110469222069, 0.05837121978402138, 0.057675689458847046, 0.054632630199193954, 0.05324631556868553, 0.045634832233190536, 0.03759213164448738, 0.03577140346169472, 0.03425324335694313, 0.03365184739232063, 0.03295694664120674, 0.030750781297683716, 0.030162213370203972, 0.03001992776989937, 0.029232122004032135, 0.021511921659111977, 0.020147914066910744, 0.018478088080883026, 0.011005829088389874, 0.010714645497500896]}",0.13996291160583496,Resources and Evaluation,0.07225289195775986
"Phonology, Morphology and Word Segmentation",GPT Perdetry Test: Generating new meanings for new words,"Human innovation in language, such as inventing new words, is a challenge for pretrained language models. We assess the ability of one large model, GPT-3, to process new words and decide on their meaning. We create a set of nonce words and prompt GPT-3 to generate their dictionary definitions. We find GPT-3 produces plausible definitions that align with human judgments. Moreover, GPT-3's definitions are sometimes preferred to those invented by humans, signaling its intriguing ability not just to adapt, but to add to the evolving vocabulary of the English language.","{'sequence': ""Human innovation in language, such as inventing new words, is a challenge for pretrained language models. We assess the ability of one large model, GPT-3, to process new words and decide on their meaning. We create a set of nonce words and prompt GPT-3 to generate their dictionary definitions. We find GPT-3 produces plausible definitions that align with human judgments. Moreover, GPT-3's definitions are sometimes preferred to those invented by humans, signaling its intriguing ability not just to adapt, but to add to the evolving vocabulary of the English language."", 'labels': ['Dialogue and Interactive Systems', 'Generation', 'Question Answering', 'Speech and Multimodality', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Information Extraction', 'NLP Applications', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Ethics and NLP', 'Summarization', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08471237868070602, 0.07720012217760086, 0.07433652132749557, 0.06043381243944168, 0.05708610638976097, 0.0517171286046505, 0.046606019139289856, 0.046311356127262115, 0.044300422072410583, 0.043731313198804855, 0.042186442762613297, 0.039474811404943466, 0.03863861411809921, 0.03751753270626068, 0.03735756501555443, 0.03698579967021942, 0.03676949068903923, 0.03309281915426254, 0.030088728293776512, 0.02650037407875061, 0.020324837416410446, 0.01884874328970909, 0.01577901281416416]}",0.08471237868070602,Dialogue and Interactive Systems,0.03735756501555443
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Universal Semantic Tagging for English and Mandarin Chinese,"Universal Semantic Tagging aims to provide lightweight unified analysis for all languages at the word level. Though the proposed annotation scheme is conceptually promising, the feasibility is only examined in four Indo-European languages. This paper is concerned with extending the annotation scheme to handle Mandarin Chinese and empirically study the plausibility of unifying meaning representations for multiple languages. We discuss a set of language-specific semantic phenomena, propose new annotation specifications and build a richly annotated corpus. The corpus consists of 1100 English-Chinese parallel sentences, where compositional semantic analysis is available for English, and another 1000 Chinese sentences which has enriched syntactic analysis. By means of the new annotations, we also evaluate a series of neural tagging models to gauge how successful semantic tagging can be: accuracies of 92.7% and 94.6% are obtained for Chinese and English respectively. The English tagging performance is remarkably better than the state-ofthe-art by 7.7%.","{'sequence': 'Universal Semantic Tagging aims to provide lightweight unified analysis for all languages at the word level. Though the proposed annotation scheme is conceptually promising, the feasibility is only examined in four Indo-European languages. This paper is concerned with extending the annotation scheme to handle Mandarin Chinese and empirically study the plausibility of unifying meaning representations for multiple languages. We discuss a set of language-specific semantic phenomena, propose new annotation specifications and build a richly annotated corpus. The corpus consists of 1100 English-Chinese parallel sentences, where compositional semantic analysis is available for English, and another 1000 Chinese sentences which has enriched syntactic analysis. By means of the new annotations, we also evaluate a series of neural tagging models to gauge how successful semantic tagging can be: accuracies of 92.7% and 94.6% are obtained for Chinese and English respectively. The English tagging performance is remarkably better than the state-ofthe-art by 7.7%.', 'labels': ['Resources and Evaluation', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Question Answering', 'Information Extraction', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'NLP Applications', 'Machine Translation and Multilinguality', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Interpretability and Analysis of Models for NLP', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09390478581190109, 0.08720081299543381, 0.0763116106390953, 0.0700671374797821, 0.06519050151109695, 0.06106678023934364, 0.05267975851893425, 0.05214700475335121, 0.04711846634745598, 0.044328272342681885, 0.04074703902006149, 0.0370250828564167, 0.036467812955379486, 0.035756666213274, 0.03453468158841133, 0.024704813957214355, 0.023202314972877502, 0.022842803969979286, 0.021229213103652, 0.021063894033432007, 0.01970129832625389, 0.018368976190686226, 0.014340412802994251]}",0.09390478581190109,Resources and Evaluation,0.05214700475335121
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser,"Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https://github. com/WowCZ/shadowgnn.","{'sequence': 'Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https://github. com/WowCZ/shadowgnn.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Information Extraction', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Generation', 'Machine Learning for NLP', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Ethics and NLP', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Discourse and Pragmatics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08181339502334595, 0.07906458526849747, 0.07213447988033295, 0.0714258924126625, 0.05873291566967964, 0.05829142406582832, 0.05175066739320755, 0.04998685047030449, 0.04713568836450577, 0.044488001614809036, 0.041837356984615326, 0.03963102027773857, 0.03942258656024933, 0.03786291927099228, 0.037274669855833054, 0.03152726963162422, 0.03038901835680008, 0.026683220639824867, 0.025892285630106926, 0.02251548320055008, 0.02199261449277401, 0.019409306347370148, 0.010738353244960308]}",0.08181339502334595,Question Answering,0.05873291566967964
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning: A Case Study on Discourse Relation Analysis,"We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed method, a model is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The model is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the model minimize the similarity between the latter representation and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in English and Japanese and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa.","{'sequence': 'We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed method, a model is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The model is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the model minimize the similarity between the latter representation and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in English and Japanese and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Question Answering', 'Machine Translation and Multilinguality', 'NLP Applications', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Summarization', 'Resources and Evaluation', 'Generation', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.10377263277769089, 0.07612048834562302, 0.06899511814117432, 0.05573905259370804, 0.05476051941514015, 0.05154724791646004, 0.05119960755109787, 0.04975353553891182, 0.04974748566746712, 0.04154563322663307, 0.04018464684486389, 0.03961271792650223, 0.03647869825363159, 0.03527234122157097, 0.03511803224682808, 0.03387879952788353, 0.03292708098888397, 0.032363150268793106, 0.03214254975318909, 0.027104105800390244, 0.017951659858226776, 0.017607565969228745, 0.016177240759134293]}",0.10377263277769089,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.10377263277769089
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",AMR Parsing with Action-Pointer Transformer,"Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention over sentences with a targetside action pointer mechanism to decouple source tokens from node representations and address alignments. We model the transitions as well as the pointer mechanism through straightforward modifications within a single Transformer architecture. Parser state and graph structure information are efficiently encoded using attention heads. We show that our action-pointer approach leads to increased expressiveness and attains large gains (+1.6 points) against the best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best SMATCH score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding.","{'sequence': 'Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention over sentences with a targetside action pointer mechanism to decouple source tokens from node representations and address alignments. We model the transitions as well as the pointer mechanism through straightforward modifications within a single Transformer architecture. Parser state and graph structure information are efficiently encoded using attention heads. We show that our action-pointer approach leads to increased expressiveness and attains large gains (+1.6 points) against the best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best SMATCH score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding.', 'labels': ['Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Speech and Multimodality', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Ethics and NLP', 'Generation', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Summarization', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09972689300775528, 0.0964372307062149, 0.06875856965780258, 0.06523947417736053, 0.06469106674194336, 0.062329746782779694, 0.04672130569815636, 0.046404339373111725, 0.04227246716618538, 0.03648382052779198, 0.035536449402570724, 0.03461900353431702, 0.03265983611345291, 0.03156665712594986, 0.030446216464042664, 0.029694387689232826, 0.02926245518028736, 0.028540311381220818, 0.026126420125365257, 0.02606119029223919, 0.02393253892660141, 0.021546443924307823, 0.020943090319633484]}",0.09972689300775528,"Semantics: Sentence-level Semantics, Textual Inference and Other areas",0.09972689300775528
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction,"We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one round of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit.","{'sequence': 'We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one round of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Speech and Multimodality', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Information Extraction', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Question Answering', 'Machine Translation and Multilinguality', 'Machine Learning for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.08873768895864487, 0.08826741576194763, 0.07584090530872345, 0.07221508026123047, 0.06373434513807297, 0.05932874605059624, 0.057218365371227264, 0.05353078618645668, 0.047803137451410294, 0.0422310046851635, 0.0419582799077034, 0.03782189264893532, 0.03321050852537155, 0.03234443441033363, 0.031201377511024475, 0.030389875173568726, 0.02688850276172161, 0.02657635696232319, 0.02543700858950615, 0.021302150562405586, 0.016795333474874496, 0.016667161136865616, 0.010499469004571438]}",0.08873768895864487,Dialogue and Interactive Systems,0.03782189264893532
"Semantics: Sentence-level Semantics, Textual Inference and Other areas",Unsupervised Concept Representation Learning for Length-Varying Text Similarity,"Measuring document similarity plays an important role in natural language processing tasks. Most existing document similarity approaches suffer from the information gap caused by context and vocabulary mismatches when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire text corpus. Moreover, a concept-based document matching method is proposed to leverage advances in the recognition of local phrase features and corpuslevel concept features. Extensive experiments on real-world data sets demonstrate that new method can achieve a considerable improvement in comparing length-varying texts. In particular, our model achieved 6.5% better F1 Score compared to the best of the baseline models for a concept-project benchmark dataset.","{'sequence': 'Measuring document similarity plays an important role in natural language processing tasks. Most existing document similarity approaches suffer from the information gap caused by context and vocabulary mismatches when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire text corpus. Moreover, a concept-based document matching method is proposed to leverage advances in the recognition of local phrase features and corpuslevel concept features. Extensive experiments on real-world data sets demonstrate that new method can achieve a considerable improvement in comparing length-varying texts. In particular, our model achieved 6.5% better F1 Score compared to the best of the baseline models for a concept-project benchmark dataset.', 'labels': ['Generation', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Summarization', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.140843465924263, 0.08292365819215775, 0.0666189044713974, 0.061574552208185196, 0.055269066244363785, 0.05034404247999191, 0.04573028162121773, 0.04524094611406326, 0.042169034481048584, 0.03993900120258331, 0.03756638616323471, 0.03489278256893158, 0.03397289663553238, 0.03394189849495888, 0.031406790018081665, 0.031359802931547165, 0.02807360701262951, 0.027885060757398605, 0.02742990292608738, 0.023052722215652466, 0.023009035736322403, 0.020132245495915413, 0.016623787581920624]}",0.140843465924263,Generation,0.05034404247999191
Dialogue and Interactive Systems,Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition,"Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pretraining a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pretrained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledgegrounded dialogue benchmarks indicate that our model achieves better performance over baseline models.","{'sequence': 'Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pretraining a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pretrained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledgegrounded dialogue benchmarks indicate that our model achieves better performance over baseline models.', 'labels': ['Speech and Multimodality', 'Generation', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'NLP Applications', 'Summarization', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.12089270353317261, 0.10148787498474121, 0.06366261839866638, 0.05192332714796066, 0.04577174410223961, 0.04536626860499382, 0.0437047965824604, 0.040421485900878906, 0.039399728178977966, 0.039294201880693436, 0.03899889811873436, 0.035962142050266266, 0.03583374246954918, 0.03579811006784439, 0.034515611827373505, 0.034217774868011475, 0.032954905182123184, 0.032825302332639694, 0.030955208465456963, 0.02787957526743412, 0.02639230340719223, 0.02258274145424366, 0.019158853217959404]}",0.12089270353317261,Speech and Multimodality,0.04577174410223961
Dialogue and Interactive Systems,Adversarial Self-Supervised Learning for Out-of-Domain Detection,"Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a selfsupervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and robustness of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.","{'sequence': 'Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a selfsupervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and robustness of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.', 'labels': ['Information Extraction', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Question Answering', 'Summarization', 'Ethics and NLP', 'Computational Social Science and Social Media', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.09815047681331635, 0.09139251708984375, 0.0767245665192604, 0.06823725253343582, 0.06778816133737564, 0.05306374281644821, 0.04902312159538269, 0.04869402199983597, 0.047333672642707825, 0.044231634587049484, 0.0420498326420784, 0.04204774647951126, 0.037780534476041794, 0.03758597746491432, 0.03544776514172554, 0.03380545973777771, 0.031893737614154816, 0.017693888396024704, 0.017582960426807404, 0.017480209469795227, 0.015882525593042374, 0.013255758211016655, 0.012854453176259995]}",0.09815047681331635,Information Extraction,0.09139251708984375
Dialogue and Interactive Systems,Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue StateTracking,"Zero-shot cross-domain dialogue state tracking (DST) enables us to handle task-oriented dialogue in unseen domains without the expense of collecting in-domain data. In this paper, we propose a slot description enhanced generative approach for zero-shot cross-domain DST. Specifically, our model first encodes dialogue context and slots with a pre-trained self-attentive encoder, and generates slot values in an auto-regressive manner. In addition, we incorporate Slot Type Informed Descriptions that capture the shared information across slots to facilitate cross-domain knowledge transfer. Experimental results on the MultiWOZ dataset show that our proposed method significantly improves existing stateof-the-art results in the zero-shot cross-domain setting.","{'sequence': 'Zero-shot cross-domain dialogue state tracking (DST) enables us to handle task-oriented dialogue in unseen domains without the expense of collecting in-domain data. In this paper, we propose a slot description enhanced generative approach for zero-shot cross-domain DST. Specifically, our model first encodes dialogue context and slots with a pre-trained self-attentive encoder, and generates slot values in an auto-regressive manner. In addition, we incorporate Slot Type Informed Descriptions that capture the shared information across slots to facilitate cross-domain knowledge transfer. Experimental results on the MultiWOZ dataset show that our proposed method significantly improves existing stateof-the-art results in the zero-shot cross-domain setting.', 'labels': ['Generation', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Question Answering', 'Ethics and NLP', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.1526772826910019, 0.07929112762212753, 0.06867951899766922, 0.0630226731300354, 0.05408059060573578, 0.05219228193163872, 0.05201290547847748, 0.045308977365493774, 0.04429864138364792, 0.04255543276667595, 0.04175976663827896, 0.041314128786325455, 0.032065413892269135, 0.031116142868995667, 0.030099235475063324, 0.029968438670039177, 0.028513478115200996, 0.020786428824067116, 0.020287299528717995, 0.019903389737010002, 0.017937324941158295, 0.01676536537706852, 0.01536415982991457]}",0.1526772826910019,Generation,0.04429864138364792
Dialogue and Interactive Systems,Hierarchical Transformer for Task Oriented Dialog Systems,"Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni et al. (2015b); Serban et al. ( 2016 ) proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of hierarchy in transformer based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments. The code and data for all experiments in this paper has been open-sourced 1 2 .","{'sequence': 'Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni et al. (2015b); Serban et al. ( 2016 ) proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of hierarchy in transformer based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments. The code and data for all experiments in this paper has been open-sourced 1 2 .', 'labels': ['Question Answering', 'Generation', 'Summarization', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'NLP Applications', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Ethics and NLP', 'Syntax: Tagging, Chunking and Parsing', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.32615163922309875, 0.19067704677581787, 0.11626652628183365, 0.02941140905022621, 0.027422571554780006, 0.02661353163421154, 0.023146864026784897, 0.02267627604305744, 0.021311499178409576, 0.01962793618440628, 0.018634628504514694, 0.018545428290963173, 0.01818610355257988, 0.017480285838246346, 0.017250588163733482, 0.017097417265176773, 0.01657833904027939, 0.0152632687240839, 0.015055160038173199, 0.01257868018001318, 0.01253265980631113, 0.01152968779206276, 0.005962501745671034]}",0.32615163922309875,Question Answering,0.027422571554780006
Information Extraction,RTFE: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion,"Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in time evolution, we treat the sequence of graphs as a Markov chain, which transitions from the previous state to the next state. RTFE takes the SKGE to initialize the embeddings of TKG. Then it recursively tracks the state transition of TKG by passing updated parameters/features between timestamps. Specifically, at each timestamp, we approximate the state transition as the gradient update process. Since RTFE learns each timestamp recursively, it can naturally transit to future timestamps. Experiments on five TKG datasets show the effectiveness of RTFE.","{'sequence': 'Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in time evolution, we treat the sequence of graphs as a Markov chain, which transitions from the previous state to the next state. RTFE takes the SKGE to initialize the embeddings of TKG. Then it recursively tracks the state transition of TKG by passing updated parameters/features between timestamps. Specifically, at each timestamp, we approximate the state transition as the gradient update process. Since RTFE learns each timestamp recursively, it can naturally transit to future timestamps. Experiments on five TKG datasets show the effectiveness of RTFE.', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Question Answering', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Generation', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Summarization', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10033415257930756, 0.09424565732479095, 0.08474648743867874, 0.07719668745994568, 0.06208271160721779, 0.057534970343112946, 0.04868203401565552, 0.046040717512369156, 0.04520426690578461, 0.04047178849577904, 0.03627874702215195, 0.03499555215239525, 0.03486417979001999, 0.03305751085281372, 0.03295847401022911, 0.03061681240797043, 0.028386199846863747, 0.028317151591181755, 0.027384396642446518, 0.020262865349650383, 0.013567942194640636, 0.012423423118889332, 0.010347206145524979]}",0.10033415257930756,Dialogue and Interactive Systems,0.09424565732479095
Information Extraction,Open Hierarchical Relation Extraction,"Open relation extraction (OpenRE) aims to extract novel relation types from open-domain corpora, which plays an important role in completing the relation schemes of knowledge bases (KBs). Most OpenRE methods cast different relation types in isolation without considering their hierarchical dependency. We argue that OpenRE is inherently in close connection with relation hierarchies. To address the bidirectional connections between OpenRE and relation hierarchy, we propose the task of open hierarchical relation extraction and present a novel OHRE framework for the task. To effectively integrate hierarchy information into relation representations for better novel relation extraction, we propose a dynamic hierarchical triplet objective and hierarchical curriculum training paradigm. We also present a top-down hierarchy expansion algorithm to add the extracted relations into existing hierarchies with reasonable interpretability. Comprehensive experiments show that OHRE outperforms state-of-the-art models by a large margin on both relation clustering and hierarchy expansion. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/OHRE.","{'sequence': 'Open relation extraction (OpenRE) aims to extract novel relation types from open-domain corpora, which plays an important role in completing the relation schemes of knowledge bases (KBs). Most OpenRE methods cast different relation types in isolation without considering their hierarchical dependency. We argue that OpenRE is inherently in close connection with relation hierarchies. To address the bidirectional connections between OpenRE and relation hierarchy, we propose the task of open hierarchical relation extraction and present a novel OHRE framework for the task. To effectively integrate hierarchy information into relation representations for better novel relation extraction, we propose a dynamic hierarchical triplet objective and hierarchical curriculum training paradigm. We also present a top-down hierarchy expansion algorithm to add the extracted relations into existing hierarchies with reasonable interpretability. Comprehensive experiments show that OHRE outperforms state-of-the-art models by a large margin on both relation clustering and hierarchy expansion. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/OHRE.', 'labels': ['Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Generation', 'NLP Applications', 'Resources and Evaluation', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Summarization', 'Semantics: Lexical Semantics', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.07328087091445923, 0.06782977283000946, 0.06522265076637268, 0.05862585827708244, 0.05818649381399155, 0.055151909589767456, 0.05395273491740227, 0.052523523569107056, 0.0487685389816761, 0.047638002783060074, 0.042071934789419174, 0.04169809818267822, 0.041478224098682404, 0.040690772235393524, 0.0390646867454052, 0.038107335567474365, 0.035284239798784256, 0.031611621379852295, 0.027938567101955414, 0.02718263864517212, 0.02393284998834133, 0.018364667892456055, 0.011394101195037365]}",0.07328087091445923,Question Answering,0.05862585827708244
Information Extraction,Jointly Extracting Explicit and Implicit Relational Triples with Reasoning Pattern Enhanced Binary Pointer Network,"Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types between the implicitly connected entity pairs can be identified with relational reasoning patterns in the real world. In this paper, we propose a unified framework to jointly extract explicit and implicit relational triples. To explore entity pairs that may be implicitly connected by relations, we propose a binary pointer network to extract overlapping relational triples relevant to each word sequentially and retain the information of previously extracted triples in an external memory. To infer the relation types of implicit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method.","{'sequence': 'Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types between the implicitly connected entity pairs can be identified with relational reasoning patterns in the real world. In this paper, we propose a unified framework to jointly extract explicit and implicit relational triples. To explore entity pairs that may be implicitly connected by relations, we propose a binary pointer network to extract overlapping relational triples relevant to each word sequentially and retain the information of previously extracted triples in an external memory. To infer the relation types of implicit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method.', 'labels': ['Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'NLP Applications', 'Ethics and NLP', 'Dialogue and Interactive Systems', 'Question Answering', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Speech and Multimodality', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Generation', 'Summarization', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond'], 'scores': [0.11936759203672409, 0.06567933410406113, 0.06505019962787628, 0.05717301368713379, 0.05435857176780701, 0.04813890531659126, 0.04783105105161667, 0.04580911621451378, 0.042183417826890945, 0.042174458503723145, 0.04192609339952469, 0.040571179240942, 0.03894064947962761, 0.03604762256145477, 0.03430350497364998, 0.034047745168209076, 0.03282151371240616, 0.03137539327144623, 0.030999107286334038, 0.026992691680788994, 0.025270352140069008, 0.021030794829130173, 0.017907561734318733]}",0.11936759203672409,Information Extraction,0.11936759203672409
Information Extraction,Multi-Grained Knowledge Distillation for Named Entity Recognition,"Although pre-trained big models (e.g., BERT, ERNIE, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel distillation scheme to efficiently transfer the knowledge learned from big models to their more affordable counterpart. Our solution highlights the construction of surrogate labels through the k-best Viterbi algorithm to distill knowledge from the teacher model. To maximally assimilate knowledge into the student model, we propose a multigrained distillation scheme, which integrates cross entropy involved in conditional random field (CRF) and fuzzy learning. To validate the effectiveness of our proposal, we conducted a comprehensive evaluation on five NER benchmarks, reporting cross-the-board performance gains relative to competing prior-arts. We further discuss ablation results to dissect our gains.","{'sequence': 'Although pre-trained big models (e.g., BERT, ERNIE, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel distillation scheme to efficiently transfer the knowledge learned from big models to their more affordable counterpart. Our solution highlights the construction of surrogate labels through the k-best Viterbi algorithm to distill knowledge from the teacher model. To maximally assimilate knowledge into the student model, we propose a multigrained distillation scheme, which integrates cross entropy involved in conditional random field (CRF) and fuzzy learning. To validate the effectiveness of our proposal, we conducted a comprehensive evaluation on five NER benchmarks, reporting cross-the-board performance gains relative to competing prior-arts. We further discuss ablation results to dissect our gains.', 'labels': ['Resources and Evaluation', 'Dialogue and Interactive Systems', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Speech and Multimodality', 'NLP Applications', 'Question Answering', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Ethics and NLP', 'Discourse and Pragmatics', 'Summarization', 'Computational Social Science and Social Media', 'Machine Translation and Multilinguality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.11531704664230347, 0.09545028209686279, 0.07581165432929993, 0.07082340121269226, 0.06933294981718063, 0.06465527415275574, 0.05058677867054939, 0.044926777482032776, 0.042351242154836655, 0.040438003838062286, 0.036711834371089935, 0.03625844419002533, 0.03521478548645973, 0.034908171743154526, 0.029981199651956558, 0.027670806273818016, 0.025852683931589127, 0.024184994399547577, 0.023949602618813515, 0.019048266112804413, 0.016491925343871117, 0.010050678625702858, 0.00998322106897831]}",0.11531704664230347,Resources and Evaluation,0.07581165432929993
Information Extraction,"SGG: Learning to Select, Guide, and Generate for Keyphrase Generation","Keyphrases, that concisely summarize the high-level topics discussed in a document, can be categorized into present keyphrase which explicitly appears in the source text, and absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Most existing keyphrase generation approaches synchronously generate present and absent keyphrases without explicitly distinguishing these two categories. In this paper, a Select-Guide-Generate (SGG) approach is proposed to deal with present and absent keyphrase generation separately with different mechanisms. Specifically, SGG is a hierarchical neural network which consists of a pointing-based selector at low layer concentrated on present keyphrase generation, a selection-guided generator at high layer dedicated to absent keyphrase generation, and a guider in the middle to transfer information from selector to generator. Experimental results on four keyphrase generation benchmarks demonstrate the effectiveness of our model, which significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks.","{'sequence': 'Keyphrases, that concisely summarize the high-level topics discussed in a document, can be categorized into present keyphrase which explicitly appears in the source text, and absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Most existing keyphrase generation approaches synchronously generate present and absent keyphrases without explicitly distinguishing these two categories. In this paper, a Select-Guide-Generate (SGG) approach is proposed to deal with present and absent keyphrase generation separately with different mechanisms. Specifically, SGG is a hierarchical neural network which consists of a pointing-based selector at low layer concentrated on present keyphrase generation, a selection-guided generator at high layer dedicated to absent keyphrase generation, and a guider in the middle to transfer information from selector to generator. Experimental results on four keyphrase generation benchmarks demonstrate the effectiveness of our model, which significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks.', 'labels': ['Generation', 'Machine Learning for NLP', 'Summarization', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Speech and Multimodality', 'Dialogue and Interactive Systems', 'NLP Applications', 'Information Extraction', 'Ethics and NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Resources and Evaluation', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10690972954034805, 0.09943205118179321, 0.06139526888728142, 0.06074920669198036, 0.06039009988307953, 0.05870283022522926, 0.05502408370375633, 0.049589574337005615, 0.04883548617362976, 0.04775266349315643, 0.0462491437792778, 0.04207739606499672, 0.03961062431335449, 0.03558063879609108, 0.03221398964524269, 0.030495543032884598, 0.030155781656503677, 0.024429943412542343, 0.016430748626589775, 0.016304196789860725, 0.014757507480680943, 0.014069187454879284, 0.008844282478094101]}",0.10690972954034805,Generation,0.04883548617362976
Information Extraction,Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter,"Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA ('TA' means tweet act, i.e., speech act in Twitter) dataset called EmoTA collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants.","{'sequence': ""Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA ('TA' means tweet act, i.e., speech act in Twitter) dataset called EmoTA collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants."", 'labels': ['Speech and Multimodality', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Generation', 'Information Extraction', 'NLP Applications', 'Question Answering', 'Summarization', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Syntax: Tagging, Chunking and Parsing', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation'], 'scores': [0.22752605378627777, 0.09399496018886566, 0.0699119046330452, 0.06231819465756416, 0.04717852547764778, 0.04704403132200241, 0.04488023370504379, 0.04391609877347946, 0.04089224338531494, 0.035887133330106735, 0.03289184719324112, 0.029167944565415382, 0.02823258936405182, 0.027490293607115746, 0.026226555928587914, 0.024921704083681107, 0.022927917540073395, 0.021909786388278008, 0.02169269695878029, 0.016511917114257812, 0.013840587809681892, 0.011184422299265862, 0.00945242214947939]}",0.22752605378627777,Speech and Multimodality,0.04704403132200241
Machine Translation and Multilinguality,Generative Imagination Elevates Machine Translation,"There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence -image for training and tuples of source sentence -image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the ""imagined representation"" to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.","{'sequence': 'There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence -image for training and tuples of source sentence -image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the ""imagined representation"" to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.', 'labels': ['Machine Translation and Multilinguality', 'Speech and Multimodality', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Semantics: Lexical Semantics', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Information Extraction', 'Phonology, Morphology and Word Segmentation', 'Discourse and Pragmatics', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Ethics and NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.14917181432247162, 0.08857230842113495, 0.07116873562335968, 0.05627990514039993, 0.047807782888412476, 0.04338642954826355, 0.0425545759499073, 0.041604313999414444, 0.039722852408885956, 0.03813188895583153, 0.03812483698129654, 0.03780017048120499, 0.036474183201789856, 0.03526191785931587, 0.03273409977555275, 0.03135879337787628, 0.03057997114956379, 0.03002963401377201, 0.02726827934384346, 0.0254714023321867, 0.021753542125225067, 0.017692165449261665, 0.017050476744771004]}",0.14917181432247162,Machine Translation and Multilinguality,0.14917181432247162
Machine Translation and Multilinguality,Non-Autoregressive Translation by Learning Target Categorical Codes,"Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines.","{'sequence': 'Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines.', 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Machine Translation and Multilinguality', 'Speech and Multimodality', 'Information Extraction', 'Interpretability and Analysis of Models for NLP', 'Question Answering', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Discourse and Pragmatics', 'NLP Applications', 'Information Retrieval and Text Mining', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Semantics: Lexical Semantics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.4052567481994629, 0.05869964137673378, 0.051832620054483414, 0.050776928663253784, 0.04440326616168022, 0.03309871256351471, 0.02961123175919056, 0.029139013960957527, 0.028712453320622444, 0.02746700309216976, 0.027347419410943985, 0.024914341047406197, 0.02235460840165615, 0.02166442759335041, 0.020201928913593292, 0.01959468051791191, 0.019548188894987106, 0.01888265274465084, 0.01781795546412468, 0.016696488484740257, 0.014740404672920704, 0.010605888441205025, 0.006633326876908541]}",0.4052567481994629,Generation,0.050776928663253784
Machine Translation and Multilinguality,Training Data Augmentation for Code-Mixed Translation,"Machine translation of user-generated codemixed inputs to English is of crucial importance in applications like web search and targeted advertising. We address the scarcity of parallel training data for training such models by designing a strategy of converting existing non-code-mixed parallel data sources to codemixed parallel data. We present an mBERT based procedure whose core learnable component is a ternary sequence labeling model, that can be trained with a limited code-mixed corpus alone. We show a 5.8 point increase in BLEU on heavily code-mixed sentences by training a translation model using our data augmentation strategy on an Hindi-English codemixed translation task.","{'sequence': 'Machine translation of user-generated codemixed inputs to English is of crucial importance in applications like web search and targeted advertising. We address the scarcity of parallel training data for training such models by designing a strategy of converting existing non-code-mixed parallel data sources to codemixed parallel data. We present an mBERT based procedure whose core learnable component is a ternary sequence labeling model, that can be trained with a limited code-mixed corpus alone. We show a 5.8 point increase in BLEU on heavily code-mixed sentences by training a translation model using our data augmentation strategy on an Hindi-English codemixed translation task.', 'labels': ['Generation', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Information Extraction', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'NLP Applications', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics', 'Ethics and NLP', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Question Answering', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13584737479686737, 0.11720316112041473, 0.07588958740234375, 0.06244023144245148, 0.05232413485646248, 0.05208354815840721, 0.05014638975262642, 0.04930036514997482, 0.045588672161102295, 0.03979272395372391, 0.03869331628084183, 0.03318118304014206, 0.028474897146224976, 0.02803380973637104, 0.026878612115979195, 0.02669944055378437, 0.026654139161109924, 0.02050643227994442, 0.02037614770233631, 0.019945064559578896, 0.019830552861094475, 0.01711115427315235, 0.012999126687645912]}",0.13584737479686737,Generation,0.11720316112041473
Machine Translation and Multilinguality,Rethinking Perturbations in Encoder-Decoders for Fast Training,"We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster. Our code is publicly available at https://github.com/takase/rethink_perturbations.","{'sequence': 'We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster. Our code is publicly available at https://github.com/takase/rethink_perturbations.', 'labels': ['Dialogue and Interactive Systems', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Language Grounding to Vision, Robotics and Beyond', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'NLP Applications', 'Ethics and NLP', 'Discourse and Pragmatics', 'Phonology, Morphology and Word Segmentation', 'Speech and Multimodality', 'Semantics: Lexical Semantics', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Extraction', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Machine Translation and Multilinguality', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.10980374366044998, 0.09576591104269028, 0.07363846153020859, 0.05594819411635399, 0.050114963203668594, 0.049565013498067856, 0.04943232983350754, 0.046774256974458694, 0.04354332387447357, 0.04151376336812973, 0.03947027772665024, 0.03938213735818863, 0.03409942612051964, 0.03305143862962723, 0.03275057673454285, 0.031300704926252365, 0.028447207063436508, 0.02844039723277092, 0.026792827993631363, 0.025234805420041084, 0.02453249879181385, 0.021684428676962852, 0.018713319674134254]}",0.10980374366044998,Dialogue and Interactive Systems,0.021684428676962852
Machine Translation and Multilinguality,Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model,"Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation.","{'sequence': 'Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation.', 'labels': ['Machine Translation and Multilinguality', 'Resources and Evaluation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Semantics: Lexical Semantics', 'Question Answering', 'Information Retrieval and Text Mining', 'Computational Social Science and Social Media', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'NLP Applications', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Discourse and Pragmatics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Ethics and NLP'], 'scores': [0.11147164553403854, 0.10462864488363266, 0.06850102543830872, 0.06542270630598068, 0.06045070290565491, 0.05167756229639053, 0.04466617479920387, 0.04357302561402321, 0.04345954582095146, 0.04316098242998123, 0.04151631519198418, 0.03999552130699158, 0.03670194745063782, 0.03167814016342163, 0.03065488301217556, 0.03051002137362957, 0.02849091775715351, 0.024671897292137146, 0.02343079075217247, 0.020959069952368736, 0.020780149847269058, 0.017205027863383293, 0.016393307596445084]}",0.11147164553403854,Machine Translation and Multilinguality,0.11147164553403854
Machine Translation and Multilinguality,Machine Translated Text Detection Through Text Similarity with Round-Trip Translation,"Translated texts have been used for malicious purposes, i.e., plagiarism or fake reviews. Existing detectors have been built around a specific translator (e.g., Google) but fail to detect a translated text from a strange translator. If we use the same translator, the translated text is similar to its round-trip translation, which is when text is translated into another language and translated back into the original language. However, a round-trip translated text is significantly different from the original text or a translated text using a strange translator. Hence, we propose a detector using text similarity with round-trip translation (TSRT). TSRT achieves 86.9% accuracy in detecting a translated text from a strange translator. It outperforms existing detectors (77.9%) and human recognition (53.3%).","{'sequence': 'Translated texts have been used for malicious purposes, i.e., plagiarism or fake reviews. Existing detectors have been built around a specific translator (e.g., Google) but fail to detect a translated text from a strange translator. If we use the same translator, the translated text is similar to its round-trip translation, which is when text is translated into another language and translated back into the original language. However, a round-trip translated text is significantly different from the original text or a translated text using a strange translator. Hence, we propose a detector using text similarity with round-trip translation (TSRT). TSRT achieves 86.9% accuracy in detecting a translated text from a strange translator. It outperforms existing detectors (77.9%) and human recognition (53.3%).', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Information Retrieval and Text Mining', 'Generation', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Summarization', 'Question Answering', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.09125137329101562, 0.0876959040760994, 0.0761224627494812, 0.06414786726236343, 0.05747534707188606, 0.05308326333761215, 0.050271932035684586, 0.04745003953576088, 0.04196096211671829, 0.041545603424310684, 0.0399533212184906, 0.039617184549570084, 0.039325643330812454, 0.03789648041129112, 0.03523172065615654, 0.03141152486205101, 0.029800986871123314, 0.028681937605142593, 0.026972634717822075, 0.02413814701139927, 0.02184320241212845, 0.020141704007983208, 0.013980761170387268]}",0.09125137329101562,Dialogue and Interactive Systems,0.04745003953576088
Question Answering,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,"Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs' inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/ thunlp/TR-BERT.","{'sequence': ""Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs' inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/ thunlp/TR-BERT."", 'labels': ['NLP Applications', 'Machine Learning for NLP', 'Dialogue and Interactive Systems', 'Ethics and NLP', 'Question Answering', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Generation', 'Speech and Multimodality', 'Computational Social Science and Social Media', 'Information Extraction', 'Semantics: Lexical Semantics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Translation and Multilinguality', 'Discourse and Pragmatics', 'Syntax: Tagging, Chunking and Parsing', 'Summarization', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Information Retrieval and Text Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.3309115469455719, 0.09480490535497665, 0.05354512482881546, 0.04662628099322319, 0.04177435487508774, 0.035767920315265656, 0.035003725439310074, 0.033027324825525284, 0.03291027247905731, 0.030531644821166992, 0.02963411808013916, 0.028196781873703003, 0.027852889150381088, 0.02532212622463703, 0.02410491742193699, 0.022725459188222885, 0.020795060321688652, 0.017760392278432846, 0.01671615056693554, 0.015414627268910408, 0.014844458550214767, 0.014039117842912674, 0.007690793368965387]}",0.3309115469455719,NLP Applications,0.04177435487508774
Question Answering,Breadth First Reasoning Graph for Multi-hop Question Answering,"Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each node from over-smoothing or being updated multiple times unnecessarily. To introduce more semantics, we also define the reasoning graph as a weighted graph with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of granularity based on the GNN. On Hot-potQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction.","{'sequence': 'Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each node from over-smoothing or being updated multiple times unnecessarily. To introduce more semantics, we also define the reasoning graph as a weighted graph with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of granularity based on the GNN. On Hot-potQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction.', 'labels': ['Question Answering', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Information Extraction', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'NLP Applications', 'Speech and Multimodality', 'Generation', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality'], 'scores': [0.18211999535560608, 0.07189364731311798, 0.07168827205896378, 0.058815568685531616, 0.049982644617557526, 0.049810029566287994, 0.04758810997009277, 0.04572170600295067, 0.04009421914815903, 0.037322111427783966, 0.0358864888548851, 0.03571741282939911, 0.03511158004403114, 0.03206123039126396, 0.03171955794095993, 0.027632230892777443, 0.027002234011888504, 0.026676641777157784, 0.02292907051742077, 0.020021267235279083, 0.017437627539038658, 0.016970664262771606, 0.01579768769443035]}",0.18211999535560608,Question Answering,0.18211999535560608
Question Answering,Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph,"Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a highresource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both languageand syntax-independence. Consequently, our model narrows the gap in zero-shot crosslingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness.","{'sequence': 'Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a highresource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both languageand syntax-independence. Consequently, our model narrows the gap in zero-shot crosslingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness.', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Information Extraction', 'Syntax: Tagging, Chunking and Parsing', 'Machine Translation and Multilinguality', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Generation', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.29367795586586, 0.06295673549175262, 0.06270404160022736, 0.05492158234119415, 0.04969945549964905, 0.04808928444981575, 0.04039984196424484, 0.03855222836136818, 0.03343585133552551, 0.03204961493611336, 0.026268834248185158, 0.025280456990003586, 0.024966184049844742, 0.023850861936807632, 0.02374817058444023, 0.023672478273510933, 0.02352101542055607, 0.02148597128689289, 0.021065300330519676, 0.020776286721229553, 0.017368478700518608, 0.016940321773290634, 0.014569099992513657]}",0.29367795586586,Question Answering,0.29367795586586
Question Answering,RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering,"In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely crossbatch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 .","{'sequence': 'In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely crossbatch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 .', 'labels': ['Question Answering', 'Information Extraction', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Generation', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'NLP Applications', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'Speech and Multimodality', 'Language Grounding to Vision, Robotics and Beyond', 'Summarization', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Ethics and NLP', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Machine Translation and Multilinguality'], 'scores': [0.4229776859283447, 0.04998694732785225, 0.04878241568803787, 0.03791625797748566, 0.03557158634066582, 0.03552698343992233, 0.030476948246359825, 0.029250822961330414, 0.027308015152812004, 0.02683061920106411, 0.026759382337331772, 0.025514736771583557, 0.02386881411075592, 0.023854056373238564, 0.020976295694708824, 0.020805461332201958, 0.020243065431714058, 0.019895246252417564, 0.01648319885134697, 0.016314199194312096, 0.016070352867245674, 0.013749645091593266, 0.010837291367352009]}",0.4229776859283447,Question Answering,0.4229776859283447
Question Answering,DAGN: Discourse-Aware Graph Network for Logical Reasoning,"Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourseaware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, Re-Clor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/Eleanor-H/DAGN.","{'sequence': 'Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourseaware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, Re-Clor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/Eleanor-H/DAGN.', 'labels': ['Dialogue and Interactive Systems', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Question Answering', 'Computational Social Science and Social Media', 'Information Retrieval and Text Mining', 'NLP Applications', 'Discourse and Pragmatics', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Semantics: Lexical Semantics', 'Interpretability and Analysis of Models for NLP', 'Resources and Evaluation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Summarization', 'Machine Translation and Multilinguality', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.0945601686835289, 0.08802372217178345, 0.08082883805036545, 0.05759881064295769, 0.05520343780517578, 0.05386883020401001, 0.04766111075878143, 0.04421708360314369, 0.04417384788393974, 0.038872260600328445, 0.03883417323231697, 0.03811974078416824, 0.03772649168968201, 0.03624767065048218, 0.03435982018709183, 0.031887367367744446, 0.02864810638129711, 0.02718048170208931, 0.026529360562562943, 0.024586038663983345, 0.024310896173119545, 0.023652927950024605, 0.022908814251422882]}",0.0945601686835289,Dialogue and Interactive Systems,0.05759881064295769
Question Answering,Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering,"In open-domain question answering (QA), retrieve-and-read mechanism has the inherent benefit of interpretability and the easiness of adding, removing, or editing knowledge compared to the parametric approaches of closedbook QA models. However, it is also known to suffer from its large storage footprint due to its document corpus and index. Here, we discuss several orthogonal strategies to drastically reduce the footprint of a retrieve-andread open-domain QA system by up to 160x. Our results indicate that retrieve-and-read can be a viable option even in a highly constrained serving environment such as edge devices, as we show that it can achieve better accuracy than a purely parametric model with comparable docker-level system size. 1","{'sequence': 'In open-domain question answering (QA), retrieve-and-read mechanism has the inherent benefit of interpretability and the easiness of adding, removing, or editing knowledge compared to the parametric approaches of closedbook QA models. However, it is also known to suffer from its large storage footprint due to its document corpus and index. Here, we discuss several orthogonal strategies to drastically reduce the footprint of a retrieve-andread open-domain QA system by up to 160x. Our results indicate that retrieve-and-read can be a viable option even in a highly constrained serving environment such as edge devices, as we show that it can achieve better accuracy than a purely parametric model with comparable docker-level system size. 1', 'labels': ['Question Answering', 'Dialogue and Interactive Systems', 'Speech and Multimodality', 'Syntax: Tagging, Chunking and Parsing', 'Generation', 'Information Retrieval and Text Mining', 'Resources and Evaluation', 'Information Extraction', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'Language Grounding to Vision, Robotics and Beyond', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'Semantics: Lexical Semantics', 'Summarization', 'Machine Learning for NLP', 'Interpretability and Analysis of Models for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Computational Social Science and Social Media'], 'scores': [0.4591161906719208, 0.07056734710931778, 0.05191127955913544, 0.03876432776451111, 0.031231554225087166, 0.029085151851177216, 0.02839972823858261, 0.028186453506350517, 0.025730980560183525, 0.024053052067756653, 0.022708235308527946, 0.02222103253006935, 0.0207934882491827, 0.020046334713697433, 0.019843561574816704, 0.016170423477888107, 0.014752439223229885, 0.014695947989821434, 0.01407979242503643, 0.013939570635557175, 0.013015853241086006, 0.011287800036370754, 0.009399443864822388]}",0.4591161906719208,Question Answering,0.4591161906719208
Question Answering,Unsupervised Multi-hop Question Answering by Question Generation,"Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multihop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https: //github.com/teacherpeterpan/ Unsupervised-Multi-hop-QA.","{'sequence': 'Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multihop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https: //github.com/teacherpeterpan/ Unsupervised-Multi-hop-QA.', 'labels': ['Question Answering', 'Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Information Extraction', 'Computational Social Science and Social Media', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Interpretability and Analysis of Models for NLP', 'Ethics and NLP', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Language Grounding to Vision, Robotics and Beyond', 'NLP Applications', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Machine Translation and Multilinguality', 'Summarization', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.13998597860336304, 0.10707046091556549, 0.07782167196273804, 0.07113661617040634, 0.05770096182823181, 0.05389662832021713, 0.04676325246691704, 0.039981551468372345, 0.03668327257037163, 0.03620034456253052, 0.03325118124485016, 0.03231046348810196, 0.03143668547272682, 0.03139903396368027, 0.026075132191181183, 0.025111529976129532, 0.024408496916294098, 0.02417820133268833, 0.023714475333690643, 0.023177776485681534, 0.020563235506415367, 0.01932186633348465, 0.017811182886362076]}",0.13998597860336304,Question Answering,0.13998597860336304
Summarization,Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents,"Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from. 1","{'sequence': 'Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from. 1', 'labels': ['Summarization', 'Information Extraction', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Dialogue and Interactive Systems', 'Interpretability and Analysis of Models for NLP', 'Semantics: Lexical Semantics', 'Syntax: Tagging, Chunking and Parsing', 'Speech and Multimodality', 'Information Retrieval and Text Mining', 'Generation', 'Question Answering', 'Machine Translation and Multilinguality', 'NLP Applications', 'Ethics and NLP', 'Machine Learning for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Resources and Evaluation', 'Discourse and Pragmatics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Language Grounding to Vision, Robotics and Beyond', 'Computational Social Science and Social Media'], 'scores': [0.2981674373149872, 0.08594236522912979, 0.07304447144269943, 0.05369752272963524, 0.04418007656931877, 0.0396624431014061, 0.039225708693265915, 0.03687606379389763, 0.035052597522735596, 0.03307190537452698, 0.031797587871551514, 0.029480431228876114, 0.02592557668685913, 0.02447785809636116, 0.02370242215692997, 0.020982131361961365, 0.01969420723617077, 0.016868162900209427, 0.01628432422876358, 0.015425282530486584, 0.013383856974542141, 0.011841317638754845, 0.011216274462640285]}",0.2981674373149872,Summarization,0.2981674373149872
Summarization,AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization,"State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2) domain-adaptive pre-training; and 3) taskadaptive pre-training. Experiments show that the effectiveness of pre-training is correlated with the similarity between the pre-training data and the target domain task. Moreover, we find that continuing pre-training could lead to the pre-trained model's catastrophic forgetting, and a learning method with less forgetting can alleviate this issue. Furthermore, results illustrate that a huge gap still exists between the low-resource and high-resource settings, which highlights the need for more advanced domain adaptation methods for the abstractive summarization task. 1 * * Equal contributions. Listing order is random. 1 The code and data are released at: https://github. com/TysonYu/AdaptSum","{'sequence': ""State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2) domain-adaptive pre-training; and 3) taskadaptive pre-training. Experiments show that the effectiveness of pre-training is correlated with the similarity between the pre-training data and the target domain task. Moreover, we find that continuing pre-training could lead to the pre-trained model's catastrophic forgetting, and a learning method with less forgetting can alleviate this issue. Furthermore, results illustrate that a huge gap still exists between the low-resource and high-resource settings, which highlights the need for more advanced domain adaptation methods for the abstractive summarization task. 1 * * Equal contributions. Listing order is random. 1 The code and data are released at: https://github. com/TysonYu/AdaptSum"", 'labels': ['Generation', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Question Answering', 'Computational Social Science and Social Media', 'Summarization', 'Ethics and NLP', 'NLP Applications', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Lexical Semantics', 'Discourse and Pragmatics', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Machine Learning for NLP', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Interpretability and Analysis of Models for NLP', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Phonology, Morphology and Word Segmentation', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Information Extraction', 'Language Grounding to Vision, Robotics and Beyond', 'Information Retrieval and Text Mining', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11202631890773773, 0.07019319385290146, 0.06975529342889786, 0.06612507253885269, 0.06066126003861427, 0.05698017030954361, 0.05594612658023834, 0.053909655660390854, 0.052042409777641296, 0.04837070778012276, 0.04717927798628807, 0.041781939566135406, 0.04028383642435074, 0.03679222986102104, 0.028030958026647568, 0.027417387813329697, 0.025825662538409233, 0.02076115272939205, 0.0197859238833189, 0.01914547197520733, 0.018879801034927368, 0.01441729161888361, 0.013688948005437851]}",0.11202631890773773,Generation,0.05698017030954361
Summarization,QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization,"Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at https://github.com/Yale-LILY/ QMSum.","{'sequence': 'Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at https://github.com/Yale-LILY/ QMSum.', 'labels': ['Summarization', 'Resources and Evaluation', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Dialogue and Interactive Systems', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Ethics and NLP', 'NLP Applications', 'Generation', 'Machine Translation and Multilinguality', 'Information Retrieval and Text Mining', 'Information Extraction', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP', 'Discourse and Pragmatics', 'Language Grounding to Vision, Robotics and Beyond', 'Phonology, Morphology and Word Segmentation', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics'], 'scores': [0.18105116486549377, 0.06686238199472427, 0.061198845505714417, 0.05955486372113228, 0.058638885617256165, 0.05821568891406059, 0.05349101126194, 0.04461211338639259, 0.041604895144701004, 0.041432999074459076, 0.04017435759305954, 0.04002806544303894, 0.03763115406036377, 0.0360051766037941, 0.02941937744617462, 0.025402823463082314, 0.024583345279097557, 0.022245483472943306, 0.020051220431923866, 0.01994122378528118, 0.016827071085572243, 0.011973593384027481, 0.009054295718669891]}",0.18105116486549377,Summarization,0.18105116486549377
Summarization,MM-AVS: A Full-Scale Dataset for Multi-modal Summarization,"Multimodal summarization becomes increasingly significant as it is the basis for question answering, Web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we present a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in English from CNN and Daily Mail. To our best knowledge, this is the first collection that spans all modalities and nearly comprises all types of materials available in this community. In addition, we devise a baseline model based on the novel dataset, which employs a newly proposed Jump-Attention mechanism based on transcripts. The experimental results validate the important assistance role of the external information for multimodal summarization.","{'sequence': 'Multimodal summarization becomes increasingly significant as it is the basis for question answering, Web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we present a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in English from CNN and Daily Mail. To our best knowledge, this is the first collection that spans all modalities and nearly comprises all types of materials available in this community. In addition, we devise a baseline model based on the novel dataset, which employs a newly proposed Jump-Attention mechanism based on transcripts. The experimental results validate the important assistance role of the external information for multimodal summarization.', 'labels': ['Question Answering', 'Speech and Multimodality', 'Summarization', 'Information Extraction', 'Information Retrieval and Text Mining', 'Discourse and Pragmatics', 'Machine Translation and Multilinguality', 'Dialogue and Interactive Systems', 'Resources and Evaluation', 'Syntax: Tagging, Chunking and Parsing', 'NLP Applications', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Generation', 'Semantics: Lexical Semantics', 'Computational Social Science and Social Media', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Language Grounding to Vision, Robotics and Beyond', 'Theory and Formalism in NLP (Linguistic and Mathematical)'], 'scores': [0.31558823585510254, 0.10624057799577713, 0.10606534034013748, 0.04774009436368942, 0.040332261472940445, 0.030947884544730186, 0.03093688003718853, 0.030895203351974487, 0.028376707807183266, 0.02808491885662079, 0.026224911212921143, 0.024450134485960007, 0.024007895961403847, 0.021642420440912247, 0.021582651883363724, 0.021204769611358643, 0.018298126757144928, 0.015319166705012321, 0.01459809672087431, 0.013196686282753944, 0.012570957653224468, 0.012401193380355835, 0.009294814430177212]}",0.31558823585510254,Question Answering,0.10606534034013748
Summarization,MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization,"This paper introduces MEDIASUM 1 , a largescale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MEDIASUM can be used in transfer learning to improve a model's performance on other dialogue summarization tasks. * Equal contribution 1 https://github.com/zcgzcgzcg1/ MediaSum/","{'sequence': ""This paper introduces MEDIASUM 1 , a largescale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MEDIASUM can be used in transfer learning to improve a model's performance on other dialogue summarization tasks. * Equal contribution 1 https://github.com/zcgzcgzcg1/ MediaSum/"", 'labels': ['Summarization', 'Speech and Multimodality', 'Information Extraction', 'Generation', 'Resources and Evaluation', 'Computational Social Science and Social Media', 'Dialogue and Interactive Systems', 'Discourse and Pragmatics', 'Semantics: Lexical Semantics', 'Information Retrieval and Text Mining', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Question Answering', 'Machine Translation and Multilinguality', 'Phonology, Morphology and Word Segmentation', 'NLP Applications', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Machine Learning for NLP'], 'scores': [0.3078981339931488, 0.07320821285247803, 0.06306429207324982, 0.049937669187784195, 0.04774189367890358, 0.03704871982336044, 0.036953989416360855, 0.03643041476607323, 0.0346265472471714, 0.03376321494579315, 0.03199893236160278, 0.03030543029308319, 0.025647852569818497, 0.025105560198426247, 0.021564483642578125, 0.021050317212939262, 0.02075039967894554, 0.019071519374847412, 0.01899808645248413, 0.01825944148004055, 0.018228815868496895, 0.014255407266318798, 0.014090691693127155]}",0.3078981339931488,Summarization,0.3078981339931488
Summarization,Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection,"Despite significant progress in neural abstractive summarization, recent studies have shown that the current models are prone to generating summaries that are unfaithful to the original context. To address the issue, we study contrast candidate generation and selection as a model-agnostic post-processing technique to correct the extrinsic hallucinations (i.e. information not present in the source text) in unfaithful summaries. We learn a discriminative correction model by generating alternative candidate summaries where named entities and quantities in the generated summary are replaced with ones with compatible semantic types from the source document. This model is then used to select the best candidate as the final output summary. Our experiments and analysis across a number of neural summarization systems show that our proposed method is effective in identifying and correcting extrinsic hallucinations. We analyze the typical hallucination phenomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction.","{'sequence': 'Despite significant progress in neural abstractive summarization, recent studies have shown that the current models are prone to generating summaries that are unfaithful to the original context. To address the issue, we study contrast candidate generation and selection as a model-agnostic post-processing technique to correct the extrinsic hallucinations (i.e. information not present in the source text) in unfaithful summaries. We learn a discriminative correction model by generating alternative candidate summaries where named entities and quantities in the generated summary are replaced with ones with compatible semantic types from the source document. This model is then used to select the best candidate as the final output summary. Our experiments and analysis across a number of neural summarization systems show that our proposed method is effective in identifying and correcting extrinsic hallucinations. We analyze the typical hallucination phenomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction.', 'labels': ['Generation', 'Summarization', 'Dialogue and Interactive Systems', 'Question Answering', 'Syntax: Tagging, Chunking and Parsing', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Information Extraction', 'Computational Social Science and Social Media', 'Interpretability and Analysis of Models for NLP', 'Discourse and Pragmatics', 'Information Retrieval and Text Mining', 'Semantics: Lexical Semantics', 'Resources and Evaluation', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Machine Learning for NLP', 'Ethics and NLP', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Language Grounding to Vision, Robotics and Beyond', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining'], 'scores': [0.11692806333303452, 0.0998639464378357, 0.0695863887667656, 0.06157201901078224, 0.0521780289709568, 0.052045147866010666, 0.04856577888131142, 0.04492512345314026, 0.040217820554971695, 0.03882000595331192, 0.03830813616514206, 0.037033047527074814, 0.034441668540239334, 0.03321925923228264, 0.03255010396242142, 0.0318530835211277, 0.030015554279088974, 0.028596172109246254, 0.025223666802048683, 0.024156175553798676, 0.022597549483180046, 0.021471470594406128, 0.01583191379904747]}",0.11692806333303452,Generation,0.0998639464378357
Summarization,Inference Time Style Control for Summarization,"How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. ( 1 ) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation. In experiments of summarizing with simplicity control, automatic evaluation and human judges both find our models producing outputs in simpler languages while still informative. We also generate news headlines with various ideological leanings, which can be distinguished by humans with a reasonable probability.","{'sequence': 'How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. ( 1 ) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation. In experiments of summarizing with simplicity control, automatic evaluation and human judges both find our models producing outputs in simpler languages while still informative. We also generate news headlines with various ideological leanings, which can be distinguished by humans with a reasonable probability.', 'labels': ['Summarization', 'Generation', 'Resources and Evaluation', 'Information Extraction', 'Dialogue and Interactive Systems', 'Question Answering', 'Semantics: Lexical Semantics', 'Speech and Multimodality', 'Machine Translation and Multilinguality', 'Computational Social Science and Social Media', 'Semantics: Sentence-level Semantics, Textual Inference and Other areas', 'Syntax: Tagging, Chunking and Parsing', 'Ethics and NLP', 'Interpretability and Analysis of Models for NLP', 'Information Retrieval and Text Mining', 'NLP Applications', 'Phonology, Morphology and Word Segmentation', 'Language Grounding to Vision, Robotics and Beyond', 'Linguistic Theories, Cognitive Modeling and Psycholinguistics', 'Machine Learning for NLP', 'Sentiment Analysis, Stylistic Analysis, and Argument Mining', 'Theory and Formalism in NLP (Linguistic and Mathematical)', 'Discourse and Pragmatics'], 'scores': [0.3241226375102997, 0.17520098388195038, 0.07208675146102905, 0.042453035712242126, 0.04118158295750618, 0.040545448660850525, 0.03688783198595047, 0.03319589048624039, 0.027739979326725006, 0.021630611270666122, 0.020273132249712944, 0.02006511762738228, 0.018204450607299805, 0.01805492863059044, 0.017756806686520576, 0.013381465338170528, 0.013094192370772362, 0.011757937259972095, 0.011474409140646458, 0.011424431577324867, 0.009997242130339146, 0.009897567331790924, 0.009573462419211864]}",0.3241226375102997,Summarization,0.3241226375102997
