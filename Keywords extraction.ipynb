{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Aw\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keybert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeybert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyBERT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerDocumentEmbeddings\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myake\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keybert'"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "# Reading in files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import scipy.sparse as sp\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "\n",
    "import yake\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ACL_2020 = pd.read_csv(\"./Data/Pred/BART/ACL_2022_bart_pred_231122.csv\")\n",
    "df_EMNLP_2020 = pd.read_csv(\"./Data/Pred/BART/EMNLP_2020_bart_pred_231122.csv\")\n",
    "\n",
    "df_ACL_2020 = df_ACL_2020[[\"Labels\", \"Paper Name\", \"abstract\"]]\n",
    "df_EMNLP_2020 = df_EMNLP_2020[[\"Labels\", \"Paper Name\", \"abstract\"]]\n",
    "\n",
    "df_ACL_2020.columns = [\"Label\", \"Title\", \"Abstract\"]\n",
    "df_EMNLP_2020.columns = [\"Label\", \"Title\", \"Abstract\"]\n",
    "\n",
    "df_ACL_2020 = df_ACL_2020.loc[lambda df_ACL_2020: ~df_ACL_2020[\"Label\"].isin([\"Student Research Workshop\", \"Theme\", \"NLP Applications\", \"System Demonstrations\"]), :]\n",
    "df_ACL_2020[\"Label\"].unique()\n",
    "\n",
    "\n",
    "df_ACL_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EMNLP_2020 = df_EMNLP_2020.loc[lambda df_EMNLP_2020: ~df_EMNLP_2020[\"Label\"].isin([\"Student Research Workshop\", \"Theme\", \"NLP Applications\", \"System Demonstrations\"]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACL_2020['Text'] = df_ACL_2020['Title'] + \" \" +df_ACL_2020['Abstract']\n",
    "df_EMNLP_2020['Text'] = df_EMNLP_2020['Title'] + \" \" +df_EMNLP_2020['Abstract']\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatiser_stemmer_stopword(text, nlp, stemmer):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    lemmatised_sentence_lst = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    lemmatised_sentence = \" \".join(lemmatised_sentence_lst)\n",
    "    stemmed_lemmatised_sentence = stemmer.stem(lemmatised_sentence)\n",
    "    \n",
    "    return stemmed_lemmatised_sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACL_2020['Lemm Stemmed Text'] = df_ACL_2020['Text'].progress_apply(lemmatiser_stemmer_stopword, args=(nlp, stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EMNLP_2020['Lemm Stemmed Text'] = df_EMNLP_2020['Text'].progress_apply(lemmatiser_stemmer_stopword, args=(nlp, stemmer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keybert_keyword_extraction(input_text, kw_model, use_maxsum=True, use_mmr=False, ngram=3, topn=10, nr_cand=20, div=0.5):\n",
    "    if use_maxsum:\n",
    "        keywords_res = kw_model.extract_keywords(input_text, keyphrase_ngram_range=(1, ngram), stop_words='english',\n",
    "                                        top_n=topn, use_maxsum=True, nr_candidates=nr_cand)\n",
    "    elif use_mmr:\n",
    "        keywords_res = kw_model.extract_keywords(input_text, keyphrase_ngram_range=(1, ngram), stop_words='english',\n",
    "                                        top_n=topn, use_mmr=True, diversity=div)\n",
    "    keyword_str = \"#\".join([kw[0] for kw in keywords_res])\n",
    "    return keyword_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yake_keyword_extraction(input_text, kw_model, useless):\n",
    "    keywords_res = kw_model.extract_keywords(input_text)\n",
    "    keyword_str = \"#\".join([kw[0] for kw in keywords_res])\n",
    "    return keyword_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_str(keyword_str):\n",
    "    keyword_str = keyword_str.replace(\" \", \"_\")\n",
    "    keyword_str = keyword_str.replace(\"#\", \" \")\n",
    "    return keyword_str\n",
    "\n",
    "#format_str(df_ACL_2020[\"Keyword\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different setting of keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plm_name: roberta-base, allenai/scibert_scivocab_uncased, allenai/specter\n",
    "plm_name = \"allenai/scibert_scivocab_uncased\"\n",
    "plm = TransformerDocumentEmbeddings(plm_name)\n",
    "kw_model = KeyBERT(model=plm)\n",
    "    \n",
    "use_maxsum = False\n",
    "topn = 10\n",
    "nr_cand = 20\n",
    "use_mmr = True\n",
    "ngram = 1\n",
    "df_ACL_2020['Keyword'] = df_ACL_2020['Text'].progress_apply(keybert_keyword_extraction, args=(kw_model, use_maxsum, use_mmr, ngram, topn, nr_cand))\n",
    "\n",
    "df_ACL_2020.to_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_nostem_261222.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plm_name: roberta-base, allenai/scibert_scivocab_uncased, allenai/specter\n",
    "plm_name = \"allenai/specter\"\n",
    "plm = TransformerDocumentEmbeddings(plm_name)\n",
    "kw_model = KeyBERT(model=plm)\n",
    "    \n",
    "use_maxsum = False\n",
    "topn = 10\n",
    "nr_cand = 20\n",
    "use_mmr = True\n",
    "ngram = 1\n",
    "df_ACL_2020['Keyword'] = df_ACL_2020['Lemm Stemmed Text'].progress_apply(keybert_keyword_extraction, args=(kw_model, use_maxsum, use_mmr, ngram, topn, nr_cand))\n",
    "\n",
    "df_ACL_2020.to_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_specter_261222.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dedup_func='seqm'\n",
    "dedup_thred=0.7\n",
    "ngram=2\n",
    "wind_size=1\n",
    "top_n=20\n",
    "kw_model = yake.KeywordExtractor(n=ngram, dedupLim=dedup_thred, dedupFunc=dedup_func, windowsSize=wind_size, top=top_n)\n",
    "useless=True\n",
    "\n",
    "df_ACL_2020['Keyword'] = df_ACL_2020['Lemm Stemmed Text'].progress_apply(yake_keyword_extraction, args=(kw_model, useless))\n",
    "\n",
    "df_ACL_2020.to_csv(\"./Data/ACL_2020_keywords_yake_030123.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EMNLP_2020['Keyword'] = df_EMNLP_2020['Lemm Stemmed Text'].progress_apply(yake_keyword_extraction, args=(kw_model, useless))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword matching based topic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords_mmr_161222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords_mmr_bigram_161222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords_maxsum_231222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords_maxsum_bigram_231222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords_maxsum_unigram_251222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_maxsum_unigram_251222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_maxsum_unigram_specter_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_maxsum_bigram_specter_261222.csv\")\n",
    "\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_specter_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_mmr_bigram_specter_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords20_mmr_unigram_specter_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords20_mmr_bigram_specter_261222.csv\")\n",
    "\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords30_mmr_unigram_specter_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords40_mmr_unigram_specter_261222.csv\")\n",
    "\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords20_mmr_unigram_specter_div7_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_roberta_261222.csv\")\n",
    "\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_specter_nostem_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_nostem_261222.csv\")\n",
    "#df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords10_mmr_unigram_roberta_nostem_261222.csv\")\n",
    "df_ACL_load = pd.read_csv(\"./Data/ACL_2020_keywords_yake_030123.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ACL_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on ACL2020\n",
    "\n",
    "Obtain topic_keywords{topic: \\[keywords\\]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_ACL_load\n",
    "text_label = \"Keyword\" # Extract keyword from which column, \"Keyword\" or \"Lemm Stemmed Text\", or \"Text\"\n",
    "topk = 10 # 10, 20, 30, 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for keyword only, convert from 'kc kc#kw2#kc kc3' to \"kc_kc kw2 kc_kc3\"\n",
    "df_train[\"Keyword\"] = df_train[\"Keyword\"].apply(format_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = df_train.groupby(['Label'], as_index=False).agg({text_label: ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_vals = tfidf_model.fit_transform(topic_docs[text_label])\n",
    "keyword_feas = tfidf_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_label == \"Keyword\": \n",
    "    ngram_range=1\n",
    "elif text_label == \"Lemm Stemmed Text\": \n",
    "    ngram_range=(1, 2)\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range).fit(topic_docs[text_label])\n",
    "count = count_vectorizer.transform(topic_docs[text_label])\n",
    "words = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTFIDFVectorizer(TfidfTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CTFIDFVectorizer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
    "        \"\"\"Learn the idf vector (global term weights) \"\"\"\n",
    "        _, n_features = X.shape\n",
    "        df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "        idf = np.log(n_samples / df)\n",
    "        self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                  shape=(n_features, n_features),\n",
    "                                  format='csr',\n",
    "                                  dtype=np.float64)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix) -> sp.csr_matrix:\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF \"\"\"\n",
    "        X = X * self._idf_diag\n",
    "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top _topk_ words per class\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(df_train)).toarray()\n",
    "\n",
    "topic_keywords = {topic_docs['Label'].iloc[label]: [words[index].replace(\"_\", \" \") for index in ctfidf[label].argsort()[-topk:]  if ctfidf[label][index]>0] for label in range(0,len(topic_docs['Label']))}\n",
    "topic_keywords_val = {topic_docs['Label'].iloc[label]: [(words[index].replace(\"_\", \" \"), ctfidf[label][index]) for index in ctfidf[label].argsort()[-topk:] if ctfidf[label][index]>0] for label in range(0,len(topic_docs['Label']))}\n",
    "\n",
    "#topic_keywords = {topic_docs['Label'].iloc[label]: [lemmatiser_stemmer_stopword(words[index].replace(\"_\", \" \"), nlp, stemmer) for index in ctfidf[label].argsort()[-topk:]  if ctfidf[label][index]>0] for label in range(0,len(topic_docs['Label']))}\n",
    "#topic_keywords_val = {topic_docs['Label'].iloc[label]: [(lemmatiser_stemmer_stopword(words[index].replace(\"_\", \" \"), nlp, stemmer), ctfidf[label][index]) for index in ctfidf[label].argsort()[-topk:] if ctfidf[label][index]>0] for label in range(0,len(topic_docs['Label']))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on ACL2020/EMNLP2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_EMNLP_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_count(text): \n",
    "\n",
    "    keyword_count_dict = {}\n",
    "    keyword_list_dict = {}\n",
    "\n",
    "    for label in topic_keywords.keys():\n",
    "\n",
    "        count = 0\n",
    "        keywords = []\n",
    "\n",
    "        list_of_key_words = topic_keywords[label]\n",
    "\n",
    "        for keyword in list_of_key_words:\n",
    "\n",
    "            count += text.count(keyword)\n",
    "            if keyword in text: keywords.append(keyword)\n",
    "\n",
    "        keyword_count_dict[label] = count\n",
    "        keyword_list_dict[label] = keywords\n",
    "    \n",
    "    return keyword_list_dict, keyword_count_dict, max(keyword_count_dict, key=keyword_count_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count keywords from \"Text\" or \"Lemm Stemmed Text\" or \"Keyword\", according to which column to extract keyword\n",
    "df_pred = df_test.apply(lambda row: keyword_count(row['Lemm Stemmed Text']), axis='columns', result_type='expand')\n",
    "df_pred.columns = [\"Matched Keywords\", \"Dictionary Output\", \"Predicted Label\"]\n",
    "\n",
    "df_test = pd.concat([df_test, df_pred], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Label Outcome\"] = df_test.apply(lambda x: x[\"Label\"] == x[\"Predicted Label\"], axis = 1)\n",
    "df_test_outcome = df_test[['Label', 'Label Outcome']].groupby(['Label', 'Label Outcome']).size().reset_index(name='Counts')\n",
    "df_test_outcome = df_test_outcome.sort_values(by = ['Label','Label Outcome'], ascending = [True, False])\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_test_outcome, x=\"Label\", y=\"Counts\", color=\"Label Outcome\", title=\"Predictions for ACL Dataset\",\n",
    "             width=900, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_labels = list(df_test[\"Label\"].unique())\n",
    "\n",
    "cm = confusion_matrix(df_test['Label'], df_test['Predicted Label'], labels = idx_labels)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = idx_labels, \n",
    "                     columns = idx_labels)\n",
    "\n",
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(cm_df, annot=True, cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(df_test['Label'], df_test['Predicted Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_accuracy = df_test_outcome.pivot(index=\"Label\", columns=\"Label Outcome\", values=\"Counts\").reset_index().fillna(0)\n",
    "df_test_accuracy[\"Accuracy\"] = df_test_accuracy[True] / (df_test_accuracy[False] + df_test_accuracy[True]) * 100\n",
    "df_test_accuracy = df_test_accuracy.sort_values(by = 'Accuracy', axis=0, ascending=False)\n",
    "df_test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b110531817a2bf0b8e1fa59ad47ae74cf2e8602bdaf1b8cf6b96ebd8cf78ed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
